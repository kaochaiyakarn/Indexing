computational view interior point methods linear programming report terlaky technische universiteit delft delft university technology der technische wiskunde en informatica faculty technical mathematics informatics issn copyright fl faculty technical mathematics informatics delft netherlands 
part journal may reproduced form print means permission faculty technical mathematics informatics delft university technology netherlands 
copies reports may obtained bureau faculty technical mathematics informatics bl delft phone 
selection reports available postscript form faculty anonymous ftp twi tudelft nl 
located directory pub publications tech reports 
accessed world wide web www twi tudelft nl twi publications overview html delft university technology report computational view interior point methods linear programming terlaky issn reports faculty technical mathematics informatics delft september systems research institute polish academy sciences warsaw poland 
mail pl terlaky faculty technical mathematics informatics delft university technology box ga delft netherlands 
mail terlaky twi tudelft nl research done author visiting department management studies university geneva 
supported fonds national de la recherche scientifique suisse 
second author leave university budapest partially supported 
copyright fl faculty technical mathematics informatics delft netherlands 
part journal may reproduced form print means written permission faculty technical mathematics informatics delft university technology netherlands 
ii issues crucial efficient implementation interior point algorithm addressed 
start prototype primal dual algorithm 
tricks efficient practice discussed detail 
include preprocessing techniques initialization approaches methods computing search directions lying linear algebra techniques centering strategies methods stepsize selection 
reasons manifestations numerical difficulties primal degeneracy optimal solutions lack feasible solutions explained comprehensive way 
motivation obtaining optimal basis practicable algorithm perform task 
advantages different methods perform analysis applicable interior point optimal solutions discussed 
important questions remain open implementations interior point methods addressed performing correct analysis detecting infeasibility resolving difficulties arising presence unbounded optimal faces 
challenging practical problem warm start recalled potentially attractive approaches suggested 
facilitate understanding different implementation strategies illustrative numerical results subset problems netlib collection 
key words linear programming interior point methods primal dual algorithm implementation numerical linear algebra 
contents prototype primal dual algorithm tricks presolve starting point linear algebra stepsize centering higher order methods stopping criteria theoretical vs practical worst case complexity remarks numerical difficulties degeneracy ipms ill conditioned normal equations matrix optimal basis identification need optimal basis 
get optimal basis 
problems solved correct analysis unbounded optimal faces infeasible problems warm start karmarkar publication new polynomial time algorithm linear programming lp drew enormous attention mathematical programming community led great activity past years resulting flood papers see bibliography 
idea crossing interior feasible region search optimum linear program sixties 
example affine scaling method logarithmic barrier method mccormick 
reasons methods time shown competitive simplex 
due storage limitations size problems solved late sixties exceed couple rows columns sizes simplex method practically 
secondly sparse symmetric solvers available time appeared seventies orthogonal projections killed efficiency interior point methods ipms 
ipms need significantly memory simplex method unacceptable requirement time 
clearly situation quite different encouraged karmarkar claim excellent efficiency new approach 
fact claims wait couple years confirmed computational results 
soon karmarkar publication gill murray saunders tomlin wright built bridge new interior point method logarithmic barrier approach 
barrier methods developed primal dual lp formulation see surveys 
early implementations pure primal dual methods gave competitive results simplex implementations 
nowadays state art ipm implementations primal dual methods concentrate primal dual methods 
megiddo proposed applying logarithmic barrier method primal dual problems time 
independently kojima mizuno developed theoretical background method gave complexity results 
early implementations showed promise encouraged research field 
extensions represent current state art primal dual implementations see lustig shanno mehrotra 
primal dual algorithm feasible ipm iterates primal dual feasible respectively 
iterates positive infeasible primal dual algorithm called infeasible ipm 
algorithm attains feasibility time optimality reached 
successfully implemented way shown practical convergence long theoretical justification behavior kojima megiddo mizuno 
method proven polynomial complexity nl 
complexity infeasible primal dual algorithm worse best known complexity nl feasible ipms see surveys widely accepted primal dual infeasible ipms efficient implementations 
infeasible ipms methods choice date state art implementations mean primal dual infeasible ipm speak primal dual algorithm 
facilitate section shall introduce prototype primal dual infeasible ipm algorithm 
common feature ipms interpreted terms path centers leads optimal solution see date 
abuse mathematics basic iteration path algorithm consists moving point neighborhood central path called target preserves property lying neighborhood central path reduces distance optimality measured estimation duality gap 
movement principle involve step target 
depending significant update target consequently just newton steps needed reach vicinity new target distinguishes short long step methods 
due considerable cost newton step usually implementations newton step allowed new target defined 
newton step requires computing orthogonal projection null space scaled linear operator ad lp constraint matrix positive diagonal scaling matrix changes subsequent iterations 
primal dual primal dual variants ipms differ way matrix defined effort compute karmarkar projection 
orthogonal projection involves inversion matrix ad time consuming linear algebra operation takes computation time single interior point iteration 
linear program specially structured structure exploited determine easily invertible preconditioner iterative method conjugate gradients algorithm implemented successfully network problems direct methods compute sparse symmetric factorization cholesky decomposition positive definite system ad bunch parlett decomposition indefinite augmented system gamma methods choice 
computing projections affine spaces crucial efficiency interior point algorithm 
shall discuss detail section addresses issues implementation ipm role presolve analysis choice starting point choice stepsizes primal dual spaces role centering higher order methods termination conditions comparison theoretical practical complexity 
section shall add remarks manifestations degeneracy ill conditioning computations projections 
years application simplex method starting discovery till karmarkar breakthrough competition operations research practitioners got seeing linear programming simplex perspective 
particular applies analysis available optimal basis solution 
fact analysis incomplete frequently incorrect see 
exist applications optimal basis necessary reoptimization integer programming 
case need arises identifying optimal basis interior point optimal solution 
fortunately done strongly polynomial time 
shall address problem optimal basis identification section 
section devoted crucial questions remain open 
sensitivity analysis interior point optimal solution generally expensive produces correct information 
discuss handle problems unbounded level sets detect infeasibility implement efficient warm start interior point algorithms 
relevant issues interior point method implementations illustrated solving subset netlib lp problem test collection version higher order primal dual method code 
computations sun sparc workstation 
frequently speak stability robustness efficiency different methods 
stability usual numerical stability meant 
talking robustness thinks algorithm gives reliable answer wide range optimally problem instances 
efficiency relates speed algorithm speed implementation 
prototype primal dual algorithm consider primal linear programming problem minimize subject ax mn dual maximize gamma subject gamma derive primal dual algorithm replace nonnegativity constraints primal formulation logarithmic barrier penalty terms objective function gives logarithmic barrier function gamma ln gamma ln order optimality conditions ax gamma gamma gamma gamma substituting gamma order optimality conditions give ax gamma st diagonal matrices elements respectively vector ones barrier parameter 
set solutions defines central path primal dual problem respectively 
having primal dual feasible solutions quality centrality measured ffi gamma gamma note central path iff ffi 
smaller ffi better points centered 
observe equations linear force primal dual feasibility solution 
equations nonlinear complementarity conditions feasibility constraints provides optimality solutions 
solution certain available complementarity gap ipm quantity measures error complementarity 
observe feasible ipms complementarity gap reduces usual duality gap 
clearly vanishes optimal solution 
single iteration basic primal dual algorithm step newton method applied order optimality conditions updated usually decreased 
algorithm terminates infeasibility complementarity gap reduced predetermined tolerance 
having newton direction obtained solving system linear equations gammai deltax deltay deltas deltaz deltat gamma gamma st gamma ax gamma gamma gamma gamma denote violations primal dual constraints respectively 
primal dual infeasible ipms require feasibility solutions nonzero optimization process 
feasibility attained process optimality reached 
easy verify step length newton direction feasibility reached 
seldom case smaller stepsize usually chosen damped newton iteration taken preserve nonnegativity case stepsize ff applied infeasibilities reduced gamma ff times 
look closer system linear equations 
elimination deltaz gamma gamma gamma deltax deltas gamma deltax deltat gamma gamma st gamma deltas gamma gamma st gamma deltax reduces gammad gamma deltax deltay gamma gamma gamma gamma gamma gamma gamma gamma st gamma gamma matrix augmented system sparse symmetric indefinite 
system linear equations solved directly bunch parlett factorization multiplying equation ad substituting second equation system reduced sparse symmetric positive definite normal equations system ad deltay ad advantages approaches discussed section 
conclude deltay uniquely determines deltax deltay gamma deltas deltaz deltat 
computing deltax deltay deltay usually direct approach applied divided phases factorization matrix easily invertible form solve exploits factorization 
usually second step order magnitude cheaper 
observation led introducing higher order terms computing direction 
computing different corrector terms resolves multiple solution linear system right hand sides reuses factorization relatively inexpensive 
successful technique incorporates higher order information primal dual algorithm comes mehrotra 
shall address technique detail section 
sake brief presentation method assume direction deltax deltay deltas deltaz deltat computed maximum stepsizes primal ff dual ff spaces maintain nonnegativity variables slightly reduced factor ff new iterate computed ff ff deltax ff ff deltas ff ff deltay ff ff deltaz ff ff deltat making step barrier parameter reduced process repeated 
formally algorithm summarized follows 
prototype algorithm input initial pair primal dual solutions respectively parameters accuracy parameter ff step size stopping criteria satisfied calculate search directions calculate new iterates 
tricks section shall concentrate issues crucial efficient implementation interior point algorithms 
presolve linear programs solved nowadays large sizes usually generated automatically modelling support tools 
formulated way necessarily suitable direct application lp solver 
advantageous analyse possible simplify formulation passing solver 
important role linear programming recognized long time ago 
worth mention presolve strongly recommended solver independently method 
importance presolve pronounced interior point solver applied due involved linear algebra operations need full rank matrix preprocessing aims main goals problem size reduction problem density reduction ensuring full rank constraint matrix problem size reduction general purpose presolve repeats logical analysis lp problem formulation reduction obtained single reduction creates possibility model simplifications 
simple operations applied 

empty rows columns removed 

singleton inequality constraints replaced bounds variables variable entry singleton equality row fixed removed 

lower upper limits constraint determined fj ij ij fj ij ij clearly satisfy ij observe due nonnegativity limits nonpositive nonnegative respectively 
inequalities tight original inequality type lp constraint constraint redundant 
contradicts lp constraint problem infeasible 
special cases equal row greater equal row equality type row equals limits lp constraint forcing 
means way satisfy constraint fix variables appear appropriate bounds 

constraint limits generate implied variable bounds 
note lp variables transformed standard form 
technique original form lp constraint form slack variable added transform standard equality row 
assume example nonredundant equal le type constraint ij ik ik ij ik ik gamma ij new implied bounds variables involved row gamma ik ik gamma ik ik tighter original ones variable bounds improved 
note technique similar particularly useful imposes finite bounds free variables 
free variables case split represented difference nonnegative variables 

singleton column row entry generate implied bounds variable referring 
bounds tight original ones variable implied free 
consequently row implied free constraint singleton free column eliminated 

nonnegative unbounded variables refering singleton columns generate bounds dual variables variable refers singleton column entry ij dual constraint inequality ij inequality solved depending sign ij produces lower upper bound 
dual variables explicit possibly infinite bounds delta delta delta lower upper limits dual constraint generated fi ij ij fi ij ij fi ij ij fi ij ij limits compared cost coefficient applied identify variables sign reduced cost gamma restricted dominated variables 
dual constraint redundant holds dominated variable fixed appropriate bound eliminated problem 
reduced cost gamma weak sign restriction nonnegative nonpositive variable weakly dominated 
surprisingly case additional conditions variable eliminated 

dual constraint limits generate implied bounds dual variables 
technique similar point applied 
implied bounds tighter original ones replace 
techniques mentioned worth run lp solver applied 
usually reduce problem size considerably identify primal dual infeasibility unboundedness 
improving sparsity problem way lp constraint matrix involved interior point iterations justifies presolve effort aims decreasing cost calculating solution equations improving accuracy solutions 
mind form equation compute search directions strongly influenced sparsity structure depends conditioning matrix requires full row rank 
sparsity usually improved 
general look nonsingular matrix mm matrix ma sparsest possible 
primal feasibility constraints case replaced equivalent formulation max suitable direct application interior point solver 
exact solution sparsity problem np complete problem efficient heuristics usually produce satisfactory nonzero reductions algorithm example looks row sparsity pattern subset sparsity pattern rows uses pivot nonzero elements rows 
sparse produce relatively dense factors 
additionally fills dramatically dense columns number dense columns excessive technique splitting shorter pieces remedy 
note augmented system approach suffers presence dense columns see section 
full rank matrix theoretically detecting rank deficiency problem 
continue gaussian elimination zero submatrix obtained 
process reliable complete pivoting prohibitively expensive due destruction sparsity structure 
practicable generalized markowitz pivoting eliminate linear dependencies reasonable cost 
codes offer option 
hand lot linear dependencies identified search duplicate rows heuristics sparser 
observe due primal degeneracy practically normal equations matrix rank deficient optimum approached case original full row rank details see section 
codes equipped safeguards problems usually recover small rank deficiency numerical experiments tables illustrate advantages presolve effort 
report problem sizes normal equations statistics number diagonal nonzeros adjacency structure aa number diagonal nonzeros cholesky factor number flops required compute iterations cpu time solve netlib tests digits accuracy cases presolve analysis respectively 
additionally table reports cpu time presolve analysis 
computational results reported obtained code 
code written fortran 
compiled compiler default optimization option run processor sun sparc workstation 
table 
original problem sizes solution statistics 
problem original problem size normal equations solution nz aa nz flops iters time fv bau bnl cycle pilot pilot sctap ship sierra analysis results collected tables shows presolve analysis valuable technique leads considerable savings solution time 
savings problem dependent significant marginal pilot 
starting point choice starting point interior point algorithm solved full satisfaction 
surprisingly points relatively close optimal solution centered lead bad performance numerical difficulties 
ipms infeasible ipms quite sensitive choice initial point 
fortunate guess possible understood linear programs reduce computational effort considerably 
hand bad choice may disastrous efficiency iterations done iterates get reasonably centered algorithm allows large steps 
starting points implementations primal dual infeasible ipms variation approximate solution auxiliary qp problem minimize subject ax predetermined weight parameter 
solution explicit formula computed cost comparable single interior point iteration 
supposed minimize norm primal solution promotes points better sense lp objective 
table 
advantages presolve analysis 
problem size presolve normal equations solution presolve nz aa nz flops iters time time fv bau bnl cycle pilot pilot sctap ship sierra solution may negative components negative components pushed positive values sufficiently bounded away zero elements smaller ffi replaced ffi say ffi 
independently initial dual solution chosen similarly satisfy dual constraint 
elements smaller ffi replaced ffi worth note code shows consistent efficiency simple starting point phenomena known primal dual method details see sections 
linear algebra iteration interior point method linear programming requires computing newton direction order optimality conditions 
turn equivalent computing projection vector null space linear operator ad 
diagonal scaling matrix depends variant algorithm computational effort remains practically interior point algorithms 
explains comparison efficiency different algorithms limited comparison iteration numbers reach desired accuracy 
shown section large sparse system newton equations reduces called augmented system 
scaling primal component search direction delta gammad gamma deltax substituting dr system delta deltay gammah ad 
usually system referred augmented system 
observe equations define unique orthogonal decomposition delta ker deltay im 
system better stability properties reduced form obtained eliminating delta deltay ag called normal equation 
direct solution needs bunch parlett factorization large sparse symmetric positive definite matrix unit lower triangular matrix block diagonal matrix diagonal blocks size 
matrix indefinite guarantee nonzero diagonal elements intermediate steps symmetric gaussian elimination 
cholesky decomposition general possible way overcome difficulties allow indefinite block pivots augmented system approach advantages 
accuracy properties 

easily generalizable exploit sparsity kkt systems arising nonseparable quadratic programming linear complementarity problems 

naturally handles free variables 
gamma gamma replaced zero 

dense columns degrade efficiency lead significant fill 
important disadvantages 
complicated implement remains average efficient counterpart applying cholesky decomposition normal equations matrix ll reason efficiency cholesky decomposition comes fact matrix positive definite sparsity preserving sequence pivots ordering symmetric gaussian elimination performed defined advance numerical operations start 
analyse phase completely separated phase 
important sparsity pattern interior point iterations analyse phase done optimization 
choose solve augmented system gaussian elimination pivots upper left diagonal block normal equations 
due stability fill reasons pivot sequences possibly including pivots preferable 
sequences influenced numerical values 
determined advance sole basis sparsity structure analysis 
choose stable pivots get improvement normal equations inspect actual numerical values time 
analyse factorize phases separated factorization expensive normal equations approach 
additionally due changes diagonal scaling matrix subsequent ipm iterations pivot sequence redefined 
practice suffices update couple iterations numerical stability experience shows rarely update needed 
advantage augmented system approach freedom pivot choice opens possibility getting sparser factors normal equations approach degrading influence dense columns avoided 
pass brief description cholesky decomposition comment accuracy competitive approaches 
system definitely stable 
improve table 
augmented system vs normal equations 
problem augmented system normal equations nz flops nz aa flops aa nz flops total flops fv bau bnl cycle pilot pilot sctap ship sierra stability approaches equipped easy implement techniques improve accuracy iterative refinement regularizing term added results bounding pivots away zero 
computational experience proves normal equations approach produces sufficiently accurate directions reach desired digits correct solutions practical linear programs 
case researchers decided incorporate augmented system approach ipm implementations 
find interesting give bit insight computational effort competitive approaches 
table reports results running netlib problems 
reports number nonzeros triangular factors effort flops compute 
case normal equations distinguish effort form aa factorize report total effort sum 
results normal equations come code results augmented system approach reproduced table 
note problem pilot solved 
undoubtedly advantageous able pick preferable approach preliminary analysis sparsity structure lp constraint matrix needs methods incorporated implementation quite unusual 
theoretically difficult efficient implementation cholesky decomposition analyse phase reordering sparsity 
goal find permutation matrix cholesky factor sparsest possible 
unfortunately np complete problem 
practice suboptimal solution usually applying reordering heuristics 
heuristics practice 
marginal time spectacular density reduction factors reached 
popular minimum degree minimum fill orderings shall briefly describe 
minimum degree ordering markowitz observed kth step sparse gaussian elimination locally best pivot candidate ij minimizes ij gamma gamma numbers nonzero entries row column kth schur complement 
value ij gives number flops required kth step gaussian elimination time estimates potential fill caused step 
pivot sequence way prevent excessive fill ins ensure low cost factorization 
walker applied strategy symmetric matrices 
symmetric positive definite systems considered pivot selection restricted diagonal elements markowitz merit function simplifies gamma leads simple rule best candidate pivot column minimum number nonzero entries 
interpreting process terms elimination graph sees equivalent choice node minimum degree gave name heuristic 
key point efficient implementation minimum degree ordering representation fill ins elimination proceeds 
algorithm keeps trace elimination process storing pivotal cliques 
clique denotes set row numbers say rows active pivotal step 
storage clique needs remembering row numbers symmetric matrix represented elements 
sparsity pattern schur complement kth step gaussian elimination needed determine minimum degree column represented implicitly sparsity pattern decomposed matrix pivotal cliques previous steps elimination 
modern implementations minimum degree ordering enhancements basic algorithm extremely powerful 
detailed discussion scope 
focus plays particularly important role 
technique takes advantage presence indistinguishable nodes called supernodes elimination graph set columns identical sparsity patterns 
storing identical cliques technique handles supernode clique leads obvious savings time consuming degree update step reordering algorithm 
note form normal equations matrix aa denotes jth column matrix explains natural tendency creating supernodes column creates subject symmetric row column permutation dense window aa larger windows produced denser columns cover smaller ones 
minimum fill ordering observe general function considerably overestimates expected number fill ins iteration gaussian elimination take account fact positions predicted fill nonzero entries exist 
possible node expensive terms produce fill elimination step mainly update existing nonzero entries schur complement 
analysis exactly predicts fill chooses pivot producing minimum number minimum fill ordering involved minimum degree ordering 
count predicted fill simulate elimination step quite expensive operation 
surprisingly general technique offer sufficient advantage minimum degree ordering justify 
discussion concentrated analysis phase decomposition 
dominating term computational effort interior point algorithm repeated times numerical factorization 
mathematical point view pivot order effort numerical factorization uniquely determined flops denotes number nonzero elements column practice efficiency depends computations organized exploit specific computer architecture parallelism vectorization cache memory basic linear algebra system blas routines lower level blas routines offer example loop unrolling technique 
higher level blas routines contain block versions decomposition algorithms take advantage efficient organization matrix vector matrix matrix products 
gain results blas may vary significantly different computers 
detailed discussion issues scope 
undoubtedly specializations interior point implementations different computer architectures draw attention lp community near 
interested reader may consult preliminary results 
variant cholesky decomposition suits particularly parallelization comes 
shall technique gives considerable time savings computer architectures 
savings vary significantly different computers 
technique consists switch dense code near decomposition 
exploits fact triangular factors practically completely dense steps gaussian elimination 
loops nonzero entries pivot column avoid indirect addressing needed handle sparse columns 
table compares efficiency implementation case switch dense mode done default case disabled 
additionally reports number decomposition flops required compute cholesky factorization aa part effort done dense mode size dense window 
sun sparc computer vectorization facilities considerable savings obtained dense window large 
obviously savings remarkably larger computer architectures take advantage specific processor features ibm risc workstation 
discussion section naturally concentrated direct approaches solving reduced kkt systems general large scale lps definitely methods choice 
saunders major cholesky feel proud summarize current state art ipms implementations 
complete discussion different methods computing projections mention iterative approaches 
classical method family conjugate gradients algorithm applied solving reduced kkt systems researchers knowledge proved competitive applied general large scale linear programs 
reason ill conditioning kkt systems 
way overcome extremely slow convergence iterative method preconditioner general linear programs way date find preconditioner acceptable computational effort 
specially structured problems preconditioner derived problem interpretation 
table 
advantages switch dense mode 
problem window dense window iters time iters time total flops dense flops window fv bau bnl cycle pilot pilot sctap ship sierra stepsize interior point methods require iterates belong neighborhood central path 
way keep condition satisfied allow small reduction barrier parameter factor fi fl fl consequence leads short steps short step methods 
approach allows proving nice theoretical properties algorithm iterates close central path leads hopelessly slow convergence worst case behavior practice 
medium step methods fl omega gamma long step methods fl omega gamma optimistic target chosen consequently iterates allowed move larger neighborhood central path damped newton steps required get close new target 
complexity long step methods worse short step methods 
practice methods offer fast convergence significantly faster worst case complexity analysis 
current implementations bother getting really close target 
target updated newton step 
worse different stepsizes primal dual spaces concern preserving nonnegativity variables 
precisely maximum possible stepsizes determined formulae ff max ff ff deltax deltas ff max ff ff deltaz deltat table 
different stepsizes different spaces 
problem ff ff ff ff fv bau bnl cycle pilot pilot sctap ship sierra netlib stepsizes slightly reduced factor ff prevent hitting boundary 
stepsize rule saves iterations number compared case theory long step methods strictly followed 
different stepsizes different spaces due kojima 
preserve polynomial complexity additional safeguards needed 
kojima megiddo mizuno prove global convergence infeasible primal dual method 
stepsize selection nonnegativity variables concerned additional conditions imposed ensure uniform progress reducing primal dual feasibility approaching optimality 
results collected table illustrate efficiency primal dual implementation measured number iterations reach optimality cases identical stepsizes chosen spaces different stepsizes allowed default 
results clearly show restricting primal dual stepsizes causes remarkable loss efficiency iterations problems 
centering higher order methods interior point algorithms compose direction step deltax deltas deltay deltaz deltat denoted delta short parts delta delta delta combine affine scaling delta centering delta components 
term delta obtained solving delta solution equation right hand side gamma gamma st centering parameter example refers centering change current complementarity gap 
term delta responsible optimization delta keeps current iterate away boundary 
path methods benefit predictor corrector technique 
technique takes account higher order terms newton direction estimates targets achieved 
undoubtedly reasons proved successful primal dual method mehrotra heuristics defining new target takes advantage knowledge current predicted complementarity gap 
shall address technique detail 
introducing higher order terms combined adaptive choice barrier parameter analysis reduction complementarity gap achievable moving predictor primal dual affine scaling direction 
affine scaling predictor direction delta solves linear system right hand side equal current violation order optimality conditions 
direction usually optimistic full step length lp problem solved step 
predictor corrector hypothetical step direction 
maximum stepsizes primal ff pa dual ff da spaces preserving nonnegativity respectively determined predicted complementarity gap ff pa deltax ff da deltaz ff pa deltas ff da deltat computed 
determine barrier parameter denotes current complementarity gap 
term measures achievable progress affine scaling direction 
small step affine scaling direction close means want improve centrality 
affine scaling direction offers considerable progress reduction complementarity gap optimistic target closer optimum chosen 
corrector direction delta computed gammai deltax deltay deltas deltaz deltat gamma deltax deltaz gamma deltas deltat direction delta determined 
single iteration second order predictor corrector primal dual method needs solves large sparse linear system different right hand sides solve solve 
note presentation predictor corrector technique follows computational practice 
abuses mathematics sense stepsizes ff ff taken account table 
efficiency higher order methods 
problem order order order order iters time iters time iters time iters time fv bau bnl cycle pilot pilot sctap ship sierra expect higher order taylor approximations 
reader interested see detailed rigorous presentation approach encouraged consult 
observe predictor corrector mechanism applied repeatedly leading higher order method 
direction delta predictor new corrector term delta computed added 
method order compute gamma corrector terms require solving linear system different right hand sides 
higher order terms usually results reduction number iterations reach optimality necessarily produce time savings due increased effort single iteration 
date second order methods average efficient 
implementations 
knows benefit practically higher order information 
possible way take advantage promote higher order terms cholesky factorization expensive compared single solve see details 
section demonstration higher order techniques 
table reports number iterations reach digits precision duality gap cpu time netlib problems different order methods 
method order pure primal dual direction comes solution 
method order classical predictor corrector corrector term computed 
method order computes gamma corrector terms 
analysis results collected table shows higher order information may reduce number iterations reach optimality 
translates computation time savings case cholesky factorizations expensive compare data table 
stopping criteria interior point algorithms terminate relative primal dual feasibility relative duality gap reduced predetermined tolerance 
case primal dual method conditions checked state digits accurate solutions gamma jjxjj gammap jjx gamma jjxjj gammap jja gamma gamma jjwjj gammap jc gamma gamma jb gamma wj gammap worth note scaling may change left hand sides equations consequently measures quality solutions significantly 
denominators left hand sides usually decrease scaling problem 
digits exact solution typically required literature 
practice extremely rare condition satisfied time conditions hold 
explanation phenomena comes analysis order optimality conditions 
observe equations impose primal dual feasibility linear 
easier satisfied newton method equations nonlinear additionally change subsequent interior point iterations 
consequently important condition really checked 
results collected table report numbers primal dual iterations primal dual relative relative optimality gap reduced gamma subset netlib problems 
columns give relative infeasibilities relative optimality gap digits optimal solution 
note infeasible problems problems unbounded level sets scaling infeasibilities appropriate 
cases iterates diverge denominator goes infinity lead false results 
proper normalization factor cases respectively 
theoretical vs practical worst case complexity theoretical worst case complexity simplex method known polynomial caused problems practice 
fact modern simplex codes rarely iterations reach optimality 
worst case complexity interior point methods polynomial situation similar 
theoretical bound log ffl iterations obtain ffl exact solution lp pessimistic 
practice number iterations log 
rare example predictor corrector primal dual infeasible ipm iterations reach gamma optimality 
case code applied solving lps netlib suite problems need iterations solved 
table 
attaining optimality 
problem iterations reach accuracy solution pr 
feas 
dl 
feas 
opt 
pr 
inf 
dl 
inf 
opt 
gap fv bau bnl cycle pilot pilot sctap ship sierra remarks numerical difficulties degeneracy ipms main computational step ipms solving system normal equations ad diagonal matrix 
ye showed central path methods limit point iterates relative interior optimal faces 
fact prove exists constant fl fl relations fl fl fl fl satisfied 
optimal partition lp problem 
note pz px convergent ipm 
limiting behavior apx px px determines asymptotic behavior linear systems 
primal dual path methods obtain information matrices px shall explain 
class methods gamma delta follows relations fl fl appropriate constant depending specific ipm 
shows condition numbers matrices px uniformly bounded bounded away zero 
matrix apx full rank matrices apx px px full rank condition numbers matrices uniformly bounded 
rank apx depends degeneracy problem discuss detail 

non degenerate respective optimal faces programs unique solutions matrices apx apx px px non singular 
linear systems conditioned fl small 

degenerate non degenerate matrix apx columns rank apx apx px px singular 
means linear system ill conditioned 
numerical problems caused ill conditioning may arise fact early recognized 
shall discuss section method embedded safeguard 

non degenerate degenerate rank apx equal apx px px non singular 
implies linear system conditioned long fl small 
observe non degenerate degeneracy status matters little numerical point view 

degenerate said partition apx rank apx consequently say rank matrices 
ill conditioned normal equations matrix folklore iteration ipm needs inversion matrix optimum approaches extremely ill conditioned 
generally true impact accuracy solutions dramatic expect 
ill conditioning coefficient matrix factor determines difficulty solving system linear equations accuracy solutions 
equally important factor relation matrix right hand side 
important result theoretically justifies common experience numerical problems rare normal equations matrix ill conditioned 
closer look right hand side normal equations system 
form ad may split systems equations ad ad ad stewart proved positive diagonal matrix solution equation belongs bounded compact set 
words normal matrix ill conditioned bad property limited influence solution vector unfortunately infeasible primal dual method nonzero clear dependence furthermore changes see 
lp primal dual feasible infeasibilities go rapidly zero 
consequently vanishes converges fixed point stewart result ensures stability lp dual infeasible dual iterates diverge converge zero diverges 
stability result apply practice serious numerical difficulties observed 
lp primal infeasible primal iterates diverge primal infeasibilities converge zero diverges 
similarly dual infeasible case stability result apply eventually converge zero solution may cause additional numerical problems 
optimal basis identification rest consider lp problem dual standard form primal variables nonnegative upper bounds 
form simplifies notations easier explain understand underlying ideas 
primal problem consider minimize subject ax dual problem maximize subject vectors matrices dimensions 
known pair primal dual feasible solutions optimal 
optimal pair strictly complementary 
case fi fi 
due complementarity property strict complementarity property equivalent ng 
partition uniquely determined problem data called optimal partition lp problem 
need optimal basis 
known lp problem solved simplex method optimal basis solution produced solution problem 
course problem degenerate multiple optimal solutions possible optimal basis solutions generated 
case ipms iri imai method contrary simplex algorithm produce optimal basis solution primal dual optimal pair primal optimal solution solution relative interior primal optimal face dual optimal solution solution relative interior dual optimal face 
ipms provide optimal solution maximal number nonzero coordinates primal dual problems see ye 
proved solutions strictly complementary define optimal partition lp problem 
existence strictly complementary primal dual optimal solutions proved goldman tucker 
tucker propose pivot algorithm generate strictly complementary pair 
contrast see optimal basis obtained primal dual optimal solution pair strongly polynomial time 
optimal basis important reasons 
example basic solutions minimal number non zero coordinates advantageous solution small number positive coordinates required practical applications 
course situations strictly complementary optimal solution knowledge optimal partition needed see greenberg 
currently sensitivity analysis knowledge optimal bases commercial packages 
people approach papers recognized potential mathematical errors negative economical consequences approach 
correct analysis possible solutions ipms different cost simplex method 
decision approach preferable depends problem instance discussed subsection 
best knowledge basic solution necessary cutting plane methods mixed integer programming 
branch bound methods problem small modification 
date simplex solvers efficient ipms optimal solution available 
numerical reasons facing large mixed integer problem crossover ipms simplex method needed 
efficient techniques crossover similar basis identification techniques 
advantages provide sufficient motivation examine generate optimal basic solution optimal near optimal solution obtained ipm 
evident question occurs case degeneracy primal dual optimal solutions unique basic solutions 
get optimal basis 
attempts create efficient methods produce optimal basis strictly complementary optimal solution available 
best theoretical algorithm turned efficient practice see due megiddo 
finds optimal basis strongly polynomial time provided optimal solutions available 
due theoretical practical importance discuss detail 
principally ipm ends strictly complementary pair optimal solutions scheme applicable slightly general case 
suppose primal dual optimal solutions available 
index refers positive coordinates index refers zero coordinates index refers positive coordinates starting solution partition looking optimal basis disjoint partition indices submatrix built columns nonsingular xb xn 
process subsequently build necessary modify satisfy sign requirements 
naturally columns indices enter linearly independent meet requirement 
process starts optimal solution alter objective values 
algorithm split phases 
maximal linearly independent subset columns built 
time coordinates purified moved continues resulted contains linearly independent columns 
indices form initial basis done 
second phase extended maximal independent subset 
basis proceed third phase 
step indices drives corresponding coordinates zero 
corresponding columns independent current moved immediately examined current extended 
steps repeated extends basis 
complete algorithm 
optimal basis identification algorithm initialization suppose primal dual optimal solutions available initially partitioned explained 
reduce positive part columns dependent find pivoting vector implies 
eliminate positive coordinate say preserving non negativity ratio test 
remove column add 
note columns independent stage 
extend basis extend rank rank rank rank column independent add extend rank find pivoting vector implies 
note satisfies 
eliminate positive coordinate say preserving dual feasibility ratio test 
remove add optimal complementary pair rank formulae bxb see basis optimal 
note gaussian elimination steps pivoting necessary perform algorithm 
amount involved depends degree degeneracy lp problem 
worst case pivots dimension space necessary identify optimal basis 
algorithm uses primal dual information generates optimal basic solutions primal dual problems 
megiddo proves surprising result complexity identifying optimal bases just primal optimal solution just dual optimal solution available equivalent solving lp problem 
result demonstrates primal dual approach important advantage compared pure primal dual methods 
algorithm clear elegant problems solved practical implementation 
example approximate optimal approximate complementary solutions tolerances safeguards needed implementations 
problem consists finding partition applying megiddo algorithm initialized ipm optimal pair part usually refers variables clear decision variable index belongs special care taken select stable basis 
megiddo algorithm efficiently implemented 
theoretically clarified guaranteed crossover take place 
problems solved great progress concerned efficiency ipm implementations relevant questions remain open 
shall concentrate important ffl implementing analysis correct way 
ffl handling problems unbounded optimal faces 
detecting primal dual infeasibility 
ffl warm start 
correct analysis implementing algorithms lp consider methods produce shadow prices ranges 
currently implementations sensitivity analysis commercial codes knowledge optimal basis 
people approach papers recognized potential mathematical errors negative economical consequences 
ranges shadow prices obtained analyzing optimal bases usually incorrect know linearity intervals optimal value function left right derivatives correct shadow prices 
correct information obtainable different cost 
proved correspondence optimal partitions linearity intervals break points value function 
values obtained cost solution smaller lps 
define lps needs description optimal face 
strictly complementary solution optimal known 
primal optimal face fx ax dual optimal face px alternatively optimal partition known just primal optimal solution dual optimal solution available optimal faces characterizable follows 
fx ax mentioned get linearity intervals correct shadow prices solution auxiliary lps needed 
illustration define lps find left right actual linearity interval ffl left ffl right single objective coefficient changes 
optimal partition known need solve linear programs ffl left ffle px ffl right ffle px optimal partition available just optimal solutions known solution lps ffl left ffle ffl right ffle necessary 
note validity resulting ranges follows concavity convexity respectively optimal value functions 
similarly correct left right shadow prices obtained solving auxiliary lp problems 
obvious interior linearity interval value function varies left right shadow prices equal equals breakpoint optimal value function left right shadow prices computed solving lp problems maxfx ax ax optimal partition approach 
optimal partition available lps solved maxfx ax ax varies corresponding values obtained similarly cost solution analogous lps 
go details 
complete analysis optimal partition see alternative approaches optimal partition available discussed 
close section point get correct ranges shadow prices primal dual variables needs substantially time incomplete incorrect optimal basis approach 
advisable select smaller set important variables perform necessary calculations just set 
unbounded optimal faces infeasible problems discussed section current implementations infeasible ipms 
theoretical disadvantages implemented infeasible ipms theoretical complexity nl nl best complexity date 
optimal face unbounded duality gap converges zero sequence solutions primal dual unboundedness optimal faces respectively diverges difficult identify optimal solution 
similar difficulties occur primal dual problems infeasible 
case manifests divergence iteration sequence infeasible ipms practice easy identify 
resolving difficulties interesting problems nowadays 
called skew symmetric self dual embedding sssd remedy 
sssd introduced ye todd mizuno standard form problems 
discussed advantages embedding showed mizuno todd ye predictor corrector algorithms solve lp problem nl iterations yielding infeasible ipm complexity 
somewhat jansen roos terlaky sssd problem symmetric form primal dual lp pair concise theory lp ipms 
presenting sssd embedding surprisingly nice properties summarized 

self duality dual problem identical primal 
solve lp lcp 

sssd feasible 
furthermore interior feasible sets non empty optimal faces bounded 
ipms applied sssd converge optimal solution 

optimality original problem detected convergence independently boundedness unboundedness optimal faces original problem 

infeasibility original problem detected convergence 
primal dual primal dual rays original problems identified prove dual primal dual primal infeasibility 

perfectly centered initial pair constructed sssd 

polynomial convergence best known complexity local quadratic convergence guaranteed 
self dual embedding exploit fully symmetry sssd embedding problems transformed symmetric form 
done increasing number variables number constraints 
may assume problem rank redundant constraints eliminated 
basis 
ax written xb gamma nxn gamma equivalently gammab gamma nxn gammab gamma xn 
likewise xb xn gamma gamma gamma written equivalently symmetric form min phi gamma gamma xn gamma gamma nxn gammab gamma xn psi note transformations need modified preprocessing problem chooses sparse hopefully produces sparse gamma obvious basis constructed best knowledge implemented 
problem finding basis optimal sense gamma sparsest possible imposes additional requirements sparsity problem see equation 
practically difficult find efficient heuristics solve 
heuristics naturally generalize techniques solve sparsity problem 
having done transformations redefine objective vector coefficient matrix right hand side vector denote respectively 
produces lp problem symmetric form min phi ax psi theta matrix ir ir linear program associated dual program max phi psi usual pair called strictly complementary ax ax gamma gamma ax gamma gamma 
formulate sssd embedding need vectors 
ir ir ir ff fi ir follows gamma ax gamma gamma ff gamma fi ff gamma worthwhile note feasible ax gamma 
feasible gamma 
abuse mathematics vectors measure amount scaled infeasibility vectors consider skew symmetric self dual lp problem sssd min fi ax gammab gammaa gammac gamma gammaff gammafi gammac ff due selection parameters positive solution interior feasible sssd problem 
denote slack variables problem sssd respectively 
note chooses solution perfectly centered initial solution sssd problem 
theorem holds see 
theorem problems sssd embedding 
sssd problem feasible primal dual feasible optimal solution 
ii optimal solution sssd 
iii sssd strictly complementary optimal solution 
iv strictly complementary optimal solutions respectively 
infeasible 
skew symmetric self dual embedding implemented 
impressive numerical results reported show approach slightly efficient primal dual method feasible problems 
additionally experience mentioned shows method detects problem infeasibility really convergence divergence 
needs slightly different preprocessing deal dense rows columns bordered matrix augmented system require special care solving sssd approach may computationally attractive alternative infeasible primal dual method 
warm start practical problems need solution sequence similar linear programs small perturbations long perturbations small naturally expect optimal solutions far restarting optimization solution old problem warm start efficient 
case practice simplex method 
contrast efficient implementation warm start ipms exploits information contained old optimal solution 
attempts solve problem shifted barriers allow infeasibility original variables applying modified barrier functions perturbing problem throw away boundary positive orthant 
may say general context ipm warm start far working satisfactorily 
hand computational experience shows applying feasible projective algorithm restart centered optimal solution sufficiently far away boundary works problems subsequently solved differ considerably 
difficulty ipm warm start comes fact old optimal solution close boundary centered 
point perturbed problem remains close boundary badly centered 
consequently ipm long sequence short steps due fact iterates get rid boundary boundary behavior 
needs centered point close old optimal efficient centering method leaving boundary overcome difficulties 
possibilities discussed 
independently approach chosen wise save centered optimal solution say gamma relative duality gap sufficiently far away boundary 
ffl efficient centering 
called target method offers flexibility choosing achievable targets 
path methods define targets central path due boundary behavior optimistic case warm start 
sequence traceable targets improves centrality allows larger steps speeds centering optimization process 
practical methods defining sequence targets studied theory established see jansen roos terlaky vial roos vial volume 
ffl centered solutions warm start sssd embedding 
spectacular properties sssd embedding listed previous section ability construct perfectly centered initial point mentioned 
old centered optimal optimal solution initial point embed perturbed problem 
seen section ff fi redefined solution stays centered 
construction allows simultaneous perturbations additionally extends handling new constraints variables added problem build cutting plane schemes 
cases keep solution unchanged old coordinates actual barrier parameter define initial value new complementary variables barrier parameter old centered solution 
results perfectly centered initial solution 
adler karmarkar resende veiga 
implementation karmarkar algorithm linear programming mathematical programming 
adler karmarkar resende veiga 
data structures programming techniques implementation karmarkar algorithm orsa journal computing 
adler monteiro 
geometric view parametric linear programming algorithmica 
altman 
efficient implementation higher order primal dual interior point method large sparse linear programs archives control sciences 
andersen ye combining interior point pivoting algorithms linear programming research report may department management studies university iowa iowa city ia usa 
demmel duff 
solving sparse linear systems sparse backward error siam journal matrix analysis applications 
duff de 

augmented system approach sparse squares problems numerische mathematik 
goffin vial 
du 
experimental behaviour interior point cutting plane algorithm convex programming application geometric programming discrete applied mathematics 
tucker 
duality theory linear programs constructive approach applications 
siam review 

parallel interior point algorithm linear programs network transputers annals operations research 
bixby 
progress linear programming orsa journal computing 
mitra williams 
analysis mathematical programming problems prior applying simplex algorithm mathematical programming 
bunch 
direct methods solving indefinite systems linear equations siam journal numerical analysis 
chang mccormick 
hierarchical algorithm making sparse matrices sparser mathematical programming 
choi monma shanno 
development primal dual interior point method orsa journal computing 
dantzig 
linear programming extensions princeton university press princeton 
iterative solution problems linear quadratic programming doklady nauk sssr 
translated soviet mathematics doklady 
duff reid 
direct methods sparse matrices oxford university press new york 
duff gould reid scott turner 
factorization sparse symmetric indefinite matrices ima journal numerical analysis 
duff reid 
solution indefinite sparse symmetric linear equations acm transactions mathematical software 
mccormick 
nonlinear programming sequential unconstrained minimization techniques wiley new york 
mehrotra 
solving symmetric indefinite systems interior point method linear programming mathematical programming 
forrest tomlin 
implementing interior point linear programming methods optimization subroutine library ibm systems journal 
freund 
theoretical efficiency shifted barrier function algorithm linear programming linear algebra applications 
george liu 
evolution minimum degree ordering algorithm siam review 
gill murray saunders tomlin wright 
projected newton barrier methods linear programming equivalence karmarkar projective method mathematical programming 
goldman tucker 
theory linear programming linear inequalities related systems kuhn tucker eds annals mathematical studies princeton university press princeton new jersey 
golub van loan 
matrix computations nd ed 
johns hopkins university press baltimore london 

splitting dense columns constraint matrix interior point methods large scale linear programming optimization 

presolve analysis linear programs prior applying interior point method technical report department management studies university geneva switzerland 

path methods linear programming siam review 
greenberg 
optimal partition linear programming solution analysis technical report mathematics department university colorado appear operations research letters 
den roos terlaky tsuchiya 
degeneracy interior point methods linear programming survey annals operations research 
ye 
convergence behavior interior point algorithms mathematical programming 
mitra 
solving large scale linear programming problems interior point method massively parallel simd computer applied parallel computing evans ed gordon breach publishers appear vol 
iri imai 
multiplicative barrier function method linear programming 
algorithmica 
jansen roos terlaky 
interior point approach parametric analysis linear programming technical report faculty technical mathematics informatics technical university delft delft netherlands 
jansen roos terlaky 
theory linear programming skew symmetric self dual problems central path technical report faculty technical mathematics informatics technical university delft delft netherlands appear optimization 
jansen roos terlaky vial 
primal dual target algorithm linear programming technical report faculty technical mathematics informatics technical university delft delft netherlands 
jong de jansen roos terlaky 
sensitivity analysis linear programming just careful technical report amer amsterdam netherlands 
karmarkar 
new polynomial time algorithm linear programming combinatorica 
karmarkar lagarias wang 
power series variants karmarkar type algorithms technical journal 
kojima mizuno 
primal dual interior point algorithm linear programming megiddo ed progress mathematical programming interior point related methods springer verlag new york 
kojima megiddo mizuno 
primal dual infeasible interior point algorithm linear programming mathematical programming 
kojima megiddo mizuno 
theoretical convergence large step primal dual interior point algorithms linear programming mathematical programming 

interior point methods mathematical programming bibliography discussion institute economy operations research fern universitat hagen box hagen germany 
mitra 
experimental investigations combining primal dual interior point method simplex lp solvers revision annals operations research 
lustig shanno 
computational experience primal dual interior point method linear programming linear algebra applications 
lustig shanno 
implementing mehrotra predictor corrector interior point method linear programming siam journal optimization 
lustig shanno 
interior point methods linear programming computational state art orsa journal computing 
markowitz 
elimination form inverse application linear programming management science 
monma shanno 
implementation primal dual interior point method linear programming orsa journal computing 
megiddo 
finding primal dual optimal bases orsa journal computing 
megiddo 
pathways optimal set linear programming megiddo ed progress mathematical programming interior point related methods springer verlag new york 
mehrotra 
implementation primal dual interior point method siam journal optimization 
mehrotra 
higher order methods performance technical report department industrial engineering management sciences northwestern university evanston usa 
mehrotra monteiro 
parametric range analysis interior point methods technical report april dept systems industrial engineering university arizona tucson az usa 
mizuno 
polynomiality kojima megiddo mizuno infeasible interior point algorithm linear programming technical report school operations research industrial engineering cornell university ithaca ny 
mizuno todd ye 
adaptive step primal dual interior point algorithms linear programming mathematics operations research 
monma morton 
computational experience dual affine variant karmarkar method linear programming operations research letters 

modified barrier functions theory methods mathematical programming 
portugal terlaky 
investigation interior point algorithms linear transportation problems report der technische wiskunde en informatica technische universiteit delft 

infeasible interior point predictor corrector algorithm linear programming technical report department mathematics university iowa iowa city ia usa 
reid 
sparsity exploiting variant bartels golub decomposition linear programming bases mathematical programming 
resende veiga 
efficient implementation network interior point method technical report february bell laboratories murray hill nj usa 
roos vial ph 
interior point methods advances linear integer programming beasley 
ed chapter oxford university press oxford england 
saunders 
major cholesky feel proud orsa journal computing 
shanno bagchi 
unified view interior point methods linear programming annals operations research 

analytic center new classes global algorithms linear smooth convex programming system modelling optimization proceedings th ifip conference pr 
eds lecture notes control information sciences vol 
pp 
springer verlag berlin germany 
stewart 
scaled projections linear algebra applications 
walker 
direct solution sparse network equations optimally ordered triangular factorization proceedings ieee 
turner 
computing projections karmarkar algorithm linear algebra applications 
vanderbei 
splitting dense columns sparse linear systems linear algebra applications 
vanderbei carpenter 
symmetric indefinite systems interior point methods technical report sor department civil engineering operations research princeton university princeton new jersey 
ward 
approaches sensitivity analysis linear programming 
annals operations research 
wu wu ye 
quadratic convergence nl iteration homogeneous self dual linear programming algorithm technical report department management studies university iowa iowa city ia usa 
xu hung 
ye 
simplified homogeneous self dual linear programming algorithm implementation technical report department management sciences university iowa usa 
yannakakis 
computing minimum fill np complete siam journal algebraic discrete methods 
ye todd mizuno 
nl iteration homogeneous self dual linear programming algorithm mathematics operations research 
zhang 
convergence infeasible interior point algorithm linear programming problems research report dept mathematics statistics university maryland baltimore county baltimore md usa 

