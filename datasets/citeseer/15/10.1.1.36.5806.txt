spatio temporal memory soms activity diffusion neil jose principe main st gainesville fl computational laboratory university florida gainesville fl discusses biologically inspired concept activity diffusion create spatio temporal memory som neural gas algorithms 
activity diffusion creates system sensitive temporal patterns trained anticipate inputs 
technique uses temporal information help remove variability inherent signal 
essence system capable creating time varying voronoi regions 

sensory processing grouped domains static dynamic problems 
static problems consist information independent time 
instance static image recognition image change time 
hand time fundamental dynamic problem 
output dynamical system example depends input current state system encapsulates entire past inputs 
temporal processing analysis modeling prediction classification systems vary time 
patterns evolve time traditionally provided challenging problems scientists engineers 
language skills speech recognition speech synthesis sound identification vision skills motion detection target tracking object recognition locomotion skills synchronized movement robotics mapping navigation process control human mechanical time series prediction applications require temporal pattern processing 
fact ability properly recognize generate temporal patterns fundamental cognition 
neural network architectures designed static problems neural network successes domain 
attempts adapting architectures temporal processing success static models 

temporal som research attempts integrating temporal information som 
major technique add temporal information input som 
example exponential averaging tapped delay lines tested 
common method layered hierarchical soms second map tries capture spatial dynamics input moving map 
researchers integrated memory inside som typically exponentially decaying memory traces 
morasso created som leaky integrators thresholds node activate pattern stable area map certain amount time 
allows map pick stationary regions input signal sequences regions detect input sequence 
chappell taylor created som neurons hold activity surface leaky integrator storage 
learning law proposed taylor lead unstable weight space 
methodology patterns binary inputs length 
improved architecture moving leaky integration synapses 
gives network better picture temporal input space stable training exponentially windowed input standard kohonen map proposed kangas 
kohonen kangas proposed architecture include context som architecture 
kangas extended concept eliminating context weights allowing nodes vicinity winner selected 
conceptually extend concept include notion attention 
theory probability selecting winner affected higher cognitive processes may considered type supervision information past activations network context 
gives components selection winner distance context higher processes sensory distance normal distance form weight input 
architecture outperformed standard som simple temporal tasks train complicated trajectories 

temporal activity diffusion believe studying biological neural networks may find key temporal processing artificial neural networks 
began search new temporal methodology studying biology 
came concept activity diffusion 

activity diffusion biology reaction diffusion equations originally proposed turing typically explain natural pattern formation turing proposal modeled patterns nature interaction chemicals called morphogens 
different morphogens react diffuse substance equation mi mi equation mi concentration morphogen time dm diffusion coefficient mi mj function typically nonlinear represents interaction reaction morphogens 
varying interaction chemicals speed diffusion complicated spatial patterns chemicals created 
reaction diffusion equations create traveling waves 
example system equations fhn describe transmission energy axon neuron 
general concept element fires activity diffused neighbors pushes just far stable state move excited state 
newly excited elements excite neighbors excited elements relax creating traveling wave activity 
particularly interesting aspect human brain gas oxide involved processes central nervous system 
process modification synaptic strength thought mechanism learning 
neurons produce post 
diffuses rapidly cm long half life seconds creating effective range 
large quantities active synapse strengthen synapse called long term potentiation ltp 
level low synaptic strength decreased long term depression site strongly 
commonly called diffusing messenger ability carry information diffusion direct electrical contact synapses larger distances normally considered nonlocal 
diffusion non linear synaptic change mechanism shown capable supporting development topographical maps need mexican hat lateral interaction 
addition possibility lateral diffusive messenger effects long life produce interesting temporal effects 
shown act memory trace brain allow temporal correlations input converted spatial connection strengths 

temporal activity diffusion neural networks reaction diffusion equations leaky integrators concept activity wavefronts created spatio temporal memory stores information activity distributed network 
applied memory technique som neural gas architecture 
call architectures self organizing map temporal activity diffusion neural gas temporal activity diffusion 
basic principles fundamentally architectures 
short term memory converts time space tad method uses diffusion create self organization time space 
approach keep fundamental operation neural network order theory knowledge accumulated add self organization time space pes network 
creating temporally correlated neighborhoods output space basic functionality network organized temporally sensitive drastically changing underlying operation 
mechanism creation temporally correlated neighborhoods diffusion mechanism 
activity diffusion similar diffusion brain 
pe group pes fire influence neighbors typically lowering threshold fire near 
underlying mechanism neural network training hebbian nature neighboring pes fire correlated fashion tend continue fire correlated fashion 
creates temporally correlated neighborhoods self organization space time 
key concept tad architectures activity diffusion output space 
firing pe network causes activity diffuse network affects training operation network 
activity diffusion moves lattice som structure 
activity diffusion spreads neighboring pes thresholds neighboring pes lowered creating situation neighboring pes fire 
define enhancement amount pe threshold lowered 
model local enhancement acts traveling wave 
significantly reduces computation diffusion equations provides mechanism temporally ordered inputs trigger spatially ordered outputs 
traveling wave decays time 
remain strong spatially neighboring pes triggered temporally ordered inputs case traveling waves reinforced 
simple dimensional case shows enhancement sequence spatially ordered winners winners order pe pe pe pe sequence random winners winners order pe pe pe pe case input noise unknown 
ordered case enhancement lower threshold pe dramatically pes making pe win competition 
unordered case enhancement weak affects pes roughly evenly 
temporal enhancement pe pe pe pe pe pe temporal enhancement pe pe pe pe pe pe temporal activity network 
activity created temporally ordered input activity created unordered input second temporal functionality included tad architectures decay output activation time 
pe fires active maintains portion exponentially decaying activity fires 
pe gradually decays wavefront creates spread time simple traveling impulse 
spreading creates robust architecture gracefully handle time warping missing noisy data 
simplify description algorithm maps activity propagate direction diffusion activity severely restricted dimensional case 
output space considered set pes connected string information passed pes string 
activity enhancement moves direction increasing pe number decays step 
implementation activity diffusion string shown includes activity decay pe activity movement net left right direction 
factors normalize total activity network 
activity diffusion mechanism serves store temporal information network 
training pes spatially ordered sequentially follow temporal sequences 
node dist inp som pe node node act dist inp dist inp som pe som pe model activity diffusion string iteration activity network determined calculating distance dot product input weights pe allowing membrane potential decay dist inp wk equation represents activity pe time dist inp wk represents distance input time weights som pe activity diffuses network creates enhancement pe 
decaying wavefront version activity diffusion modeled scaled version activity previous pe previous time called enhancement en act equation decay constant applied decay enhancement 
winning som pe selected winner arg max dist inp en equation enhancement activity propagated left 
parameter spatio temporal parameter determines amount temporal wavefront lower threshold pe firing 
increasing lowers threshold neighboring pes point winner guaranteed neighbor current winner forces input patterns sequential output map 
interesting note system operates standard som system operates avalanche network 
winner selected trained neighbors hebbian manner normalization follows neigh inp equation wk neighborhood function neigh defines closeness winner typically gaussian function learning rate defined 
current implementation spatio temporal parameter learning rate neighborhood size annealed better convergence 
architecture creates spatially distributed memory 
memory system described equation ek act equation equation shows results matching activity act contribute enhancement 
traveling waves create decaying exponentials moves space moves time 
past history node added enhancement recursive self loop 
wavefront motion added enhancement diagonal movement left right channel scaled 
farther node diagonal farther back time influence enhancement 

simple illustrative example simple descriptive test case involves input composed dimensional vectors randomly distributed 
embedded input shaped sequences located upper right hand corner input space 
uniform noise added target sequences 
standard som maps input space maps pes regard temporal order simply needs cover input space structure 
show happens plot position input space represented weights pe remember weights pe center point voronoi region contains inputs trigger pe 
neighborhood relationship pes important connect neighboring pes line 
som result string pes string pes stretched manipulated training algorithm entire input space mapped minimum distortion error maintaining neighborhood relationships string broken 
orientation output important long covers input minimal residual energy 
typical example shown left side note slightly higher density input shaped region 
temporal activity added som mapping additional constraint temporal neighbors sequential winners fire sequentially 
string cover input space follow prevalent temporal patterns input 
shown right side 
notice sequential nodes aligned cover shaped temporal patterns input 
kohonen mapping temporal enhancement kohonen mapping temporal enhancement dimensional mapping dimensional input space spatio temporal coupling single string network trained represent single pattern multiple patterns 
multiple patterns require string long 
long string may difficult train properly weave way input space moving pattern 
additional flexibility added breaking large string smaller strings 
multiple strings considered array output nodes neighborhood function 
allows network follow multiple trajectories long complicated trajectories simplified manner 
landmark discrimination robotics application 
self organize clustering phoneme sequences 
application created hardware system temporally organized phonemes spoken words real time 

neural gas algorithm temporal activity diffusion neural gas algorithm similar som algorithm imposition predefined neighborhood structure output pes 
neural gas pes trained soft max rule soft max applied ranking distance vectors distance winning pe lattice 
neural gas algorithm shown converge quickly low distortion errors smaller means maximum entropy clustering som algorithm 
predefined neighborhood structure som reason works better disjoint complicated input spaces 
dynamics algorithm algorithm neural gas algorithm predefined lattice structure 
allows flexibility create diffusion structure trained best fit input data 
diffuses activity secondary connection matrix trained temporal hebbian learning 
flexible structure decouples spatial component temporal component network 
neighboring nodes time needed relatively close space order system train properly time space coupled 
longer restriction 
space time mapping coupling space time directly controllable 
interesting concept falls structure ability network focus temporal correlations 
temporal correlation thought simple concept anticipation 
human brain uses information past enhance recognition expected patterns 
instance conversation speaker uses context past determine expect hear 
methodology greatly improve recognition noisy input signals speech 

details algorithm works follows calculate distance di input pes 
temporal activity network similar diffusive wavefronts wavefronts scaled connection strengths pes 
temporal activity diffuses space defined connection matrix follows act winner acti acti equation max acti activity pe time decay constant pi connection strength pe pe parameter smoothes activity giving importance past activity network max normalizes connection strengths 
previous winner followed known path network higher activity influence selection 
rest algorithm identical 
output modified spatio temporal activity pes highest activity trained neural gas update rule pi ki pi equation learning rate step size 
exponential neighborhood parameter defining width exponential 
ki ranking pei modified distance input 
connection strengths trained temporal hebbian learning normalization 
temporal hebbian learning hebbian learning applied time pes fire sequentially enhance connection strength 
rule pes remain active period time fire current previous winners active time 
current implementation connection strengths updated similar conscience algorithm competitive learning min arg pi pi min equation strength connection winner winner increased small constant connections decreased fraction maintains constant energy set connections total number pes network 
operation weights fixed signal time structure creates temporal wavefronts network allow plasticity recognition 
temporal activity mixed standard spatial activity distance input weights spatio temporal parameter 
identical input values may fire different pes depending temporal past signal 
shows voronoi diagrams network different temporal histories 
particular diagrams number voronoi region represents pe number particular region located center static voronoi region center weights pe 
diagrams show regions input space fire pe network 
training data included random inputs interspersed temporal diagonal lines moving bottom left top right bottom right top left 
left side shows voronoi diagram presentation random noise network 
input pattern seen training input temporal wavefronts created voronoi diagram similar static voronoi diagram 
right side shows voronoi diagram presentation bottom left top right diagonal line 
temporal wavefront grew amplitude time pe fired 
training network connection strength pe pe large compared pes 
temporal wavefront flowed preferentially pe enhancing possibilities winning competition 
voronoi diagram previous winners beta voronoi diagram previous winners beta voronoi diagrams enhancement notice large region right side expected winner 
plasticity similar way humans recognize temporal patterns speech 
notice network uses temporal information previous training anticipate input 
anticipated result detected network expecting see 
important point static dynamic conditions dramatically different 
dynamic centroids vectors important temporal information changes entire characteristics vector quantization creating data dependent voronoi regions 
animation demonstrate operation voronoi regions better static figures 

vector quantization speech data goal application recognize spoken english digits 
vector quantize sampled frequency representation digit 
corpus set speakers saying digits 
speakers training testing 
speakers graduate students professors electrical engineering department university florida 
speakers represent wide variety accents making task significantly difficult think 
preprocessing comprised calculating cepstral coefficients ms frames overlapped ms khz sampling 
cepstral coefficients raised sine control bearing cepstral variability reliable discrimination sounds 
cepstral coefficients mean filtered time reduce number input vectors 
major difference vq method standard vq method algorithm trained enhance patterns trained 
options incorporate temporal characteristics architecture 
typically vector quantizer quantize word corpus 
done vq 
case network need store temporal information digits single network 
possible task simplified training separate network digit network stores temporal characteristics single digit 
similarly separate mlp detect digit 
shows block diagram system 
speech signal speech signal feature vector generation feature vector generation smoothed cepstral coefficients smoothed cepstral coefficients vq codebook generation vector quantization vq codebook generation digit codebook entries sequence codebook vectors sets codebooks mlp digit recognition typical vector quantization training system vector quantization sequence codebook vectors mlp digit recognition vector quantization training system 
block diagram digit recognition system 
standard digit recognition system recognition system trained vq networks 
process done feeding network input consisting target digits spoken training speakers interspersed random vectors digits 
training activity wavefronts easily seen plot maximum activity network time 
usually picks wavefront activity network quite 
shows activity digit network training data 
instances word spoken highlighted dashed lines 
input data interspersed presentations es random vectors digits 
clearly activity network higher word network 
notice certain speakers adequately match global average 
instance speaker near sample create large activity spike network 
larger systems solved multiple networks digit allowing variation speakers 
vq networks trained vector quantized digit speakers training set test set 
vector quantized data standard neural gas algorithm test results 
order remove variability caused different rates words phonemes spoken passed sequence vectors spoken word gamma memory 
gamma memory taps giving depth samples corresponds maximum length spoken digit corpus minimum vectors 
output taps memory fed mlp hidden pes activity maximum activity training digit 
maximum activity network digit sigmoidal output pe 
networks trained simultaneously winner take pe select network largest output 
digit detector largest output declared winner compared desired signal indicates digit spoken 
spoken digit frequency information vq digit vq digit 
vq digit gamma memory embedding gamma memory embedding 
gamma memory embedding 
digit recognition system mlp detector digit mlp detector digit 
mlp detector digit winner determines spoken digit entire system trained different sets data 
original data preprocessor train system 
allows validate vector quantization reduces variability data allows easier recognition digits 
second neural gas algorithm vector quantize input data 
third network vector quantizer 
order remove random variations initial conditions system trained tested different times results averaged 
table shows results training 
key figures number misclassifications test set 
mlps universal mappers sufficiently large mlp learn classify virtually data set 
common problem mlps 
network classification training set poor classification test set 
true indication performance mlps performance test set 
table summary digit recognition system performance training testing training testing percent correct system type mse mse classification test set vector quantization neural gas vq vq table shows vq system reduced number errors testing set neural gas vq system system vector quantization 
performance due reduction variability systems 
vector quantization technique removes variability signal clustering inputs representing input cluster single vector 
vq system takes step temporal information enhance clustering 
temporal sequence feature vectors plays important role vector quantization 
additional comparison hidden markov model hmm states trained original input 
hmm trained cycles starting different initial conditions 
average results hmm correct test set 

tad algorithm uses temporal plasticity induced diffusion activity time space 
creates unique spatio temporal memory som neural gas architecture 
activity diffusion couples space time single set dynamics help disambiguate static spatial information temporal information 
creates time varying voronoi diagrams past input signal 
dynamic vector quantization helps reduce variability inherent input anticipating training inputs 


kangas time delayed self organizing maps proceedings international joint conference neural networks pp 
part 

kangas phoneme recognition time dependent versions self organizing maps proceedings international conference acoustic speech signal processing vol 
pp 


morasso analysis continuous temporal sequences map sequential leaky integrators proceedings icnn pp 


shastri hierarchical model reflexive processing application visual trajectory classification international computer science institute berkeley ca technical report tr june 

chappell taylor temporal kohonen map neural networks vol 
pp 


extending kohonen self organising map adaptive parameters temporal neurons ph thesis university college london department computer science february 

kohonen architecture proceedings international conference artificial neural networks pp 


kangas temporal knowledge locations activations self organizing map artificial neural networks pp 


selective attention self organizing maps proceedings neural networks applications marseille france 

dynamic extensions self organizing maps proceedings international conference artificial neural networks sorrento springer london 

neurons continuous varying activation self organizing maps natural artificial neural computation lecture notes computer science vol 
pp 
springer verlag 

turing chemical basis morphogenesis phil 
transactions royal society london ser 
vol 
pp 


tyson singular perturbation theory traveling waves excitable media review physica vol 
pp 

taylor oxide development long range horizontal connectivity neural networks world vol 
pp 


taylor oxide cortical map formation international artificial neural networks 

autonomous robotic agent neural network learning autonomous mapping navigation strategies unpublished master thesis university portugal 

principe self organizing temporal pattern recognizer application robot landmark recognition sintra spatiotemporal models biological artificial systems workshop november 

bode wave propagation neural coupling mechanism hardware self organizing feature maps representation temporal sequences ieee neural networks signal processing proceedings pp 


martinetz schulten neural gas network vector quantization application time series prediction ieee transactions neural networks vol 
july pp 

