linguistic competence clearly illustrates processing sequences events fundamental aspect human cognition 
reason sequence learning behavior currently attracts considerable attention cognitive psychology computational theory 
typical sequence learning situations participants asked react element sequentially structured visual sequences events 
important issue context determine essentially associative processes sufficient understand human performance powerful learning mechanisms necessary 
address issue explore human participants connectionist models capable learning sequential material involves complex disjoint contingencies 
show popular simple recurrent network model elman shown account variety empirical findings cleeremans fails account human performance experimental situations meant test model specific predictions 
previous research cleeremans briefly described structure center embedded sequential structures manipulated strictly identical probabilistically different function elements surrounding embedding 
srn learn second case human subjects insensitive manipulation 
new experiment described tested idea performance benefits starting small effects elman contrasting conditions training regimen incremental 
srn capable learning case human subjects able learn 
suggest alternative model auto associative recurrent network way overcome srn model failure account empirical findings 
past years sequence learning major paradigms study elementary learning processes particularly context implicit learning research see berry cleeremans reviews 
typical sequence learning situations participants asked react element sequentially structured visual sequences events nissen 
large literature showing human subjects exhibit detailed sensitivity incremental sequence learning axel cleeremans ulb 
ac 
arnaud ulb 
ac 
de recherche en sciences universit libre de bruxelles avenue 
roosevelt cp bruxelles belgium sequential constraints material differences reaction time stimuli predictable temporal context set previous elements sequence 
important issue context determine essentially associative processes sufficient understand human performance powerful learning mechanisms necessary 
issue typically approached exposing participants complex material involves disjoint temporal contingencies contingencies elements sequence separated number irrelevant elements 
instance reed johnson trained participants called second order conditional sequences element sequence predicted identity element 
research focused specifically question determining human participants maintain information long distance contingencies embedded material illustrated natural language expressions dog chased cat playful dogs chased cat playful expressions share embedding completely irrelevant determining number verb 
processing expressions information number head dog vs dogs maintained memory processing embedding information completed 
expressions interesting challenges popular sequential connectionist architectures simple recurrent network henceforth srn 
srn proposed elman subsequently adapted cleeremans mcclelland simulate sequential effects reaction time tasks shown 
network uses back propagation learn predict element sequence current element representation temporal context network elaborated 
uses information provided called context units step contain copy network hidden unit activation vector previous time step 
training relative activation output units representing possible successor come srn context element hidden units current element simple recurrent network srn 
see text details 
approximate optimal conditional probabilities associated appearance current context interpreted representing implicit preparation event network model human sequence learning performance 
previous see cleeremans mcclelland cleeremans shown srn able account variance sequential choice reaction time data 
srn suffers important limitation ability learn sequential material 
key aspect learning srn material need prediction relevant step representation maintained context layer see servan schreiber cleeremans mcclelland 
words element sequence useful predicting probabilistically 
specific limitation shared architectures similar complexity jordan network jordan buffer networks see cleeremans 
human participants perform material interesting diagnostic value determining model best fit account human sequence learning performance 
issue focus rest 
start reviewing existing data light previous research srn 
report experiment meant compare human simulated performance conditions srn known fail 
overcoming srn limitations previous research issue revealed number important facts srn limitations 
particular authors attempted show srn sensitivity prediction relevance overcome changing features stimulus environment network exposed training 
arguments developed 
may argue natural language situations seldom correspond artificially hard situation described 
example illustrates naturalistic argument showing embedded structures fact dependent information conveyed head subtle ways dog chased tail playful dogs chased playful sentences embeddings longer switched expressions number head constraints extent embeddings follow 
gist argument completely independent embeddings exception rule typical embeddings contain information relevant processing subsequent information 
idea servan schreiber cleeremans mcclelland trained srn sequential material generated finite state grammar producing structures form expressions element contingent head separated number embedded elements symmetrical condition servan schreiber simulations embedding identical regardless head occurred 
asymmetrical condition probability occurrence embedded elements varied function head 
conditions srn assessed predict tail neutral embeddings 
servan schreiber srn master material asymmetrical condition 
srn maintain information irrelevant embedded elements embedding probabilistically dependent head 
cleeremans tested human subjects time task design described grammar illustrated generate center embedded structures similar ones described previous paragraphs 
subjects able successfully anticipate tail occur head conditions 
contrast srn able encode long distance contingencies outer elements asymmetrical condition embeddings probabilistically dependent head 
results suggest srn limitations overcome changing probability structure stimulus environment exposed human participants appear suffer limitations 
second way overcome srn limitations proposed elman 
elman srn able learn kind complex hierarchically organized information typically occurs natural language training incremental network progressively exposed complex sequential contingencies contained stimulus material 
quote elman network fails learn task entire data set training data selected simple sentences network succeeded mastering going master complex sentences 
elman directly manipulating network memory forcing finite state grammar generate stimulus material 
see text details 
activations context units reset progressively larger intervals training produced equivalent beneficial effects result prompted elman frame argument terms development observation capacity children short term memory increases age 
propose explore idea context sequence learning situations 
report experiment designed test effects training regimen sequential reaction time task 
contrasted conditions involved stimulus material different training incremental non incremental 
detail experiment section 
experimental design method experiment consisted training sessions subjects exposed serial choice rt task 
session consisted blocks trials total trials 
trial stimulus appeared positions arranged horizontally computer screen subjects press fast accurately possible corresponding key 
sequential structure material manipulated generating sequence finite state grammar illustrated described 
sequences contained different long distance contingencies elements head tail separated varying number embedded elements 
determine training regimen impact performance contrasted conditions 
flat training condition distribution embedding lengths training 
subjects exposed complex material right start 
contrast incremental training condition training started short embeddings proportion long embeddings increased progressively training 
subjects twelve subjects randomly assigned condition 
subjects paid participation experiment earn additional bonus performance 
apparatus display experiment run powerpc macintosh computers 
display consisted dots arranged horizontal line computer screen separated intervals cm 
screen position corresponded key computer keyboard 
spatial configuration keys fully compatible screen positions 
stimulus small black circle cm high appeared white screen background centered cm dots 
rsi msec 
procedure procedure experiment followed closely design described jim nez ndez cleeremans 
subjects exposed sessions day consecutive days 
subjects kept unaware fact material contained sequential contingencies merely told experiment effects prolonged practice motor performance 
instructions stressed accuracy speed 
short user controlled rest breaks occurred experimental blocks 
breaks subjects feedback performance informed bonus money earned far 
amount computed block accuracy speed 
stimulus material stimuli generated finite state grammar illustrated 
trials stimulus generation proceeded phases 
arc coming current node randomly selected label recorded 
current node initialized randomly start block updated trial node pointed selected arc recorded label determine screen position stimulus appear latin square design label corresponded screen position exactly subjects condition 
grammar generates sequences share form designates head element designates embedded element designates tail element 
tail element sequence served head sequence 
grammar heads tails instantiated labels subsequently refer outer elements 
grammar designed frequency flat embedding length incremental initial frequency distribution embeddings containing elements flat incremental conditions 
particular head followed embedding outer elements head tended strongly associated legal tails 
instance appears head sequence appear tail sequence tend appear tail element cases remaining cases 
difference tail likelihood provides simple way assessing subjects sensitive regularities contained material 
difference reactions times elicited vs tails clearly indicate participants encoded information head element head provides information distribution tails 
illustrates heads tails separated embedding 
embedding instantiated different tokens labels 
grammar designed element mandatory 
subsequent elements embedding chosen random constraint direct repetitions element forbidden 
embedded elements totally irrelevant respect task predicting tail 
random number additional embedded elements appear probability loops grammar 
flat condition set entire experiment 
means embedded element chance followed embedded element 
incremental condition contrast probability stay loop increased steps sessions 
shows initial frequency distribution embeddings length 
see short embeddings frequent incremental condition flat condition distribution reverses embeddings length higher 
distributions condition progressively converged training identical sessions 
results subjects exposed choice reaction time task involving complex sequential contingencies incrementally incremental condition flat condition 
report results corresponding simulation srn model 
simulation results assess srn able account rt performance experiment trained model material human subjects number trials parameters architecture cleeremans mcclelland 
srn hidden units local representations input output pools unit corresponded stimuli 
account short term priming effects network dual connection weights described cleeremans mcclelland 
network trained predict element continuous sequence stimuli generated exactly conditions human subjects 
step label generated grammar network setting activation corresponding input unit 
activation allowed spread units network error response actual successor current stimulus modify weights 
training running average activation output unit recorded trial transformed luce ratios normalize responses 
purpose comparing simulated observed responses assumed normalized activations output units represent response tendencies linear reduction rt proportional relative strength unit corresponding correct response 
network responses subtracted increases response strength compatible reduction rt transformed easy comparison human data 
results illustrated left panels 
data represents differences response strengths associated tails sequences containing embedded elements flat condition top panel incremental condition bottom panel 
clear srn incapable learning shortest contingencies flat condition differences responses tails regardless length embedding 
network simply fails learn 
contrast model appears capable successfully predicting tail element sequences containing single embedded element incremental condition producing responses stronger tail occur head vs training 
predicted model quite sensitive difference training stimulus material 
human participants similarly sensitive difference 
focus section 
luce ratio differences session human performance human data illustrated middle panels 
see subjects appear learn flat condition incremental condition 
determine participants able discriminate tails embeddings different lengths conducted anova data obtained sessions experiment 
data represented 
indicates participants appear sensitive likelihood tails occurring embeddings length performance appears quite similar conditions 
averaging conditions tails msec advantage tails embedding length msec advantage embedding length 
impressions confirmed results mixed measures anova condition factor flat vs incremental condition session levels embedding length levels tail probability vs repeated measures factors 
condition session failed reach significance 
analysis revealed significant effect tail probability mse indicating participants tended produce faster responses reacting tail compared responses tails 
suggested data shown flat rt differences session incremental luce ratio differences srn human data session human simulated data srn model left panels human subjects middle panels right panels plotted separately embeddings length squares circles triangles 
top panels flat condition 
bottom panels incremental condition 
data separately transformed data obtained source conditions 
see text additional details 
sensitivity tail likelihood interacted significantly embedding length mse 
contrasts conducted separately different levels embedding length showed participants exhibited significant differences responses tails embeddings length embeddings involving elements 
contrast srn data participants appear learn material equally conditions 
discussion learn disjoint temporal contingencies 
learning influenced training regimen 
addressed issues exploring human simulated performance experiment meant test specific prediction srn model sequence processing borne empirically 
experiment involved assessing reaction time performance complex sequential material containing center embedded elements incrementally 
elman showed srn learn material complex instances introduced progressively training 
contrast human data showed participants learn material conditions instances containing embedded elements 
rt difference results combined research described suggest srn exhibits specific limitations human participants sensitive human participants generally capable mastering sequential material srn unable learn 
necessarily srn inadequate model human performance sequential choice reaction time tasks model clearly captures central aspects performance situations see cleeremans review prompt look alternative architectures build srn strengths overcoming limitations 
architecture proposed 
auto associative recurrent network henceforth illustrated 
name suggests network essentially srn required act encoder current element context information 
time step network required produce current element context information addition predicting element sequence 
requirement forces network maintain srn context srn context current element hidden units current element flat embedding length incremental reaction time differences responses tails averaged sessions experiment sequences containing embeddings length 
element auto associative recurrent network 
see text details 
information previously sequence elements tend forgotten standard srn performing prediction task 
showed capable mastering languages srn master 
applied data produces pattern results shown right panels 
see network successful reproducing important aspect human performance learn conditions albeit model learn human participants 
likewise applied experiment involving symmetrical vs asymmetrical embeddings cleeremans described learns srn fails 
research clearly necessary results encourage continue explore properties model sequence learning choice reaction time tasks 
importantly model just srn involves elementary associative learning mechanisms 
authors note axel cleeremans research associate national fund scientific research belgium 
arnaud scientific collaborator national fund scientific research belgium 
supported axel cleeremans 
berry 

implicit learning theoretical empirical issues erlbaum 
cleeremans 

mechanisms implicit learning connectionist models sequence processing 
cambridge ma mit press 
cleeremans mcclelland 

learning structure event sequences 

elman 

finding structure time 
cognitive science 
elman 

learning development neural networks importance starting small 
cognition 
jim nez ndez cleeremans 

comparing direct indirect measures sequence learning 
lmc 
jordan 

attractor dynamics parallelism connectionist sequential machine 
proceedings th 
annual conference cognitive science society 
hillsdale nj erlbaum 


forced simple recurrent neural network grammatical inference 
proceedings fourteenth annual conference cognitive science society nj lea 
nissen 

attentional requirements learning evidence performance measures 
cognitive psychology 
reed johnson 

assessing implicit learning indirect tests determining learned sequence structure 
lmc 
servan schreiber cleeremans mcclelland 

graded state machines representation temporal contingencies simple recurrent networks 
machine learning 

