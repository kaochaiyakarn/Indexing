lessons neural network training overfitting may harder expected proceedings fourteenth national conference artificial intelligence aaai aaai press menlo park california pp 

copyright aaai 
lessons neural network training overfitting may harder expected steve lawrence lee giles ah chung tsoi nec research independence way princeton nj faculty informatics uni 
australia research nj nec com ah chung tsoi edu au reasons neural networks popular ai machine learning models 
important aspects machine learning models model generalizes unseen data model scales problem complexity 
controlled task known optimal training error investigate convergence backpropagation bp algorithm 
find optimal solution typically 
furthermore observe networks larger expected result lower training generalization error 
result supported real world example 
investigate training behavior analyzing weights trained networks excess degrees freedom seen little harm aid convergence contrasting interpolation characteristics multi layer perceptron neural networks mlps polynomial models overfitting behavior different mlp biased smoother solutions 
analyze relevant theory outlining reasons significant practical differences 
results bring question common beliefs neural network training regarding convergence optimal network size suggest alternate guidelines practical lower fear excess degrees freedom help direct methods creation parsimonious solutions importance mlp bp bias possibly worse performance improved training algorithms 
neural networks popular ai machine learning models written 
common belief number parameters network related number data points expressive power network 
results suggest characteristics training algorithm considered 
generalization overfitting neural networks ai machine learning models prone overfitting 
illustrates concept polynomial approximation 
training dataset created contained points equation sin uniformly distributed random variable 
equation evaluated 
dataset fit polynomial models orders 
copyright american association artificial intelligence www aaai org 
rights reserved 
order approximation poor 
order approximation reasonably 
order number parameters increases significant overfitting increasingly poor generalization evident 
order approximated function fits training data interpolation training points poor 
overfitting important problem mlps devoted preventing overfitting techniques model selection early stopping weight decay pruning 
approximation training data target function noise approximation training data target function noise order order approximation training data target function noise approximation training data target function noise order order 
polynomial interpolation function sin range order model increased 
uniformly distributed random variable 
significant overfitting seen orders 
theory selection model size maximizes generalization important topic 
theories determining optimal network size nic network information criterion generalization aic akaike information criterion akaike widely statistical inference generalized final prediction error gpe proposed moody vc dimension vapnik measure expressive power network 
nic relies single defined minimum fitting function unreliable local minima ripley 
little published computational experience nic gpe 
evaluation prohibitively expensive large networks 
vc bounds calculated various network types 
early vc dimension handles case discrete outputs 
case real valued outputs general notion dimension required 
pseudo dimension defined considering loss function measures deviation predictions target values 
vc bounds conservative provide generalization guarantees simultaneously probability distribution training algorithm 
computation vc bounds practical networks difficult 
student teacher task investigate empirical performance task crane know optimal solution carefully control problem 
task follows 
mlp input nodes mh hidden nodes mo output nodes teacher network denoted mh mo initialized uniform random weights range gammak bias weights range gamma 

tr training data points te test points created selecting gaussian random inputs zero mean unit variance propagating network find corresponding outputs 
te 

training data set train new mlps student networks architecture mo varied mh 
mh initial weights new networks set randomly procedure suggested haykin equal weights network create dataset 
theoretically mh optimal training set error zero case noise added data 
simulation results section investigates training generalization behavior networks student teacher task teacher network size fixed student network size increasing 
cases data created teacher network architecture chosen represent typical network number inputs greater number hidden nodes specific values chosen total training time simulations reasonable random weight maximum value 
student networks architecture varied 
theoretically optimal training set error networks tested zero networks trained obtained optimal error backpropagation bp rumelhart hinton williams theta updates configuration mlp tested simulations different starting condition random weights 
method controlling generalization maximum number updates order alternative optimization techniques conjugate gradient improve convergence cases 
techniques lose advantage larger problems may detrimental training algorithm bias bp may beneficial see 
demonstrate case advocate practice networks trained identical number stochastic updates theta 
expected overfitting occur 
initial learning rate reduced linearly zero training 
standard mlp 
batch update investigated convergence poor training times extended order magnitude 
quadratic cost function 
considering networks hidden units contain degrees freedom necessary zero error reasonable expectation performance worse average number hidden units increased 
shows training test set error number hidden units student network varied 
results box whiskers plots usual mean plus minus standard deviation plots 
performed experiments different values tr number training points 
average best generalization error corresponds networks hidden units respectively tr number parameters networks greater case hidden units numbers parameters varied 
interest observe effect noise problem 
shows results case training points gaussian noise added input data standard deviation equal standard deviation input data 
similar trend observed 
distribution results gaussian alternative means presenting results mean standard deviation informative giles lawrence 
box whiskers plots tukey show interquartile range iqr box median bar box 
whiskers extend ends box minimum maximum values 
median iqr simple statistics sensitive outliers mean standard deviation 
median value middle arranging distribution order smallest largest value 
data divided equal groups median iqr difference medians groups 
iqr contains points 
caruana tutorial nips caruana generalization results variety problems size networks varied small large 
small large related number parameters model consideration distribution data error surface 
caruana reported large networks rarely worse small networks problems investigated 
results partially correlate observation 
caruana suggested backprop ignores excess parameters 
trend varies teacher network size number inputs hidden nodes outputs nature target function example optimal size networks perform best certain tasks cases advantage larger networks greater 
number hidden nodes number hidden nodes training points number hidden nodes number hidden nodes training points number hidden nodes number hidden nodes training points noise 
error networks topology noise training points 
points shown results similar points case 
graphs left training errors graphs right test errors 
abscissa corresponds number hidden nodes 
box whiskers plots shown left case mean plus minus standard deviation shown right case 
results taken indicate oversized networks 
indicate oversized networks may generalize 
additionally results indicate training successful larger networks possible larger networks generalize better smaller networks 
observations 
remains desirable find solutions smallest number parameters 

similar result expected globally optimal solution small networks hidden unit networks trained zero error expected networks extra degrees freedom result worse performance 

distribution results important 
example observe advantage larger networks training points decreased considering minimum error mean error 

number trials important 
sufficiently trials performed possible find near optimal solution optimal size networks limit infinite number random starting points finding global optimum guaranteed appropriate initialization 
advantage larger size networks expected disappear 

note deliberately control generalization capability networks validation set weight decay maximum number updates 
solutions fit training data generalize 
contrary expected results indicate possible oversized networks provide better generalization 
successive pruning retraining larger network may arrive network similar size smaller networks improved training generalization error 
note bp find optimal solution cases 
interest solution scales problem complexity 
parameter controlled investigate 
increased function mapping generally complex smooth 
experiments increasing show solution progressively worse respect optimal error zero increased 
analysis operation bp supports results 
degrees freedom rules degrees freedom model proposed selecting topology mlp number parameters network significantly number examples parameter mlp comfortably store bits information 
network tend memorize data 
rules aim prevent overfitting unreliable optimal number parameters depend factors quality solution distribution data points amount noise bias training algorithm nature function approximated 
specific rules mentioned commonly believed accurate 
stipulation number parameters num ber examples typically believed true common datasets 
results indicate case 
number hidden nodes 
face recognition example best generalizing network times parameters training points parameters 
face recognition example section presents results real data 
shows results training mlp classify people images faces training set contains images person total training patterns test set contained different set images person 
small window stepped images image samples point quantized dimensional self organizing map 
outputs self organizing map image sample inputs mlp 
case networks trained updates 
networks contain parameters number training points hidden layer sizes number weights best training error best generalization error corresponds largest model 
note generalization controlled example validation set weight decay overfitting expected sufficiently large networks sufficiently successful training 
simulated serial machines larger networks require longer training times number updates 
interest compare happens smaller networks trained longer larger networks 
problems investigated training equal time equal numbers updates significantly affect results 
proposed practical face recognition technique 
database orl database contains set faces taken april april olivetti research laboratory cambridge available www cam orl uk html 
different images distinct subjects database 
variations facial expression facial details 
images taken dark homogeneous background subjects right frontal position tolerance tilting rotation degrees 
variation scale 
images greyscale levels resolution 
polynomial mlp interpolation shows results mlp approximate training set earlier polynomial approximation example polynomial case smallest network hidden unit weights including bias weights approximate data 
hidden units weights approximation reasonably 
contrast polynomial case networks hidden units weights hidden units weights resulted reasonably approximations 
particular simple example mlp networks trained backpropagation lead large degree overfitting times parameters data points 
certainly true overfitting serious problem mlps 
example highlights possibility mlps trained backpropagation may biased smoother approximations 
list number possibilities lead bias 
training mlp np complete general known practical training algorithms mlps results sub optimal solutions due local minima result attaining sub optimal solution network resources efficiently 
experiments controlled task indicated sub optimal solutions smaller weights average lawrence giles tsoi 
intuitive explanation weights typically start reasonably small reason may get trapped local minima reaching large values 

mlps universal approximators hornik stinchcombe white 
universal approximation result requires infinite number hidden nodes 
number hidden nodes network may incapable representing required function implement simpler function approximates required function 

weight decay krogh hertz weight elimination weigend rumelhart huberman mlp training aim minimize cost function penalizes large weights 
techniques tend result networks smaller weights 

commonly recommended technique mlp classification set training targets away bounds activation function 
tanh activation function haykin 
mlp networks course resistant training details follows 
single hidden layer mlp backpropagation stochastic training updates learning rate schedule initial learning rate 
results show bp training results sub optimal solutions 
commonly solutions referred local minima written proven 
yu 
local minima create trouble bp error surface features plateaus flat spots troublesome 
error surface different problems may local minima may far amenable gradient descent optimization 
overfitting 
example repeating experiment evaluating equation creating data points overfitting seen hidden nodes 
approximation training data target function noise approximation training data target function noise hidden node hidden nodes approximation training data target function noise approximation training data target function noise hidden nodes hidden nodes 
mlp interpolation function sin range number hidden nodes increased 
uniformly distributed random variable gamma 
large degree overfitting observed 
network size degrees freedom simple explanation larger networks provide improved training generalization error extra degrees freedom aid convergence addition extra parameters decrease chance stuck local minima plateaus 
van der 
section presents visualization technique showing weights student networks network size varied 
smaller task aid visualization teacher network topology student networks contained hidden nodes 
training points 
figures show weights student networks case gaussian noise standard deviation input standard deviation added inputs performed experiments produced similar results 
diagrams plotted follows columns correspond weights hidden nodes bias input nodes 
rows organized groups space group 
number groups equal number hidden nodes student network 
rows group top row corresponds teacher network bottom row corresponds student network 
idea compare weights teacher student networks 
couple difficulties arise comparison resolved follows 
firstly reason hidden node teacher network correspond hidden node student network problem resolved finding best matching set weights student network hidden unit teacher network matching hidden nodes accordingly 
matches ordered quality match top rows shows teacher network hidden node best approximated student hidden node 
likewise worst match bottom 
second problem trying match weights hidden nodes input nodes take account output layer weights exactly hidden node function computed different weights hidden nodes weights scaled output layer weights scaled accordingly 
case output considered solution simple hidden layer weights scaled respective output layer weight 
individual weight scaled appropriate output weight plotted follows square shaded proportion magnitude weight white equals black equals maximum value weights networks 
negative weights indicated white square inside outer black square surrounds weight 
observations teacher network weights matched closely larger networks consider fourth fifth best matching groups rows extra weights larger networks contribute final approximation minor way hidden units larger networks appear redundantly case may related artificial nature task results indicate pruning optionally retraining larger networks may perform 
backpropagation result underutilization network resources certain cases parameters may ineffective partially effective due sub optimal convergence 
hidden units hidden units hidden units 
weights training networks hidden units case gaussian noise standard deviation standard deviation inputs 
case results shown networks different random starting weights 
plotting method described text 
learning theory results contradiction statistical learning theory 
vapnik states machines small vc dimension required avoid overfitting 
states difficult approximate training data problem mlp approximation goal find appropriate network size order minimize tradeoff overfitting poor approximation 
vapnik suggests priori knowledge may required small training error small generalization error 
case linear output neurons barron derived bound total risk mlp estimator mh mhm tr log tr absolute moment fourier magnitude distribution target function measure complexity tradeoff observed accuracy best approximation requires larger avoidance overfitting requires smaller tr ratio 
left hand term approximation error corresponds error target function closest function mlp implement 
noise free artificial task approximation error zero 
equation selected optimal network size note results reported sigmoidal linear output neurons 
theory practical results differ 
domain applicability theory cover practical case assumptions incorporated theory reasonable 
specifically theory take account limited training time different rates convergence different sub optimal solutions 
bartlett correlates results reported 
bartlett comments vc bounds loose neural networks perform successfully training sets considerably smaller number network parameters 
bartlett shows classification number training samples needs grow ignoring log factors avoid overfitting bound total weight magnitude neuron number layers network 
result explicit weight decay implicit bias smaller weights leads phenomenon observed larger networks may generalize better generalization possible larger networks trained successfully smaller networks reduced difficulty local minima 
task considered distribution weights training moves smaller weights size student network increases 
seen backpropagation fails find optimal solution cases 
furthermore networks weights expected result lower training generalization error certain cases 
overfitting behavior significantly different mlp polynomial models mlps trained bp biased smoother solutions 
infinite time appropriate alternate training algorithm optimal solution mlp 
examples illustrate mode failure exhibited fact beneficial result better generalization improved algorithms implicit smoothness bias created network structure training algorithm matches desired target function 
bias may account part success mlps encountered competing methods real world problems 
acknowledgments acknowledge useful discussions comments caruana hanson heath horne lin 
muller 
opinions expressed authors 
olivetti research laboratory compiling maintaining orl database 
akaike 
information theory extension maximum likelihood principle 
petrov eds proceeding nd international symposium information theory 
budapest 

barron 
complexity regularization application artificial neural networks 
ed nonparametric functional estimation related topics 
dordrecht netherlands kluwer academic publishers 

bartlett 
sample complexity pattern classification neural networks size weights important size network 
technical report australian national university 
caruana 
generalization vs net size 
denver neural information processing systems tutorial 
crane pearson 
characterizing neural network error surfaces sequential quadratic programming algorithm 
machines learn 
giles lawrence 
presenting analyzing results ai experiments data averaging data snooping 
proceedings fourteenth national conference artificial intelligence aaai 
menlo park california aaai press 

haykin 
neural networks comprehensive foundation 
new york ny macmillan 
hornik stinchcombe white 
multilayer feedforward networks universal approximators 
neural networks 
krogh hertz 
simple weight decay improve generalization 
moody hanson lippmann eds advances neural information processing systems volume 
san mateo ca morgan kaufmann 

van der 
neural networks 
university amsterdam fifth edition 
lawrence giles tsoi 
size neural network gives optimal generalization 
convergence properties backpropagation 
technical report umiacs tr cs tr institute advanced computer studies university maryland college park md 
moody 
effective number parameters analysis generalization regularization nonlinear learning systems 
moody hanson lippmann eds advances neural information processing systems volume 
san mateo ca morgan kaufmann 

ripley 
statistical ideas selecting network architectures 
invited presentation neural information processing systems 
rumelhart hinton williams 
learning internal representations error propagation 
rumelhart mcclelland eds parallel distributed processing volume 
cambridge mit press 
chapter 
tukey 
exploratory data analysis 
reading ma addison wesley 
vapnik 
nature statistical learning theory 
springer 
weigend rumelhart huberman 
generalization application forecasting 
lippmann moody touretzky eds advances neural information processing systems volume 
san mateo ca morgan kaufmann 

yu 

backpropagation error surface local minima 
ieee transactions neural networks 
