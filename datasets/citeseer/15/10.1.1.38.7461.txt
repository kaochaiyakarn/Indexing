reinforcement learning continuous time space kenji doya atr human information processing research laboratories seika soraku kyoto japan january appear neural computation presents reinforcement learning framework continuoustime dynamical systems priori discretization time state action 
hamilton jacobi bellman hjb equation discounted reward problems derive algorithms estimating value functions improving policies function approximators 
process value function estimation formulated minimization continuous time form temporal difference td error 
update methods backward euler approximation exponential eligibility traces derived correspondences conventional residual gradient td td algorithms shown 
policy improvement methods continuous actor critic method value gradient greedy policy formulated 
special case nonlinear feedback control law value gradient model input gain derived 
advantage updating model free algorithm derived previously formulated hjb framework 
performance proposed algorithms tested nonlinear control task swinging pendulum limited torque 
shown simulations task accomplished continuous actor critic method number trials times fewer conventional discrete actor critic method continuous policy update methods value gradient policy known learned current address kawato dynamic brain project japan science technology 
seika soraku kyoto japan 
phone 
fax 
mail doya atr jp dynamic model performs times better actor critic method value function update exponential eligibility traces efficient stable euler approximation 
algorithms tested higher dimensional task cart pole swing 
task accomplished trials value gradient policy learned dynamic model 
temporal difference td family reinforcement learning rl algorithms barto sutton sutton barto provides effective approach control decision problems optimal solutions analytically unavailable difficult obtain 
number successful applications large scale problems board games tesauro dispatch problems crites barto zhang dietterich singh bertsekas robot navigation mataric reported see kaelbling 
sutton barto review 
progress rl research far constrained discrete formulation problem discrete actions taken discrete time steps observation discrete state system 
interesting real world control tasks driving car riding require smooth continuous actions taken response high dimensional real valued sensory input 
applications rl continuous problems common approach discretize time state action apply rl algorithm discrete stochastic system 
discretization approach drawbacks 
coarse discretization control output smooth resulting poor performance 

fine discretization number states number iteration steps huge necessitates large memory storage learning trials 

order keep number states manageable elaborate partitioning variables prior knowledge 
efforts eliminate difficulties appropriate function approximators gordon sutton tsitsiklis van roy adaptive state partitioning aggregation methods moore singh asada multiple time scale methods sutton 
consider alternative approach learning algorithms formulated continuous time dynamical systems resorting explicit discretization time state action 
continuous framework possible advantages 
smooth control performance achieved 

efficient control policy derived gradient value function werbos 

need guess partition state action time task function approximation numerical integration algorithms find right granularity 
attempts extending rl algorithms continuous cases 
bradtke showed convergence results learning algorithms discrete time continuous state systems linear dynamics quadratic costs 
bradtke duff derived td algorithm continuous time discrete state systems semi markov decision problems 
baird proposed advantage updating method extending qlearning continuous time continuous state problems 
consider optimization problems continuous time systems bellman hjb equation continuous time counterpart bellman equation discrete time systems provides sound theoretical basis see bertsekas fleming 
methods learning optimal value function satisfies hjb equation studied grid discretization space time peterson convergence proofs shown sizes taken zero bourgine 
direct implementation methods impractical high dimensional state space 
hjb method uses function approximators dayan singh proposed learning gradients value function learning value function method applicable non discounted reward problems 
presents set rl algorithms nonlinear dynamical systems hamilton jacobi bellman equation infinite horizon discounted reward problems 
series simulations devised evaluate effectiveness continuous function approximators 
consider methods learning value function basis minimizing continuous time form td error 
update algorithms derived single step exponentially weighed eligibility traces 
relationships algorithms residual gradient baird td td algorithms sutton discrete cases shown 
formulate methods improving policy value function continuous actor critic method value gradient policy 
specifically model available input gain system dynamics derive closed form feedback policy suitable real time implementation 
relationship advantage updating baird discussed 
performance proposed methods evaluated nonlinear control tasks swinging pendulum limited torque atkeson doya normalized gaussian basis function networks representing value function policy model 
test performance discrete actor critic continuous actor critic value gradient methods performance value function update methods effects learning parameters including action cost exploration noise landscape reward function 
test algorithms challenging task cart pole swing doya state space higher dimensional system input gain state dependent 
optimal value function discounted reward task consider continuous time deterministic system ae state ae action control input 
denote immediate reward state action goal find policy control law maximizes cumulative rewards ds initial state 
note follow system dynamics policy 
called value function state time constant discounting rewards 
important feature infinite horizon formulation value function optimal policy depend explicitly time convenient estimating function approximators 
discounted reward unnecessary assume state attracted zero reward state 
value function optimal policy defined max ds denotes time course 
principle optimality condition optimal value function time max discounted version hamilton jacobi bellman equation see appendix 
optimal policy action maximizes right hand side hjb equation arg max reinforcement learning formulated process bringing current policy value function estimate closer optimal policy optimal value function generally involves components 
estimate value function current policy 
improve policy making greedy respect current estimate value function consider algorithms processes sections 
learning value function learning value function continuous state space mandatory form function approximator 
denote current estimate value function parameter function approximator simply 
framework td learning estimate value function updated self consistency condition local time space 
differentiating definition note hold policy including optimal policy 
possible updates value function estimate instantaneous td error ffi 
positive td error corrected increase decrease time derivative exponentially weighted increase 
current estimate value function perfect satisfy consistency condition 
condition satisfied prediction adjusted decrease inconsistency ffi continuous time counterpart td error barto sutton 
updating level slope order bring td error zero tune level value function time derivative illustrated 
consider objective function baird jffi definition chain rule gradient objective function respect parameter ffi ffi gradient descent algorithm jffi learning rate 
potential problem update algorithm symmetry time 
boundary condition value function appropriate update past estimates affecting estimates 
consider methods implementing back td errors 
backward euler differentiation residual gradient td way implementing back td errors backward euler approximation time derivative 
substituting ffi gradient squared td error respect parameter ffi straightforward gradient descent algorithm jffi alternative way update explicitly changing jffi euler discretized td error coincides conventional td error ffi discount factor fl rescaling values 
update schemes correspond residual gradient baird harmon td algorithms respectively 
note time step euler differentiation equal control cycle physical system 
exponential eligibility trace td consider instantaneous td error corrected change value function time 
suppose impulse reward ffi time definition corresponding temporal profile value function value function linear respect reward desired correction value function instantaneous td error ffi ffi illustrated 
update ffi dt jffi dt consider exponentially weighted integral derivatives eligibility trace parameter class learning algorithms derived jffi time constant eligibility trace 
discretize time step coincides eligibility trace update td fle improving policy consider ways improving policy associated value function 
way stochastically improve policy actor critic method td error effective reinforcement signal 
way take greedy policy respect current value function arg max knowledge reward system dynamics 
continuous actor critic derive continuous version actor critic method barto 
comparing see td error maximized greedy action 
accordingly actor critic method td error reinforcement signal policy improvement 
consider policy implemented actor oen function approximator parameter vector noise monotonically increasing output function 
parameters updated stochastic real valued srv unit algorithm gullapalli ffi value gradient policy discrete problems greedy policy ply search action maximizes sum immediate reward value state 
continuous case right hand side minimized continuous set actions instant general computationally expensive 
reinforcement convex respect action system dynamics linear respect action optimization problem unique solution derive closed form expression greedy policy 
assume reward separated parts reward state environment unknown cost action chosen part learning strategy 
specifically consider case cost function action variable case condition greedy action th column vector input gain matrix system dynamics 
assume input gain dependent system linear respect input action cost function convex 
equation unique solution monotonic function 
accordingly greedy policy represented vector notation represents steepest ascent direction value function transformed transpose model direction action space actual amplitude action determined gain function 
note gradient calculated back propagation value function represented multi layer network 
assumption linearity respect input valid newtonian mechanical systems acceleration proportional force gain matrix calculated inertia matrix 
dynamics linear reward quadratic value function quadratic coincides optimal feedback law linear quadratic regulator lqr see bertsekas 
feedback control sigmoid output function common constraint control tasks amplitude action force torque bounded 
constraint incorporated policy appropriate choice action cost 
suppose amplitude action limited ju max 
define action cost max du sigmoid function saturates 
case greedy feedback policy results feedback control sigmoid output function max limit policy bang bang control law max sign advantage updating model dynamics available learning watkins select greedy action directly learning term maximized hjb equation idea implemented advantage updating method baird harmon value function advantage function updated 
optimal advantage function represented current hjb formulation takes maximum value zero optimal action advantage function updated max max ffi constraint max 
main difference advantage updating value gradient policy described value advantage updated value model updated derivatives 
input gain model known easy learn closed form policy approach advantageous simplifies process maximizing right hand side hjb equation 
simulations tested performance continuous rl algorithms nonlinear control tasks pendulum swing task cart pole swing task 
tasks compared performance control schemes 
actor critic control learning 

value gradient policy exact gain matrix 

value gradient policy concurrent learning input gain matrix 
value functions updated exponential eligibility trace experiments 
value policy functions implemented normalized gaussian networks described appendix sigmoid output function arctan hopfield 
order promote exploration incorporated noise term oen policies see equations appendix 
low pass filtered noise denotes normal gaussian noise 
size perturbation oe tapered performance improved gullapalli 
took modulation scheme oe oe min max ii minimal maximal levels expected reward 
physical systems simulated fourth order runge kutta method learning dynamics simulated euler method time step sec 
pendulum swing limited torque tested continuous time rl algorithms task pendulum swinging upwards limited torque atkeson doya 
control degree freedom system non trivial maximal output torque max smaller maximal load torque 
controller swing pendulum times build momentum decelerate pendulum early prevent pendulum falling 
reward height tip pendulum cos 
policy value functions implemented normalized gaussian networks basis functions cover dimensional state space 
modeling system dynamics bases state action space 
trial started initial state selected randomly 
trial lasted seconds pendulum rotated mg control pendulum limited torque 
dynamics ml 
sin physical parameters max 
learning parameters oe simulations specified 

failure trial terminated reward second 
measure swing performance defined time pendulum stayed trial regarded successful seconds 
number trials achieving successful trials measure learning speed 
illustrates landscape value function typical trajectory value gradient policy 
trajectory starts bottom basin corresponds pendulum hanging spirals hill ridge value function reaches peak corresponds pendulum standing upright 
actor critic value gradient physical model compared performance continuous rl algorithms discrete actor critic algorithm barto 
shows time course learning simulation runs shows average number trials needed successful swing ups 
discrete actor critic algorithm took times trials continuous actor critic 
note continuous algorithms simulated time step discrete algorithm 
consequently performance difference due better spatial generalization normalized gaussian networks 
landscape value function pendulum swing task 
white line shows example swing trajectory 
state space cylinder connected 
centers normalized gaussian basis functions located uniform grid covers area 
trials trials trials trials comparison time course learning different control schemes discrete actor critic continuous actor critic value gradient policy exact model value gradient policy learned model note different scales 
time pendulum stayed 
discrete actor critic state space evenly discretized boxes action binary max 
learning parameters fl 
trials comparison learning speeds discrete continuous actor critic policies exact learned physical models 
ordinate number trials successful swing ups 
continuous algorithms performed basis functions discrete algorithm achieve task grids 
result shown obtained grid discretization state 
continuous algorithms learning fastest value gradient policy exact input gain 
concurrent learning input gain model resulted slower learning 
actor critic slowest 
due effective exploitation value function gradient policy compared stochastically improved policy actor critic 
methods value function update compared methods value function update algorithms greedy policy exact gain model 
algorithms attained comparable performances optimal settings method exponential eligibility trace performed widest range time constant learning rate 
tested purely symmetric update method performance unstable learning rates value gradient tuned carefully 
trials dt trials dt trials comparison different value function update methods different settings time constants 
residual gradient eq 

single step eligibility trace eq 

exponential eligibility eq 

learning rate roughly optimized method setting time step euler approximation time constant eligibility trace 
performance unstable methods 

trials control cost coef 
cosq cosq trials effects parameters policy 
control cost coefficient 
reward function perturbation size oe action cost graded reward exploration tested performance depended action cost shape reward function size exploration noise oe compares performance different action costs 
learning slower large cost torque weak output early stage 
results bang bang control tended consistent sigmoid control small costs 
summarizes effects reward function exploration noise 
binary reward function cos task difficult learn 
better perfor mance observed negative binary reward function 
difference drastic fixed initial state noise oe success achieved positive binary reward 
better performance negative reward due initialization value function 
value function near learned value gradient policy drives state unexplored areas assigned higher values default 
cart pole swing task tested learning schemes challenging task cart pole swing strongly nonlinear extension common cart pole balancing task barto 
physical parameters cart pole barto pole arbitrary angle balanced 
marked differences previous task dimension state space higher input gain state dependent 
state vector position velocity cart 
value policy functions implemented normalized gaussian networks bases 
basis network modeling system dynamics 
reward cos cart bumped track pole rotated terminal reward second 
trial lasted seconds 
illustrates control performance learning trials greedy policy value gradient learned input gain model 
shows value function state space 
squares represents subspace different values 
see shaped ridge value function similar seen pendulum swing task 
note lower values positive negative signal danger bumping track 
shows critical components input gain vector gain represents force applied cart transformed torque pole 
gain model successfully capture change sign upward orientation downward orientation pole 
comparison number trials necessary successful swing ups 
value gradient greedy policies performed times faster 
performances exact learned input gains comparable examples cart pole swing trajectories 
arrows indicate initial position pole 
typical swing bottom position 
small perturbation 
cart moves right keeps pole upright 
larger perturbation cart initially tries keep pole upright brakes avoid collision track swings pole left side 
learning parameters oe 
df du landscape value function cart pole swing task 
learned gain model trials comparison number trials successful swing ups value gradient policy exact learned physical models 
case 
learning physical model relatively easy compared learning value function 
discussion results simulations summarized follows 
swing task accomplished continuous actor critic number trials times fewer conventional discrete actor critic figures 
continuous methods value gradient policy known learned dynamic model performed significantly better actor critic figures 
value function update methods exponential eligibility traces efficient stable methods euler approximation 
reward related parameters landscape baseline level reward function greatly affect speed learning 
value gradient method worked input gain state dependent figures major rl methods actor critic learning modelbased look ahead learning extended continuous time cases advantage updating baird 
presents continuous time counterparts methods hjb equation provides complete repertoire continuous rl methods 
major contribution derivation closed form policy value gradient dynamic model 
critical issue advantage updating need finding maximum advantage function control cycle computationally expensive special cases linear quadratic problems harmon 
illustrated simulation value gradient policy applied broad class physical control problems priori learned models system dynamics 
usefulness value gradients rl considered werbos discretetime cases 
value gradients proposed dayan singh motivation eliminate need updating value function advantage updating 
method value gradients updated updating value function applicable non discounted problems 
system policy stochastic hjb equation include second order partial derivatives value function max tr covariance matrix system noise see fleming 
simulations methods deterministic hjb equation worked incorporated noise terms policies promote exploration 
reason noise small contribution second order term minor 
reason second order term smoothing effect value function implicitly achieved smooth function approximator 
point needs investigation 
convergent properties hjb rl algorithms shown deterministic stochastic bourgine cases grid discretization space time 
convergent properties continuous rl algorithms combined function approximators remain studied 
continuous rl algorithm numerically implemented finite time step shown sections equivalent discrete time td algorithm convergent properties shown function approximators gordon tsitsiklis van roy 
example convergence td algorithms shown linear function approximator line sampling tsitsiklis van roy case simulations 
result considers value function approximation policy guarantee convergence entire rl process satisfactory solution 
example swing tasks learning got stuck locally optimal solution endless rotation pendulum penalty rotation 
fixed smooth basis functions limitation steep cliffs value policy functions achieved 
despite negative didactic examples tsitsiklis van roy methods dynamically allocate reshape basis functions successfully continuous rl algorithms example swing task schaal stand task link robot morimoto doya 
elucidation conditions proposed continuous rl algorithms successfully example properties function approximators methods exploration remains subject empirical theoretical studies 
acknowledgments kawato stefan schaal chris atkeson helpful discussions 
appendix hjb equation discounted reward optimality principle divide integral parts solve short term optimization problem max ds small term approximated second term taylor expanded substituting collecting left hand side optimality condition max dividing sides zero condition optimal value function max appendix normalized gaussian network value function represented jjs jj vectors define center size th basis function 
note basis functions located ends grids extended sigmoid functions effect normalization 
current simulations centers fixed grid analogous boxes approach barto discrete rl 
grid allocation basis functions enables efficient calculation activation outer product activation vectors individual input variables 
actor critic method policy implemented max oen component wise sigmoid function noise 
value gradient methods policy max oen implement input gain model network trained predict time derivative state weights updated input gain system dynamics fi fi fi fi fi asada noda hosoda 

action sensor space categorization robot learning 
proceedings ieee rsj international conference intelligent robots systems pages 
atkeson 

local trajectory optimizers speed global optimization dynamic programming 
cowan tesauro alspector editors advances neural information processing system volume pages 
morgan kaufmann san mateo ca usa 
baird 

advantage updating 
technical report wl tr wright laboratory wright patterson air force base oh usa 
baird 

residual algorithms reinforcement learning function approximation 
prieditis russel editors machine learning proceedings twelfth international conference san francisco 
morgan kaufmann 
barto sutton anderson 

neuronlike adaptive elements solve difficult learning control problems 
ieee transactions systems man cybernetics 
bradtke 

reinforcement learning applied linear quadratic regulation 
giles hanson cowan editors advances neural information processing systems pages 
morgan kaufmann san mateo ca usa 
bradtke duff 

reinforcement learning methods continuoustime markov decision problems 
tesauro touretzky leen editors advances neural information processing systems pages 
mit press cambridge ma usa 
crites barto 

improving elevator performance reinforcement learning 
touretzky mozer hasselmo editors advances neural information processing systems pages 
mit press cambridge ma 
dayan singh 

improving policies measuring merits 
touretzky mozer hasselmo editors advances neural information processing systems pages 
mit press cambridge ma usa 
doya 

temporal difference learning continuous time space 
touretzky mozer hasselmo editors advances neural information processing systems pages 
mit press 
doya 

efficient nonlinear control actor tutor architecture 
mozer jordan editor advances neural information processing systems pages 
mit press 
fleming 

controlled markov processes viscosity solutions 
applications mathematics 
springer verlag new york 
gordon 

stable function approximation dynamic programming 
prieditis russel editors machine learning proceedings twelfth international conference 
morgan kaufmann san francisco 
gordon 

stable fitted reinforcement learning 
touretzky mozer hasselmo editors advances neural information processing systems pages 
mit press cambridge ma usa 
gullapalli 

stochastic reinforcement learning algorithm learning real valued functions 
neural networks 
harmon baird iii klopf 

reinforcement learning applied differential game 
adaptive behavior 
hopfield 

neurons graded response collective computational properties state neurons 
proceedings national academy science 
kaelbling littman moore 

reinforcement learning survey 
journal artificial intelligence research 
mataric 

reward functions accelerated learning 
cohen hirsh editors proceedings th international conference machine learning 
morgan kaufmann 
moore 

parti game algorithm variable resolution reinforcement learning multidimensional state spaces 
cowan tesauro alspector editors advances neural information processing systems pages 
morgan kaufmann san francisco ca usa 
morimoto doya 

reinforcement learning dynamic motor sequence learning stand 
proceedings ieee rsj international conference intelligent robots systems volume pages 


convergent reinforcement learning algorithm continuous case finite difference method 
proceedings international joint conference artificial intelligence pages 
bourgine 

reinforcement learning continuous stochastic control problems 
jordan kearns solla editors advances neural information processing systems pages cambridge ma usa 
mit press 


adaptive choice grid time reinforcement learning 
jordan kearns solla editors advances neural information processing systems pages cambridge ma usa 
mit press 
peterson 

line estimation optimal value function hjb estimators 
giles hanson cowan editors advances neural information processing systems pages 
morgan kaufmann san mateo ca usa 
schaal 

learning demonstration 
mozer jordan petsche editors advances neural information processing systems pages cambridge ma usa 
mit press 
singh bertsekas 

reinforcement learning dynamic channel allocation cellular telephone systems 
mozer jordan petsche editors advances neural information processing systems pages 
mit press cambridge ma usa 
singh jaakkola jordan 

reinforcement learning soft state aggregation 
tesauro touretzky leen editors advances neural information processing systems pages 
mit press cambridge ma usa 
sutton 

learning predict methods temporal difference 
machine learning 
sutton 

td models modeling world mixture time scales 
proceedings th international conference machine learning pages 
morgan kaufmann 
sutton 

generalization reinforcement learning successful examples sparse coarse coding 
touretzky mozer hasselmo editors advances neural information processing systems pages 
mit press cambridge ma usa 
sutton barto 

reinforcement learning 
mit press cambridge ma usa 
tesauro 

td gammon self teaching backgammon program achieves play 
neural computation 
tsitsiklis van roy 

analysis temporal difference learning function approximation 
mozer jordan petsche editors advances neural information processing systems pages cambridge ma usa 
mit press 
watkins 

learning delayed rewards 
phd thesis cambridge university 
werbos 

menu designs reinforcement learning time 
miller sutton werbos editors neural networks control pages 
mit press cambridge ma 
zhang dietterich 

high performance job shop scheduling td network 
touretzky mozer hasselmo editors advances neural information processing systems 
mit press cambridge ma usa 

