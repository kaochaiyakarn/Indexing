approximate statistical tests comparing supervised classification learning algorithms thomas dietterich tgd cs orst edu department computer science oregon state university corvallis december reviews approximate statistical tests determining learning algorithm performs particular learning task 
tests compared experimentally determine probability incorrectly detecting difference difference exists type error 
widely statistical tests shown high probability type error certain situations 
tests test difference proportions paired differences test random train test splits 
third test paired differences test fold cross validation exhibits somewhat elevated probability type error 
fourth test mcnemar test shown low type error 
fifth test new test cv iterations fold cross validation 
experiments show test acceptable type error 
measures power ability detect algorithm differences exist tests 
cross validated test powerful 
cv test shown slightly powerful mcnemar test 
choice best test determined computational cost running learning algorithm 
algorithms executed mcnemar test test acceptable type error 
algorithms executed times cv test recommended slightly powerful directly measures variation due choice training set 
research development application machine learning algorithms classification tasks questions arise statistical methods needed 
purpose investigate questions demonstrate existing statistical methods inadequate question propose new statistical test shows acceptable performance initial experiments 
understand question raised helpful consider taxonomy different kinds statistical questions arise machine learning 
gives taxonomy statistical questions 
root tree 
issue consider studying single application domain studying multiple domains 
applied research single domain interest goal find best classifier best learning algorithm apply domain 
fundamental goal research machine learning find learning algorithms wide range application domains 
return issue moment consider single domain case 
single domain different sets questions depending analyzing classifiers algorithms 
classifier function input example assigns example classes 
learning algorithm function set examples classes constructs classifier 
particular application setting primary goal usually find best classifier estimate accuracy examples 
example suppose working medical instrumentation wish manufacture sell instrument classifying blood cells 
time designing instrument gather large collection blood cells human expert classify cell 
apply learning algorithm produce classifier set classified cells 
classifier implemented instrument sold 
want instrument contain accurate classifier find 
applications select best learning algorithm finding best classifier 
example suppose want sell electronic mail system learns recognize filter junk mail 
user receives email message consider junk mail flag message 
periodically learning algorithm included program analyze accumulated examples junk non junk email update filtering rules 
job determine learning algorithm include program 
level taxonomy distinguishes fundamental tasks estimating accuracy choosing classifiers algorithms 
market blood cell diagnosis system claim accuracy 
measure accuracy 
course design system want choose best classifier set available classifiers 
lowest level taxonomy concerns amount data available 
large amount data set aside serve test set evaluating classifiers 
simpler statistical methods applied case 
situations amount data limited need input learning algorithms 
means form resampling cross validation bootstrap perform statistical analysis 
reviewed general structure taxonomy consider statistical questions 
assume data points examples drawn independently fixed probability distribution defined particular application problem 
question suppose large sample data classifier classifier may small sample large sample small sample large sample small sample large sample small sample large sample predict choose analyze algorithms single domain multiple domains predict choose analyze classifiers classifier accuracy classifiers algorithm accuracy algorithms taxonomy statistical questions machine learning 
boxed node question subject 
constructed part data data remaining separate test set 
measure accuracy test set construct binomial confidence interval cochran efron tibshirani kohavi 
note question classifier produced method interviewing expert need produced learning algorithm 
question small data set suppose apply learning algorithm construct classifier ca accurately ca classify new examples 
separate test set direct way answer question 
frequently applied strategy convert question question predict accuracy algorithm trained randomly selected data sets approximately size 
predict accuracy ca obtained training question classifiers ca cb data separate test set determine classifier accurate new test examples 
question answered measuring accuracy classifier separate test set applying mcnemar test described 
question classifiers ca cb produced feeding small data set learning algorithms classifier accurate classifying new examples 
separate set test data answer question directly 
researchers taken approach converting problem question learning algorithms question 
determine algorithm usually produces accurate classifiers trained data sets approximately size select classifier ca cb created algorithm 
question learning algorithm large data set accuracy classifiers produced trained new training sets specified size 
question received attention literature 
approach advocated delve project hinton neal tibshirani delve team members rasmussen subdivide test set disjoint training sets size trained training sets resulting classifiers tested test set 
average performance test set estimates accuracy new runs 
question learning algorithm small data set accuracy classifiers produced trained new training sets size 
kohavi shows stratified fold cross validation produces fairly estimates case 
note resampling approach train training sets exactly size train data sets slightly fewer examples size fold cross validation rely assumption performance learning algorithms changes smoothly changes size training data 
assumption checked experimentally performing additional cross validation studies smaller training sets checked directly training sets size results shape learning curves show cases smoothness assumption violated haussler kearns seung tishby 
observed hold experimentally applications 
question learning algorithms large data set algorithm produce accurate classifiers trained data sets specified size 
question received attention delve team studied question regression problems 
divide disjoint training sets single test set 
algorithm trained training set resulting classifiers tested test set 
analysis variance performed includes terms choice learning algorithm choice training set individual test example 
quasi test applied determine effect due choice learning algorithms significantly non zero 
question learning algorithms small data set algorithm produce accurate classifiers trained data sets size 
purpose describe compare statistical tests answering question 
small necessary holdout resampling methods 
mentioned question means answer question exactly making assumption performance learning algorithms changes smoothly changes size training set 
specifically need assume relative difference performance algorithms changes slowly changes size training set 
question learning algorithms data sets domains algorithm produce accurate classifiers trained examples new domains 
fundamental difficult question machine learning 
researchers applied simple sign test wilcoxon signed ranks test try answer question single runs cross validation estimates tests take account uncertainty individual comparisons 
effectively want combine results answers question answer associated uncertainty 
important question research 
questions important experimental research learning algorithms 
develops new learning algorithm modification existing algorithm answers questions determine new algorithm better existing algorithms 
unfortunately data sets experimental research small allow posing question 
focuses developed statistical tests question 
define compare statistical tests question 
proceeding derivation statistical tests worth noting questions posed extended classification algorithms misclassification rates 
example decision making settings important estimate conditional probability new example belongs classes 
measure accuracy probability estimates log loss questions rephrased terms determining expected log loss classifier algorithm 
similarly questions rephrased terms determining classifier algorithm smaller log loss 
unaware statistical research specifically addressing questions case log loss 
neural network applications task predict continuous response variable 
problems squared error usually natural loss function questions rephrased terms determining expected mean squared error predictor algorithm 
similarly questions rephrased terms determining predictor algorithm smaller mean squared error 
question addressed constructing confidence interval normal distribution depending size test set 
question addressed constructing confidence interval expected difference 
mentioned delve project developed analysis variance techniques questions 
appropriate statistical tests small sample questions established 
statistical tests regression methods may suggest ways designing statistical tests log loss case 
important area research 
design evaluate statistical tests step identify sources variation controlled test 
case considering important sources variation 
random variation selection test data evaluate learning algorithms 
particular randomly drawn test data set classifier may outperform population classifiers perform identically 
particularly pressing problem small test data sets 
second source random variation results selection training data 
particular randomly drawn training set algorithm may perform average algorithms accuracy 
small changes training set adding deleting data points may cause large changes classifier produced learning algorithm 
breiman called behavior instability shown serious problem decision tree algorithms cart breiman friedman olshen stone 
third source variance internal randomness learning algorithm 
consider example widely backpropagation algorithm training feed forward neural networks 
algorithm usually initialized set random weights improves 
resulting learned network depends critically random starting state kolen pollack 
case training data changed algorithm produce different hypothesis executed different random starting state 
source random variation handled statistical tests random classification error 
fixed fraction test data points randomly mislabeled learning algorithm achieve error rate statistical test fooled sources variation 
test conclude algorithms different percentage correct classifications different average trained training set fixed size tested data points population 
accomplish statistical testing procedure account sources variation 
account test data variation possibility random classification error statistical procedure consider size test set consequences changes test set 
account training data variation internal randomness statistical procedure execute learning algorithm multiple times measure variation accuracy resulting classifiers 
begins describing statistical tests bearing question mcnemar test test difference proportions resampled test cross validated test new test called cv test 
describes simulation study seeks measure probability test incorrectly detect difference difference exists type error 
results simulation study show mcnemar test cross validated test cv test acceptable type error 
type error resampled test bad test expensive computationally consider 
type error difference proportions test unacceptable cases cheap evaluate retained study 
simulation study somewhat idealized address aspects training data variation 
obtain realistic evaluation remaining tests conducted set experiments real learning algorithms realistic data sets 
measured type error power tests 
results show cross validated test consistently elevated type error 
difference proportions test acceptable type error low power 
remaining tests type error reasonable power 
cv test slightly powerful mcnemar test times expensive perform 
concludes cv test test choice inexpensive learning algorithms mcnemar test better expensive algorithms 
lisp code implementing tests available author 
formal preliminaries assume exists set possible data points called population 
exists target function classifies classes 
loss generality assume results depend assumption concern example classified correctly incorrectly 
application setting sample drawn randomly fixed probability distribution collection training examples constructed labeling 
training example form hx applications may source classification noise randomly sets label incorrect value 
learning algorithm takes input set training examples outputs classifier true error rate classifier probability misclassify example drawn randomly practice error rate estimated available sample subdividing training set test set error rate provides estimate true error rate population null hypothesis tested randomly drawn training set fixed size learning algorithms error rate test example randomly drawn random draws distribution classifier output algorithm trained training set classifier output algorithm trained null hypothesis written notation indicates probability taken respect random draws training set test example statistical tests describe statistical tests main subject 
simple holdout tests consider tests resampling available data 
mcnemar test apply mcnemar test everitt divide available sample data training set test set train algorithms training set yielding classifiers test classifiers test set 
example record classified construct contingency table number examples number examples misclassified misclassified number examples number examples misclassified misclassified notation total number examples test set null hypothesis algorithms error rate means mcnemar test test goodness fit compares distribution counts expected null hypothesis observed counts 
expected counts null hypothesis statistic distributed approximately degree freedom incorporates continuity correction term gamma numerator account fact statistic discrete distribution continuous jn gamma gamma null hypothesis correct probability quantity greater 
may reject null hypothesis favor hypothesis algorithms different performance trained particular training set note test shortcomings regard question 
directly measure variability due choice training set internal randomness learning algorithm 
single training set chosen algorithms compared training set 
mcnemar test applied believe sources variability small 
second directly compare performance algorithms training sets size jsj sets size jrj substantially smaller jsj ensure sufficiently large test set 
assume relative difference observed training sets size jrj hold training sets size jsj 
test difference proportions second simple statistical test measuring difference error rate algorithm error rate algorithm cochran 
specifically proportion test examples incorrectly classified algorithm proportion test examples incorrectly classified algorithm assumption underlying statistical test algorithm classifies example test set probability misclassification number misclassifications test examples binomial random variable mean npa variance gamma binomial distribution approximated normal distribution reasonable values furthermore difference independent normally distributed random variables normally distributed 
quantity gamma viewed normally distributed assume measured error rates independent 
null hypothesis mean zero standard error se gamma average error probabilities 
analysis obtain statistic gamma gamma approximately standard normal distribution 
reject null hypothesis jzj sided test probability incorrectly rejecting null hypothesis 
test researchers including author dietterich hild bakiri 
problems test 
measured test set independent 
second test shares drawbacks mcnemar test measure variation due choice training set internal variation learning algorithm directly measure performance algorithms training sets size jsj smaller training set size jrj 
lack independence corrected changing estimate standard error se estimate focuses probability disagreement algorithms cochran 
resulting statistic written jn gamma gamma recognize square root statistic mcnemar test 
experimentally analyzed uncorrected statistic statistic current wanted determine badly incorrect independence assumption affects accuracy test 
small sample sizes exact versions mcnemar test test difference proportions avoid normal approximations 
resampled paired test statistical test consider currently popular machine learning literature 
series usually trials conducted 
trial available sample randomly divided training set specified size typically thirds data test set learning algorithms trained resulting classifiers tested respectively observed proportion test examples misclassified algorithm respectively trial assume differences gamma drawn independently normal distribution apply student test computing statistic delta gammap gamma null hypothesis statistic distribution gamma degrees freedom 
trials null hypothesis rejected jtj 
potential drawbacks approach 
observed individual differences normal distribution independent 
second independent test sets trials overlap training sets trials overlap 
see violations assumptions underlying test cause severe problems test unsafe 
fold cross validated paired test test identical previous constructing pair training test sets randomly dividing randomly divide disjoint sets equal size 
conduct trials 
trial test set training set union statistic computed 
advantage approach test set independent 
test suffers problem training sets overlap 
fold cross validation pair training sets shares examples 
overlap may prevent statistical test obtaining estimate amount variation observed training set completely independent previous training sets 
illustrate point consider nearest neighbor algorithm 
suppose training set contains clusters points large cluster belonging class small cluster belonging class 
perform fold cross validation subdivide training data disjoint sets 
points smaller cluster go sets runs nearest neighbor algorithm elevated error rates small cluster test set point misclassified 
small cluster training set points may incorrectly treated nearest neighbors test set points increases error rate 
conversely small cluster evenly divided sets error rates improve test point corresponding nearby training point provide correct classification 
way see performance folds cross validation correlated independent 
verified experimentally fold cross validation letter recognition task total training examples experiment null hypothesis true described 
measured correlation coefficient differences error rates folds cross validation observed value test significantly different gamma hand error rates drawn independent fold cross validations independent data sets correlation coefficient gamma test significantly different zero 
cv paired test initial experiments fold cross validated paired test attempted determine statistic large cases 
numerator statistic estimates mean difference performance algorithms folds denominator estimates variance differences 
synthetic data constructed non overlapping training sets measured mean variance training sets 
variance slightly underestimated training sets overlapped means occasionally poorly estimated cause large values 
problem traced correlations different folds described 
replaced numerator statistic observed difference single fold fold cross validation statistic behaved 
led cv paired test 
test perform replications fold cross validation 
replication available data randomly partitioned equal sized sets learning algorithm trained set tested set 
produces error estimates trained tested trained tested 
subtracting corresponding error estimates gives estimated differences gamma gamma differences estimated variance gamma gamma 
variance computed th replication replications 
define statistic call cv statistic 
claim null hypothesis approximately distribution degrees freedom 
argument goes follows 
standard normal random variable random variable gamma degrees freedom 
definition quantity gamma distribution gamma degrees freedom independent 
usual statistic derived starting set random variables having normal distribution mean variance oe sample mean gamma sum squared deviations mean 
define gamma oe oe known results probability theory state standard normal distribution distribution gamma degrees freedom 
remarkable result probability theory independent provided original drawn normal distribution 
plug equation follows gamma gamma oe oe delta gamma gamma gamma gives usual definition statistic 
construct analogy follows 
null hypothesis numerator difference identically distributed proportions safely treat approximately normal random variable zero mean unknown standard deviation oe underlying test set contained points 
oe 
null hypothesis oe distribution degree freedom additional assumption independent 
assumption false seen differences proportions measured opposite folds fold cross validation 
assumption independence probably appropriate fold cross validation fold cross validation fold case training sets completely non overlapping cross validation test sets non overlapping 
chose fold cross validation gives large test sets disjoint training sets 
large test set needed paired difference disjoint training sets help independent 
drawback course learning algorithms trained training sets half size training sets question seek relative performance 
set oe tested experimentally resulting estimate variance noisy zero 
similar situations combining results multiple cross validations help stabilize estimate perform fold cross validations define oe oe assume fold cross validation independent sum independent random variables having distribution degree freedom 
summation property distribution means distribution degrees freedom 
independence assumption false fold cross validation computed training data 
experimental tests showed problematic various independence assumptions underlying cv test 
equation assumption variance estimates independent assumed proved usual distribution derivation observed differences proportions mean observed differences 
discussed mean difference tends overestimate true difference lack independence different folds cross validation 

predicted value quantile qq plot comparing distribution values values distribution degrees freedom 
points fall line distributions matched 
assumptions plug equation obtain summarize assumptions approximations involved derivation 
employ normal approximation binomial distribution 
second assume pairwise independence third assume independence 
assume independence numerator denominator statistic 
way evaluate cv statistic experimentally quantile quantile plot qq plot shown 
qq plot shows computed values cv statistic case null hypothesis known apply exp task described 
generate qq plot values sorted assigned quantiles rank sorted list divided 
inverse cumulative distribution degrees freedom compute quantile value distributed random variable taken rank 
value coordinate original value coordinate 
words observed point ordinal position points compute value points truly drawn distribution 
points distribution degrees freedom lie line shows fairly fit line 
tails distribution somewhat conservative 
choice replications cross validation arbitrary 
exploratory studies showed fewer replications increased risk type error 
possible explanation competing problems 
fewer replications noise measurement troublesome 
replications lack independence troublesome 
best value number replications open question 
completes description statistical tests null hypothesis learning algorithms error rate population sample population 
simulation experiment design turn experimental evaluation methods 
purpose simulation measure probability type error algorithms 
type error occurs null hypothesis true difference learning algorithms learning algorithm rejects null hypothesis 
measure probability type error constructed simulated learning problems 
understand problems useful think abstractly behavior learning algorithms 
simulating behavior learning algorithms consider population data points suppose training set size fixed 
learning algorithm define ffl probability classifier produced trained randomly drawn training set fixed size misclassify ffl correctly classified classifiers produced ffl misclassified 
shows measured values ffl population points respect decision tree algorithm quinlan trained randomly drawn training sets examples 
points sorted ffl values 
ffl values simulate behavior randomly drawn test set points point misclassifying probability ffl 
exactly reproduce behavior assumes misclassification errors independent test example fact classifications data points close tend highly correlated 
simulated procedure average error rate real algorithm exhibit similar degree variation random trial 
simulate learning algorithms various properties defining population points assigning value ffl point 
want learning algorithm high variance assign values ffl near value giving maximum variance binomial random variable 
want learning algorithms error rate population ensure average value ffl population algorithms 
studies wanted construct simulated learning problems provide worstcase statistical tests 
accomplish sought maximize main sources random variation variation resulting choice test data sets variation resulting choice training sets 
ignored issue classification noise 
affects training test data equally incorporated error rate 
ignored internal epsilon data point measured values ffl population data points 
randomness learning algorithms manifest way training set variance causing learning algorithm produce different classifiers 
tests type error designed sets ffl values ffl algorithm ffl algorithm established target error rate ffl chose distinct values ffl ffl ffl ffl 
generated population points 
half population assigned ffl ffl ffl ffl 
remaining half population reversed assigned ffl ffl ffl ffl 
shows configuration ffl values 
size population irrelevant sampling replacement kinds points 
important thing population evenly divided kinds training points 
effect algorithm error rate ffl algorithm total variance 
test example algorithms different error rates 
effect random choice test data sets apparent 
test data set exactly equally divided half population second half population apparent advantage algorithm 
statistical tests need avoid fooled apparent difference error rates 
experimental measurements confirmed choices ffl ffl best job simultaneously maximizing algorithm variance algorithm variation achieving desired error rate ffl 
experiments value ffl investigated ffl ffl ffl 
data point algorithm algorithm designed values ffl ffl error rate ffl 
details simulation statistical test simulation divided series trials 
trial data set size randomly drawn replacement analyzed statistical tests described 
goal measure proportion trials null hypothesis rejected 
tests mcnemar test normal test difference proportions data set randomly divided training set containing thirds test set containing remaining third training set ignored execute learning algorithms 
performance algorithm test set simulated classifying test example randomly value ffl ffl 
random number range drawn 
ffl considered misclassified algorithm second random number drawn considered misclassified algorithm number ffl 
results classifications processed appropriate statistical test 
tests performed sided tests confidence level 
resampled paired test process randomly splitting data set thirds repeated times 
test set classified described previous paragraph 
differences error rates algorithms collected employed test 
fold cross validated paired test data set divided random subsets equal size 
test sets classified algorithms random procedure described 
classification process simulate random variation quality training set generated random value fi range gamma mcnemar prop test resampled cv probability type error probability type error statistical test 
adjacent bars test represent probability type error ffl 
error bars show confidence intervals probabilities 
horizontal line shows target probability 
added value fi ffl ffl generating classifications 
results classifications collected subjected test 
cv test replications fold cross validation performed statistic constructed described 
important note experiment simulate training set variance 
particular model effect overlapping training sets behavior cross validated test cv test 
correct shortcoming experiments real learning algorithms 
results shows probability making type error procedures data set contains examples error rate ffl varied 
tests probability type error exceeds target value difference proportions test resampled test 
remaining tests acceptable probability making type error simulation 
resampled test higher probability type error tests 
results fact randomly drawn data set contain imbalance points half population compared second half population 
resampled test detect magnify difference statistically significant 
probability making type error test increased increasing number resampled training test splits 
shows effect various numbers resampled splits probability type error number replications resampled probability type error resampled test fold cross validated test number resampling replications varied 
error rate ffl 
resampled test cross validated test 
notice cross validated test exhibit problem 
difference proportions test suffers essentially problem 
sample unrepresentative measured difference proportions large especially ffl near 
interesting mcnemar test share problem 
key difference difference proportions test looks difference proportions absolute values 
consider theta contingency tables tables difference error rates difference proportions test treats identically rejects null hypothesis 
mcnemar test finds significant difference left table finds extremely significant difference right table 
left table mcnemar test asks question probability tosses fair coin receive heads tails 
right table asks question probability tosses fair coin receive heads tails 
way constructed simulated learning problems produce tables left especially ffl near 
note corrected version difference proportions test suffered problem 
poor behavior high cost resampled test excluded experiments 
biggest drawback simulation capture measure training set variance variance resulting internal behavior learning algorithm 
address problems conducted second set experiments real learning algorithms real data sets 
experiments realistic data methods evaluate type error rates statistical tests real learning algorithms needed find learning algorithms identical performance trained training sets size 
needed learning algorithms efficient experiments replicated times 
achieve chose release quinlan nearest neighbor nn algorithm dasarathy 
selected difficult problems exp problem developed kong letter recognition data set frey slate pima indians diabetes task merz murphy 
course nn performance data sets 
exp letter recognition nn performs better reverse true pima data set 
step damage learning algorithms performance identical 
exp letter recognition tasks modified distance metric employed nn weighted euclidean distance bad weights 
allowed reduce performance nn matched data sets 
equalize performance algorithms pima data set modified classifying new instances random classification errors specified rate 
precisely data set processed follows 
exp generated calibration set examples spaced uniform grid resolution 
generated data sets size simulate separate trials 
data sets randomly drew subsets size 
sizes chosen sizes training sets fold cross validated test mcnemar difference tests cv test respectively tests initial data set examples 
size training set adjusted distance metric nn average performance data sets measured calibration examples matched average performance 
letter recognition randomly subdivided examples calibration set experimental set 
drew data sets size randomly experimental set examples 
data sets drew random subsets size 
size training set adjusted distance metric nn average performance data sets measured calibration examples matched average performance 
pima indians diabetes data set drew data sets size available examples 
data sets remaining examples retained calibration 
data sets size subsampled produce random subsets size 
size training set measured average error rate nn data sets tested calibration examples corresponding data set 
adjusted random noise rate average error rates identical 
mcnemar prop test cv probability type error type error rates statistical tests 
bars test correspond exp letter recognition pima data sets 
error bars confidence intervals true type error rate 
type error results shows measured type error rates statistical tests 
fold test test type error exceeds 
tests difference proportions test prop test show acceptable type error rates 
power measurements type error important consideration choosing statistical test 
important criterion goal confident observed performance difference real 
goal detect difference learning algorithms power statistical test important 
power test probability reject null hypothesis null hypothesis false 
measure power tests distance metric nearest neighbor exp letter recognition tasks random classification error rate pima achieve various differences performance nn 
specifically 
measured performance nn trained examples tested appropriate calibration examples denote error rates ffl ffl nn 
chose various target error rates extremes 
target error rate ffl target computed fraction ffl target ffl gamma ffl gamma ffl nn 
calibrated error rates nn training sets size value applied 
words adjusted learning curve nearest neighbor algorithm damaged distance metric positioned fixed fraction way learning curves error rate training set size nn learning curves interpolated nn purpose measuring power 
nn distance metric 
shows calibrated learning curves letter recognition task various values figures plot power curves statistical tests 
curves show cross validated test powerful tests 
goal confident difference algorithms cross validated test test choice type error unacceptable 
tests acceptable type error cv test powerful 
note performance learning algorithms differs percentage points letter recognition task statistical tests able detect third time 
discussion experiments suggest cv test powerful statistical tests acceptable type error 
test satisfying assesses effect choice training set running learning algorithms different training sets choice test set measuring performance test sets 
despite fact mcnemar test assess effect varying training sets performs 
various experiments saw type error rate mcnemar test exceed target level 
contrast observe cases cv differences proportions tests fooled 
cv test fail cases error rates measured various fold crossvalidation replications vary wildly difference error rates unchanged 
able observe simulated data experiments error rates probability rejecting difference performance nn mcnemar prop test cv power statistical tests exp task 
horizontal axis plots number percentage points algorithms nn differ trained training sets size 

observe experiments realistic data 
wild variations cause bad estimates variance 
advisable check measured error rates applying test 
difference proportions test fail cases learning algorithms different regions poor performance error rates close 
encounter problem experiments realistic data nn different algorithms 
suspect errors committed learning algorithms near decision boundaries 
learning algorithms comparable error rates similar regions poor performance pathology observed simulated data experiments arise practice 
superior performance mcnemar test incorrect assumptions underlying version difference proportions test justification employing uncorrected difference proportions test 
fold cross validated test high type error 
high power recommended cases type ii error failure detect real difference algorithms important 
starting point question learning algorithms small data set algorithm produce accurate classifiers trained data sets size drawn population 
unfortunately statistical tests described evaluated answer question 
statistical tests require probability rejecting difference performance nn mcnemar prop test cv power statistical tests letter recognition task 
horizontal axis plots number percentage points algorithms nn differ trained training sets size 
holdout resampling methods consequence tell relative performance learning algorithms training sets size jrj jsj jrj size training set employed statistical test 
addition fundamental problem statistical tests shortcomings 
derivation cv test requires large number independence assumptions known violated 
mcnemar test difference proportions test measure variation resulting choice training sets internal randomness algorithm measure important sources variation 
cross validated test violates assumptions underlying test training sets overlap 
consequence problems statistical tests described viewed approximate heuristic tests rigorously correct statistical methods 
relied experimental evaluations methods regarded tentative experiments learning algorithms data sets 
experiments lead recommend cv test situations learning algorithms efficient run times mcnemar test situations learning algorithms run 
tests similar power 
experiments revealed shortcomings statistical tests confidently conclude resampled test employed 
test high probability type error results obtained test trusted 
experiments suggest caution interpreting results fold cross validated test 
test elevated probability type error twice target level nearly probability rejecting difference performance nn mcnemar prop test cv power statistical tests pima task 
horizontal axis plots number percentage points algorithms nn differ trained training sets size 
severe problem resampled test 
hope results useful scientists machine learning neural network communities develop understand improve machine learning algorithms 
author learned fold cross validated test ronny kohavi personal communication author jim osu statistics department statistical consulting assistance radford neal ronny kohavi tom mitchell helpful suggestions 
author grateful referees careful reading identified errors earlier version manuscript 
research supported iri iri national science foundation number office naval research 
breiman friedman olshen stone 

classification regression trees 
wadsworth international group 
breiman 

heuristics instability stabilization model selection 
tech 
rep department statistics university california berkeley ca 
breiman 

bagging predictors 
machine learning 
dasarathy 
ed 

nearest neighbor nn norms nn pattern classification techniques 
ieee computer society press los alamitos ca 
dietterich hild bakiri 

comparison id backpropagation english text speech mapping 
machine learning 
efron tibshirani 

bootstrap 
chapman hall new york ny 
everitt 

analysis contingency tables 
chapman hall london 
frey slate 

letter recognition holland style adaptive classifiers 
machine learning 
haussler kearns seung tishby 

rigorous learning curve bounds statistical mechanics 
proc 
th annu 
acm workshop comput 
learning theory pp 

acm press new york ny 
hinton neal tibshirani delve team members 
assessing learning procedures delve 
tech 
rep university toronto department computer science www cs utoronto ca neuron delve delve html 
kohavi 

wrappers performance enhancement oblivious decision graphs 
ph thesis stanford university 
kolen pollack 

back propagation sensitive initial conditions 
advances neural information processing systems vol 
pp 
san francisco ca 
morgan kaufmann 
kong dietterich 

error correcting output coding corrects bias variance 
prieditis russell 
eds twelfth international conference machine learning pp 
san francisco ca 
morgan kaufmann 


analysis variance experimental design 
springer verlag new york 
merz murphy 

uci repository machine learning databases 
www ics uci edu mlearn mlrepository html 
quinlan 

programs empirical learning 
morgan kaufmann san francisco ca 
rasmussen 

evaluation gaussian processes methods non linear regression 
ph thesis university toronto department computer science toronto canada 
cochran 

statistical methods 
iowa state university press ames ia eighth edition 

