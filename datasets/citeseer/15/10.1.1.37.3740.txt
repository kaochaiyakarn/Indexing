learning hidden markov model structure information extraction seymore ri cmu edu andrew mccallum zy mccallum com ronald rosenfeld roni cs cmu edu school computer science carnegie mellon university pittsburgh pa just research henry street pittsburgh pa statistical machine learning techniques proven fields speech recognition just applied information extraction domain 
explore hidden markov models information extraction tasks specifically focusing learn model structure data best labeled unlabeled data 
show manually constructed model contains multiple states extraction field outperforms model state field discuss strategies learning model structure automatically data 
demonstrate distantly labeled data set model parameters provides significant improvement extraction accuracy 
models applied task extracting important fields headers computer science research papers achieve extraction accuracy 
hidden markov modeling powerful statistical machine learning technique just gain information extraction tasks 
hidden markov models hmms offer advantages having strong statistical foundations suited natural language domains handling new data robustly computationally efficient develop evaluate due existence established training algorithms 
disadvantages hmms need priori notion model topology statistical technique large amounts training data 
focuses aspects hmms information extraction 
investigate learning model structure data 
applications hmms assume fixed model structure number states transitions states selected hand priori domain 
argue information extraction correct model topology apparent typical solution state class may optimal 
second examine role labeled unlabeled data training hmms 
introduce concept distantly labeled data labeled data domain labels partially overlap target domain 
show distantly labeled data consistently improves classification accuracy 
hidden markov models relatively new information extraction enjoyed success related natural language tasks 
widely part speech tagging kupiec applied topic detection tracking yamron dialog act modeling stolcke shriberg 
systems hmms information extraction include leek extracts gene names locations scientific abstracts nymble system bikel named entity extraction 
systems consider automatically determining model structure data state class hand built models assembled inspecting training examples 
freitag mccallum hand build multiple hmms field extracted focus modeling immediate prefix suffix internal structure field contrast focus learning structure hmm extract relevant fields account field sequence 
hmms centered task extracting information headers computer science research papers 
header research consists words preceding main body includes title author names affiliations addresses 
automatically extracting fields useful constructing searchable database computer science research 
models describe part cora computer science research search engine mccallum available www cora com 
remainder structured follows review basics hidden markov models 
discuss learn model structure data examine estimate model parameters labeled unlabeled distantly labeled data 
experimental results extracting important fields headers computer science research papers 
conclude breakdown errors hmms making discussion improving models 
information extraction hidden markov models hidden markov models provide natural framework modeling production headers research papers 
want label word header belonging class title author date keyword 
modeling entire header classes extract hmm 
task varies classical extraction task identifying small set target words large document containing uninformative text 
discrete output order hmms composed set states specified initial final states set transitions states discrete vocabulary output symbols sigma foe oe oe model generates string initial state transitioning new state emitting output symbol transitioning state emitting symbol transition final state 
parameters model transition probabilities state follows emission probabilities oe state emits particular output symbol 
probability string emitted hmm computed sum possible paths xjm gamma restricted respectively string token 
forward algorithm calculate probability rabiner 
observable output system sequence symbols states emit underlying state sequence hidden 
common goal learning problems hmms recover state sequence xjm highest probability having produced observation sequence xjm arg max gamma fortunately viterbi algorithm viterbi efficiently recovers state sequence 
hmms may information extraction research headers formulating model way state associated class want extract title author affiliation 
state emits words class specific unigram distribution 
learn class specific unigram distributions state transition probabilities training data 
order label new header classes treat words header observations recover state sequence viterbi algorithm 
state produces word class tag word 
example hmm annotated class labels transition probabilities shown 
learning model structure data order build hmm information extraction decide states model contain transitions states allowed 
reasonable initial model state class allow transitions state state fully connected model 
model may optimal cases 
specific hidden sequence structure expected extraction domain may better building model multiple states class transitions state 
model finer distinctions likelihood encountering class particular location document model specific local emission distribution differences states class 
alternative simply assigning state class learn model structure training data 
training data labeled class information build maximally specific model 
word training data assigned state transitions state word follows 
state associated class label word token 
transition placed start state state training instance state training instance state 
model starting point variety state merging techniques 
propose simple types merges generalize maximally specific model 
neighbor merging combines states share transition class label 
example sequence adjacent title states single header merged single title state 
multiple neighbor states class label merged self transition loop introduced probability represents expected state duration class 
second merging merges states label share transitions common state 
merging reduces branching factor maximally specific model 
apply merging models undergone neighbor merging 
example start state selecting transitions title states merged model merge children title states transition start state title state remain 
merged model extraction directly state merges automatically hand generalize model 
model structure learned automatically data starting maximally specific neighbor merged merged model technique bayesian model merging stolcke 
bayesian model merging seeks find model structure maximizes probability model training data iteratively merging states optimal tradeoff fit data model size reached 
relationship expressed keyword note address email affiliation date author note title start example hmm 
state emits words class specific multinomial distribution 
bayes rule jd djm djm calculated forward algorithm approximated probability viterbi paths 
model prior formulated reflect preference smaller models 
implementing bayesian model merging learning appropriate model structure extraction tasks accomplished automatically 
labeled unlabeled distantly labeled data model structure selected transition emission parameters need estimated training data 
obtaining unlabeled training data generally difficult acquiring labeled training data problematic 
labeled data expensive tedious produce manual effort involved 
valuable counts class transitions counts word occurring class oe derive maximum likelihood estimates parameters hmm oe oe ae sigma ae smoothing distributions necessary avoid probabilities zero transitions emissions occur training data 
unlabeled data hand baum welch training algorithm baum train model parameters 
baum welch algorithm iterative expectation maximization em algorithm initial parameter configuration adjusts model parameters locally maximize likelihood unlabeled data 
baum welch training suffers fact finds local maxima sensitive initial parameter settings 
third source valuable training data refer distantly labeled data 
possible find data labeled purpose partially applied domain hand 
cases may portion labels relevant corresponding data added model estimation process helpful way 
example bibtex files bibliography databases contain labeled citation information 
labels occur citations title author occur headers papers labeled data training emission distributions header extraction 
bibtex fields relevant header extraction task data include information sequences classes headers 
experiments goal information extraction experiments extract relevant information headers computer science research papers 
define header research words section usually page whichever occurs 
automatically located regular expression matching changed single token 
likewise single token added header intro page indicate case terminated header 
special classes words identified simple regular expressions converted special tokens email web zipcode number 
punctuation case newline information removed text 
target classes wish identify include fifteen categories title author affiliation address note email date intro phone keywords web degree publication number page 
intro page classes represented state outputs token intro page respectively 
degree class captures language associated ph master theses submitted partial fulfillment thesis 
note field commonly accounts phrases copyright notices citations 
type source word tokens labeled headers unlabeled headers distantly labeled bibtex files table sources amounts training data 
headers manually tagged class labels 
headers discarded due poor formatting rest split header word token labeled training set header word token test set 
unlabeled headers composed word tokens designated unlabeled training data 
training data acquired bibtex files collected web 
files consist words contribute header classes address affiliation author date email keyword note title web 
training data sources amounts summarized table 
class emission distributions trained labeled training data combination labeled distantly labeled data linear interpolation labeled distantly labeled data 
case word counts labeled distantly labeled data added deriving emission distributions 
case separate emission distributions trained labeled distantly labeled data distributions interpolated mixture weights derived leave expectation maximization labeled data 
emission distribution training case fixed vocabulary derived words training data 
labeled data results word vocabulary labeled distantly labeled data contain words 
maximum likelihood emission estimates computed class smoothed absolute discounting ney essen kneser avoid probabilities zero vocabulary words observed particular class 
unknown word token unk 
added vocabularies model vocabulary words 
words testing data vocabulary mapped token 
probability unknown word estimated separately class assigned portion discount mass proportional fraction singleton words observed current class 
model selection build hmm models varying model structures training conditions test models finding viterbi paths test set headers 
performance measured word classification accuracy percentage header words emitted state label words true label 
accuracy model states trans full self ml smooth table extraction accuracy models state class 
set models state class 
emission distributions trained class labeled data combination labeled distantly labeled data interpolation labeled distantly labeled data 
extraction accuracy results models reported table 
full model fully connected model transitions assigned uniform probabilities 
relies emission distributions choose best path model achieves 
self model similar self transition probability set maximum likelihood estimate labeled data transitions set uniformly 
model benefits additional information expected number words emitted state accuracy jumps 
ml model sets transition parameters maximum likelihood estimates achieves best result set models 
smooth model adds additional smoothing count transition transitions non zero probabilities smoothing transition probabilities improve tagging accuracy 
models combination labeled unlabeled data negatively affects performance relative labeled data results 
interpolation distantly labeled data labeled data consistently provides percentage points improvement accuracy training labeled data 
refer back ml model results comparisons best representative models state class 
want see models structures derived data outperform ml model 
consider models built combination automated manual techniques 
starting model states built randomly selected labeled training headers states class label manually merged iterative manner 
headers keep manual state selection process manageable 
transition counts preserved merges maximum likelihood transition probabilities estimated 
state uses smoothed class emission distribution estimated interpolation labeled distantly labeled data 
extraction accuracy number states multi state ml extraction accuracy multi state models states merged 
accuracy model states trans ml merged merged table extraction accuracy models learned data compared best model uses state class 
formance measured number states decreases plotted 
performance ml model indicated 
models multiple states class outperform ml model particularly states 
best performance obtained model containing states 
refer model merged model 
result shows complex model structure benefits extraction performance hmms header task 
compare result performance state merged model created entirely automatically labeled training data 
summary results ml model merged model merged model table 
merged merged models outperform ml model training conditions model performing best 
expect bayesian model merging result fully automated construction procedure produces models performing better manually created merged model 
investigate incorporate unlabeled data parameter training scheme 
starting ml merged models run baum welch training unlabeled data 
initial parameters set maximum likelihood transition probabilities labeled data interpolated emission distributions 
baum welch training produces new transition emission parameter values locally maximize likelihood unlabeled data 
models tested different conditions extraction results model merged acc 
pp acc 
pp initial varies table extraction accuracy test set perplexity pp ml merged models baum welch training 
test set shown table 
perplexity measure hmms model data lower value indicates model assigns higher likelihood observations test set 
initial result performance models initial parameter estimates 
results case table 
vocabulary words occur unlabeled data probability zero newly estimated emission distributions resulting baum welch training new distributions need smoothed initial estimates 
state newly estimated emission distribution linearly interpolated initial distribution mixture weight setting distributions state weight 
alternatively viterbi paths labeled training data computed model emission distributions 
words emitted state estimate optimal mixture weights local initial distributions em algorithm 
mixture weights varies case 
smoothed baum welch emission estimates degrade classification performance ml merged models 
lack improvement classification accuracy partly explained fact baum welch training maximizes likelihood unlabeled data classification accuracy 
better modeling capabilities pointed improvement test set perplexity 
perplexity test set improves initial settings baum welch reestimation improves careful selection emission distribution mixture weights 
merialdo finds similar effect tagging accuracy training part speech taggers baum welch training starting initial parameter estimates 
error breakdown conclude experiments breakdown errors best performing models 
table shows errors class ml merged models emission distributions trained labeled interpolated data 
classes distantly labeled training data indicated bold 
classes title author noticeable increase ml merged tag address affiliation author date degree email keyword note phone title web table individual class results ml merged models 
classes noted bold occur distantly labeled data 
accuracy distantly labeled data included 
poorest performing individual classes degree publication number web classes 
web class particularly low accuracy merged model limited web class examples training headers probably kept web state having transitions states necessary 
experiments show hidden markov models extracting important information headers research papers 
achieve accuracy classes headers class specific accuracies titles authors 
demonstrated models contain state class provide increased extraction accuracy models state class 
improvement due specific transition context modeling possible states 
expect beneficial localized emission distributions capture distribution variations dependent position class header 
distantly labeled data proven valuable providing robust parameter estimates 
interpolation distantly labeled data provides consistent increase extraction accuracy headers 
cases little labeled training data available data helpful resource 
forthcoming experiments include bayesian model merging learn model structure completely automatically data advantage additional header features positions words page 
expect inclusion layout information particularly improve extraction accuracy 
plan model internal state structure order better capture words absorbed state 
possibly useful affiliation proposed internal model structure states 
state structure displayed 
case distributions words modeled explicitly internal state emits words 
expect improvements contribute development accurate models research header extraction 
baum 
inequality associated maximization technique statistical estimation probabilistic functions markov process 
inequalities 
bikel miller schwartz weischedel 
nymble high performance learning name finder 
proceedings anlp 
freitag mccallum 
information extraction hmms shrinkage 
proceedings aaai workshop machine learning information extraction 
kupiec 
robust part speech tagging hidden markov model 
computer speech language 
leek 
information extraction hidden markov models 
master thesis uc san diego 
mccallum nigam rennie seymore 
machine learning approach building domainspecific search engines 
proceedings sixteenth international joint conference artificial intelligence 
merialdo 
tagging english text probabilistic model 
computational linguistics 
ney essen kneser 
structuring probabilistic dependencies stochastic language modeling 
computer speech language 
rabiner 
tutorial hidden markov models selected applications speech recognition 
proceedings ieee 
stolcke shriberg 
dialog act modeling conversational speech 
applying machine learning discourse processing aaai spring symposium number ss 
menlo park ca aaai press 
stolcke 
bayesian learning probabilistic language models 
ph dissertation university california berkeley ca 
viterbi 
error bounds convolutional codes optimum decoding algorithm 
ieee transactions information theory 
yamron carp gillick lowe van 
hidden markov model approach text segmentation event tracking 
proceedings ieee icassp 
