design framework highly concurrent systems matt welsh steven gribble eric brewer david culler computer science division university california berkeley berkeley ca usa gribble brewer culler cs berkeley edu building highly concurrent systems large scale internet services requires managing information flows maintaining peak throughput demand exceeds resource availability 
addition platform supporting internet services provide high availability able cope burstiness load 
approaches building concurrent systems proposed generally fall categories threaded eventdriven programming 
propose threads events ends design spectrum best implementation strategy applications 
general purpose design framework building highly concurrent systems design components tasks queues thread pools encapsulate concurrency performance fault isolation software engineering benefits threads events 
set design patterns applied map application implementation components 
addition provide analysis systems including internet services platform highly available distributed persistent data store constructed framework demonstrating benefit building reasoning concurrent applications 
large internet services deal concurrency unprecedented scale 
number concurrent sessions hits day internet sites translates higher number network requests placing enormous demands underlying resources 
microsoft web sites receive hits users day lycos page views users daily 
research sponsored advanced research projects agency dabt equipment intel 
demand internet services grows functionality new system design techniques manage load 
addition high concurrency internet services properties necessitate fresh look systems designed burstiness continuous demand human scale access latency 
burstiness load fundamental internet dealing overload conditions designed system 
internet services exhibit high availability downtime minutes year 
access latencies internet services human scale limited wan modem access times important engineering tradeoff optimize high throughput low latency 
building highly concurrent systems inherently difficult 
structuring code achieve high throughput supported existing programming models 
threads commonly device expressing concurrency high resource usage scalability limits thread implementations led developers prefer event driven approach 
event driven systems generally built scratch particular applications depend mechanisms supported languages operating systems 
addition event driven programming concurrency complex develop debug threads 
threads events best viewed opposite ends design spectrum key developing highly concurrent systems operate middle spectrum 
event driven techniques useful obtaining high concurrency building real systems threads valuable cases required exploiting multiprocessor parallelism dealing blocking mechanisms 
developers aware spectrum exists utilizing thread event oriented approaches concur 
dimensions spectrum currently understood 
propose general purpose design framework building highly concurrent systems 
key idea framework event driven programming high throughput leverage threads limited quantities parallelism ease programming 
addition framework addresses requirements applications high availability maintenance high throughput load 
achieved introducing fault boundaries application components conditioning load placed system resources 
framework provides means reason structural performance characteristics system 
analyze different systems terms framework including distributed persistent store scalable internet services platform 
analysis demonstrates design framework provides useful model building reasoning concurrent systems 
motivation robust throughput explore space concurrent programming styles consider hypothetical server illustrated receives tasks second number clients imposes server side delay seconds task returning response overlaps tasks possible 
denote task completion rate server concrete example server web proxy cache request cache misses large latency page fetched authoritative server time task doesn consume cpu cycles 
response client receives immediately issues task server system 
prevalent strategies handling concurrency modern systems threads events 
threading allows programmers write straight line code rely operating system overlap computation transparently switching threads 
alternative events allows programmers manage concurrency explicitly structuring code single threaded handler reacts events non blocking completions application specific messages timer events 
explore turn formulate robust hybrid design pattern leads general design framework 
closed loop implies completion rate tasks sec server latency sec task arrival rate tasks sec concurrent tasks server tasks concurrent server model server receives tasks second handles task latency seconds service response rate tasks second 
system closed loop service response causes tasks injected server steady state 
threaded servers simple threaded implementation server uses single dedicated thread service network hands incoming tasks individual task handling threads step stages processing task 
handler thread created task 
optimization simple scheme creates pool threads advance dispatches tasks threads pool amortizing high cost thread creation destruction 
steady state number threads execute concurrently server task latency increases corresponding increase number concurrent threads needed absorb latency maintaining fixed throughput likewise number threads scales linearly throughput fixed latency 
threads dominant form expressing concurrency 
thread support standardized operating systems wellestablished incorporated modern languages java 
programmers comfortable coding sequential programming style threads tools relatively mature 
addition threads allow applications scale number processors smp system operating system schedule threads execute concurrently separate processors 
thread programming presents number correctness tuning challenges 
synchronization primi closed loop implies completion rate tasks sec thread sleep secs task arrival rate tasks sec concurrent threads server threads dispatch create threaded server task arrives server thread dispatched statically created pool new thread created handle task 
time total threads executing concurrently tives locks mutexes condition variables common source bugs 
lock contention cause serious performance degradation number threads competing lock increases 
regardless threaded server crafted number threads system grows operating system overhead scheduling aggregate memory footprint increases leading decrease performance system 
typically maximum number threads system support performance degradation occurs 
phenomenon demonstrated clearly 
thread limit large general purpose timesharing adequate tremendous concurrency requirements internet service 
event driven servers event driven implementation server uses single thread non blocking interfaces subsystems timer utilities juggle concurrent tasks shown 
event driven systems typically structured program thread loops continuously processing events different types queue 
thread blocks polls queue wait new events 
event driven programming set inherent challenges 
sequential flow task longer handled single thread thread processes tasks disjoint stages 
debugging difficult stack traces longer represent control flow processing particular task 
max server throughput tasks sec threads executing server threaded server throughput degradation benchmark fast client issuing concurrent byte tasks single tcp connection threaded server ms mhz ultra sparc running solaris 
arrival rate determines number concurrent threads sufficient threads preallocated load 
number concurrent threads increases throughput increases throughput system degrades substantially 
task state bundled task stored local variables stack threaded system 
event packages standardized debugging tools eventdriven programmers 
event driven programming avoids bugs associated synchronization race conditions deadlocks 
events generally take advantage smp systems performance multiple event processing threads 
event processing threads block regardless mechanisms 
page faults garbage collection common sources thread suspension generally unavoidable 
addition may impossible application code non blocking standard library components third party code export blocking interfaces 
cases threads valuable provide mechanism obtaining concurrency blocking interfaces 
event driven systems tend robust load little degradation throughput offered load increases system deliver 
handling events bundling task state efficient peak throughput high 
shows throughput achieved event driven implementation network service function load 
throughput exceeds threaded server importantly degrade increased concurrency 
number tasks increases server throughput increases pipeline fills bottleneck cpu closed loop implies completion rate tasks sec task arrival rate tasks sec timer queue latency seconds event driven server task arrives server placed main event queue 
dedicated thread serving queue sets second timer task timer implemented queue processed thread 
timer fires timer event placed main event queue causing main server thread generate response 
case saturated 
number tasks pipeline increased excess tasks absorbed queues system main event queue server network stack queues associated client server transport connection 
throughput server remains constant situation latency task increases 
concurrency explicit event driven approach associated directly queues 
multiple tasks various stages exposed queues programmers applicationspecific knowledge reorder event processing prioritization efficiency reasons 
thread event spectrum highly concurrent services provide new fuel debate threads events key observation design space concurrency isn limited points 
spectrum extremes possible build hybrid systems strategically exploit properties 
goal obtain benefit threads limit number prevent performance degradation 
question asked threads needed number fit operating range system 
simplest example hybrid thread event server shown 
limits number concurrent threads running system server throughput tasks sec tasks client server pipeline event driven server throughput benchmark setup shows event driven server throughput function number tasks pipeline 
event driven server thread receiving tasks thread handling timer events 
throughput flattens excess threaded server system saturates throughput degrade increased concurrent load 
threads preallocated thread pool buffers incoming tasks event queue thread pool feeds 
task handled single thread standard blocking interfaces developed familiar tools 
excess concurrency absorbed queue response time increases load throughput degrade 
hybrid approach possible determine implementation sufficient meet target efficiency throughput 
returning example service concurrency demand system serviced threads pool 
operating regime hybrid server performs event driven server shown 
impossible service concurrency demand creating threads shown 
size thread pool exceeds throughput degrades regardless thread pool large concurrency load 
set exceed excess tasks accumulate task queue absorbs bursts increases latency process task 
shows performance microbenchmark implementation hybrid server various values demonstrating increases hybrid system unable meet concurrency demand note tasks buffered incoming queue throughput degrade long size thread pool chosen shown 
throughput simultaneous threads throughput simultaneous threads throughput simultaneous threads ms ms ms server throughput tasks sec tasks client server pipeline throughput hybrid event thread system illustrate theoretical performance hybrid server larger smaller concurrency demand 
shows measurements benchmark augmented placing queue front thread pool different values 
shows throughput hybrid server optimal operating point server 
ms 
middle plateau corresponds point pipeline filled form server 
right hand plateau signifies stages pipeline merged 
note axis linear scale logarithmic scales 
closed loop implies completion rate tasks sec thread sleep secs task arrival rate tasks sec concurrent threads server threads thread pool threads event queue absorb bursts tasks hybrid thread event system server uses constant size thread pool threads service tasks arrival rate incoming task queue task experiences service latency seconds 
number tasks received hybrid server exceeds size thread pool excess tasks buffered incoming task queue 
case new design framework parameters consider design space highly concurrent systems 
strike right balance event driven threads concurrency issues fault isolation data sharing ease programming important concerns 
number choices design difficult application designers construct new applications easily reason perform 
initial investigation spectrum concurrency models indicates need systematic techniques mapping application design implementation provides efficiency robustness despite inherent limits underlying operating system 
feel design space represents full spectrum threads events adequately explored attempts understand characteristics tradeoffs inherent building applications space 
continue build new concurrency models ad hoc manner specific applications propose general purpose design framework addresses holistic requirements highly concurrent systems 
framework provides structure build reason largescale server applications 
small set specific design principles application designers certain assertions performance fault isolation properties system 
design framework framework components general purpose building blocks constructing applications framework tasks thread pools queues 
tasks task fundamental unit framework 
typed message contains description done data required complete task 
example task retrieve contents web page data associated task url 
tasks processed series stages individual components application 
example web server retrieve static html page task url parsed page looked local cache necessary read disk 
stages task executed sequence parallel combination 
decomposing task series stages possible distribute stages multiple physical resources reason flow tasks load balancing fault isolation purposes 
thread pools thread pool collection threads machine operate continually processing tasks 
logically thread pool associated set task types thread pool executes piece code consumes task processes dispatches outgoing tasks thread pool 
thread pool thread 
thread pools source execution contexts framework 
modularity accomplished structuring applications set thread pools processing particular set task types 
parallelism exploited thread run separate cpu multiprocessor system 
queues queues means communication thread pools 
queue logically consists list tasks thread pools pull tasks incoming task queue dispatch tasks pushing incoming queues thread pools 
operation thread pools composed inserting queue allowing tasks pass 
call thread pool coupled incoming task queue task handler 
queues act separation mechanism thread pools introducing explicit control boundary 
thread cross boundary pass data boundary enqueuing task possible constrain execution threads task handler 
desirable wrap pipeline combine replicate design patterns 
depicts design patterns construct applications framework 
reasons 
applications easier debug thread pool internal state visible thread pools 
second eliminate cases threads escape piece code may return example library performs blocking operations 
addition queues provide mechanism overflow absorption backpressure fairness 
queues act buffer incoming tasks number outweighs number threads available process 
backpressure implemented having queue reject new entries raising error condition full 
important allows excess load rejected system buffering arbitrary amount 
fairness accomplished scheduling thread pools incoming queue length 
design patterns turn question map application implementation components 
decision thread pools queues important impact performance fault tolerance properties resulting application 
propose design patterns applied construct application framework 
patterns encapsulate fundamental properties framework components describe build application 
wrap wrap pattern involves wrapping set threads queue interface illustrated 
thread processes single task number stages may block times 
applying wrap places single input queue front set threads effectively creating task handler 
operation processing task handler robust load number threads inside task handler fixed value prevents thread overhead degrading performance additional tasks serviced threads accumulate queue 
wrap pattern example thread event hybrid system section 
example thread pool processed single task blocking call sleep operation 
fixing number threads pool value placing queue front peak throughput maintained despite threads sleeping task handler 
pipeline pipeline pattern takes single threaded piece code splits multiple pipeline stages introducing queue thread pool boundary various points 
illustrated 
example queues introduced blocking call operation call appear non blocking separate threads responsible processing task side call 
pipeline pattern uses 
limit number threads allocated handling operations 
pipeline stage task arrival rate tasks second task latency seconds number threads needed maintain completion rate stage section 
consider pipeline stage limited number concurrent tasks handle 
example unix filesystems generally handle fixed number concurrent read write requests saturated 
call limit width pipeline stage denote value width pipeline stage places upper bound number threads yield concurrency benefit stage 
need supply stage threads additional threads stage remain idle 
completion rate stage limited width limits completion rate task handler width increased replication discussed 
pipeline provides way limiting threads needed particular pipeline stage stage width wasting threads remain idle 
breaking processing task separate pipeline stages allows size thread pool tailored stage allows stage replicated separate physical resources see achieve greater parallelism 
second pipeline pattern increase locality 
cache locality increasingly important concern building high throughput systems performance gap cache main memory increases 
addition multiple processors taxes available memory bandwidth lead lengthy stall times cache taken 
thread task system instruction cache tends take misses thread control passes unrelated code modules process task 
addition context switch occurs due thread preemption blocking call say threads invariably flush waiting thread state cache 
original thread resumes execution need take cache misses order bring code state back cache 
situation threads system competing limited cache space 
applying pipeline pattern increase data instruction cache locality avoid performance hit 
pipeline stage process convoy tasks keeping instruction cache warm code data cache warm shared data process convoy 
addition pipeline stage opportunity service incoming tasks order optimizes data cache locality 
example queues serviced order tasks arrived may data cache 
pipeline increases task processing latency recall goal framework optimize aggregate throughput time process individual task 
combine combine pattern combines separate task handlers single task handler shared thread pool 
seen number threads system high performance degrade 
combine allow multiple task handlers share thread pool purposes thread conservation 
consider set sequential pipeline stages resulting pipeline isolate blocking operation thread pool 
third stages cpu bound width number cpus system 
thread pools size single thread pool shared applying combine 
sense combine inverse pipeline 
replicate replicate pattern copy existing task handler 
additionally instantiates task handler new set physical resources places failure boundary copies 
replicate achieve parallelism fault isolation 
replicating task handler physical resources combined width replicas increased 
eliminate bottleneck pipeline task processing stages 
failure boundary introduced replicas running separate address spaces separate machines 
done replicas highly available fails continue processing tasks 
replication raises concerns distributed state management 
failure network link cluster lead partitioning troublesome task handlers residing different cluster nodes need maintain consistent state 
ways avoid problem 
employ various distributed consistency group membership protocols 
engineer cluster interconnect eliminate partitioning 
approach taken dds see section inktomi search engine 
applying design patterns section provide set heuristics determine design patterns applied application 
patterns effect performance fault isolation properties application 
addition number constraints met keep application operational range 
obtaining desired application properties viewed solving multivariate constraint problem 
goal maximize service task completion rate minimize probability application failure 
various constraints include physical resource limits number cluster nodes available application assumed fixed value 
likewise cpu speed amount memory disk network bandwidth parameters assumed fixed configuration 
thread limits seen thread implementations generally impose limit number threads system support performance degradation occurs call extent limit applies depends thread implementation 
user level threads package may allow threads process kernel level threads may limited machine 
latency width values task processing stage drive decision apply design patterns 
starting thread task implementation design rules transform implementation obtains benefits framework 
apply wrap introduce load conditioning placing application threads task handler structure allows number threads limited optimum value additional tasks absorbed queue 
transformation produce hybrid system section 
apply pipeline avoid wasting threads transformation possible increase throughput isolating code low width task limit number threads task handler example apply pipeline point filesystem calls limit number threads filesystem task handler number concurrent accesses filesystem handle 
effectively frees additional threads placed task handlers 
apply pipeline cache performance discussed pipeline increase cache locality isolating code related task processing stages task handler 
cache locality benefit structuring applications way depends major factors 
amount code shared data internally task handler processing convoy tasks 
determines potential cache locality benefit achieved internally task handler 
second amount data carried task handlers task pushed queue 
determines potential data cache locality benefit processing tasks order 
performance effect pipelining task processing measured directly determine particular application structure 
apply replicate fault tolerance replication task handlers multiple resources increases reliability application intro redundancy task processing stages 
replicate task handler times probability failure task handler time interval probability replicas fail time see replica tion exponential effect reliability set task handlers 
benefit replication depends factors 
physical resources available support replication 
wish replicate cluster nodes times achieve particular failure probability clearly cluster nodes available 
task handlers replicated rely shared state raises design concerns discussed 
apply replicate scale concurrency replicate effectively increase concurrency width particular task handler running multiple instances separate resources 
general replicating task handler times increases width collective replicas task handler latency increases task completion rate combined replicas eliminate throughput bottleneck 
apply combine limit number threads node pipelining replicating thread task application may large number task handlers private thread pool 
latency task handlers low sense share thread pools reducing number threads 
discussed important limit number threads node value measured system 
combine merges thread pools disparate task handlers node allowing threads conserved 
heuristics assumes application designer keeping mind inherent resource limits underlying platform 
example task handler placed node increase total number threads node heuristics help structure application framework means exhaustive 
structure application constructed framework illustrated 
see thread pools incoming task queue running separate machines 
communication arrows intended example tasks flow application components real application involve components wrap pipeline replicate combine application resulting design patterns 
data flows left right tasks entering application leftmost queue 
dashed boxes represent machine boundaries 
design pattern labelled 
communication paths 
principles apart design patterns framework reveals principles followed engineering system 
task handlers stateless possible 
allows task handler lock free state shared threads 
addition allows task handlers easily created restarted demand example response load spike failure 
second data associated tasks passed value possible 
data sharing task handlers raises number concerns 
consistency shared data maintained locks similar mechanism locks lead race conditions long acquisition wait times contended turn reduces concurrency 
passing data problematic task handlers located different addresses spaces machines 
distributed shared memory dsm space sharing transparent dsm mechanisms complex raise concurrency concerns 
data sharing requires task handlers agree responsible deallocating data longer 
garbage collected environment single address space straightforward garbage collection explicit coordination required 
importantly data sharing reduces fault isolation 
task handler fails leaves shared data inconsistent state task handlers sharing data able recover situation risk failure 
alternative passing value pass originator relinquishing access 
means reducing data sharing space partition application state multiple pipeline stages replicas task handler process private partition application state sharing state locks maintain consistency 
third principle fate sharing avoided 
task handlers share physical resources share fate physical resources fail node crashes task handlers fail 
clearly replicas task handler kept separate physical nodes separate address spaces avoid fate sharing 
note task handlers linked terms load just terms failures 
example replica task handler fails replicas required take load order maintain throughput 
fourth principle admission control done front door application interface outside world 
denying delaying task entry application topmost layer task handlers able stay operational range terms resource demand 
approach requires task handlers implement backpressure rejecting new tasks queue lengths long 
external admission control mechanism slow task handler overloaded tasks queue application 
increases latency resource demand system 
analysis framework demonstrate utility analyze number existing systems terms design framework 
systems built framework explicitly spelled designed framework key features mind 
distributed data structures scalable available cluster storage layer internet services 
system exposes conventional data structure apis hash tables trees service authors replicates state data structures cluster sake scalability availability 
persistent state managed software components called bricks applications run cluster bricks distributed hashtable rpc stubs single node ht buffer cache core disk operating system core network core disk file system raw disk core disk file system raw disk core network network stack core network network stack distributed hash tables illustrates structure distributed hash table brick process thick arrows represent request tasks thin arrows represent completion tasks 
shows ideal operating system core implementation os exports non blocking interface 
shows current implementation blocking interfaces exposed java class libraries force dispatch threads handle requests fixed sized thread pool 
dedicated threads block listening incoming sockets 
communicate distributed data structure dds libraries 
libraries handle distribution replication recovery data shielding services complexity 
dds bricks designed hybrid thread event framework tailored high concurrency throughput required internet services 
illustrates bricks design 
dds layer implemented java linux cluster operating system 
brick single process composed layers separated thread boundary 
tasks placed queues bridge boundaries 
bottom layer brick abstraction library called core 
library provides non blocking interfaces disk network implementations queues events thread pools 
buffer cache layer sits top disk interface implement hash table layer 
expose single node hash table rest cluster asynchronous rpc layer networking interfaces core 
buffer cache single node hash table asynchronous rpc layer single dedicated thread waits tasks layer task queue 
task arrives layer task state retrieved task passed event handler layer 
data locks necessary due single threaded design layers implication event handlers block 
state distributed data structure horizontally partitioned 
sake availability partitions replicated cluster nodes 
replicas kept consistent phase commits yielding copy equivalence 
ideal implementation core layer shown 
operating system exposes non blocking interfaces disk network allowing core implemented singlethreaded state machine 
unfortunately java class libraries expose non blocking interfaces version linux cluster provide non blocking file system interfaces 
core uses thread pool shown provide non blocking interfaces high layers 
task placed core queue thread dispatched pool issue blocking request 
thread pool large handle width disk inter cluster network case disk width approximately threads network width times number cluster nodes threads dedicated network connection reading writing 
note number cluster nodes grows number threads required network layer increases 
dds example complex hybrid thread event system fits framework 
layer dds brick event driven task handler single thread 
lowest layer dds uses thread pool gain concurrency blocking interfaces example wrap pattern 
pipeline pattern separate higher layers dds replicate pattern replicate bricks multiple cluster nodes 
fact single thread pool serving disk network exemplifies combine pattern 
vspace vspace platform scalable flexible internet services 
vspace uses workstation clusters java programming language provide high level programming model hides details vspace early stages development concrete performance results available 
internet service construction application developers 
vspace borrows ideas embodied multispace tacc goes step providing data persistence distributed data structure layer event driven execution model high concurrency 
vspace applications constructed set workers run nodes workstation cluster 
workers operate processing tasks generally consists small amount data 
task associated task type specifies contents workers register system ability handle task types 
example protocol worker handle tasks type disk storage worker handle types 
addition workers dispatch outgoing tasks workers handle 
vspace model maps neatly design framework 
vspace workers correspond directly task handlers queues implemented task dispatch operation vspace cause tasks flow workers node cluster network nodes 
vspace worker thread pool size running multiple workers single node achieve high concurrency 
vspace pipeline pattern decompose applications workers replicate achieve scalability fault tolerance nodes 
wrap introduce single logical task queue workers node 
vspace workers soft state managed distributed data structure layer failure cluster node affect nodes workers node restarted 
load balancing accomplished interposing task dispatch operation vspace determines suitable cluster node worker node handle new task 
determination load information periodically gathered cluster nodes 
flash harvest framework analyze successful concurrent system designs 
flash web server harvest web cache asynchronous event driven model proposed 
conceived specific application mapped design framework 
flash component web server responds particular types events socket connections filesystem accesses 
main server process responsible continually dispatching events components implemented library calls 
certain operations case filesystem access asynchronous interfaces main server process handles events dispatching helper processes ipc 
helper processes issue blocking requests return event main process completion 
harvest structure similar single threaded event driven exception ftp protocol implemented separate process 
flash harvest main thread helper processes considered task handlers framework produced applying pipeline pattern 
queues implemented unix ipc mechanisms pipe 
task handlers thread pool size multiple helper processes created replicate pattern increase throughput 
framework suggests improvements flash harvest designs pipeline decompose event processing stages increase bandwidth replicate applied produce clustered version application 
related programming models proposed building scalable highly concurrent systems 
systems attempt cover wide range requirements addressed framework 
application servers including bea weblogic objectspace voyager ibm websphere aim support scalable concurrent applications industry standard programming interfaces enterprise java beans java servlets 
generally application servers middleware component presentation system web server back database mainframe 
systems provide variety application programming interfaces internal structure generally thread 
threads network connections interestingly harvest asynchronous disk read mechanism relied select call unix implementations including solaris linux freebsd returns immediately regardless requested data fact memory 
application thread block waiting data read disk non blocking file descriptors 
squid corrected bug separate thread pool issue disk requests 
database connections usually pooled limit resource consumption server replication servers provide scalability fault isolation 
systems tend implemented standard java virtual machines operating systems little control low level concurrency mechanisms 
systems require resources handle load bursts 
structure framework influenced greatly tacc sns propose scalable service models collections workers replicated cluster 
structural components task handlers queues evident systems adopt event driven approach achieving robust high concurrency 
rely threads obtain scalability growing number cluster nodes 
jaws web server combines event driven concurrency mechanism high level programming construct proactor pattern intended simplify development highly concurrent event driven applications 
jaws directly address fault isolation clustering proactor pattern describe event driven programming model useful tool applied task handlers framework 
kaashoek propose specializing operating system architectures server applications internet services specific example 
focuses low level aspects performance servers disk network access overhead realizes benefit event driven concurrency model 
application specific handlers install application level event handlers kernel added performance 
approach complements design framework providing novel kernel level functionality improve performance 
looked improving threaded event driven programming models 
banga suggest changes unix system call interface better support event driven applications scheduler activations reduce cost kernel thread implementations 
click modular packet router uses software architecture similar framework packet processing stages implemented separate code modules private state 
click modules communicate queues function calls threads cross module boundaries 
click domain specific system obtaining high concurrency packet router general lower level framework 
click uses push pull semantics flow control packet processing modules send data downstream request data pushed upstream 
rationale pull processing push semantics require packets queued stages ready process packet example network port busy 
push pull distinction important threads cross module boundaries case click 
framework imposes queue modules threads push data downstream queues pull data upstream incoming queue 
click implemented framework creating thread pool boundary wrap pattern queue exists click modules 
click modules communicate function calls operate single task handler framework 
believe design framework enable construction new class highly concurrent applications 
process building systems design 
include novel internet services new segment database storage manager secure consistent highly available global filesystem 
applications share concurrency availability requirements targeted framework 
addition continue explore design tradeoffs framework 
approaches loadbalancing resource management cluster investigated depth 
particular interested economic models manage arbitration competing applications sharing physical resources 
building event driven systems supported better high level language features managing state consistency scheduling 
current java new language abstractions supporting framework reveal avenues research 
goal map design space highly concurrent systems framework provides way reason performance fault isolation software engineering characteristics 
framework simple components tasks queues thread pools capture benefits threaded event driven concurrency models 
set implementation guidelines expressed small number design patterns allow application designers directly obtain benefit framework components 
demands highly concurrent systems internet services new challenges operating system language designers 
high concurrency high throughput burstiness fault tolerance required applications existing systems generally fail provide features form ready application developers 
design framework presents straightforward programming model allows applications constructed existing techniques threads non blocking clustering suggest language designs support framework class programming schema 
anderson bershad lazowska levy 
scheduler activations effective kernel support user level management parallelism 
acm transactions computer systems february 
gaurav banga jeffrey mogul peter druschel 
scalable explicit event delivery mechanism unix 
proceedings usenix annual technical conference monterey ca june 
bea systems 
bea weblogic 
www com products weblogic 
chankhunthod danzig neerdaels schwartz worrell 
hierarchical internet object cache 
proceedings usenix annual technical conference pages january 
chawathe eric brewer 
system support scalable fault tolerant internet services 
proceedings middleware september 
inktomi 
inktomi search engine 
www inktomi com products portal search 
armando fox steven gribble chawathe eric brewer paul gauthier 
cluster scalable network services 
proceedings th acm symposium operating systems principles st malo france october 
ian goldberg steven gribble david wagner eric brewer 
ninja jukebox 
proceedings nd usenix symposium internet technologies systems boulder october 
gosling joy steele 
java language specification 
addison wesley reading ma 
steven gribble eric brewer joseph hellerstein david culler 
scalable distributed data structures internet service construction 
submitted fourth symposium operating system design implementation osdi october 
steven gribble matt welsh david culler eric brewer 
multispace evolutionary platform infrastructural services 
proceedings th usenix annual technical conference monterey california 
joe hellerstein eric brewer mike franklin 
telegraph universal system information 
db cs berkeley edu telegraph 
james hu irfan pyarali douglas schmidt 
high performance web servers windows nt design performance 
proceedings usenix windows nt workshop august 
james hu irfan pyarali douglas schmidt 
applying proactor pattern high performance web servers 
proceedings th international conference parallel distributed computing systems october 
ibm 
ibm websphere application server 
www ibm com software 
sun microsystems java servlet api 
java sun com products servlet index html 
frans kaashoek dawson engler gregory ganger deborah wallach 
server operating systems 
proceedings sigops european workshop september 
keleher dwarkadas cox zwaenepoel 
treadmarks distributed shared memory standard workstations operating systems 
proceedings winter usenix conference pages january 
robert morris eddie kohler john jannotti frans kaashoek 
click modular router 
proceedings th acm symposium operating systems principles sosp pages kiawah island south carolina december 
moser amir melliar smith agarwal 
extended virtual synchrony 
proceedings fourteenth international conference distributed computing systems pages poznan poland june 
national laboratory applied network research 
squid internet object cache 
www squid cache org 
objectspace objectspace voyager 
www objectspace com products voyager htm 
vivek pai peter druschel willy zwaenepoel 
flash efficient portable web server 
proceedings annual usenix technical conference june 
stonebraker devine litwin pfeffer sah staelin 
economic paradigm query processing data migration mariposa 
proceedings rd international conference parallel distributed information systems september 
sun microsystems enterprise java beans technology 
java sun com products ejb 
uc berkeley oceanstore project 
oceanstore cs berkeley edu 
robbert van renesse kenneth birman maffeis 
horus flexible group communication system 
communications acm april 
hogg huberman kephart stornetta 
spawn distributed computational economy 
ieee transactions software engineering february 
deborah wallach dawson engler frans kaashoek 
application specific handlers high performance messaging 
proceedings acm sigcomm conference applications technologies architectures protocols computer communication pages stanford california august 
matt welsh jason hill rob von behren alec woo 
querying large collections music similarity 
technical report ucb csd berkeley computer science division november 
