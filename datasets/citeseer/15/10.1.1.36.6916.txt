mutual information metric entropy cumulative relative entropy risk david haussler uc santa cruz manfred opper universitat december submitted annals statistics abbreviated title mutual information risk ams subject classifications primary secondary 
key words phrases mutual information hellinger distance relative entropy metric entropy minimax risk bayes risk density estimation kullback leibler distance 
supported nsf iri 
email addresses haussler cse ucsc edu supported heisenberg fellowship dfg 
email addresses opper physik uni de assume fp thetag set probability distributions common dominating measure complete separable metric space state theta chosen nature 
statistician gets independent observations distributed time observations gamma statistician produces estimated distribution suffers loss 
cumulative risk statistician average total loss time special interest information theory data compression mathematical finance computational learning theory statistical mechanics special case loss relative entropy true distribution estimated distribution cumulative bayes risk time mutual information random parameter theta observations new bounds mutual information terms laplace transform hellinger distance pairs distributions indexed parameters theta 
bounds cumulative minimax risk terms metric entropy theta respect hellinger distance 
assumptions required bounds general depend choice dominating measure 
apply finite infinite dimensional theta 
apply cases infinite dimensional cases compact cases distributions smooth parametric cases asymptotic normality posterior distribution fails 
classical statistics concerned estimation probability distributions independent identically distributed observations drawn distributions 
denote true distribution generating observations estimated distribution obtained seeing gamma independent observations success statistical procedure defined terms loss function measures difference true distribution estimated distribution loss function proven importance fields including information theory data compression mathematical finance computational learning theory statistical mechanics 
relative entropy function 
fields special importance cumulative relative entropy loss suffered sequential estimation setting total observations observations arrive time time new refined estimate unknown true distribution gamma previous observations 
setting study 
average cumulative loss sequences observations generated true distribution cumulative relative entropy risk 
family fp thetag distributions types risk interest statistics 
minimax risk minimum worst case risk possible true distributions theta minimum possible sequential estimation strategies 
bayes risk minimum average case risk possible true distributions drawn prior distribution theta minimum possible sequential estimation strategies 
cumulative relative entropy loss bayes risk fundamental information theoretic interpretation mutual information random variable representing choice parameter true distribution random variable observations 
provides beautiful connection information theory statistics 
connection extends fields discussed 
data compression cumulative relative entropy risk redundancy expected excess code length best adaptive coding method compared best coding method prior knowledge true distribution 
minimax risk called information channel capacity 
mathematical finance gambling theory cumulative relative entropy risk measures expected reduction logarithm compounded wealth due lack knowledge true distribution 
computational learning theory risk average additional loss suffered adaptive algorithm predicts observation arrives previous observations compared algorithm predictions knowing true distribution 
assume observation time predicted predictive probability distribution formed adaptive algorithm previous gamma observations tth observation arrives loss negative logarithm probability statistical mechanics bayes risk related free energy 
provide upper lower bounds bayes risk cumulative relative entropy loss form laplace integrals hellinger distance pairs distributions fp thetag 
illustrate bounds number special cases characterize asymptotic growth rate minimax risk terms metric entropy fp thetag hellinger distance 
methods advantage simplicity proofs amounting little simple applications jensen inequality 
results quite general 
bounds apply finite infinite dimensional theta 
apply cases space observations infinite dimensional cases compact cases distributions smooth parametric cases asymptotic normality posterior distribution fails 
bounds fairly tight 
smooth parametric cases general bounds crude give precise estimates low order additive constants obtained clarke barron 
organized follows 
sections give precise definitions risks evaluate discuss conditions required bounds hold 
compare bounds obtained previously authors 
bounds section followed examples sections showing applied 
section give characterization minimax risk 
discuss possible section 
basic definitions notation assumptions notation assumptions 
complete separable metric space 
probability distributions discussed assumed defined oe algebra borel sets theta set theta probability distribution assume theta distributions associated distinct sense borel set ae 
addition assume fixed oe finite measure dominates theta borel set implies 
implicitly assumption distribution mentioned results dominated results depend choice dominating measure distribution radon nikodym derivative dq abbreviated simply dq convention le cam text 
furthermore integrals results assumed specific notation taken respect measure indicated 
function distribution expectation denoted dq special case countable counting measure probability mass function need treat probability distributions theta refer prior distributions 
theta associated distinct distribution complete separable metric space define prior distributions theta respect borel sets topology weak convergence measures 
assume set fp thetag measurable topology 
prior distributions theta assumed borel distributions type suprema priors assumed respect borel distributions type 
discussion issues appendix 
integer real valued functions say lim lim inf lim sup 
logarithms natural logarithms specified 
assume log log nonnegative finite number 
employ functions values extended reals gamma particular extended log function obtained defining log gamma log 
expectations extended real valued functions defined take value positive probability value gamma positive probability 
expectation value positive probability similarly gamma 
statement problem game estimating probability distribution view problem estimating probability distribution set distributions fp thetag game nature plays statistician 
nature picks theta 
refer true state nature 
sequence random variables observed distributed particular sequence values observed random variables denoted time statistician forms estimate jy gamma unknown distribution values gamma gamma particular gamma distribution called predictive distribution time set predictive distributions gamma called predictive strategy statistician denoted simply note formulation statistician estimate parameter distribution represents 
allows statistician necessary predictive distributions set fp thetag 
statistician suffers loss predictive distribution place true distribution define loss kl divergence relative entropy true distribution predictive distribution 
general distributions relative entropy defined dkl jjq dp log dp dq statistician uses strategy risk statistician time state nature average loss gamma dp gamma dkl jj cumulative risk observations examines cumulative risk henceforth referred simply risk risk time referred instantaneous risk 
cumulative risk particularly simple interpretation 
strategy define distribution jy gamma way identify prediction strategies joint distributions gamma dp gamma gamma dp log dp jy gamma dkl jj chain rule relative entropy see 
course statistician seeks strategy minimizes risk 
approach assume nature strategic adversary selects worst case particular strategy statistician 
case best strategy statistician minimizes worst case risk value game minimax risk minimax inf sup theta strategy achieves minimax value called minimax strategy 
approach bayesian approach seeks minimize average risk 
imagine nature chooses random prior probability distribution theta 
statistician seeks minimize average risk value game bayes risk bayes inf theta strategy achieves value called bayes strategy 
bayesian approach random variables theta giving choice state nature giving sequence observations 
joint distribution defines behavior nature 
marginal distribution defined theta particular importance 
breaking product conditional distributions write bayes jy gamma bayes jy gamma gamma gamma distributions bayes called predictive posterior distributions 
form bayes strategy relative entropy loss call bayes see note difference average risk arbitrary strategy strategy bayes theta dkl jj gamma dkl jjm theta dp log dp gamma log dp dm dm log dm dkl jj follows bayes risk relative entropy loss bayes theta dkl jjm theta mutual information parameter theta observations 
see general definition discussion mutual information 
turns simple universal relationship bayes risk bayes minimax risk minimax result obtained limited effort general results early le cam 
special cases result derived gallager davisson leon garcia general result 
theorem minimax sup bayes supremum taken borel probability measures parameter space theta 
minimax inf sup theta bayes authors studied bayes risk bayes equivalent mutual information theta case parametric family distributions fp thetag 
early showed theta log real line conditional distributions smooth family densities indexed real valued parameter vector compact set theta dimension certain conditions apply 
case able estimate lower order additive terms approximation involve fisher information entropy prior 
related results clarke 
clarke barron gave detailed analysis applications risk bayes strategy function true state nature discussing relation bayes risk notion redundancy information theory giving applications hypothesis testing portfolio selection theory 
results extended bayes minimax risk see 
related lower bounds quoted obtained rissanen certain asymptotic normality assumptions 
amari developed extensive theory relates risk true state nature certain differential geometric properties parameter space theta neighborhood involving fisher information related quantities see 
authors looked value relative entropy risk nonparametric cases 
issue consistent estimation general probability distribution respect relative entropy addressed 
nonparametric case extensive done bounding risk loss functions see 
extensive summarize note authors taken general approach take notions metric entropy defined specifically hellinger distance obtaining bounds 
authors applied methodology relative entropy risk wong shen see corollary barron yang 
somewhat complementary treats instantaneous risk focus cumulative risk 
tools wong shen employ considerably sophisticated involving bracket entropy methods empirical processes appears boundedness assumptions theorem bit stronger see discussion section 
different assumptions different methods fano inequality obtain related general results 
describe new approach employing hellinger metric certain laplace integrals bounding bayes minimax risks cumulative relative entropy loss 
theta bounds fairly tight 
show theta satisfy general conditions 
describe conditions needed ff define ff affinity distributions ae ff dp ff dq gammaff obtain useful bounds minimax risk need assume theta exist ff constant distribution theta ae ff call ff affinity boundedness condition theta 
fixing ff separate condition type defined ff 
condition needed bayes risk upper bound similar need expectation ae ff drawn random prior 
related boundedness condition investigation consistent density estimation respect relative entropy risk 
assumed constant distribution theta dkl called relative entropy boundedness condition 
clear minimax risk minimax infinite theta relative entropy boundedness condition satisfied 
condition necessary analysis minimax risk nontrivial 
fact shown ff boundedness condition impose noted dkl jjq dominate assumption results minimax risk strictly stronger relative entropy boundedness condition ff 
sense stronger relative entropy boundedness condition shown simple function ff affinity divergence defined approaches relative entropy ff approaches 
furthermore shown ff affinity boundedness condition weaker integrable envelop condition sup theta dp minimax analysis relative entropy risk 
discussion relationship conditions section 
note setting ff avoid confusion usage ff 
sections discuss sense boundedness conditions may viewed assuming minimax risk finite alternate definitions loss function case ff affinity place relative entropy 
viewpoint allows bound minimax risk cumulative relative entropy loss terms function plus additive constant minimax risk ff affinity loss ff place relative entropy loss see lemma 
bounds mutual information relative entropy distance mixture obtain minimax risk supremum bayes risks focus attention bayes risk 
noted bayes risk bayes mutual information theta random variable theta giving choice prior observations give general bounds mutual information 
addition risk particular state nature bayes strategy bayes bayes dkl jjm seek bounds quantity 
bounds address general problem bounding relative entropy distance fold product distribution mixture distributions 
obtaining bounds notions distance probability distributions ff affinities 
family distances divergences introduced renyi 
real ff distributions divergence order ff defined ff jjq ff gamma log dp ff dq gammaff ff related set distances defined ff gamma ff gamma dp ff dq gammaff gamma ff gamma ff dq gamma dp ff dq gammaff mentioned section distributions theta common dominating measure weaker relative entropy boundedness condition 
minimax risk trivially infinite assumption showing obtain essentially loss generality making assumption 
ffx gamma ff gamma ff gammaff ff integrand nonnegative rightmost definition ff showing ff 
essentially holder inequality 
gamma log gamma follows ff jjq ff ff jjq 
gamma log gamma near quantities similar ff affinity dp ff dq gammaff close 
case ff define jjq dkl jjq dq gamma dp gamma dp log dq dp log gamma follows gamma gamma log integrand rightmost expression nonnegative 
shown ff ff jjq increasing ff ff 
important special case distances squared hellinger distance hl dp gamma dq distances divergences discussed distance dhl squareroot defined hl metric symmetric satisfies triangle inequality 
metric give bounds risk estimation procedures statistics authors including le cam van de 
basic bounds main theorem gives bounds theta dkl jjm terms logarithms laplace transforms divergence value ff relative entropy ff 
theorem prior measure theta ff 
theta arbitrary conditional distribution fold product 
gamma theta log theta gamman gammaff ff jjp bayes theta gamma theta log theta jjq 
fl exists subset theta fl theta measure gamma gammafl prior theta fl gamma log theta gamman gammaff ff jjp gamma fl bayes dkl jjm gamma log theta jjq fl upper bound part similar results mentioned case best knowledge lower bound results part new 
proof series lemmas calculations 
prove upper bounds parts theorem lower bounds 
establishing bounds part show set measure gammafl lower bound fails similarly upper bound 
bounds hold complement union sets measure gamma gammafl upper bounds 
requires lemma previously utilized framework statistical physics 
lemma measure set measure set real valued function gamma dq log dp gamma log dp dq proof note holder inequality real valued functions ff dp ffu gammaff dp ff gammaff dp ff dp gammaff logs shows log dp convex result follows applying jensen inequality 
simple lemma suggested meir feder 
measure product space theta conditional distribution vjw marginal distribution lemma random variables nonnegative function dp 
dp log 
fl pr dp vjw log fl gammafl proof part dp log gamma set positive measure 
note jensen inequality dp log log dp employ convention log 
second part case set positive measure conditional distribution similarly trivial note pr dp vjw log fl pr dp vjw log fl gammafl dp dp vjw log gammafl dp dp vjw gammafl inequality follows markov inequality second jensen inequality 
establishing upper bounds lemma theta theta dq dmn assume dm removed domain finite 
conditions lemma satisfied function nonnegative theta thetay dp theta dq dm theta dq theta employing lemma choice chain inequalities holds set measure gammafl dkl jjm dp log dp dm dp log dp theta dq log theta dq dm dp log dp theta dq fl gamma dp log theta dq dp fl gamma dp log theta log dq dp fl gamma log theta dp log dq dp fl gamma log theta gammad kl jjq fl gamma log theta kl jjq fl inequality follows lemma part second lemma 
equality follows fact kl divergence additive product independent distributions see 
note convention log set dp simply removed equality reintroduced exponent second inequality avoiding division zero cases 
similarly theta dq set positive measure respect upper bounds second line infinite result holds trivially 
set measure zero theta dq ignored avoiding division zero regard 
dkl establishes upper bound part theorem 
upper bound part theorem established similar manner 
note theta theta dp log dp dm theta dp log dp theta dq log theta dq dm theta dp log dp theta dq inequality follows lemma part 
remainder proof consists identical chain inequalities proof upper bound part take expectation term fl 
turn lower bounds 
lemma new far tell 
measure product space theta conditional distribution vjw marginal distribution define gamma dp log dp dp vjw dp vjw note mutual information lemma dp dp vjw 
gamma dp dp vjw dp vjw dp dp vjw dp vjw dp dp vjw happens dp vjw set measure zero 
convention gamma log gamma log set contribute ignored 
furthermore dp dp vjw dp vjw vjw dp dp vjw 
similar reasoning individual set vjw ignored evaluating possible dp dp vjw dp dp vjw dp dp dp vjw gammar dp dp vjw delta jensen inequality 
defined 

dp vjw dp vjw log dp dp vjw dp vjw gamma log dp dp vjw dp vjw flg gammafl proof follows lemma function dp dp vjw dp vjw dp dp vjw dp vjw dp gamma vjw dp dp vjw dp dp vjw conditions lemma satisfied nonnegative dp vjw dp dp dp gamma vjw dp dp vjw dp dp vjw dp dp vjw dp gamma vjw dp dp vjw dp dp vjw dp dp vjw dp dp vjw dp dp vjw dp dp vjw note fy theta dp positive measure distribution dkl jjm 
lower bound holds trivially 
set measure zero ignored part lemma theta show inequalities hold set measure gammafl dkl jjm gamma dp log theta dp dp gamma dp log theta dp dp gamma fl gamma log theta dp dp dp gamma fl gamma log theta dp gamma dp gamma fl gamma log theta dp gamma dp gamma fl gamma log theta gamman log dp gamma dp gamma fl gamma log theta gamma jjp gamma fl proof upper bound avoid division zero apply lemma remove set dp line reintroduce fourth line 
setting ff gamma establishes lower bound part 
upper bound lower bound part established easily removing gammafl terms expectation chain inequalities part lemma line 
establishes lower bounds completes proof theorem 
brief comments theorem order 
note part fl grow suitable way obtain bounds asymptotically hold theta 
stronger result obtained chose fl gammafl holds example fl grow faster log borel cantelli lemma shows theta bounds violated finite number times 
noted important special case upper bound part theorem holds fl omit steps derivation case fl introduced 
strengthened upper bound lower bound hold set measure gamma gammafl case 
note part related part way strong redundancy capacity theorem universal coding related usual theorems concerning average redundancy 
possible state variant theorem ff distances 
particular choice family distributions appear theorem 
possible choice explored theorem 
need definition 
ff define ff gamma ff gamma log gamma ff gamma ff gamma gammaff define ff 
easily verified ff strictly decreasing approaches approaches 
ff theta sup theta ff dp dp clearly constant depend choice dominating measure corollary ff 
gamma theta log theta gamman gammaff ff bayes theta gamma theta log theta ff theta ff 
fl exists subset theta fl theta measure gamma gammafl prior theta fl gamma log theta gamman gammaff ff gamma fl bayes dkl jjm gamma log theta ff theta ff fl proof 
ff jjq ff lower bounds follow directly lower bounds theorem 
upper bounds need lemma simple extension lemma 
lemma distributions ff dkl jjq sup ff dq dp ff proof 
dp dq set zero measure dominating measure dkl jjq result holds 
suffices consider case ff 
fy dp 
separating gamma factoring dp equations case dkl jjq ff gamma ff gammas dp dq dp gamma log dq dp gamma dq gammas dp ff gamma ff dq dp gamma dq dp gammaff dq sup ff dq dp ff 
upper bounds corollary follow theorem lemma setting dp uniformly bounded zero infinity choice dominating measure ff theta finite corollary applied 
cases ff theta ff making upper bound corollary useless 
case occurs theta dominated example zero zero dominated ff theta cases lack domination occur 
example theta open interval ff theta distributions fail mutually dominate inf theta dp dp 
cases handled results section 
affinity boundedness condition prove version corollary cases ff theta ff 
new theorem requires bayes version weaker affinity boundedness condition described section 
fixed prior define bayes ae inf theta dp gamma bayes risk game studying relative entropy loss replaced affinity loss fixed number observations 
jensen inequality verified bayes ae minimizing bayes strategy distribution defined du theta dp theta dp 
individual risk bayes strategy ae dp du gamma dp theta dp gamma bayes risk bayes ae theta dp du gamma theta dp theta dp gamma theta dp theorem 
theorem ff 
assume bayes ae 

gamma theta log theta gamman gammaff ff bayes theta gamma theta log theta gamma log gammaff ff ff bayes ae 
fl exists subset theta fl theta measure gamma gammafl prior theta fl gamma log theta gamman gammaff ff gamma fl bayes dkl jjm gamma log theta gamma log gammaff ff ff ae fl case fixed ff function 
furthermore results hold replacing quantity ff ff jjp 
proof 
replaced follows fact ff jjq ff ff pointed proof corollary 
prove result need lemma lemma assume ff 
distributions dp du gamma gamma ffl ffl log log ffl log ffl ffl gammaff gammaff dkl jjq log ffl ff ffl ff ffl log ffl gamma ff ff ffl ffl ff ff gamma ff gamma gammaff gamma ff proof lemma appendix 
bayes strategy defined 
bayes ae welldefined 
theta gamma ffl ffl gamma clear ff ffl ff gammaff ffl 
lemma sufficiently large dkl jjq log ffl ff ffl ff ffl log ffl gamma ff ff ffl ffl ae log ff gamma ff ae log gamma ff ff ff ae theta ae bayes ae result follows theorem 
noticed related result theorem explicit relationship ff affinities result 
note attempt optimize constants theorem 
theta sup theta dp call sup theta dp envelop function theta 
note theta independent choice dominating measure 
theta dp sup theta dp follows bayes ae theta dp theta 
theta integrable envelop function theta bayes ae bounds part theorem hold bayes ae replaced theta 
clear theta finite bounded set densities fp thetag uniformly upper bounded 
theorem applies cases 
theorem applies cases theta infinite example case section 
characterize types theta priors covered theorem define function theta theta log bayes ae theta bayes risk observation relative entropy loss 
shown theta theta nondecreasing function values theta finite lim theta verify property note lim bayes ae 
hospital rule lim theta bayes ae verified direct calculation quantity mutual information theta bayes clear bayes infinite bayes infinite 
possible cases pair theta sup theta dp measurable measurable function definition theta 

theta 
case bayes ae theorem applies may get bounds bayes 
theta 
case bayes problem bounding quantity trivial 

theta theta 
case say pair theta irregular 
nontrivial cases theorem apply 
expected irregular theta show practice possible construct 
example theta theta define gamma log log 
log log 
shown theta irregular 
examples illustrate theorems applying simple problems 
classical case point theta vector real numbers theta compact set prior specified density 
apply theorem fix theta fl interior theta 
assume prior continuous positive assume fp smooth family probabilities fisher information matrix defined ij dp log dp log dp exists positive definite 
case focus bounds risk bounds mutual information 
simplest choice sufficient obtain useful bound smooth case 
large obviously main contributions inner expectations theorem come small neighborhoods certain regularity conditions laplace method evaluate expectations asymptotically 
perform taylor expansion exponents theorem second order difference partial derivatives ff jjp ff jjp ffj ij note results valid ff 
laplace method yield lower bound theta gamman gammaff ff jjp gamma ff gammaff ij gamma ij gamma similar expression obtained upper bound 
evaluating gaussian integrals get log gamma log log gamma log ff gamma ff gamma fl bayes log gamma log log note asymptotically lower bound optimized setting ff case large bounds differ constant approximately equal log small fl 
classical case clarke barron determined exact answer bayes log gamma log log gamma simpler methods give best known additive constants bounds classical case provide bounds large pointed clarke barron scaling log bayes risk smooth parametric families strongly related asymptotic normality properly normalized posterior distribution 
interesting look nonregular families probabilities posterior fails converge nontrivial limit 
conditions necessary convergence see 
example nonsmooth densities study simple family dp gamma gamma fy obviously dkl jjp fisher information exist 
previous analysis applicable resort sophisticated upper bounds 
specializing ff easily find gamma gammaj gamma jjp gamma result clearly shows difference smooth families 
distances behave locally quadratic function close linear scaling 
different scaling risk mutual information expected 
explicit result theorem easily obtained prior gammaj note envelope theta integrable obtain direct bounds ae theta 
upper bound bayes ae inf theta dp gamma suffices choose distribution bound expectation dp du gamma set du case theta 
evaluate bounds fact gamma gammaj gamma gammaj gamma gamma gammaj set fl upper bounds comments theorem 
ff get log gamma fl bayes log log fl asymptotic scaling log risk observed 
gives factor difference compared risk smooth dimensional family densities 
consider example parameter space space observations infinite dimensional 
assume unknown real continuous function corrupted gaussian white noise process 
statistician observes random functions conditioned independent realizations process dz standard wiener process covariance min 
case easy calculate divergences explicitly ff 
measure corresponding random process dominating measure wiener measure 
cameron martin formula radon nikodym derivative dp exp oe dw gamma oe dx inserting definition divergences obtain ff jjp ff oe gamma dx case prior space functions gaussian measure realization gaussian random process bounds evaluated closed form 
restrict case mutual information theta fact gaussian processes gamma theta log theta gamma gamma dx log eigenvalues process interval 
specializing wiener process get gamma log gamma log cosh gamma tanh setting ff get log cosh oe oe tanh oe theta log cosh oe oe tanh oe asymptotically theta notice examples case asymptotically best bounds obtained value ff 
general large value laplace transform theta gamman gammaff ff jjp lower bound theorem largely determined ff jjp near zero close holds corresponding laplace transform theta jjp upper bound 
shown distributions get close sense dp dq uniformly jjq gamma ff ff jjq ff gamma ff expect get best asymptotic lower bound theorem choosing ff minimize ff gammaff choice desirable property mentioned ff distance ff corollary theorem squared hellinger distance nice metric properties exploit applications bounds 
reasons follows simplicity restrict case ff notation hl bounds cumulative risk countable theta recall assumed distinct theta conditional densities dp dp differ set positive measure dhl 
assumption essential loss generality replace theta set equivalence classes property iff dp dp set measure zero natural way changing risks interested calculating 
suppose theta countable say theta theta gamma log denote entropy random variable theta distributed prior measure entropy theta may infinite 
corollary bayes theta theta lim bayes theta proof recall bayes theta 
theta infinite clearly lim sup theta theta assume theta finite 
theta jy gamma dm jy log jy conditional entropy theta note quantity nonnegative 
theta finite easily verified theta theta gamma theta jy see lim sup theta theta case 
lower bound theorem ff fatou lemma lim inf theta lim inf gamma log gamma hl gamma lim inf log gamma hl gamma log theta result generalizes similar result corollary removing additional conditions assumed 
general results including corollary follow results pinsker book see 
applying theorem supremum corollary follows theta finite minimax log thetaj lim minimax log thetaj 
follows theta infinite lim minimax 
case theta finite results renyi show difference theta gamma theta converges zero exponentially fast obtain result follows 
corollary theta gamma theta thetaj gamma max jj thetaj dp dp proof theorem theta gamma log dp dp gamma log gamma log dp dp theta gamma dp dp theta gamma thetaj gamma max jj thetaj dp dp second inequality follows gamma log gammax 
assuming densities dp dp different application cauchy inequality yields dp dp corollary shows exponential convergence 
note theorem corollary characterize mutual information theta bayes risk general case theta uncountably infinite finite dimensional 
demonstrated :10.1.1.123.2638
sequel focus minimax risk 
bounds minimax risk covering packing numbers metric entropy theta dhl mentioned assume distinct states nature theta conditional distributions differ set positive measure 
assumption theta metric space 
show bounds minimax risk obtained looking properties metric space 
packing covering numbers associated metric entropy introduced kolmogorov commonly theory empirical processes see 
definitions ae complete separable metric space 
definition metric entropy called kolmogorov ffl entropy partition pi collection borel subsets pairwise disjoint union diameter set diam sup ae 
diameter partition supremum diameters sets partition 
ffl ffl ae denote cardinality smallest finite partition diameter ffl finite partition exists 
metric entropy ae defined ffl ae log ffl ae say totally bounded ffl ae ffl 
definition packing covering numbers ffl ffl cover subset exists ae ffl 
ffl ae denote cardinality smallest finite ffl cover finite cover exists 
ffl ffl separated subset subset distinct ae ffl 
ffl ae denote cardinality largest finite ffl separated subset arbitrarily large sets exist 
lemma easily verified 
lemma ffl ffl ae ffl ae ffl ae ffl ae follows metric entropy ffl condition defining total boundedness defined packing covering numbers place ffl constant factor ffl 
kolmogorov introduced notion dimension metric space seminal 
metric ae omitted notation understood context 
definition upper lower metric dimensions defined dim lim sup ffl ffl log ffl dim lim inf ffl ffl log ffl respectively 
dim dim value denoted dim called metric dimension dim lim ffl ffl log ffl totally bounded say finite dimensional dim infinite dimensional 
results theorems section ff obtain bounds minimax risk minimax terms metric entropy space theta 
ffl ffl supf dkl jjp hl theta hl fflg minimax ae inf sup theta dp gamma minimax analog bayes ae section obtain general bounds bayes risk 
minimax risk game studying relative entropy loss replaced affinity loss fixed number observations 
lemma assume theta totally bounded 

minimax sup ffl gamma log ffl theta gamma nffl sup ffl minfk ffl theta nffl gamma log 
minimax inf ffl ffl theta ffl nffl inf ffl ffl theta theta nffl furthermore minimax ae minimax inf ffl ffl theta ffl log minimax ae case function 
proof establish inequality part ffl separated subset theta maximal size discrete prior distribution theta uniform elements theorem corollary minimax bayes gamma theta log theta gamma nh gamma log gamma nh log gamma log gamma gamma nffl gamma log gamma nffl holds ffl follows minimax sup ffl gamma log ffl theta gamma nffl complete proof part simply note gamma log gamma log max gamma log minf gamma log gamma log yg 
follows minimax sup ffl ffl theta nffl gamma log ffl log ffl log ffl replacing ffl ffl second inequality follows 
turn upper bounds part 
pi partition theta diameter ffl 
prior measure theta 
theorem upper bound theorem follows 
minimax sup bayes sup ae gamma theta log theta kl jjp oe sup gamma log kl jjp sup gamma log gammab ffl nffl sup gamma log ffl nffl log ffl nffl second inequality follows ignoring ith term inner sum index outer sum noting diameter ffl dkl jjp ffl ffl ffl equality follows fact entropy finite distribution maximal uniform distribution 
particular partition diameter ffl chosen arbitrarily chain inequalities follows minimax ffl theta ffl nffl ffl 
establishes inequality part 
second inequality follows ffl theta ffl 
third inequality sup bayes ae place minimax ae follows argument similar inequality theorem 
maximin minimax sup bayes ae minimax ae obtain result stated theorem 
method obtaining upper bound result familiar see 
method obtaining lower bound choosing discrete prior separated set similar respects standard lower bound methods fano inequality lemma see method particularly clean framework giving fairly match upper bound 
cases ffl may continuous function ffl may obvious kinds asymptotic bounds risk minimax implied lemma 
cases definitions 
fix totally bounded theta continuous nondecreasing unbounded functions lim inf ffl ffl theta ffl lim sup ffl ffl theta ffl positive real ffl unique solution equation ffl nffl ffl unique solution equation ffl nffl ffl nffl ffl nffl lemma 
lemma integer 
lim inf minimax 
lim ffl ffl function lim sup minimax ng exists minimax ae lim sup minimax ng log proof lemma definitions lim inf minimax lim inf min ffl ffl min lim inf ffl lim inf ffl min lim inf ffl ng 
lim ffl ffl 
lim sup minimax ng lim sup ffl lim sup ffl lim sup inequality follows similarly inequality lemma 
essentially log close asymptotically arranged lemma shows asymptotic growth rates minimax obtained solving equation ffl theta nffl general approach developed le cam context loss functions 
illustrate applying lemma establish simple relationship metric dimension theta asymptotic growth rate minimax risk minimax theorem assume exists minimax ae 

theta finite minimax log thetaj 
dim theta minimax log 
dim theta minimax log 
dim theta theta totally bounded minimax minimax log 
theta totally bounded minimax minimax minimax proof mentioned corollary part follows corollary theorem 
parts second half part follow easily lemma plugging appropriate rates solving illustrate part similar 
dim theta may choose log solving log ffl nffl find ffl ffl log log upper bounds parts require assumption exists minimax ae 
lower bound lemma follows lim inf minimax log log second upper bound lemma follows lim sup minimax log log result part follows 
verify half part part note minimax risk minimax nondecreasing furthermore minimax finite grow linearly seen series inequalities 
minimax inf dist sup theta dkl jjr inf dist sup theta dkl jjq inf dist sup theta dkl jjq nr minimax theta minimax minimax finite bounded nr minimax theta totally bounded minimax part lemma 
minimax case 
theta totally bounded ffl theta infinite ffl 
case lower bound part lemma shows minimax nffl 
minimax case infinite 
theorem generalizes standard results case theta finite dimensional vector valued parameter space give information infinite dimensional case 
mentioned authors studied minimax risk infinite dimensional nonparametric density estimation loss functions related metric entropy theta hellinger distance 
lemma give general characterization asymptotic growth rate minimax risk minimax terms metric entropy theta infinite dimensional cases 
infinite dimensional cases growing log ffl metric entropy ffl theta usually grows ffl ff log ffl fi ff fi 
classical example 
theta lipschitz class ffi densities satisfying sup having derivatives dp order lipschitz condition th derivative gamma dp gamma ffi 
functions ffi uniformly bounded integrable envelope function minimax ae 
furthermore functions ffi uniformly bounded distances equivalent 
shown barron yang restriction uniformly lower bounded densities hellinger distance equivalent distances change metric entropy asymptotically 
result clements metric entropy theta distance ffl theta ffl gamma ffi ffl theta ffl gamma ffi asymptotic growth rate minimax risk minimax example determined consequence lemma 
theorem assume exists minimax ae 
continuous nondecreasing function defined positive reals fl 
lim cx fl 
lim cx log fl 
ffl theta ffl minimax 
ff ffl theta ffl ff ffl lim ffl ffl minimax ff ff ff ff lim inf minimax ff ff ff ff lim sup minimax ff ff ff ff log ff ff proof consider part 
ffl theta ffl ff ffl may choose ax ff bx ff suitable constants solving find ff ff property 
ffl ff ff property ff ff ff ff similar reasoning ff ff ff ff lower bound lemma property follows lim inf minimax ff ff ff ff second upper bound lemma follows unbounded increasing function lim sup minimax ff ff ng log ff ff log ff ff part follows easily property function 
part note upper bound lemma lim ffl ffl log factors removed lim sup yielding desired result 
part follows similar argument essentially setting ff terms denominators expressions go away tracking lim inf lim sup precisely 
note finite dimensional cases ffl log ffl ffl part theorem gives minimax log obtained previous theorem 
part generalizes infinite dimensional cases ffl log ffl fi fi 
illustrate part note case theta lipschitz class described uniform lower bound densities condition lim ffl ffl holds theorem fact ffl theta ffl gamma ffi get minimax ffi note ff lower bounds theorem show minimax approaches linear growth rate fastest possible finite minimax risk 
theorem covers interesting growth rates 
theorem applicable cases 
particular shown condition minimax ae result preceding results section removed 
example condition violated theta defined example 
case theta totally bounded minimax ffl ffl yield theorem estimated rate minimax factor course lower bounds theorem preceding results valid case special assumptions case see tight 
discussion open problems shown relatively weak assumptions particular exists distribution affinity uniformly bounded theta obtain explicit bounds mutual information theta true parameter observations terms laplace transform hellinger distance theta obtain bounds cumulative minimax risk estimating distribution theta relative entropy loss terms metric entropy theta respect hellinger distance 
fact case upper bounds depend assumptions lower bounds hold theta 
show example assumptions needed get type general characterizations mutual information minimax risk terms hellinger distance obtain 
remains open get useful characterization quantities cases assumptions hold get precise bounds 
show general bounds instantaneous risk estimating distribution various loss functions derived simple manner bounds cumulative relative entropy risk 
resulting bounds usually tight obtained direct methods specific theta approach advantage giving simple unified general treatment problem sophisticated mathematical methods jensen inequality needed derive results 
hope explore applications results specific estimation problems concept learning pattern classification problems examined current machine learning neural network research 
initial results lines see 
directions research pursue 
apart general tightening bounds include treating case observations extending results giving bounds individual theorems case distribution theta close distribution theta giving complete characterization mutual information theta terms metric entropy properties theta infinite dimensional case done finite dimensional case :10.1.1.123.2638
appendix give proof lemma 
lemma assume ff 
distributions dp du gamma gamma ffl ffl log log ffl log ffl ffl gammaff gammaff dkl jjq log ffl ff ffl ff ffl log ffl gamma ff ff ffl ffl ff ff gamma ff gamma gammaff gamma ff proof 
easily verified fact ff ff positive decreasing fy dp 
gamma dq dp du dp equations definition ff dkl jjq gammay dp ff ff dq consider cases gamma 
ffl ffl 
note gammaffl dr dp ffl du dp fflt case ffl ff ff ffl ffl log ffl gamma ff ffl log ffl ff ffl ff decreasing 

ffl ffl 
case ff log gamma ff log ff log ffl log ff log ffl ff ffl log ff inequality fact ffl ff decreasing previous inequality fact fflt log increasing 
ffl ffl ffl dp log follows dkl jjq log ffl ff ffl gammay dpf ff dq ffl log ffl ff ffl ff ffl ff gammay dpf ff dq log ffl ff ffl 
note ff gamma ff gamma dp ff gamma ffl dr gammaff gamma ff gamma dp ff gamma ffl dr gammaff gamma ff gamma dp ff dr gammaff gamma ff gamma gamma ffl gammaff ff gamma ff gamma gamma ffl gammaff ff ffl gamma ff dkl jjq log ffl ff ffl ff ffl log ffl gamma ff ff ffl ffl note ffl implies ffl log log ffl log ffl implies log fl fl fl ffl 
log log ffl log ffl ffl ffl ffl dp log ffl ffl dp ffl log ffl ffl ffl dp ffl ffl ffl dp du gamma ffl dp du gamma ffl result follows inequality 
authors andrew barron inspiring problems helpful discussions ideas 
shun ichi amari meir feder yoav freund michael kearns sebastian seung tom cover bin yu le cam helpful conversations laszlo anonymous referee comments earlier version 
barron van der meulen 
distribution estimation consistent total variation types information divergence 
ieee transactions information theory 
amari 
differential geometry curved exponential families curvatures information loss 
annals statistics 
amari murata 
statistical theory learning curves entropic loss 
neural computation 
barron 
strong ergodic theorem densities generalized shannon theorem 
annals probability 
barron 
cover gopinath editors open problems communication computation chapter 
bayes rules consistent information pages 

barron 
exponential convergence posterior probabilities implications bayes estimators density functions 
technical report dept statistics ill urbana champaign 
barron clarke haussler 
information bounds risk bayesian predictions redundancy universal codes 
proc 
international symposium information theory 
barron clarke haussler 
information bounds risk bayesian predictions redundancy universal codes 
proc 
international symposium information theory jan 
barron cover 
bound financial value information 
ieee trans 
information theory 
barron yang 
information theoretic lower bounds convergence rates nonparametric estimators 
unpublished manuscript 

approximation dans les th eorie de estimation 
zeitschrift fuer und gebiete 

estimating density hellinger distance strange facts 
probability theory related fields 
massart 
rates convergence minimum contrast estimators 
probability theory related fields 
cam 
extension wald theory statistical decision functions 
annals mathematical statistics 
cameron martin 
transformation wiener integrals translations 
ann 
math 
clarke 
asymptotic cumulative risk bayes risk entropy loss applications 
phd thesis dept statistics university ill 
clarke barron 
information theoretic asymptotics bayes methods 
ieee transactions information theory 
clarke barron 
prior asymptotically favorable entropy risk 
statistical planning inference 
clements 
entropy sets real valued functions 
pacific math 
cover thomas 
elements information theory 
wiley 
davisson leon garcia 
source matching approach finding minimax codes 
ieee transactions information theory 
devroye 
nonparametric density estimation view 
wiley 
diaconis freedman 
consistency bayes estimates 
ann 
statist 
dudley 
course empirical processes 
lecture notes mathematics 

information contained sequence observations 
problems information transmission 
feder freund mansour 
optimal universal learning prediction probabilistic concepts 
proc 
ieee information theory conference page 
ieee 
gallager 
source coding side information universal coding 
technical report lids mit laboratory information decision systems 
ghosh 
statistical decision theory related topics gupta berger editors stability convergence posterior non regular problems 
springer verlag 
gin zinn 
limit theorems empirical processes 
annals probability 

density estimation view kolmogorov ideas approximation theory 
annals statistics 
haussler 
general minimax result relative entropy 
ieee trans 
info th 
appear 
haussler barron 
bayes methods line prediction gamma values 
proceedings third nec symposium computation cognition 
siam 
haussler kearns schapire 
bounds sample complexity bayesian learning information theory vc dimension 
machine learning 
haussler opper :10.1.1.123.2638
general bounds mutual information parameter conditionally independent observations 
proceedings seventh annual acm workshop computational learning theory 
haussler opper 
mutual information metric entropy risk estimation probability distributions 
technical report ucsc crl univ calif computer research lab santa cruz ca 

information sample parameter 
second int 
symp 
information theory pages 

developments nonparametric density estimation 
jasa 
kolmogorov 
ffl entropy ffl capacity sets functional spaces 
amer 
math 
soc 
translations ser 


asymptotic properties predictive distributions 
technical report math 
eng 
physics 
ali van der meulen 
universal source code infinite alphabet 
ieee transactions information theory 

asymptotic methods statistical decision theory 
springer 
meir merhav 
stochastic complexity learning realizable unrealizable rules 
machine learning 
merhav feder 
strong version redundancy capacity theorem universal coding 
ieee trans 
info th 
opper haussler 
calculation learning curve bayes optimal classification algorithm learning perceptron noise 
proc 
th annu 
workshop comput 
learning theory pages san mateo ca 
morgan kaufmann 
opper haussler 
bounds predictive errors statistical mechanics supervised learning 
physical review letters 
pinsker 
information information stability random variables processes transl 
holden day 
pollard 
empirical processes theory applications volume nsf cbms regional conference series probability statistics 
institute math 
stat 
am 
stat 
assoc 
renyi 
measures entropy information 
editor fourth berkeley sym 
math stat 
prob pages 

renyi 
amount information concerning unknown parameter sequence observations 
publ 
math 
inst 

acad 
sci 
rissanen 
stochastic complexity modeling 
annals statistics 
rissanen speed yu 
density estimation stochastic complexity 
ieee trans 
info 
th 

proof refinements inequality feynman 
math 
phys 
van 
hellinger consistency certain nonparametric maximum likelihood estimators 
annals statistics 
wong shen 
probability inequalities likelihood ratios convergence rates sieve mle 
annals statistics 
yamanishi 
loss bound model line stochastic prediction algorithms 
information computation 
yu 
lower bounds expected redundancy nonparametric classes 
ieee trans 
info 
th 
zhu rohwer 
information geometric measurements generalization 
technical report ncrg aston england neural computing research group 

