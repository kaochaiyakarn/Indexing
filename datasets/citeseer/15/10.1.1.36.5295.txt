journal artificial intelligence research submitted published experimental evidence utility occam razor geoffrey webb webb deakin edu au school computing mathematics deakin university vic australia 
presents new experimental evidence utility occam razor 
systematic procedure post processing decision trees produced 
procedure derived rejecting occam razor attending assumption similar objects belong class 
increases decision tree complexity altering performance tree training data inferred 
resulting complex decision trees demonstrated average variety common learning tasks higher predictive accuracy complex original decision trees 
result raises considerable doubt utility occam razor commonly applied modern machine learning 

fourteenth century william occam stated plurality assumed necessity 
principle known occam razor 
occam razor originally intended basis determining ontology 
modern times widely reinterpreted adopted epistemological principle means selecting alternative theories ontologies 
modern occam razor widely employed classification learning 
utility principle subject widespread theoretical experimental attack 
adds debate providing experimental evidence utility modern interpretation occam razor 
evidence takes form systematic procedure adding non redundant complexity classifiers manner demonstrated frequently improve predictive accuracy 
modern interpretation occam razor characterized hypotheses explain simpler preferred 
specify aspect theory measured simplicity 
syntactic semantic epistemological pragmatic simplicity alternative criteria employed bunge 
practice common occam razor machine learning seeks minimize surface syntactic complexity 
interpretation addresses 
assumed occam razor usually applied expectation application general lead particular form advantage 
widely accepted articulation precisely occam razor applied advantages expected application classification learning 
literature contain statements capture widely adopted approach fl ai access foundation morgan kaufmann publishers 
rights reserved 
webb principle 
blumer ehrenfeucht haussler warmuth suggest occam razor adopt goal discovering simplest hypothesis consistent sample data expectation simplest hypothesis perform observations taken source 
quinlan states choice decision trees correct training set sensible prefer simpler grounds capture structure inherent problem 
simpler tree expected classify correctly objects outside training set 
statements necessarily accepted proponents occam razor capture form occam razor seeks address learning bias classifiers minimize surface syntactic complexity expectation maximizing predictive accuracy 
statements occam razor restrict classifiers correctly classify objects training set 
modern machine learning systems incorporate learning biases tolerate small levels misclassification training data clark niblett michalski quinlan example 
context extending scope definition decision trees classifiers general reasonable modify quinlan statement choice plausible classifiers perform identically training set simpler classifier expected classify correctly objects outside training set 
referred occam thesis 
concept identical performance training set defined different ways 
tempting opt definition requires identical error rates classifiers applied training set 
strict interpretation allow classifiers differing error rates long difference statistical confidence limit 
maximize applicability results adopt strict interpretation identical performance object training set classifiers provide classification noted occam thesis claiming classifiers equal empirical support complex greater predictive accuracy previously unseen objects 
claiming frequently complex higher predictive accuracy 
examines arguments occam thesis 
presents new empirical evidence thesis 
evidence acquired learning algorithm post processes decision trees learnt 
post processor developed rejecting occam thesis attending assumption similarity predictive class 
post processor systematically adds complexity decision trees altering performance training data 
demonstrated lead increase predictive accuracy previously unseen objects range real world learning tasks 
evidence taken incompatible occam thesis 
experimental evidence utility occam razor 
previous theoretical experimental provide context new evidence occam thesis worth briefly examining previous relevant theoretical experimental 
relevant outline provided reasons contribution may failed persuade side debate 
law conservation generalization performance conservation law generalization performance schaffer proves learning bias outperform bias space possible learning tasks follows occam razor valuable learning bias subset possible learning tasks 
argued set real world learning tasks subset 
predicated accepting proposition set real world learning tasks distinguished set possible learning tasks respects render conservation law inapplicable 
rao gordon spears argue case learning tasks universe uniformly distributed space possible learning tasks 

argument support proposition follows 
real world learning tasks defined people machine learning systems 
task constructors sought ensure independent variables class attributes related dependent variables attributes ways captured space classifiers available learning system 
actual machine learning tasks drawn randomly space possible learning tasks 
human involvement formulation problems ensures 
simple thought experiment support proposition consider learning task class attribute generated random number generator way relates attributes 
majority machine learning researchers slightest systems failed perform trained data 
example consider learning task class attribute simple count number missing attribute values object 
assume learning task submitted system quinlan develops classifiers mechanism testing classification attribute value missing 
majority machine learning researchers systems failed perform circumstances 
machine learning simply unsuited tasks 
knowledgeable user apply machine learning data expectation obtaining useful classifier 
explores applicability occam thesis real world learning tasks 
theoretical objections occam thesis machine learning systems explicitly implicitly employ occam razor 
addition universal machine learning principle occam razor widely 
law proved discrete valued learning tasks reason believe apply continuous valued tasks webb accepted general scientific practice 
persisted despite occam razor subjected extensive philosophical theoretical empirical attack suggests attacks persuasive 
philosophical front summarize bunge complexity theory classifier depends entirely language encoded 
claim acceptability theory depends language happens expressed appears 
obvious theoretical relationship syntactic complexity quality theory possibility world intrinsically simple occam razor enables discovery intrinsic simplicity 
world intrinsically simple reason simplicity correspond syntactic simplicity arbitrary language 
merely state complex explanation preferable specify criterion preferable 
implicit assumption underlying machine learning research appears things equal complex classifiers general accurate blumer quinlan 
occam thesis seeks 
straight forward interpretation syntactic measure predict expected accuracy appears absurd 
classifiers identical meaning age pos age age pos possible accuracies differ matter greatly complexities differ 
simple example highlights apparent dominance semantics syntax determination predictive accuracy 
previous experimental evidence occam thesis empirical front number experimental results appeared conflict occam thesis 
murphy pazzani demonstrated number artificial classification learning tasks simplest consistent decision trees lower predictive accuracy slightly complex consistent trees 
experimentation showed results dependent complexity target concept 
bias simplicity performed target concept best described simple classifier bias complexity performed target concept best described complex classifier murphy 
addition simplest classifiers obtained better average consistent classifiers predictive accuracy data augmented irrelevant attributes attributes strongly correlated target concept required classification 
webb results suggest wide range learning tasks uci repository learning tasks murphy aha relative generality classifiers better predictor classification performance relative surface syntactic complexity 
argued results demonstrate strategy selecting simplest pair theories lead maximization predictive accuracy demonstrate selecting simplest available theories fail maximize predictive accuracy 
schaffer shown pruning techniques reduce complexity decreasing resubstitution accuracy increase predictive accuracy experimental evidence utility occam razor decrease predictive accuracy inferred decision trees 
proponent occam thesis explain results terms positive effect application occam razor reduction complexity counter balanced negative effect reduction empirical support resubstitution accuracy 
holte acker porter shown specializing small disjuncts rules low empirical support exclude areas instance space occupied training objects frequently decreases error rate unseen objects covered disjuncts 
specialization involves increasing complexity viewed contrary occam thesis 
research shows total error rates classifiers disjuncts embedded increases disjuncts specialized 
proponent occam thesis dismiss relevance results arguing thesis applies complete classifiers elements classifiers 
theoretical experimental support occam thesis theoretical experimental objections occam thesis exists body apparent theoretical empirical support 
attempts provide theoretical support occam thesis machine learning context blumer pearl fayyad irani 
proofs apply equally systematic learning bias favors small subset hypothesis space 
argued equally support preference classifiers high complexity schaffer berkman sandholm 
holte compared learning simple classification rules sophisticated learner complex decision trees 
number tasks uci repository machine learning datasets murphy aha simple rules achieved accuracies percentage points complex trees 
considered supportive occam thesis 
case simple rules outperform complex decision trees 
demonstrated exist learning bias consistently outperformed studied 
final argument considered support occam thesis majority machine learning systems employ form occam razor appear perform practice 
demonstrated better performance obtained occam razor abandoned 

new experimental evidence occam thesis theoretical experimental objections occam thesis appear greatly diminished machine learning community occam razor 
seeks support objections occam thesis robust general experimental counter evidence 
presents systematic procedure increasing complexity inferred decision trees modifying performance training data 
procedure takes form post processor decision trees produced quinlan 
application procedure range learning tasks uci repository learning tasks murphy aha demonstrated result average webb increased predictive accuracy inferred decision trees applied previously unseen data 
theoretical basis decision tree post processor similarity assumption common assumption machine learning objects similar high probability belonging class rendell 
techniques described rely assumption theoretical justification occam thesis 
starting similarity assumption machine learning viewed inference suitable similarity metric learning task 
decision tree viewed partitioning instance space 
partition represented leaf contains objects similar relevant respects expected belong class 
raises issue similarity measured 
instance learning methods aha kibler albert tend map instance space ndimensional geometric space employ geometric distance measures space measure similarity 
approach problematic number grounds 
assumes underlying metrics different attributes 
possible determine priori difference years age signifies greater lesser difference similarity difference inch height 
second assumes possible provide priori definitions similarity respect single attribute 
really universal prescription value similar value value 
case relevant similarity metric log surface value case similar 
wish employ induction learn classifiers expressed particular language appear forced assume language question manner captures relevant aspect similarity 
potential leaf decision tree presents plausible similarity metric objects fall leaf similar respect 
empirical evaluation performance leaf training set infer relevance similarity metric induction task hand 
leaf covers large number objects class classes provides evidence similarity respect tests define predictive illustrates simple instance space partition quinlan imposes thereon 
note forms nodes continuous attributes consist test cut value test takes form respect cut value attribute infers relevant similarity metric relates attribute 
partition shown dashed line placed value attribute accept occam thesis accept similarity assumption reason believe area instance space lightly shaded belong class determined class 
uses occam thesis justify termination partitioning instance space soon decision tree accounts adequately training set 
consequence experimental evidence utility occam razor 
simple instance space large areas instance space occupied objects training set may left partitions similarity assumption provides little support 
example respect argued relevant similarity metric respect region similarity respect entire instance space objects values belong class 
objects 
contrast objects values provide evidence objects area instance space belong class 
tests represents plausible similarity metric basis available evidence 
object region similar plausible respect positive negative objects 
objects similar relevant respects high probability belonging class information available plausible object similar positive negative objects appear probable object negative positive 
disagreement similarity assumption case contrasts example area instance space 
region similarity assumption suggests partition appropriate plausible similarity metrics indicate object region similar positive objects post processor developed research analyses decision trees produced order identify regions occupied objects training set evidence terms similarity assumption favoring relabeling 
provide example implausible similarity metric consider similarity metric defined root node similar 
plausible great level dissimilarity classes respect metric 
relevant similarity metric distribution training examples representative distribution objects domain similarity assumption violated similar objects probability just belonging class 
probability calculated follows 
probabilities object respectively 
object probability belonging class object similar 
object probability belonging class object similar 
probability object belonging class similar object theta theta 
numbers involved simple example course small reach high level confidence example intended illustrative 
webb different class assigned 
regions identified new branches added decision tree creating new partitionings instance space 
trees provide identical performance respect training set regions instance space occupied objects training set affected 
difficult see plausible metric complexity interpret addition branches increasing complexity tree 
result post processor adds complexity decision tree altering tree applies training data 
occam thesis predicts general lower predictive accuracy similarity assumption predicts general increase predictive accuracy 
seen prediction consistent experimental evidence 
post processor process applied continuous discrete attributes current implementation addresses continuous attributes 
post processor operates examining leaf tree turn 
attribute considered turn 
possible thresholds region instance space occupied objects explored 
minimum min maximum max determined values possible objects reach lies branch split threshold split provides upper limit max values lies branch threshold provides lower limit min 
node lie branch max 
node lie branch min gamma 
objects training set values range min max considered operations 
value observed training set attribute allowable range outside actual range values objects evidence evaluated support region threshold 
level support threshold evaluated laplacian accuracy estimate niblett bratko 
leaf relates binary classification object belongs class question binary form laplace 
threshold attribute leaf evidence support labeling partition class maximum value ancestor node formula number objects min number objects belong class evidence support labeling partition threshold calculated identically exception objects max considered 
maximum evidence new labeling exceeds evidence current labeling region new branch added appropriate threshold creating new leaf node labeled appropriate class 
addition evidence favor current labeling gathered evidence support current labeling region calculated laplace accuracy experimental evidence utility occam razor estimate considering objects leaf number objects leaf number objects belong class node labeled 
approach ensures new partitions define true regions 
attribute value possible partition possible objects domain values greater objects values equal reach node partitioned objects training set fall new partition 
particular ensures new cuts simple duplications existing cuts ancestors current node 
modification adds non redundant complexity tree 
algorithm 
implemented modification release called 
source code modifications available line appendix 
multiple sets values equally satisfy specified constraints maximize laplace function values deeper tree selected closer root single node preference values depends order attributes definition data preference values dependent data order 
selection strategies side effect implementation system 
reason believe experimental results differ general strategies select competing constraints 
default develops decision trees time run unpruned pruned simplified decision tree 
produces post processed versions trees 
evaluation evaluate post processor applied datasets containing continuous attributes uci machine learning repository murphy aha held due previous machine learning experimentation local repository deakin university 
datasets believed broadly representative repository 
experimentation eleven data sets additional data sets sick euthyroid discordant results retrieved uci repository added study order investigate specific issues discussed 
resulting thirteen datasets described table 
second column contains number attributes object described 
proportion continuous 
fourth column indicates proportion attribute values data missing unknown 
fifth column indicates number objects data set contains 
sixth column indicates proportion belong class represented objects data set 
final column indicates number classes data set describes 
note glass type dataset uses float float class classification commonly class classification 
data set divided training evaluation sets times 
training set consisted data randomly selected 
evaluation set consisted remaining data 
applied resulting data sets trials training evaluation set pairs 
webb cases denote set training examples reach node value denote value attribute training example pos denote number objects class set training examples laplace pos jxj set training examples jxj number training examples class 
denote minimum value cut attribute ancestor node lies branch 
cut 
determines upper bound values may reach denote maximum value cut attribute ancestor node lies branch 
cut gamma 
determines lower bound values may reach post process leaf dominated class 
find values ancestor continuous attribute cases value min cases value class maximize laplace fx cases value value 

find values ancestor continuous attribute cases value max cases value class maximize laplace fx cases value value 

laplace cases replace node test ii 
set branch lead new leaf class iii 
set branch lead laplace cases replace node test ii 
set branch lead new leaf class iii 
set branch lead post processing algorithm experimental evidence utility occam razor table uci data sets experimentation 
contin 
common 
name attrs 
uous missing objects class classes breast cancer wisconsin cleveland heart disease credit rating discordant results echocardiogram glass type hepatitis hungarian heart disease hypothyroid iris new thyroid pima indians diabetes sick euthyroid table summarizes percentage predictive accuracy obtained unpruned decision trees generated 
presents mean standard deviation set trials respect data set results tailed matched pairs test comparing means 
twelve thirteen data sets obtained higher mean accuracy 
remaining data set hypothyroid obtained higher mean predictive accuracy cs albeit small margin measured decimal places respective mean accuracies respectively 
data sets advantage statistically significant level advantage respect discordant results data small apparent measured decimal place measured decimal places values respectively 
advantage hypothyroid data statistically significant level 
differences mean predictive accuracy hungarian heart disease new thyroid sick euthyroid data sets significant level 
table uses format table summarize predictive accuracy obtained pruned decision trees generated 
twelve data sets obtained higher mean predictive accuracy 
remaining data set hypothyroid obtained higher mean predictive accuracy magnitude difference small apparent level precision displayed measured decimal places mean accuracies 
data sets advantage statistically significant level difference apparent precision decimal places discordant results data respectively 
advantage hypothyroid data statistically significant level 
differences webb table percentage predictive accuracy unpruned decision trees 
name breast cancer wisconsin cleveland heart disease credit rating discordant results echocardiogram glass type hepatitis hungarian heart disease hypothyroid iris new thyroid pima indians diabetes sick euthyroid table percentage accuracy pruned decision trees 
name breast cancer wisconsin cleveland heart disease credit rating discordant results echocardiogram glass type hepatitis hungarian heart disease hypothyroid iris new thyroid pima indians diabetes sick euthyroid breast cancer wisconsin echocardiogram hungarian heart disease iris new thyroid sick euthyroid statistically significant level 
completing experimentation initial eleven data sets results hypothyroid data stood stark contrast 
raised possibility distinguishing features hypothyroid data experimental evidence utility occam razor accounted difference performance 
table indicates data set clearly distinguishable initial data sets respects ffl having attributes ffl containing greater proportion discrete attributes directly addressed ffl containing objects ffl having greater proportion objects belong common class ffl having classes ffl producing decision trees extremely high predictive accuracy post processing 
explore issues discordant results sick euthyroid data sets retrieved uci repository added study 
data sets identical hypothyroid data set exception different class attribute 
data sets contain objects described attributes 
addition discordant results sick euthyroid data little illuminate issue 
data sets changes accuracy small magnitude 
hypothyroid significant advantage 
sick euthyroid significant advantage system 
discordant results data significant advantage 
question distinguishing feature hypothyroid data explains observed results remains unanswered 
investigation issue lies scope current remains interesting direction research 
results suggest post processing frequently increases predictive accuracy type data uci repository 
comparisons significant increase fifteen significant decrease 
sign test reveals rate success significant level 
tables summarize number nodes decision trees developed 
table addresses unpruned decision trees table addresses pruned decision trees 
postprocessing modification replaces single leaf split leaves 
modification performed leaf original tree 
data sets decision trees significantly complex original decision trees 
cases post processing increased mean number nodes decision trees approximately 
demonstrates post processing causing substantial change 

discussion primary objective research occam thesis 
uses post processor disregards occam thesis theoretically founded similarity assumption 
experimentation post processor webb table number nodes unpruned decision trees 
name breast cancer wisconsin cleveland heart disease credit rating discordant results echocardiogram glass type hepatitis hungarian heart disease hypothyroid iris new thyroid pima indians diabetes sick euthyroid table number nodes pruned decision trees 
name breast cancer wisconsin cleveland heart disease credit rating discordant results echocardiogram glass type hepatitis hungarian heart disease hypothyroid iris new thyroid pima indians diabetes sick euthyroid demonstrated possible develop systematic procedures range realworld learning tasks increase predictive accuracy inferred decision trees result changes substantially increase complexity altering performance training data 
general difficult attack occam thesis due absence widely agreed formulation thereof 
far apparent occam thesis experimental evidence utility occam razor 
modified simple instance space recast accommodate experimental results provide practical learning bias 
directions research implications research reach relevance occam razor 
postprocessor appears practical utility increasing quality inferred decision trees 
objective research improve predictive accuracy occam thesis post processor modified number ways 
modification enable addition multiple partitions single leaf original tree 
selects single modification maximum support 
design decision originated desire minimize likelihood performing modifications decrease accuracy 
principle appear desirable select modifications strong support inserted tree order level supporting evidence 
greater increases accuracy expected removed constraint post processing alter performance decision tree respect training set 
case new partitions may employ objects regions instance space provide evidence support adding partitions correct misclassifications small numbers objects leaf node original tree 
similarity assumption provide strong evidence repartitioning 
situation occur example respect learning problem illustrated additional object class attribute values 
illustrated 
case create indicated partitions 
unable relabel area containing additional object due constraint alter performance original decision tree respect training set 
addition object prevents relabeling shaded region basis similarity assumption improves evidence support relabeling 
extended post processor encourage model inductive inference decision trees 
role similar system identify clusters webb objects instance space grouped single leaf node 
second stage analyze regions instance space lie outside clusters order allocate classes regions 
current decision tree learners motivated occam thesis ignore second stage leaving regions outside identified clusters associated classes assigned product cluster identification process 
related research number researchers developed learning systems viewed considering evidence neighboring regions instance space order derive classifications regions instance space occupied examples training set 
ting explicitly examining training set directly explore neighborhood object classified 
system uses instance learning classification nodes decision tree low empirical support small disjuncts 
number systems viewed considering evidence neighboring regions classification 
systems learn apply multiple classifiers ali brunk pazzani gascuel oliver hand 
context point region instance space occupied training objects covered multiple leaves rules 
leaf rule greatest empirical support classification 
uses distinct criteria evaluating potential splits 
standard stage tree induction employs information measure select splits 
post processor uses laplace accuracy estimate 
similar uses dual criteria investigated 
quinlan employs laplace accuracy estimate considering neighboring regions instance space estimate accuracy small disjuncts 
brodley employ resubstitution accuracy select splits near leaves induction decision trees 
adding split leaf specializing respect class leaf generalizing respect class new leaf 
holte 
explored number techniques specializing small disjuncts 
differs leaves candidates specialization just low empirical support 
differs manner selects specialization perform considering evidence support alternative splits just strength evidence support individual potential conditions current disjunct 
bias versus variance breiman friedman olshen stone provide analysis complexity induction terms trade bias variance 
classifier partitions instance space regions 
regions large degree fit accurate partitioning instance space poor increasing error rates 
effect called bias 
regions small probability individual regions labeled wrong class increased 
effect called variance increases error rates 
analysis due variance fine partitioning instance space tends increase experimental evidence utility occam razor error rate due bias coarse partitioning tends increase error rate 
increasing complexity decision tree creates finer partitionings instance space 
analysis argue addition undue complexity decision trees ground increase variance error rate 
success decreasing error rate demonstrates successfully managing bias variance trade introduces complexity decision tree 
evidence neighboring regions instance space successful increasing error rate resulting variance lower rate decreases error rate resulting bias 
success demonstrates adding undue complexity decision trees 
minimum encoding length induction minimum encoding length approaches perform induction seeking theory enables compact encoding theory available data 
key approaches developed minimum message length mml wallace boulton minimum description length mdl rissanen 
approaches admit probabilistic interpretations 
prior probabilities theories data minimization mml encoding closely approximates maximization posterior probability wallace freeman 
mdl code length defines upper bound unconditional likelihood rissanen 
approaches differ mdl employs universal prior rissanen explicitly justifies terms occam razor mml allows specification distinct appropriate priors induction task 
practice default prior usually employed mml appears derive justification occam razor 
mdl mml default prior add complexity decision tree doing justified solely basis evidence neighboring regions instance space 
evidence study appears support potential desirability doing 
casts doubt utility universal prior employed mdl default prior usually employed mml respect maximizing predictive accuracy 
noted probabilistic interpretation minimum encoding length techniques indicates encoding length minimization represents maximization posterior probability unconditional likelihood 
maximization factors necessarily directly linked maximizing predictive accuracy 
appropriate application grafting pruning important note calls question value learning biases penalize complexity way provide support learning biases encourage complexity sake 
new nodes decision tree empirical support doing 
results way argue appropriate decision tree pruning 
generate pruned trees removes branches statistical estimates upper bounds error rates indicate increase branch removed 
webb argued reduces complexity empirical support doing 
interesting note thirteen data sets examined post processing pruned trees resulted higher average predictive accuracy post processing unpruned trees 
results suggest pruning grafting play valuable role applied appropriately 

presents systematic procedure adding complexity inferred decision trees altering performance training data 
procedure demonstrated lead increases predictive accuracy range learning tasks applied pruned unpruned trees inferred 
thirteen learning tasks examined procedure lead statistically significant loss accuracy case magnitude difference mean accuracy extremely small 
face provides strong experimental evidence occam thesis 
post processing technique developed rejecting occam thesis attending similarity assumption similar objects high probability belonging class 
procedure developed constrained need ensure revised decision tree performed identically original decision tree respect training data 
constraint arose desire obtain experimental evidence occam thesis 
possible constraint removed basic techniques outlined result greater improvements predictive accuracy reported 
research considered version occam razor favors minimization syntactic complexity expectation tend increase predictive accuracy 
interpretations occam razor possible minimize semantic complexity 
bunge provided philosophical objections formulations occam razor sought investigate 
version occam razor examined research widely machine learning apparent success 
objections principle substantiated research raise question apparent success flawed 
webb suggests apparent success principle due manner syntactic complexity usually associated relevant qualities inferred classifiers generality prior probability 
thesis accepted key challenges facing machine learning understand deeper qualities employ understanding place machine learning theoretical footing 
offers small contribution direction demonstrating minimization surface syntactic complexity general maximize predictive accuracy inferred classifiers 
important realize thrust notwithstanding occam razor useful learning bias employ 
frequently pragmatic reasons preferring simple hypothesis 
simple hypothesis general easier understand communicate employ 
preference simple experimental evidence utility occam razor hypotheses justified terms expected predictive accuracy may justified pragmatic grounds 
research supported australian research council 
am grateful charlie david dowe doug ross quinlan anonymous reviewers extremely valuable comments benefited greatly 
aha kibler albert 

instance learning algorithms 
machine learning 
ali brunk pazzani 

learning multiple descriptions concept 
proceedings tools artificial intelligence new orleans la berkman sandholm 

minimized decision tree re examination 
technical report university massachusetts amherst computer science department amherst mass blumer ehrenfeucht haussler warmuth 

occam razor 
information processing letters 
breiman friedman olshen stone 

classification regression trees 
wadsworth international belmont ca 
brodley 

automatic selection split criterion tree growing node selection 
proceedings twelth international conference machine learning pp 
city ca 
morgan kaufmann 
bunge 

myth simplicity 
prentice hall englewood cliffs nj 
clark niblett 

cn induction algorithm 
machine learning 
fayyad irani 

minimized decision tree 
aaai proceedings eighth national conference artificial intelligence pp 
boston ma 


mathematical theory explanation statistical applications 
proceedings royal society london series 
holte 

simple classification rules perform commonly datasets 
machine learning 
holte acker porter 

concept learning problem small disjuncts 
proceedings eleventh international joint conference artificial intelligence pp 
detroit 
morgan kaufmann 
webb 

increasing performance consistency classification trees accuracy criterion leaves 
proceedings twelth international conference machine learning pp 
city ca 
morgan kaufmann 
michalski 

theory methodology inductive learning 
michalski carbonell mitchell 
eds machine learning artificial intelligence approach pp 

springer verlag berlin 
murphy 

empirical analysis benefit decision tree size biases function concept distribution 
tech 
rep department information computer science university california irvine 
murphy aha 

uci repository machine learning databases 
machine readable data repository 
university california department information computer science irvine ca 
murphy pazzani 

exploring decision forest empirical investigation occam razor decision tree induction 
journal artificial intelligence research 
niblett bratko 

learning decision rules noisy domains 

ed research development expert systems iii pp 

cambridge university press cambridge 
gascuel 

learning decision committees 
proceedings twelth international conference machine learning pp 
city ca 
morgan kaufmann 
oliver hand 

pruning averaging decision trees 
proceedings twelth international conference machine learning pp 
city ca 
morgan kaufmann 
pearl 

connection complexity credibility inferred models 
international journal general systems 
quinlan 

induction decision trees 
machine learning 
quinlan 

learning logical definitions relations 
machine learning 
quinlan 

improved estimates accuracy small disjuncts 
machine learning 
quinlan 

programs machine learning 
morgan kaufmann los altos 
rao gordon spears 

generalization action really equal opposite reaction 
analysis conservation law generalization performance 
proceedings twelth international conference machine learning pp 
city ca 
morgan kaufmann 
experimental evidence utility occam razor rendell 

learning hard concepts constructive induction framework rationale 
computational intelligence 
rissanen 

universal prior integers estimation minimum description length 
annals statistics 
rissanen 

stochastic complexity 
journal royal statistical society series 
schaffer 

sparse data effect overfitting avoidance decision tree induction 
aaai proceedings tenth national conference artificial intelligence pp 
san jose ca 
aaai press 
schaffer 

overfitting avoidance bias 
machine learning 
schaffer 

conservation law generalization performance 
proceedings international conference machine learning san mateo ca 
morgan kaufmann 
ting 

problem small disjuncts remedy decision trees 
proceedings tenth canadian conference artificial intelligence pp 

morgan kaufmann 
wallace boulton 

information measure classification 
computer journal 
wallace freeman 

estimation inference compact coding 
journal royal statistical society series 
webb 

generality significant complexity alternatives occam razor 
zhang lukose 
eds ai proceedings seventh australian joint conference artificial intelligence pp 

world scientific 

