networks spiking neurons third generation neural network models wolfgang maass institute theoretical computer science technische universitat graz graz austria mail maass igi tu graz ac tel 
fax august computational power formal models networks spiking neurons compared neural network models mcculloch pitts neurons threshold gates respectively sigmoidal gates 
particular shown networks spiking neurons regard number neurons needed computationally powerful neural network models 
concrete biologically relevant function exhibited computed single spiking neuron biologically reasonable values parameters requires hundreds hidden units sigmoidal neural net 
hand known function computed small sigmoidal neural net computed small network spiking neurons 
article assume prior knowledge spiking neurons contains extensive list currently available literature computations networks spiking neurons relevant results neurobiology 
keywords spiking neuron integrate fire neuron computational complexity sigmoidal neural nets lower bounds definitions motivations classifies neural network models computational units distinguish different generations 
generation neurons computational units 
referred perceptrons threshold gates 
give rise variety neural network models multilayer perceptrons called threshold circuits hopfield nets boltzmann machines 
characteristic feature models give digital output 
fact universal computations digital input output boolean function computed multi layer perceptron single hidden layer 
second generation computational units apply activation function continuous set possible output values weighted sum polynomial inputs 
common activation functions sigmoid function oe gammay linear saturated function piecewise polynomial activation functions consider piecewise exponential activation functions pieces defined expressions involving exponentiation definition oe 
typical examples networks second generation feedforward recurrent sigmoidal neural nets networks radial basis function units 
nets able compute help thresholding network output arbitrary boolean functions 
shown neural nets second generation compute certain boolean functions fewer gates neural nets generation 
addition neural nets second generation able compute functions analog input output 
fact universal analog computations sense continuous function compact domain range approximated arbitrarily regard uniform convergence norm network type single hidden layer 
characteristic feature second generation neural network models support learning algorithms gradient descent backprop 
biological interpretation neural nets second generation views output sigmoidal unit representation current firing rate biological neuron 
biological neurons especially higher cortical areas known fire various intermediate frequencies minimum maximum frequency neural nets second generation regard firing rate interpretation biologically realistic models generation 
regard fast analog computations networks neurons cortex firing rate interpretation questionable 
perrett 
thorpe 
demonstrated visual pattern analysis pattern classification carried humans just msec spite fact involves minimum synaptic stages retina temporal lobe 
speed visual processing measured rolls 
macaque monkeys 
furthermore shown single cortical area involved visual processing complete computation just msec 
hand firing rates neurons involved computations usually hz msec needed just sample current firing rate neuron 
coding analog variables firing rates quite dubious context fast cortical computations 
simultaneous recordings seconds firing times neurons monkey striate cortex kruger 
firing denoted short vertical bar separate row neuron 
comparison marked length interval msec vertical lines 
time span known suffice completion complex multilayer cortical computations 
hand experimental evidence accumulated years indicates biological neural systems timing single action potentials spikes encode information 
experimental results neurobiology lead investigation third generation neural network models employ spiking neurons integrate fire neurons computational units 
started carry experiments related new types electronic hardware pulse stream vlsi see 
new chips encode analog variables time differences pulses practical advantages encoding methods 
goal understanding capabilities limitations new type analog neural hardware provides additional motivation theoretical investigation third generation neural network models 
may view threshold circuits neural nets generation models digital computation networks spiking neurons bit coded firing neuron certain short time window non firing neuron time window see 
coding scheme threshold circuit provides reasonably model network spiking neurons firing times neurons provide input bits spiking neuron synchronized msec 
apparently strongly synchronized activity occur biological neural systems see argue typical mode operation 
mathematical models integrate fire neurons spiking neurons called traced back see 
exist number variations model described compared survey see 
regard relationship mathematical models known behaviour biological neurons refer 
mathematical models spiking neurons provide complete description extremely complex computational function biological neuron 
computational units previous generations neural network models simplified models focus just aspects biological neurons 
comparison previous models substantially realistic 
particular describe better actual output biological neuron allow investigate theoretical level possibilities time resource computation communication 
timing computation steps usually models preceding generations assumed synchronization assumed stochastic asynchronicity timing individual computation steps plays key role computations networks spiking neurons 
fact output spiking neuron consists set points time fires fx 
simplest deterministic model spiking neuron assumes neuron fires potential models electric membrane potential trigger zone neuron reaches certain threshold theta potential sum socalled excitatory postsynaptic potentials epsp inhibitory postsynaptic potentials result firing neurons connected synapse neuron firing presynaptic neuron time contributes potential time amount modelled term delta gamma consists weight response function gamma biologically realistic shapes response functions indicated 
typical shape response functions epsp biological neuron 
weight term delta gamma reflects strength called efficacy neurobiology synapse neuron neuron context learning replace function addition conjectured rapid changes value essential computations biological neural systems 
simplicity view just constant 
restriction non negative values motivated assumption biological synapse excitatory inhibitory change sign course learning process 
addition biological neurons response functions gamma postsynaptic neurons excitatory positive inhibitory negative 
obviously constraints basically impact theoretical complexity investigations just consider pairs excitatory inhibitory neurons single neurons cares small constant factors size networks wants model actual architecture cortical circuits see 
mathematically convenient assume potential value absence postsynaptic potentials threshold value theta typical biological neuron resting membrane potential mv firing threshold neuron mv postsynaptic potential epsp changes membrane potential temporarily mv 
neuron fired time fire msec matter large current potential absolute refractory period 
msec reluctant fire firing requires larger value usual relative refractory period 
refractory effects modelled suitable threshold function theta gamma time firing deterministic noise free version spiking neuron model assumes fires crosses function theta gamma typical shape function theta gamma biological neuron indicated 
assume theta gamma theta large values gamma consider article computations models networks spiking neurons assume neuron fire gamma large threshold function returned resting value theta shape theta relevant arguments provided theta theta sufficiently large typical shape threshold function biological neuron 
formal spiking neuron network snn introduced consists finite set spiking neurons set theta synapses weight response function synapse hu vi fx threshold function theta neuron set firing times neuron potential trigger zone neuron time hu vi fu delta gamma noise free model neuron fires time soon reaches theta gamma time firing specified subset input input neurons assumes firing times spike trains neurons input defined preceding convention outside 
firing times neurons determined previously described rules output network form spike trains neurons specified set output neurons output experiments shown vitro biological neurons fire slightly varying delays response repetitions current injection 
certain conditions neurons known fire reliable manner 
considers stochastic noisy version snn model difference gamma theta gamma just governs probability neuron fires time choice exact firing times left unknown stochastic processes may example occur fire time interval gamma theta gamma fires spontaneously time gamma theta gamma previously described noisy version snn model basically identical spike response model common mathematical models networks spiking neurons see 
subtle differences exist models regard treatment refractory effects reset membrane potential firing 
differences irrelevant results considered article 
theoretical results stable states synfire chains associative memory networks spiking neurons refer 
results computations stochastic spiking neurons firing rate coding results information transmitted spiking neurons 
computations somewhat different model stochastic spiking neuron studied see discussion 
possible phases periodically firing neurons dynamic binding variables investigated 
article terms analog numerical real valued interchangeably denote variables range interval simplicity assume neural nets generations considered feedforward architecture 
simulation separation results mathematically simplest range snn models firing deterministic response functions threshold functions piecewise constant step functions indicated 
refer version type version snn model captures quite intended capabilities artificial spiking neurons pulse stream vlsi 
discuss snn models type assume threshold functions continuous piecewise linear 
examples simplest nontrivial response functions type indicated 
linear segments quite response threshold functions biological neurons continuous piecewise linear functions spiking neurons type 
response threshold functions spiking neuron type computation boolean functions observe case boolean input model computationally powerful neural nets generation 
assume input bits snn input neurons fires specific time fire assume output bit snn firing non firing specified output neuron specified time window 
simulate layered feedforward neural net generation snn type basically architecture wants respect biologically motivated constraint neuron trigger epsp gate simulated pair consisting excitatory inhibitory spiking neuron get input 
need possibility assign different values delays delta neuron model time passes firing effect see different neurons hu vi biological neuron delays delta may different depending length axon distance synapse trigger zone distribution ion channels dendritic tree fact frequently assumed delays delta delta parameters tuned learning algorithm biological neural systems see 
theoretical results indicate expressive power neuron type variable delays larger neuron type variable weights vc dimension theta log case theta case 
possibility employ certain neurons different delays delta different neurons show snn type fact computationally powerful neural nets similar size second generation 
purpose consider concrete boolean function cd defined cd ng function appears relevant biological context formalizes form pattern matching respectively coincidence detection 
single spiking neuron type reasonable type compute cd just choose delays input nodes input nodes way delta delta delta larger delta nonzero parts response functions overlap fire simultaneously 
weights chosen equal 
side point single spiking neuron type type compute function cd noise robust fashion small deviations firing times input neurons delays input neurons weights firing threshold affect correctness output 
achieve suffices assign firing threshold theta spiking neuron value delta maximal value epsp 
theorem threshold circuit computes cd log gates 
sigmoidal neural net piecewise polynomial activation functions computes cd omega gamma gates 
case piecewise exponential activation functions oe gets lower bound omega gamma 
proof input nodes receives values input variables 
show fact slightly stronger result claimed lower bounds hold numbers gates direct edge input nodes case layered neural nets lower bounds number gates hidden layer 
consider computations fixed vector assigned input nodes output may viewed function assignments input nodes consider set assignments exactly input variables value computes cd obvious different choices network computes different functions proof part fix linear order oe computation nodes computation node receives apart input nodes edges computation nodes precede linear order 
consider arbitrary computation node set assignments computation node computes function regard assignments inputs input nodes assignments note computation node set assignments values received gate computation nodes depend chosen assignment weighted sum values received direct edges input nodes computation nodes precede oe assumes different values different assignments arbitrary assignments obviously output depends value weighted sum weighted sum values receives direct edges input nodes theta threshold threshold gate minimal theta assume different values including value theta consequently different fixed assignments node compute different functions yields partition equivalence classes apply argument equivalence classes node regard linear order oe 
starts construction computation node th node gets partition equivalence classes 
hand fact computes cd implies output node computes assignment different function partitions different equivalence classes number computation nodes direct edge input nodes satisfies log proof part construct related sigmoidal neural net show high vc dimension contain substantial number sigmoidal gates 
proof structure koiran somewhat different context 
considers just input nodes different fixed assignments shift threshold computation nodes direct edges consider variation input nodes deleted thresholds abovementioned gates viewed programmable parameters weights usual sense vc dimension theory neural networks brief survey see 
fact computes cd implies shatters regard different assignments programmable parameters 
vc dimension hand results goldberg jerrum karpinski macintyre imply case number programmable parameters satisfies case piecewise polynomial activation functions respectively case piecewise exponential activation functions 
computation functions analog input boolean output shown boolean inputs network spiking neurons type full computational power neural net generation similar size fact powerful 
neural nets generations able process numerical inputs just boolean inputs networks spiking neurons natural encode numerical input variable firing time input gamma delta input neuron see constant input parameter depends time input arrives values input variables similarly expects numerical output realized snn firing certain output neuron time output gamma delta output input independent values input variables 
refer method encoding analog variables timing single spikes linear temporal coding 
computation functions boolean output employ output convention apply rounding considers firing output neuron certain fixed time output 
concrete example interesting function analog input boolean output element distinctness function ed defined ed jx gamma arbitrary encodes value input variable firing time input gamma delta input neuron sufficiently large values constant single spiking neuron compute ed delta delta ng holds reasonable type response function type type considered 
point ed computed single spiking neuron noise robust fashion 
max maximal value assumed epsp maximal value achieved sum epsp arrive temporal difference choosing value theta delta max firing threshold neuron achieves definitely fires definitely fire jx gamma different inputs temporal coding 
addition choice theta neuron gives correct output membrane potential firing threshold arrival times input epsp subject noise 
furthermore safety margin delta max gamma increased value max chosen large max noise robust computation ed spiking neuron possible way function ed defined gamma jg value input hx matter neuron fires 
clause arbitrary definition ed sure hair trigger situations avoided spiking neuron computes ed theorem layered threshold circuit computes ed omega gamma delta log gates hidden layer 
proof number gates hidden layer 
corresponding halfspaces partition input space different polytopes intersections halfspaces gives output inputs polytope 
consideration allow polytopes intersections closed open halfspaces 
consider 
inputs delta delta delta ng represent 
permutations ng suffices show lies different polytope implies 
assume contradiction permutations lie polytope construction threshold circuit gives output convex gives output points line connects points 
yields contradiction ed ed ed point line order analyze complexity functions boolean output sigmoidal neural nets needs fix suitable convention rounding real valued output nets 
order subsequent lower bound result strong possible may assume weakest possible rounding convention arbitrary parameter theta real valued output output node net rounded theta separating interval required outputs rounded respectively way cd show neural net second generation computes ed needs omega gamma gates 
lower bound improved gamma theorem 
proof stronger separation result exploits bound vc dimension sontag better upper bound maximal number set different inputs general position shattered sigmoidal neural net programmable parameters 
order apply result lower bound argument construct arbitrary sigmoidal neural net computes ed related net shatters set gamma inputs 
theorem sigmoidal neural net computes ed gamma gamma hidden units 
proof arbitrary sigmoidal neural net gates computes ed consider set size gamma 
sufficiently large numbers delta pairwise distance set gamma numbers max delta pairwise distance assumption decide arbitrary inputs delta different 
variation weights edges input variable multiplied assigning suitable fixed sets gamma pairwise different numbers delta gamma input variables computes characteristic function considers programmable parameters weights edges input variable thresholds gates connected gamma input variables shatters programmable parameters 
general setting subsequent argument programmable parameters occurrences factor weights may counted single programmable parameter 
set size gamma chosen arbitrarily apply result sontag implies gamma gamma gamma computation nodes gamma gamma hidden units 
remarks 
lower bound omega gamma theorem largest lower bound size sigmoidal neural nets far achieved just ed concrete function 
best previously known lower bound omega gamma function due koiran 

result section sontag implies upper bound lower bound preceding theorem remain valid neural net computing ed employs sigmoidal gates threshold gates 
apparently neurons cortex weights synapses large just synchronous epsp suffice increase potential firing threshold theta neuron regard common mathematical model spiking neuron overestimates computational capabilities biological neuron 
realistic assume simultaneously arriving epsp cause neuron fire see discussion 
consider variation ed function ed ed exists value interval length contains values input variables arbitrary common model spiking neuron membrane potential assumed linear sum postsynaptic potentials 
certainly idealization isolated epsp arrive synapses far away trigger zone located axon subject exponential decay way trigger zone 
isolated epsp hardly impact membrane potential trigger zone 
hand epsp arrive synchronously adjacent synapses boosted hot spots dendritic tree may significant impact membrane potential trigger zone 
defined ed way spite nonlinear effects integration epsp quite plausible biological neuron compute ed temporal coding fairly large value neuron computing ed needs fire blocks consisting adjacent synapses receive synchronous epsp 
furthermore hair trigger situation avoided requirements case neuron receives just synchronous synchronous epsp 
non firing required case neuron receives epsp time interval length order prove lower bound number hidden units arbitrary neural nets compute ed sigmoidal threshold gates proceeds proof theorem 
considers arbitrary sets size gamma divides remaining gamma input variables gamma blocks variables receive input value 
variation identifies input variables multiplies weights common factor computes ed network computation nodes shatters help programmable parameters 
sontag result yields gamma gamma plugs common estimate number synapses biological neuron preceding inequality yields lower bound number gamma hidden units prefers plug somewhat different values abovementioned constants preceding proof ed respectively variation ed reflects different choices parameters involved yields lower bound hundreds minimal size sigmoidal neural net computes function 
demonstrated substantial difference computational power biological neurons sigmoidal neurons computational units second generation 
numerical inputs previously sketched simulation threshold circuits neural nets generation network spiking neurons type fails 
surprisingly prove simulation possible 
function 
numerical inputs exists way simulating arbitrary threshold circuit gates network spiking neurons type consider threshold circuit outputs inputs 
obviously achieved circuit just threshold gates circuit outputs shown function restriction fl fl computed network spiking neurons type matter neurons computation time employs 
follows general characterization computational power networks spiking neurons type numerical inputs terms computational power restriction called gamma ram common model random access machine ram 
arrived limit computational power spiking neurons type numerical inputs 
question arises limitation indicates weakness spiking neurons general just weakness extremely simple response threshold functions type answering question consider spiking neurons continuous piecewise linear piecewise constant response threshold functions refer spiking neurons type examples simplest nontrivial response functions type spiking neuron indicated 
regard computational power spiking neurons type difference allows piecewise constant piecewise linear general types threshold functions theta long consider feedforward computations threshold functions theta value small arguments 
addition concrete shape response functions type irrelevant 
response functions epsp spiking neuron type particular shape triangle important results article 
show contrast abovementioned negative result neural nets type network spiking neurons response functions type indicated simulate threshold gate real valued input variables 
simulation exploits important effect spiking neurons type realized spiking neurons type incoming epsp shift firing time neuron continuous manner 
precisely certain range parameters involved firing time neuron response firings presynaptic neurons times input gamma delta written form output gamma hu vi sign delta delta output depend values sign case epsp sign gamma case 
neuron outputs weighted sum hu vi sign delta delta temporal coding response analog inputs temporal coding 
equation reveals somewhat surprising fact context temporal coding weights synapses spiking neurons able play role computational units generations neural network 
subsequent layers hidden layer layered neural net generation receive just boolean inputs network inputs real valued 
subsequent layers easily simulated spiking neurons type indicated 
subtle serious problem arises wants simulate threshold circuits boolean inputs outputs type boolean circuit spiking neurons type response functions substantially closer biological prototypes response functions type obvious spiking neuron type simulate boolean gate receives synchronized input spikes 
problem layer spiking neurons type receives boolean input synchronized input spikes coding spike corresponds spike corresponds neurons layer fire synchronized manner slightly different times depend concrete input bits 
root problem arise spiking neurons type fact potential sum epsp type continuous piecewise linear slopes linear pieces depend particular number epsp receives simultaneously concrete boolean input interpretation 
precise time crosses threshold theta general depend boolean input spiking neuron 
causes serious problem simulation multilayer threshold circuits multilayer boolean circuits snn type neurons considered layer firing represent simulation boolean circuit fire synchronized manner simulation threshold gates simpler boolean gates layer spiking neurons type impossible 
theorem threshold circuit gates having real valued inputs simulated network spiking neurons type proof consider arbitrary threshold gate inputs hx outputs ff ff 
show simulated network having constant number spiking neurons type regard temporal coding network inputs sufficiently small value constant 
employs construction simulation linear respectively sigmoidal gate yields spiking neuron firing time represents weighted sum ff temporal coding 
particular fires fixed time depend ff ff time 
arrange resulting epsp arrives subsequent spiking neuron receives addition epsp auxiliary spiking neuron firing time depends input suitable choice weights delays neuron fire fires time obviously simulate way layer threshold circuit order simulate subsequent layers spiking neurons type employ construction 
previously described spiking neurons represent outputs gates layer firing corresponding gate outputs precise time fires case depends boolean outputs gates inputs spiking neurons type simulate subsequent layers construction employ synchronization module constructed proof theorem 
results networks spiking neurons type shown preceding section contrast snn type networks spiking neurons type simulate neural nets generation case real valued network input 
question arises networks spiking neurons type simulate respectively approximate neural nets second generation real valued input output 
question answered affirmatively showing regard temporal coding real valued variables continuous function approximated arbitrary closely regard uniform convergence hidden layer network spiking neurons type fact result holds just simple scheme linear temporal coding described section scheme coding analog variables timing single spikes continuously related scheme 
example holds neuron fires time gamma delta encode analog number gammax addition exists evidence practically relevant analog function approximated small networks spiking neurons type large number results regarding practical applications learning backprop sigmoidal neural nets suggest relevant target functions applications learnt approximated sigmoidal neural nets small number sigmoidal gates 
additional empirical evidence suggests precise form sigmoidal function important number sigmoidal gates needed 
argue target functions arise application problems general approximated quite sigmoidal neural nets small number sigmoidal units employ linear saturated activation function approximation result implies case approximated quite network spiking neurons type may say regard circuit complexity computing analog functions networks spiking neurons type powerful neural nets second generation 
furthermore previously described lower bounds size neural nets generations nets compute functions cd ed ed imply networks spiking neurons type fact strictly powerful neural nets generations order achieve separation results snn type neural nets generations just remains verify single spiking neuron type single spiking neuron type compute cd ed ed refer details proofs abovementioned simulation results 
seen proofs positive results computational power snn type require response threshold functions piecewise linear type 
suffices assume epsp small linearly increasing segment small linearly decreasing segment 
properties approximately satisfied epsp biological neurons see 
complete characterization computational power snn type terms restriction called ram familiar model random access machine 
addition shown simulation sigmoidal neural nets snn carried biologically realistic model stochastic noisy spiking neuron 
easy see functions cd ed ed considered computed single noisy spiking neuron type furthermore shown noisy spiking neurons type principle carry arbitrary digital computations desired degree reliability 
noise certainly affects computational power networks spiking neurons analog input refer regard limits computational power networks noisy spiking neurons analog input 
analyzed article computational power networks spiking neurons regard temporal coding single spikes 
turns computational model computational power neural nets generations multilayer perceptrons sigmoidal neural nets similar size 
furthermore exhibited concrete functions require computation significantly fewer neurons network spiking neurons 
proof theorem appears independent interest theory sigmoidal neural nets provides strongest lower bound result sigmoidal neural nets currently known 
improves largest previously known lower bound omega gamma omega gamma new lower bound result interest technical point view provides known application results sontag dimension neural nets 
new notion dimension neural net certain sense dual familiar concept vapnik chervonenkis dimension neural net replaces exists set inputs 
sets inputs 
definition dimension 
article indicate theoretical investigation networks spiking neurons new research topic 
fact long tradition theoretical neurobiology biophysics theoretical physics 
mathematically rigorous analysis computational power networks spiking neurons far missing 
believe analysis helpful understanding organization computations complex biological neural systems 
addition analysis appears helpful evaluating potential capabilities various designs artificial networks spiking neurons particular silicon implementations integrated circuits compute pulses 
example results article show exist drastic differences computational capabilities networks spiking neurons operate rectangular pulses type operate triangular pulses type 
eduardo sontag anonymous referee helpful comments 
abeles neural circuits cerebral cortex cambridge university press 
abeles bergman margalit spatiotemporal firing patterns frontal cortex behaving monkeys neurophysiology vol 
pp 
ed brain theory spatio temporal aspects brain function elsevier 
barrow paradigm logical performance training recurrent refractory neural networks neural parallel scientific computations vol 
pp 
arbib ed handbook brain theory neural networks cambridge 
koch newsome reliable temporal modulation cortical spike trains awake monkey proc 
symposium dynamics neural processing washington usa 
bialek rieke reliability information transmission spiking neurons trends neuroscience vol 
pp 
bienenstock model neocortex network computation neural systems vol 
pp 
bower book genesis exploring realistic neural models general neural simulation system springer new york 
churchland sejnowski computational brain mit press 
bialek non boltzmann dynamics networks spiking neurons advances neural information processing systems vol 
morgan kaufmann san mateo pp 
dasgupta schnitger power approximating comparison activation functions advances neural information processing systems vol 
morgan kaufmann san mateo pp 
fields design fabrication test new vlsi hybrid analog digital neural processing element ieee trans 
neural networks vol 
pp 
douglas koch martin recurrent excitation neocortical circuits science vol 
pp 
ferster cracking neuronal code science vol 

gerstner associative memory network biological neurons advances neural information processing systems vol 
morgan kaufmann san mateo pp gerstner time structure activity neural network models phys 
rev vol 
pp 
gerstner van hemmen describe neuronal activity spikes rates assemblies advances neural information processing systems vol 
morgan kaufmann san mateo pp 
gerstner ritz van hemmen biologically motivated analytically soluble model collective oscillations cortex theory weak locking biol 
cybern vol 
pp 
goldberg jerrum bounding vapnik chervonenkis dimension concept classes parameterized real numbers machine learning vol 
pp 
herrmann hertz bennett analysis synfire chains preprint www dk preprints html 
hopfield pattern recognition computation action potential timing stimulus representations nature vol 
pp 
hopfield herz rapid local synchronization action potentials computation coupled integrate fire neurons proc 
natl 
acad 
sci vol 
pp 
moore koch delay line motion detection chip advances neural information processing systems vol 
morgan kaufmann san mateo pp 
jahnke roth simd dataflow architecture neurocomputer spike processing neural networks pp 
leong analog vlsi time encoded pattern classifier proc 
th australian conference neural networks canberra australia pp 
johnston wu foundations cellular neurophysiology cambridge 
judd pulse propagation networks neural network model uses temporal coding action potentials neural networks vol 
pp 
karpinski macintyre polynomial bounds vc dimension sigmoidal general pfaffian neural networks appear comp 
system sciences 
kempter gerstner van hemmen wagner temporal coding sub millisecond range model barn owl auditory pathway advances neural information processing systems vol 
mit press cambridge pp 
koch poggio multiplying synapses neurons single neuron computation mckenna davis eds academic press boston pp 
koiran vc dimension circuit complexity proc 
th ieee conference computational complexity pp 
kruger investigation monkey striate cortex spike train correlations layers neurophysiology vol 
pp 
recherches sur excitation des comme une polarization physiol 

gen vol 
pp 
lin pinkus multilayer feedforward networks activation function approximate function neural networks vol 
pp 
determination precision spike timing visual cortex cats biol 
cybernetics vol 
pp 
maass vapnik chervonenkis dimension neural nets handbook brain theory neural networks arbib ed mit press cambridge pp 
maass computational complexity networks spiking neurons advances neural information processing systems vol 
mit press cambridge pp 
maass lower bounds computational power networks spiking neurons neural computation vol 
pp 
maass analog computations networks spiking neurons proc 
th italian workshop neural nets world scientific press 
maass computational power noisy spiking neurons advances neural information processing systems vol 
mit press cambridge pp 
maass efficient implementation sigmoidal neural nets temporal coding noisy spiking neurons appear neural computation 
ftp host archive cis ohio state edu ftp filename pub maass sigmoidal spiking ps maass ruf relevance shape postsynaptic potentials computational power spiking neurons proc 
international conference artificial neural networks icann ec cie paris pp 
maass schnitger sontag computational power sigmoid versus boolean threshold circuits proc 
nd annual ieee symposium foundations computer science pp 
extended version appeared theoretical advances neural computation learning roychowdhury siu editors kluwer academic publishers boston pp 
maass effect analog noise discrete time analog computations advances neural information processing systems vol 
mit press cambridge 
maass schmitt bounds vc dimension spiking neuron variable delays preparation 
vlsi analogs neuronal visual processing synthesis form function phd dissertation cal inst 
technology 
analog vlsi system stereoscopic vision kluwer academic publishers boston 
sejnowski reliability spike timing neocortical neurons science vol 
pp 
mead analog vlsi neural systems addison wesley reading 
wu cole programmable impulse neural circuits ieee trans 
neural networks vol 
pp 
murray tarassenko analogue neural vlsi pulse stream approach chapman hall 
elias discrimination spike patterns dendritic processing network silicon proceedings th annual conference computational neuroscience academic press san diego appear 
perrett rolls visual neurons responsive faces monkey temporal cortex experimental brain research vol 
pp 
pratt pulse computation phd thesis dept elec 
eng 
comp 
sci mit cambridge usa 
rieke van bialek spikes exploring neural code mit press cambridge 
ritz gerstner van hemmen biologically motivated analytically soluble model collective cortex ii 
applications binding pattern segmentation biol 
cybernetics vol 
pp 
rolls brain mechanisms invariant visual recognition learning behavioural processes vol 
pp 
rolls processing speed cerebral cortex neurophysiology visual backward masking proc 
roy 
soc 
vol 
pp 
sejnowski time new neural code nature vol 
pp 
shastri ajjanagadde simple associations systematic reasoning connectionist representation rules variables dynamic bindings temporal synchrony behavioural brain sciences vol 
pp 
shawe taylor jeavons van probabilistic bit stream neural chip theory connection science vol 
pp 
shepherd neurobiology rd ed oxford university press new york 
shepherd ed synaptic organization brain rd ed oxford university press new york 
singer synchronization neuronal responses putative binding mechanism handbook brain theory neural networks arbib ed mit press cambridge pp 
sub millisecond coincidence detection active dendritic tree neuroscience vol 
pp 
sontag shattering sets points general position requires gamma parameters preprint feb see www math rutgers edu sontag papers html 
stevens zador information spiking neuron advances neural information processing systems vol 
mit press cambridge pp 
taylor mathematical analysis competitive network attention mathematical approaches neural networks taylor ed north holland amsterdam pp 
thorpe biological constraints connectionist modelling connectionism perspective pfeifer steels eds elsevier north holland pp 
tuckwell theoretical neurobiology vol 
cambridge university press cambridge 
valiant circuits mind oxford university press 
newcomb eds silicon implementations pulse coded neural networks kluwer academic publishers 
zhao stochastic bit stream neural networks theory simulations applications phd 
thesis dept comp 
sci royal holloway university london 

