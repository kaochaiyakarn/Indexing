information geometry em em algorithms neural networks shun ichi amari department mathematical engineering information physics faculty engineering university tokyo ku tokyo japan supported part aid scientific research priority areas higher order brain functions ministry education science culture japan 
requests reprints sent author department mathematical engineering university tokyo ku tokyo japan fax 
running title geometry em algorithm information geometry em em algorithms neural networks shun ichi amari order realize input output relation noise contaminated examples effective stochastic model neural networks 
model network includes hidden units activation values specified observed 
useful estimate hidden variables observed specified input output data stochastic model 
algorithms em em algorithms far proposed purpose 
em algorithm iterative statistical technique conditional expectation em algorithm geometrical information geometry 
em algorithm minimizes iteratively kullback leibler divergence manifold neural networks 
algorithms equivalent cases 
gives unified information geometrical framework studying stochastic models neural networks em em algorithms proves condition guarantees equivalence 
examples include boltzmann machines hidden units mixtures experts stochastic multilayer perceptron normal mixture model hidden markov model 
key words em algorithm information geometry stochastic model neural networks learning identification neural network projection projection hidden variable neural networks remarked universal approximators nonlinear functions trained examples input output data 
data includes noise input output relation described stochastically terms conditional probability yjx output input 
neural networks stochastic nature boltzmann machine behaviors described probability distributions stochastic dynamics 
network deterministic effective train stochastic network behaves deterministically execution mode 
stochastic model deterministic neural networks 
suggests usefulness statistical ideas neural networks see amari cheng titterington ripley white 
quite different important idea developing theory neural networks originates geometry 
consider neural network including modifiable parameters connection weights summarized vector form 
set possible neural networks realized changing forms dimensional manifold plays role coordinate system called manifold neural networks 
geometry neural manifold useful understanding total capability class networks 
network stochastic nature network accompanied probability distribution conditional probability distribution yjx 
information geometry amari csisz ar connects sources ideas 
originated information structure manifold probability distributions developed new mathematical subject new differential geometrical notions 
successfully applied various information sciences statistical sciences amari barndorff nielsen cox reid murray rice kass amari information theory amari han amari systems theory amari amari 
applications neural networks started amari amari 
unifies statistical geometrical ideas studying neural networks including hidden units unobservable variables trying establish new information geometric framework 
learning neural networks desired inputoutput relation specified examples activation values hidden units unobserved left unspecified 
convenient determined filled adequately total network behavior agreement input output data 
hidden variable problem regarded kind credit assignment problem 
approaches far proposed solve problem 

consider manifold related conditional probability distributions realizable neural networks 
realizable neural networks form submanifold neural networks hand observed data suggests distribution empirical distribution observation specification incomplete observed specified partial data defines candidates points manifold form submanifold best neural network minimizes distance realizable observed kullback leibler divergence distance measure 
dual alternative minimization problem network minimizes divergence selected optimal 
time point minimizes divergence gives estimated data complementing partial observed data 
approach proposed csisz ar 
idea amari byrne 
see amari neal hinton 
may called em algorithm realized geometrically geodesic geodesic projections explained 
approach statistical em expectation maximization algorithm 
iterative algorithm obtaining maximum likelihood estimator conditional expectation missing data choose candidate point em algorithm applied boltzmann machines byrne hierarchical mixture experts jordan jacobs see jacobs mixtures experts 
new formulation acceleration method proposed connection neal hinton 
see jordan xu xu jordan 
elucidates relation statistical em algorithm geometrical em algorithm reported short notes amari 
cases prove sufficient condition guarantees equivalence 
give simple example algorithms give different solutions 
geometric method easier understand characteristics em algorithm various versions 
propose learning version em algorithm data observed sequentially 
learning algorithm general slower batch algorithm accumulated data available time 
flexible changing environment algorithm simpler 
number examples explain information geometry approach 
examples important 
boltzmann machines hidden units hidden units improve conditional probability distribution output units conditioned inputs 

mixtures expert networks input signal space automatically divided regions signals region processed expert network corresponding region 
hidden input output data signal processed expert network 
self organizing network proposed jacobs generalized jordan jacobs 
see jordan xu xu jordan hinton 

stochastic multilayer perceptron 
theory gives new learning algorithm different backpropagation method 
new algorithm flexible better global convergence property backpropagation independent study stochastic perceptron rumelhart personal communication 

normal mixture 
studied statistics 
radial basis function method closely related 
neal hinton gave new interpretation connecting geometry 
intend detailed study interesting models related algorithms aims theoretical elucidation geometrical structures underlying em em algorithms information geometry em algorithm 
geometrical aspects suggests various new algorithms including learning 
study computational aspect algorithms shows computer simulated results 
remain studied separately applying framework proposed 
applications hidden markov random fields geman geman besag green geman kehagias dynamics boltzmann machines asymmetric connections important subjects research 
organized follows 
sections devoted exponential family curved exponential family basic statistical models 
various important neural networks boltzmann machine multilayer perceptron mixture expert networks shown described statistical framework 
section statistical preliminary likelihood estimation explained terms geometry 
readers familiar neural networks statistics may skip sections 
introduces main framework theory 
set neural networks shown embedded manifold related probability distributions submanifold 
partial observed data defines submanifold problem find network filled data minimize kullback leibler divergence statistical em algorithm geometrical em algorithm introduced section 
analyzed section main part terms information geometry 
algorithms shown equivalent practical cases section 
section treats learning procedures section studies dual geodesic gradient flow appendix gives short intuitive information geometry 
exponential families neural networks exponential families consider family probability distributions random variable may vector probability density functions specified dimensional parameter 
probability density functions written form expf functions family fp called exponential family cox barndorff nielsen 
may treat new vector random variable distribution specified expf respect suitable measure 
treats case vector composed visible part hidden part 
case 
set probability distributions regarded dimensional manifold space plays role coordinate system introduced point distribution specified amari 
called natural canonical parameter exponential family 
simple examples example 
normal distributions 
normal random variable subject oe mean variance oe oe oe exp oe set normal distributions forms dimensional manifold coordinate system oe 
exponential family putting oe oe distribution rewritten expf respect delta measure ffi dr dr oe log oe log log example 
discrete distributions 
discrete random variable values set ng 
probability vector ig distribution specified vector satisfying 
set discrete distributions fpg dimensional manifold 
fig shows triangle 
exponential family 
putting log ffi ffi ffi expf log exp example 
stable distributions boltzmann machines 
boltzmann machine stochastic neural network consisting neurons 
neurons connected symmetric connection weights ij ackley hinton sejnowski aarts korst 
self connection zero ii 
put ii sake notational convenience bias term ith unit 
state boltzmann machine takes binary values 
state transition boltzmann machine stochastic forming markov chain 
stable distribution state boltzmann machine connection matrix written expf ij set boltzmann machines 
plays role coordinate system correspondence boltzmann machines probability distributions form identified set probability distributions form 
set exponential family expf ij ij ij ij ij ij ij expf normalization constant corresponding free energy statistical physics 
generating function statistics 
example 
normal mixture hidden variables 
consider normal distributions subject oe discrete random variable values kg probabilities ig real random variable depending subject normal distribution oe joint distribution written ffi oe expf oe usually observed marginal distribution oe expf oe called normal mixture distribution 
form regarded radial basis expansion 
logarithm probability written eliminating ffi ffi log oe oe ffi log log oe oe oe oe xffi oe oe ffi oe oe log oe oe putting oe oe ffi log oe oe oe oe xffi oe oe ffi oe oe show model exponential family expf ij ij set normal mixtures exponential family 
example 
markov chain 
consider ergodic markov chain finite state set fs sequence states assume initial state sake simplicity 
state transition matrix ji ji jy probability random sequence logarithm log log ji numbers state transition ji fy random variables depending rewritten ji log ji ij independent constraints ij ji number state process fy tg precisely holds exactly holds error depending state matter treat asymptotic properties markov chain large 
exact theory constructed framework curved exponential family 
variables ij rewritten terms ij eliminating ij ji log probability written ji ji log log log log ji log ji log log log putting ji ji log set markov chains represented exponential family expf set random markov fields represented exponential form 
mixtures expert neural nets example 
mixture expert nets 
simplest example mixtures expert nets jacobs jordan jacobs 
various generalizations including hierarchical mixture jordan jacobs 
generalization shown example 
stochastic neural networks called experts receiving common input emitting binary output simplest case assume simple stochastic neuron emits binary output depending weighted sum input amari 
probability written jx sigmoidal function exp yu exp exp family conditional distributions exponential family mixture experts composed expert networks gating network selects expert nets processing input receives input output random variable values kg 
output gating network gating network decides signal processed network final output equal case fig 
general output written ffi output gating network determined stochastically softmax function ijx exp xg connection weight vector ith gating output 
loss generality may put invariant transformation joint probability zjx expf ffi yw exp exp putting ffi ffi zjx expf family fp zjx conditional distributions mixtures experts exponential family fixed 
consider case various input subsection 
shown simplest case binary stochastic neuron 
jacobs jordan jacobs treated general case conditional distribution output belongs general exponential family example jx expf binary neuron case putting exp typical example analog stochastic neuron output written sigmoidal function random noise subject oe 
case oe oe oe ff log oe joint probability mixture general written zjx expf ij ij conditional probability total net exponential family 
mixture experts stochastic model 
trained examples stochastic techniques 
deterministically mode 
case expert emits expectation stochastic gating network gives weights ffi final output weighted sum possible gating network plays winner take rule choosing candidate network largest probability example 
mixture hierarchical general gating net 
important mixtures exponential family expert nets exponential family 
jordan jacobs showed hierarchical mixture successively constructed mixtures expert nets component expert nets 
exponential family higher order hierarchical mixture constructed manner 
show hierarchical step 
mixtures experts nets consisting expert nets 
output gating network construct mixture variable total gating network 
mixture selected network selected final output equal output ffi ffi shows hierarchical structure generalize gating mechanism selected hierarchically depending xu jordan hinton proposed different gating mechanism computationally tractable 
propose gating mechanism 
component expert nets 
denote variable gating network takes values set mg set partitioned subsets fi fi 
takes value component network selected ffi ffi 
exponential family conditional distribution obtained similar way zjx expf ffi yw statistics ffi ffi case similar replaced gating net hierarchical structure simple flexible 
explain deterministic limit softmax function replaced max function winner take mechanism 
gating network maximum selector divides set fxg input signals subregions signal processed known gating network realizes laguerre voronoi division zhuang amari little general voronoi division 
hierarchical mixture divides region laguerre voronoi subregions depending values belongs processed hierarchical structure capable forming complex division simple laguerre voronoi convex 
hand proposed net divides laguerre voronoi regions region signals processed consists unions corresponding subregions non convex case universal sense division approximated method sufficiently large 
gating network realizes flexible complex division simple manner 
product space conditional probability zjx depends input signal mixture expert nets 
set conditional distributions forms exponential family canonical parameter depends shown expf 
neural network trained giving various input signals corresponding output behaviors fixed 
independent data joint conditional distribution written pf jx expf forms exponential family larger dimensions 
fp manifold conditional probability distributions corresponding tth input joint conditional distributions product space random variable canonical parameter rewritten expf take arbitrary values determined restricted common network parameters form case mixture experts 
possible restricted subregion words distributions realized neural networks cover entire exponential family limited subregion submanifold curved exponential family explained section 
curved exponential families curved exponential families curved submanifold exponential family called curved exponential family 
fp dimensional exponential family dimensional submanifold 
um parameter coordinate system curved exponential family fp consists distributions expf function distributions parameterized distribution belongs coordinates point terms geometry submanifold fig 
composed points written parametric representation inner coordinate system simple examples example 
normal multiplication model 
normal random variable subject normal distribution mean variance 
input signal magnitude unknown system order know signal amplified damped 
signal contaminated noise input 
multiplied unknown quantity want know 
final response signal 
problem estimate amplification factor observed subject 
set distributions curved exponential family embedded fn oe distribution mean variance oe independent specified common oe forms curve coordinate terms natural parameter coordinates points satisfy shape example 
multiple observations 
consider multiple observation expf coordinates product space parameters underlying neural network consisting example mixture expert nets distribution realized neural net parameter coordinates form 
set distributions forms submanifold curved exponential family inner coordinate system dimension number increases limit tends infinity number network parameters fixed 
stable distributions boltzmann machines example 
boltzmann machine general probability distributions 
order study behaviors boltzmann machines hidden units need introduce larger manifold consisting probability distributions state space fxg amari 
distributions realizable boltzmann machines stable distributions 
distribution takes expand log expf xx expf shows set distributions exponential family coordinates ij random variables ij ik distributions realized boltzmann machines part stable distributions boltzmann machines form expf ij ij putting ii ij ij see set probability distributions realizable boltzmann machines submanifold defined ij ij inner coordinates ij linear submanifold case forms exponential family 
curved flat submanifold stochastic multilayer perceptron example 
stochastic multilayer perceptron cf 
amari 
consider hidden layer perceptron single output unit fig 
input vector outputs hidden units binary values 
probability input written synaptic weight vector th hidden element 
threshold term included adding constant input ij output unit receives signal hidden layer emits binary output probability depending hidden signal yj synaptic weight vector output unit 
constant input added bias term included input signal conditional probability variables zj jx denotes parameter summarizing wm wm randomly generated subject probability distribution total probability distribution written zj may unknown 
logarithm zj log zj log zj log see maximizing logarithm total probability respect maximizing logarithm conditional probability 
yv exp exp log denote values dimensional vector components take 
introducing delta function ffi relations ffi exp ffi exp ffi note equal corresponding bias term put consistency 
regard ffi random variables dimensional vector random variable indexed values 
log likelihood rewritten zjx ffi exp ffi exp show family conditional distributions fp zjx curved exponential family 
put xffi exp ge corresponding bias term 
vector valued 
rjx entirely free non linear vector function wm 
curved exponential family 
case multiple observations joint distributions form curved exponential family embedded product space 
noted depend special case 
possible condense product space single shown section 
remarked learning takes place stochastic model perceptron examples generated noiseless deterministic network jw jvj diverge infinity 
need control carefully magnitudes parameters 
remarked neural models exponential curved exponential families 
show analog stochastic perceptron see amari 
input output hidden units final output 
analog sigmoidal function example exp output th hidden unit final output independent normal random noises subject oe 
zj exp oe fz yj exp oe fy logarithm joint conditional probability distribution written oe zjx yf ff summarized form bilinear form extended random variable function parameters 
distributions zjx belong curved exponential family 
possible generalize information geometry responsible cases introducing manifold functions mention see amari amari 
information geometry em algorithm applicable case csisz ar 
remarked stochastic perceptron reduces ordinary analog perceptron stochastic outputs replaced expected values 
ordinary multilayer perceptron trained stochastic method training phase 
case analog multilayer perceptron conventional backpropagation learning rule designed minimize empirical error fy specified output output network parameter analog stochastic perceptron execution mode expected value output output ff deterministic 
hand modified maximize likelihood function examples learning mode 
general difficult write likelihood function explicitly compare square loss 
calculate likelihood function noise oe small show difference conventional square loss statistical loss stochastic perceptron 
joint probability zjx expf oe fz fy conditional probability integrating respect putting oe yjx exp jn ff oen dn oe small expand ff oen oef oe ff output network noise prime differentiation 
obtain log yjx oe jvj stochastic perceptron expected minimize loss stoch fy oe jvj shows factor oe multiplied ordinary squared error 
reasonable automatically effect decreasing loss range saturated 
learning stochastic perceptron shown give performance preliminary computer simulation 
geometry observation estimation expectation parameter consider geometry observation exponential family curved exponential family embedded presumed hidden variables exist preliminary hidden variable case studied 
distribution specified parameter random variable 
expectation random variable respect distribution rp denotes expectation respect 
differentiating identity expf gd respect easily shown known transformation 
coordinate system parameter specify distributions 
call expectation parameter 
equation solved negative entropy distribution specified log fr explicitly see amari 
example 
examples coordinates 
case normal distributions example expectation parameter oe case discrete distributions example ffi exp exp case fq example ij expectation respect ij second moments 
parameter useful studying maximum likelihood estimator 
observed value random variable exponential family fp section assumed random variables observable 
maximizes likelihood function logarithm differentiating proved satisfy solving equation 
coordinates directly 
coordinates point 
terms coordinates directly exponential family observed gives 
want obtain need calculate 
coordinate determines distribution point call observed point 
coordinates observed value 
looks trivial exponential family useful curved exponential family case repeated observations shown subsection 
repeated observation observed point independent random variables subject distribution exponential family joint probability distribution provided independently generated 
introducing joint probability written expf extended exponential family expectation coordinates observed observed point expectation coordinates corresponding solving case neural networks determined tth input common network parameter corresponding coordinates written equations define curved exponential family case subject distribution holds 
case joint distribution summarized expf possible reduce extended direct product space single introduce new random variable arithmetic mean random variables 
joint probability transformed distribution rise transformation random variables expf integration taken region arithmetic mean shows probability arithmetic mean implying sufficient statistic estimating see standard textbooks statistics example cox rao 
distributions form type exponential family scale factor 
possible discuss repeated observations estimation framework manifold referring product space holds case need consider general case 
maximum likelihood estimator observed data maximizing exponential family referring differentiation solution terms coordinates simply observed 
call distribution point coordinate system observed point 
observed data simply summarized single observed point coordinate system loss information sense fisher 
estimation curved exponential family consider maximum likelihood estimation curved exponential family embedded case observed data summarized observed point point necessarily belong corresponding distribution maximizing log likelihood respect neglect term depend 
known maximizing likelihood equivalent minimizing divergence 
kl divergence distribution distribution jj log log entropy distribution coordinates maximizing log likelihood equivalent minimizing kl divergence jj observed point points belonging implies point closest observed point fig 
geometrically projecting explained appendix 
solving likelihood equation fr introducing matrix likelihood equation fr kl divergence decomposed point closest observed point likelihood equation fr examples observed points example 
observed point normal mixture 
case repeated observation 
independent data 
summarized ffi ffi ffi function tth observation coordinates expectation oe oe exponential family observed data coordinate system 
obtained solving oe minimizes 
show case stochastic perceptron multiple observations summarized observed point example 
observed point multilayer perceptron 
case stochastic multilayer perceptron random variables summarized set sufficient statistics ffi ffi composed 
coordinates 
depend common coordinates fe xffi efx expectation respect 
observed point sufficient statistics solving 
example mixture expert nets multiple observations summarized original simple example 
observed point mixture experts 
independent observations generated independently generated inputs case summarize corresponding random variables arithmetic mean depends conditional distribution identical 
perceptron case identical distributions summarize sufficient statistics case unconditional distributions form exponential family 
need consider larger direct product space exponential families manifold jx 
observations define observed point product space large dimensionality discussions estimation 
hidden variables data submanifold partial observations treats case parts random variables observed 
example activations hidden neurons observed 
neural learning desired input output relation specified specification desired outputs hidden neurons 
determined adequately input output relation approximated 
subsection studies simplest case constituents sufficient statistics observed 
sufficient statistic vector function basic random variables say case neural networks 
observable specifiable 
observed 
general represent sufficient statistic terms observed 
missing identify observed point uniquely 
candidates observed point takes arbitrary values 
candidate points form submanifold called observed data submanifold 
defined follows fjj fixed observed value arbitrary cases linear tr discrete vector variable linearize linear combinations coordinates 
cases linear submanifold coordinate system 
consider simplest case divided parts visible hidden parts observed hidden 
part missing observed gives observed data submanifold simple way fj forms dimensional submanifold fig called data submanifold observed submanifold partial observation dimension number remarked linear coordinate system plays role inner coordinate system data observed reduces single observed point general case divided linear linear transformation non singular matrix aj ar divided observed variables arbitrary 
coordinates coordinates dually coupled 
coordinates need related coordinates contravariantly 
probability distribution written expf new canonical parameter expectation parameter 
show example 
example 
data submanifold normal mixture 
sake simplicity assume oe oe normal mixture example 
general case analyzed quite 
case log likelihood written log ffi log xffi log full curved exponential family random variable coordinates coordinates respectively ffi xffi log observed fffi observed data submanifold jj ff xff ff free parameters corresponding unobserved ffi 
noted ffi ff takes real values satisfying ff ff 
linear submanifold convenient coordinate system xj linear transformation transformation matrix depends observed value data submanifold written jj ff arbitrary new coordinates 
divided corresponding coordinates noted transformation depends observed data submanifold product space case multiple repeated observations partial observation determines data submanifold fj unobserved random variables remain free parameters tg component linear linear submanifold product space case normal mixture full observed data summarized summarize ae case 
transformation general depends observed data coordinates different summarize visual part arithmetic mean 
need treat summarizing 
summarization possible divided directly visible hidden parts 
example 
data submanifold stochastic perceptron 
random variable time composed observed point written ffi ffi observable ffi 
observed data submanifold fj ff ff ff free parameters satisfying ff data submanifold linear submanifold coordinates 
take component neglecting suffix coordinates partitioned yj ff assumed ith component components observable ff hidden 
transformation depends depends 
example 
boltzmann machines hidden units 
boltzmann machine neural units divided parts visible units hidden units 
visible units include input output units behaviors interested 
hidden units connected visible units hidden units modify behaviors input output units direct interest behaviors hidden units 
divide state vector visible hidden parts new random variables introduced ij ij ij denote connection weights bias terms ij ii hh ij hh ii ij ij example connection weights jth hidden unit ith visible unit 
probability distribution realized visible units boltzmann machine hidden units 
precisely dividing visible units input units output units required realize conditional distribution jx 
input distribution outside distribution realized jx jx truly realizable boltzmann machine necessarily case required approximate boltzmann machine far possible 
specified explicitly outside data manifold consists distributions marginal distribution equal prescribed fq submanifold fq shown coordinates manifold probability distributions 
hold divided parts 
data submanifold rewritten linear form coordinates components corresponding specified components corresponding jx remain free parameters explicitly independent examples observed construct coordinates 
need treat case 
example 
hidden markov chain 
consider markov chain states fs state set divided subsets am fs am fs assume state set markov chain outputs letter alphabet fa corresponding state sequence observe output sequence probability obtained summation taken case function generally treat case stochastically determined subject conditional probability js ig sequence forms stochastic process necessary markov chain 
process called hidden markov model markov chain underlies process observe state 
observe output letter sequence function state sequence stochastically generated hidden model sufficient statistic pair observe hidden 
generalized hidden markov random field see geman geman besag green 
class hidden markov models larger markov chains spite simple mechanism generation 
widely applications particular speech recognition 
called baum welch algorithm baum em algorithm 
geman kehagias tries apply hidden markov random field vision research 
old problem state identification hidden markov model solved ito amari kobayashi 
analyze markov model information geometry expected provide fruitful perspectives 
manifold probability distributions ergodic time series state sequence set markov chains flat submanifold hand observed sequence gives observed submanifold problem find underlying markov chain minimizes 
time distribution minimizes obtained projection shown 
related conditional probability gives markov chain geman kehagias 
case random markov field hidden random markov field calculation conditional probability projection corresponds image restoration 
em algorithm em algorithm em algorithm required obtain best parameter partially observed data algorithms known solve hidden variable problem 
em algorithm 
em algorithm statistical technique calculating maximum likelihood estimator partially observed data dempster laird rudin 
may regarded iterative procedure estimate true parameter missing data time 
show original idea em algorithm generalization called gem algorithm dempster laird rudin 
fp curved exponential family data generated 
data observed obtained maximizing log likelihood observed data log sufficient statistic includes hidden part complete data available 
idea estimate unknown observed conditional expectation rj temporal candidate conditional expectation taken distribution log likelihood function estimated log 
estimated observed point estimated log likelihood function depends current candidate point search better candidate maximizing log likelihood estimated equivalently minimizing kl divergence guessed data point respect algorithm estimating point point iteratively search better candidates 
expectation em algorithm conditional expectation obtain candidate fig 
em algorithm consists step step follows 

choose arbitrary initial guess initial guessed distribution coordinates 
repeat 

step candidate probability distribution calculate conditional expectation condition observed gives ith candidate observed point coordinates 
step calculate candidate observed point point minimizes maximizes estimated log likelihood 
gives st candidate 
partial observed data summarized single form data submanifold product space step estimate correlated estimated separately giving known likelihood increased iteration steps 
em algorithm converges maximum likelihood function visible variables hidden variables eliminated 
converges local maximum likelihood equation 
necessarily mean converges maximum likelihood estimator may converge local maximum likelihood 
pointed em algorithm better global convergence property gradient similar method maximizing log likelihood obtain directly 
geometric em algorithm best estimate minimizes kullback divergence observed data manifold model natural study problem information geometrical point view 
complete data observed maximum likelihood estimation searches point closest observed point sense minimizing divergence observation partial identify observed point observed partial information gives data submanifold 
natural search pair points minimizes divergence min observation complete reduces observed point dual minimization proposed csisz ar general perspective 
amari byrne neal hinton studied problems various fields 
information geometry proves point minimizes projection see appendix 
projection geodesic connecting orthogonal exponential family curve geodesic linear coordinates 
orthogonality defined terms fisher riemannian information metric appendix 
dually point minimizes projection projection geodesic connecting orthogonal exponential family curve geodesic linear coordinates 
considerations formulate geometric em algorithm projection algorithm follows fig 


choose arbitrary initial guess gives initial distribution repeat 

step project gives minimizes 
step project gives minimizes observed submanifold situation 
point point ae coordinates shows point minimizes composed points minimizes projection projecting component projection applied componentwise em em algorithms look similar 
section devoted elucidation relation 
information geometry em em algorithms properties projection section establishes geometry em em algorithms 
intriguing see algorithms equivalent 
show elucidate relation projection conditional expectation 
show geometrical properties projections 
known fact statistics amari projection observed point gives maximum likelihood estimator 
summarize theorem 
theorem 
observed point statistical model projection gives maximizes likelihood 
projection unique flat 
properties projection shown theorem 
theorem 
flat submanifold projection unique 
represented separated form coordinates fj arbitrary point coordinates partitioned projected point coordinates denoted respectively 
properties hold visual part hidden part coordinates kept invariant projection conditional probability hidden variable equal conditional expectation hidden variables eq proof 
property trivial prove hidden part coordinates equal corresponding part divergence rewritten eq log eq eq submanifold free variable fixed value projection minimizes differentiation right hand side vanishes minimum point holds 
conditional distribution conditioned written expf expf gd expf normalization factor depends conditional expectation depend projection coordinates conditional probabilities equal eq geometry em algorithm theorem shows step step procedure giving steps 
order obtain geometrical interpretation step define transformation coordinates divided 
put eq jr eq conditional function 
mapping maps point 
keeps invariant replaces unconditional expectation conditional expectation conditioned expectation 
defined fixing observed value maps 
step interpreted follows 
candidate point step gives point part equal observed part conditional expectation jr hidden conditional expectation projected conditional expectation kept invariant projection theorem 
step implies project obtaining transform fq replacing conditional expectation step rewritten step project obtain transform give fq gives alternative description em algorithm 
theorem 
em algorithm formulated dual minimization steps 

step search point minimizes kff 
step search point minimizes clear identity map em em algorithms 
holds conditional expectation conditioned equal unconditional expectation theorem 
em em algorithms equivalent conditional expectation eq jr linear proof 
eq jr eq eq linear function written br constant constant matrix eq eq identity algorithms equivalent 
contrary assume identity eq eq holds point fq arbitrary distribution belonging defined fp arbitrary distribution projection equal family distributions geodesic connecting family exponential family conditional distribution jr projection distribution fq decomposed jr depends conditional distribution implies dr dr choose condition dr immediately shows linear function 
example em em algorithms different shown section algorithms equivalent important cases 
show simple example algorithms different 
example 
geometry normal multiplication model 
independent normal random variables subject 
sufficient statistics assume observed hidden 
statistical model fn curved exponential family imbedded fn oe example 
corresponding coordinates oe observed data manifold fn oe oe arbitrary see fig 
treat case simplicity sake search mapping oe point observed conditional probability distribution jr follows 
joint conditional distribution conditioned pq jr exp oe ffi normalization constant 
integrating respect pq jr exp oe know variance reduces oe 
obviously conditional distribution conditionally 
conditional expectation eq jr oe conditional expectation conditioned eq fi fi fi fi fi oe different unconditional expectation eq oe shows identity maps oe fq oe projection point follows 
coordinates point oe hand step candidate gives jr confirmed fq problem intersects point 
em algorithm converges best candidate hand em algorithm converges minimum 
general fq solution em algorithm em algorithm gives large em algorithm em algorithm asymptotically equivalent 
asymptotic equivalence holds general need bothered practically 
em em algorithms equivalent cases natural em em 
identity map em em algorithms different giving different estimates natural better 
em algorithm gives maximum likelihood estimator known bias corrected higher order efficient amari em algorithm look natural statistical inference point view 
neural network model approximates input output relation statistical inference 
problem realize neural network approximate input output behavior possible 
behavior examples summarized data submanifold point equally explain input output data difference lies hidden variables care 
em algorithm minimizes divergence give natural answer 
clearer consider case data submanifold flat linear coordinates 
case sufficient statistics nonlinear data submanifold fjj fixed flat linear submanifold em algorithm works case projection gives point minimizes hand point conditional expectation js belong satisfy constraint due observation conditional expectation js define point point belongs convergence guaranteed 
shown projection point conditional probability conditional expectation provided flat linear coordinates 
usually conditional expectation easily calculated projection conditional expectation 
curved flat submanifold tangent conditional distribution conditioned sense em algorithm regarded linear approximation em algorithm 
linearization trick guaranteeing equivalence em em algorithms algorithms equivalent cases 
show equivalent 
random variable 
function linear ff gy trick function provided variable takes value finite set element introduce new vector variable indexed fk kg km ffi function linear extended variable km shows visible random variable binary visible random variable finite set represented trivially extended vector form conditional expectation linear visible variable em em algorithms equivalent 
theorem 
em em algorithms equivalent binary stochastic perceptron mixture expert nets boltzmann machine normal mixture model 
proof 
case binary stochastic perceptron mixture binary expert nets component manifold set conditional probabilities zjx jx 
visible variable cases automatically linear algorithms equivalent 
case mixture binary experts observed new coordinates yj visible parts hidden parts 
corresponding random variables yr ffi visible variable case boltzmann machine visible state components binary 
visible variables expectations give binary equivalence algorithms shown 
prove case normal mixture model 
component observed data missing 
data submanifold fj ff ff ff parameter corresponding ffi 
candidate point conditional expectation missing variable ffi ff ffi expf expf eq ffi jx projection distribution belongs coordinates satisfy 
shows distribution trivial distribution satisfying ffi jx eq ffi ff depend linear constant case identity map normal mixture model providing equivalence algorithms 
prove equivalence case mixture analog experts quite similar way 
linearization function space takes real vector values linearization trick works ffi dm case random variable extended ffi ffi element generalized function space fg examples observed summarized empirical distribution ffi formulate hidden variable problem function space prove em em algorithms equivalent function space 
formulation obtained csisz ar neal hinton 
fp function space density functions random variables visible hidden 
fp parametric model specified parameter observed summarize function space ffi empirical distribution observed data 
decompose jr observed empirical distribution jr remains free 
observed data submanifold consists functions jr projection minimizes easy show projection emp jr jr general hand extending hidden random variable function ffi conditional expectation step ffi jr ffi dm jr giving conditional probability 
algorithms shown csisz ar neal hinton 
theorem 
em em algorithms equivalent function space 
step step obtain conditional probability jr 
step maximize eq log jr jr log jr dr respect gives convenient consider function space 
need keep data summarizing sufficient statistics 
asymptotic equivalence em em algorithms number observations large linear approximation holds asymptotically law large numbers 
guaranteed large deviation theory case approximation problem central limit theorem model faithful 
show theorem 
theorem 
number observations large em em algorithms asymptotically equivalent 
proof 
prove case observations observations summarized arithmetic mean 
point point flat 
minimizes point coordinates denoted 
virtue large deviation theory see probability density probability point observed true distribution written expf tk asymptotically 
conditional distribution exp expf tk gdr expf defined minimized socalled saddle point approximation method 
conditional distribution concentrated conditional expectation conditional expectation gives asymptotically point projection 
large deviation theory approximation problem necessarily close exists data generated close 
case directly apply central limit theorem prove result 
non case data submanifold joint probability density written jp exp point coordinates ae entropy distribution case entropy term reduced transform density mean case need apply non version theory large deviations 
general case projection conditional expectation 
difference asymptotically absorbed step 
suggests proof theorem general case easy construct rigorous proof 
learning procedures rewrite em em algorithm line learning form partial observation available time batch algorithm uses data stored 
general convergence slower algorithm simpler learning 
learning robust changes environmental structures 
propose learning algorithm data summarized single observed point algorithm essentially neal hinton jordan jacob 
algorithm applicable stochastic perceptron 
estimator time guessed observed point time partial observation time learning procedure follows 
step step calculate projection conditional expectation conditioned 
gives guess unobserved modify guessed observed point constant decreasing sequence step calculate put gives calculation usually easy gradient method obtain log likelihood gradient bf matrix 
gradient method applied incremental step incremental step acceleration method applicable 
scoring method gives bf inverse fisher information matrix observed data summarized single propose learning algorithm 
step step calculate projection coordinates conditional expectation 
gives guessed incremental step fr case log likelihood written step guess observed variable incremental step newest data calculate gradient old data may discarded 
differential incremental forms em algorithm geometrical point view em em algorithms search local minimum divergence min min kff pg moves moves associate dual gradient flows single function 
pair points move directions decreasing respectively 
dual gradient flows fig 
parameter specify point coordinates coordinates observed part restricted parameter dual gradient curves 
gradient flows coordinates dt dt gm gd fisher information matrices respectively 
easy prove relations gradient flows written dt dt coordinates respectively 
flows geodesic flows respectively fujiwara amari 
discretizing geodesic flows metric tensor terms incremental algorithm information geometry em algorithm em algorithm constructed 
proved equivalent practical cases equivalent asymptotically 
unified geometrical framework em em algorithms 
possible formulate learning algorithm version em algorithm 
boltzmann machine mixture experts stochastic multilayer perceptron normal mixture models treated examples 
framework opens new area research connecting neural networks statistics geometry 
appendix information geometry dual geometry exponential family invariant geometrical structures general manifold probability distributions studied detail amari murray rice order obtain intrinsic properties statistical model 
geometrical theory successfully applied various fields information sciences statistics amari kass systems theory amari amari information theory amari han amari neural networks amari amari 
mathematicians studying new geometrical structure differential geometry simon kurose 
riemannian manifold equipped couple dual affine connections 
duality affine connections new notion introduced differential geometry originated information science 
mention differential geometrical concepts riemannian metric affine connection curvature detail 
show types curve geodesic dually flat manifold 
orthogonality curves shown intuitively 
show fundamental theorems dually flat manifold 
readers asked refer amari murray rice related works 
explain exponential mixture geodesic intuitive way 
see xa formal definition covariant derivative affine connection 
treat manifold discrete probabilities example takes ng 
probability distribution denoted ig 
probability distributions special curves connecting manifold connect log log linearly exponential family fp log log log log log new random variable normalization factor parameter curve 
curved regarded straight line geodesic connecting exponential family standpoint 
terms coordinate system coordinates written coordinates respectively 
exponential geodesic linear curve coordinates 
mixture family fp connecting distributions curve tp curve regarded straight line geodesic mixture standpoint 
proper meanings 
easy show coordinates written mixture geodesic linear curve coordinates 
generalizing idea give definitions flatness manifold exponential family coordinate systems consider curve connecting points linearly coordinates curved regarded straight line exponential standpoint called exponential geodesic geodesic 
particular coordinate curve coordinate system geodesic 
implies manifold flat having affine coordinate system flatness point view 
called coordinate system 
hand curve connecting distributions linear coordinate system curve said mixture geodesic geodesic connecting coordinate curves coordinate system geodesics 
coordinate transformation general nonlinear geodesic geodesic general 
different criteria flatness 
exponential family flat criteria called dually flat manifold 
dually flat manifold deep differential geometrical structures 
orthogonality fisher information explain tangent space riemannian metric order define orthogonality curves 
tangent vector coordinate curve direction changes change fig 
mathematicians denote symbolically tangent space point vector space spanned fe coordinate tangent direction coordinate curve denoted tangent space spanned fe introduce inner product tangent space write inner product ij ij positive definite matrix 
natural define ij log log denotes expectation respect rao 
ij called fisher information matrix plays central role theoretical statistics 
coordinate system inner product terms basis vectors ij log log proved ij inverse matrix manifold said riemannian inner product defined tangent space point manifold probability distributions riemannian manifold having different dually coupled criteria flatness 
curve 
tangent curve vector denotes dt 
curve denoted coordinate system tangent vector written terms fe curves intersect orthogonal inner product tangent vectors vanishes ij rewritten coordinates ij show important duality relation bases fe fe proved ij ij implies theorem 
theorem 
bases fe fe dual reciprocal sense ffi ij holds point ffi ij kronecker delta 
inner product vectors easily obtained representing dual bases inner product ha bi shows usefulness dual bases 
set probability distributions exponential family regarded dimensional manifold having coordinate systems 
dually flat riemannian manifold play special dual role 
manifold shown amari murray rice coordinate systems connected legendre transformation defined relation known entropy distribution specified information geometry manifold exponential family dually flat riemannian space 
precisely dually coupled affine connections respect riemannian metric riemann curvatures vanish respect connections levi connection non zero curvature 
manifold dually flat proved invariant divergence measure defined points divergence derived geometric structure underlying manifold 
case manifold exponential family written qi coordinates respectively proved equal kl divergence log divergence symmetric general equality holds dp close dp half square riemannian distance dp ij coordinates dp respectively 
essential role divergence shown generalized pythagoras theorem fig amari amari 
theorem 
points dually flat manifold geodesic connecting orthogonal geodesic connecting self dual flat reduces euclidean space 
divergence half squared euclidean distance case theorem reduces ordinary pythagoras theorem 
important corollaries follow 
submanifold point search point closest sense divergence fig 
euclidean orthogonal projection dually flat manifold criteria closeness solutions problem 
solve problem explain concept projections 
geodesic connecting orthogonal point said projection generalized pythagoras theorem shows projection gives extremal point 
corollary 
point minimizes projection projection unique flat submanifold 
corollary 
point minimizes projection projection unique flat manifold 
projections coordinate forms follows 
flat submanifold 
adequate coordinate system divide defined fj arbitrary correspondingly divided 
point coordinates projection coordinates respectively 
hand theorem shows part invariant projection 
order obtain need solve equations similar dual formulation projection flat 
dual differential geometry briefly describe dual differential geometry probability distributions readers familiar differential geometry 
manifold defined riemannian metric 
affine connection defined covariant derivative rx vector fields determined give covariant derivative vector fields natural basis connected coordinate system 
vector field determined component form ijk hr index quantity ijk called coefficients underlying affine connection 
riemannian manifold levi riemannian connection defined ijk jk ik ij torsion free affine connection preserving riemannian metric 
affine connection defined geodesic curve ik jki denotes dt 
geodesic minimum length curve connecting points riemannian connection 
curve connecting points tangent vector field curve satisfies vector said transported tq parallel curve affine connection 
write parallel transport conservation metric implies ha dual geometry defines new torsion free affine connections different riemannian 
manifold probability distributions define invariant tensor ijk log log log ff connection defined ff ijk ijk ff ijk ff connection riemannian connection 
ff connection called exponential connection ff connection called mixture connection 
dual sense preserve metric ha exponential family special sense dually flat riemann curvatures vanish ff 
case affine coordinate system ijk vanishes 
case geodesic equation reduces bt geodesic 
dually affine coordinate system ijk vanishes 
geodesic written bt coordinate system 
dually flat manifold theorem holds 
theorem 
dually flat exist affine coordinate systems potential functions metric ij ij coordinate systems connected legendre transformation natural bases dual ffi ij stated invariant divergence defined generalized pythagoras theorem holds 
information geometry studies new geometrical structures existing manifolds probability distributions 
generalized fibre bundle structure amari conformal structure okamoto amari takeuchi 
applied successfully various fields information sciences mentioned earlier 
related completely integrable dynamical systems fujiwara amari 
aarts korst 

simulated annealing boltzmann machines chichester wiley 
ackley hinton sejnowski 

learning algorithm boltzmann machines cognitive science 
amari 

differential geometry curved exponential families curvatures information loss annals statistics pp 
amari 

differential geometrical methods statistics springer lecture notes statistics springer 
amari 

differential geometry parametric family invertible linear systems riemannian metric dual affine connections divergence mathematical systems theory pp 
amari 

differential geometrical theory statistics 
amari eds differential geometry statistical inference ims monograph series vol chap pp cal ims 
amari 

fisher information restriction shannon information multiterminal situations annals institute statistical mathematics 
amari 

mathematical foundations neurocomputing proceedings ieee pp 
amari 

geometry manifold higher order neurons neural networks pp 
amari 

em algorithm information geometry neural network learning neural computation appear 
amari barndorff nielsen kass lauritzen rao 

differential geometry statistical inferences ims lecture notes monograph series vol hayward california ims 
amari 

information geometry estimating functions semiparametric statistical models 
university tokyo 
amari 

information geometry boltzmann machines ieee trans 
neural networks pp 
amari han 

statistical inference multi terminal rate restrictions differential geometrical approach ieee trans 
information theory pp 
barndorff nielsen 

information exponential families statistical theory john wiley chichester 
barndorff nielsen 

parametric statistical model likelihood springer lecture notes statistics vol springer 
barndorff nielsen cox reid 

role differential geometry statistical theory international statistical review 
barndorff nielsen 

approximating exponential models annals institute statistical mathematics 
baum petrie soules weiss 

maximization technique occuring statistical analysis probabilistic functions markov chains annals mathematical statistics 
besag green 

spatial statistics bayesian computation journal royal statistical society 
byrne 

alternating minimization boltzmann machine learning ieee trans 
neural networks pp 


statistical decision rules optimal inference russian moscow nauka translated english rhode island ams 
cox 

theoretical statistics london chapman hall 
csisz ar 
divergence geometry probability distributions minimization problems annals probability 
csisz ar 

information geometry alternating minimization procedures eds statistics decisions supplementary issue munich oldenburg verlag 
dempster laird rudin 

maximum likelihood incomplete data em algorithm royal statist 
soc 
fujiwara amari 

dynamical systems framework information geometry preprint 
geman geman 

stochastic relaxation gibbs distributions bayesian restoration images ieee trans 
pattern analysis machine intelligence pami 
geman kehagias 

hidden markov random fields appear 
ito amari kobayashi 

identifiability hidden markov information sources minimum degrees freedom ieee trans 
information theory 
jordan jacobs 

mixtures experts em algorithm neural computation appear 
jordan xu 

convergence results em approach mixtures experts architectures neural networks jacobs jordan hinton 

adaptive mixtures local experts neural computation pp 
kass 

geometry asymptotic inference discussions statistical science pp 
amari 

estimation network parameters semiparametric stochastic perceptron neural computation appear 
geman kehagias 

hidden markov random field appear 
kurose 

dual connections affine geometry mathematische vol 
murray rice 

differential geometry statistics london chapman hall 
amari 

differential geometry smooth families probability distributions 
neal hinton 

new view em algorithm justifies incremental variants appear 
simon 

notes conjugate connections preprint 
amari 

differential geometric structures stable feedback systems dual connections proc 
nd workshop systems structure control pp prague 
okamoto amari takeuchi 
asymptotic theory sequential estimation differential geometrical approach annals statistics pp 
rao 

information accuracy attainable estimation statistical parameters bull 
calcutta 
math 
soc 
rao 

linear statistical inference applications new york john wiley 
ripley 

neural networks related method journal royal statistical society appear 


limit theorems large deviations dordrecht kluwer academic 


new criterion selecting models partially observed data proc 
th international workshop artificial intelligence statistics pp 
white 

learning artificial networks statistical perspective neural computation 
xu jordan hinton 

new gating net mixture experts em algorithm piecewise function approximations preprint 
zhuang amari 

piecewise linear division signal space multilayer neural networks maximum detector japanese trans 
inst 
electr 
inf 
comm 
eng 
legends fig manifold probability distributions fig mixture expert nets fig curved exponential family fig stochastic perceptron fig maximum likelihood estimation fig data submanifold fixed fig em algorithm fig em algorithm fig example em em algorithms different fig dual gradient flows fig tangent space fig pythagoras theorem fig projection 
