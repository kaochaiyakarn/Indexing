unsupervised bayesian distance measure petri kontkanen jussi petri myllymaki henry tirri complex systems computation group box department computer science fin university helsinki finland www cs helsinki fi research 
introduce distance measure idea vectors considered similar lead similar predictive probability distributions 
suggested approach avoids scaling problem inherent alternative techniques method automatically transforms original attribute space probability space numbers lie 
method flexible sense allows different attribute types discrete continuous consistent framework 
study validity suggested measure ran series experiments publicly available data sets 
empirical results demonstrate unsupervised distance measure sensible sense discovering hidden clustering structure data 
machine learning techniques usually aim compressing available sample data compact representations called models 
models solving different explorative data mining predictive inference tasks 
opposed traditional model machine learning lazy learning refers methods defer essential computation including compression sample data model models specific prediction task completely determined 
type predictive inference known case instance reasoning reflecting fact methods set sample instances case base central role predictive inference process 
model approach predictive inference problems solved building sample data possibly domain specific prior information mathematical model set parameters implements function solving problem hand 
lazy learning approach hand model explicitly defined prediction problems solved comparing input vector samples available case base 
actual prediction typically done distance function vectors closest input vector affect result 
means predictive accuracy point view determining distance function key issue building cbr systems 
defining distance function difficult solve problems related eds ewcbr lnai pp 
springer verlag berlin heidelberg unsupervised bayesian distance measure attribute scaling handling different attribute types 
argue probability theory offers general framework problems disappear approach offers time theoretically solid approach building accurate predictive models 
discuss alternative ways linking probabilistic predictive models lazy learning framework 
able formalize lazy learning concept 
lazy learning scenario appears sight model free important realize view flawed distance functions implicitly model set assumptions determine distance high dimensional vectors defined 
instance easy see euclidean distance underlying probabilistic model normally distributed independent variables mahalanobis distance see assumes multivariate normal model 
consequently lazy learning simply defined model free approach machine learning 
attempt formal definition lazy learning suggested basic idea refrain single model set models mathematically corresponds requirement marginalizing integrating possible model families individual models families 
type integration implicitly suggests lazy learning type approach 
see note model approach sample data compressed set numbers called sufficient statistics numbers required determining model parameters phrase sufficient 
lazy learning approach hand integration model families means single set sufficient statistics sufficient statistics respect single family models may sufficient respect family 
suggests define lazy learning set methods minimal sufficient statistics data lazy learning methods inherently sample data words case nature 
inspired general theoretical framework suggested novel distance measure case reasoning tasks 
lazy model free general form computational reasons system implemented single model basis suggested distance measure 
metric mainly planned particular subtask case reasoning case retrieval symmetric nature 
furthermore approach point probabilities different events clear similarity properties probability distributions individual probabilities 
proposed analyzed supervised probabilistic model data reduction scheme vectors considered similar lead similar predictive distributions corresponding attribute value pairs input probabilistic model 
metric similarity metric symmetric suitable example visualizing petri kontkanen high dimensional data demonstrated 
seen section metric inherently supervised nature applied unsupervised domains 
method essentially single probabilistic model case sense discussed 
overcome limitations section introduce novel unsupervised bayesian distance measure extension earlier supervised approach 
order validate approach empirical results obtained distance measure reported section 
experiments reported unsupervised distance measure computed pool supervised naive bayes classifier models 
validity suggested distance measure evaluated set subjective visual tests set simple crossvalidation experiments 
supervised bayesian distance metric fx xn denote case base collection vectors assume vector consists values attributes xm sequel assume attributes discrete discretized extending approach continuous attributes left goal 
discussed earlier argue exist thing model free reasoning suggest assumptions concerning domain space explicitly listed exploited formal models problem domain 
model mean parametric probabilistic model form parameterized instance model produces probability distribution xm jm space possible data vectors presentation concrete remainder assume models represent different bayesian network structures bayesian network models see 
general idea suggested summarized follows vectors considered similar lead similar predictive distributions corresponding attribute value pairs input bayesian network model idea precise define predictive distribution informal definition 
predictive distribution determined respect special target variable xm resulting conditional distribution xm xm gamma data vectors considered similar corresponding predictive distributions similar xm gammam xm gammam gammam denotes attribute values vector value class variable xm unsupervised bayesian distance measure type similarity measures lead supervised distance measures easily change focus metric changing target variable xm scheme scale invariant moved original attribute space probability space numbers lie 
allows handle different attribute types discrete continuous consistent framework 
furthermore framework fulfills requirement stated earlier approach theoretically solid basis domain assumptions formalized model scheme leaves question defining similarity measure predictive distributions 
standard solution computing distance distributions kullback leibler divergence see 
asymmetric measure basic form distance metric geometric sense 
empirical experiments reported observed simpler distance metric yields results practice dm gamma mapm mapm mapm denotes maximum posterior probability value target variable xm respect predictive distribution conditioned values variables xm gamma mapm arg max xm gammam slightly different distance function straightforward logarithmic transformation predictive probabilities dm gamma log mapm mapm noted extending general approach cases target variables straightforward 
unsupervised bayesian distance metric distance metric described section inherently supervised nature requires choose domain variables target variable 
consequently approach directly cases natural candidate target variable exists process data purely unsupervised manner 
unsupervised domains propose unsupervised extension suggested supervised distance metric consequently distance vectors computed variables turn target variable summing petri kontkanen resulting supervised distance measures computed formula 
intuitively speaking means vectors considered similar probable outcome cases individual supervised prediction tasks conditional distributions gamma xm sum logarithms definition unsupervised distance function means basically treating separate supervised prediction tasks independently 
scenario leaves pragmatic important question model structure defining conditional distributions 
traditionally question answered bayesian framework model maximizing posterior probability 
assuming uniform prior model structures equivalent model highest marginal likelihood evidence arg max arg max theta theta theta certain technical assumptions model selection criterion computed closed form 
required conditional distribution computed marginalizing joint probability distribution appropriately xm xim gammam xm xim gammam gammam xm gammam see approach consistent definition lazy learning sense marginalize possible parameter instantiations 
follow line reasoning see marginalize possible model structures yielding assuming uniform prior model structures 
consequently weight prediction obtained model structure marginal likelihood xjm 
obvious computing sum practice feasible number possible bayesian networks super exponential 
sum approximated efficiently find important individual terms model structures high marginal likelihood 
noted finding high evidence model structures extremely difficult problem 
means practical situations dealing model structures unsupervised bayesian distance measure possibly poor models true joint domain probability distribution probabilities obtained correct 
demonstrated trying find models joint probability distribution supervised classification domains sense try find model set models errors affect accuracy conditional distribution little possible allow joint probability distribution predictions concerning variable quite inaccurate 
reason suggest single model structure determining distance measure supervised models mm chosen respect corresponding predictive task 
unfortunately discussed finding accurate bayesian network models supervised prediction tasks difficult problem 
return issue section suggest computationally efficient heuristics solving problem 
empirical results setup illustrate validity suggested similarity metric performed series experiments publicly available classification data sets uci data repository 
data sets main properties listed table 
preprocessing phase experimental setup continuous attributes data sets discretized straightforward application means algorithm 
consequently respect empirical study reported data sets discrete 
computing pairwise distances vectors equation need determine predictive distributions xm gammam mm 
discussed section finding bayesian network models mm supervised prediction tasks difficult practice 
hand demonstrated example structurally simple naive bayes classifier performs surprisingly real world classification domains despite fact model extremely fast construct 
reason series experiments predictive models mm corresponded naive bayes models constructed respect target variable xm model parameters set expected values maximum probability values 
parameter values lead predictive distribution equivalent obtained integrating parameter values demonstrated results improved classification accuracy cases amount data small 
illustrate validity suggested unsupervised distance measure data vectors data set visualized dimensional space technique called sammon mapping see 
method data vectors placed low dimensional space manner pairwise petri kontkanen table 
data sets experiments 
dataset size attrs 
classes australian credit breast cancer wisconsin breast cancer credit screening pima indians diabetes german credit heart disease cleveland heart disease hungarian heart disease statlog hepatitis ionosphere iris plant liver disorders lymphography mole fever patient thyroid disease vehicle silhouettes congressional voting records wine recognition distances space match closely possible pairwise distances original high dimensional space 
objective function corresponding relative error caused dimensionality reduction typically quite complex finding optimal visualization sense possible practice left approximative solutions 
pointed visualization context necessary aim absolutely optimal solution visualization purposes reasonable approximation usually quite sufficient 
find effectively approximations optimal visualization challenging research problem discussed detail 
experiments reported simple iterative stochastic greedy algorithm step visual locations randomly chosen data vectors optimized connecting line objective function optimized locally 
practical relevance visualization indirectly measured data mining process domain experts try capture interesting regularities visual image 
case domain experts available evaluate visualization technique different manner 
set experiments done assuming clustering provided class labels uci data sets reasonable clustering sense clustering regarded come seen class labels originally 
line rea unsupervised bayesian distance measure classification data set pruned removing class labels column containing values class variable xm remaining data visualized dimensional space unsupervised approached 
produced visual images colored class labels visualization process 
resulting image visually pleasing sense different classes different colors nicely separated picture said able recover original clustering totally unsupervised manner class label information 
results empirical results show suggested unsupervised distance measure sensible sense discovering underlying cluster structure produced visual images pass class coloring clarity test explained 
black white example produced visualizations 
full library colored images url www cs helsinki fi research projects visual ewcbr 
possible caveat experimental procedure basically priori reason unsupervised visual image produced reflect clustering provided class labels especially original clustering poor probabilistic modeling point view 
way measure goodness clustering provided class labels evaluate predictive accuracy naive bayes model model essentially clustering data class variable xm leave cross validated classification results naive bayes classifier second column table 
conjecture class color clarity test resulted visual images may fail data sets performance naive bayes classifier poor 
experimental results confirm hypothesis cases leave cross validated classification accuracy nb classifier poor absolute sense liver disorders data set relative sense respect default classification accuracy patient data set class labeled colored images somewhat blurred 
emphasize mean unsupervised visualization technique failed cases relatively cases somewhat artificial empirical setup practically sensible place 
means data cases clustered visual image produced result probabilistic model producing accurate predictions naive bayes classifier 
interesting idea studied 
believe people agree produced images exception cases discussed visually pleasing sense original classes clearly separable image 
raises petri kontkanen fig 

ionosphere data set example unsupervised visualizations obtained suggested distance measure 
question quality visualization measured objectively 
intuitively measure data visual image clustered hidden class label 
suggest done example simple nn nearest neighbor method data point classified member class containing representatives nearest data points 
results type nn classification method summarized table 
method nn refers simple nearest neighbor classifier distances dimensional recall class labels omitted data vectors computed formula 
table see classification accuracy obtained nn classifier quite comparable cases better 
accuracy nb classifier nearest neighbor classification done purely unsupervised manner recall distance measure computed class labels 
surprising considering fact nn classifier just random naive choice performing cbr type classification idea demonstrate potentiality suggested distance measure 
unsupervised bayesian distance measure table 
leave cross validated classification results obtained data sets 
dataset default nb nn australian credit breast cancer wisconsin breast cancer credit screening pima indians diabetes german credit heart disease cleveland heart disease hungarian heart disease statlog hepatitis ionosphere iris plant liver disorders lymphography mole fever patient thyroid disease vehicle silhouettes congressional voting records wine recognition admit empirical setup quite satisfactory classifiers different space 
corrects deficiency simply incorporating class label information sammon mapping process observed nn classifier outperforms nb classifier consistently case 
important notice type empirical setup absolutely fair incorporates class label information sammon mapping process means performing leave cross validation information test vector included training data acceptable 
comparison completely fair respect sammon mapping procedure run times separately test vector loo cross validation cycle 
computationally quite demanding left task 
results clearly suggest probabilistic distance approach offers promising approach producing accurate classifiers 
hypothesis supported argument described section unsupervised distance measure computed pool gamma naive bayes models model variables xm gamma standard naive bayes classifier uses single nb model constructed respect class variable xm relatively unrealistic independence assumptions naive bayes approach hold petri kontkanen resulting predictive distributions may inaccurate 
conjecture errors caused violation assumptions smoothed unsupervised case result distance measure essentially sum supervised distance measures produced independent nb classifiers 
unsupervised extension supervised bayesian distance metric suggested earlier 
suggested method idea vectors considered similar lead similar predictive probability distributions 
earlier supervised approach predictive distribution defined respect specific target variables group variables suggested unsupervised metric computed manner changing target variable variables serves turn role 
distance measure validated series experiments hidden structure data discovered producing dimensional visual picture domain 
furthermore distance measure applied simple nearest neighbor classifier results support visual observations distance measure obviously captures interesting regularities data 
emphasized classification results surprisingly respect obtained naive bayes classifier considering fact comparing results fair nearest neighbor classifier worked totally unsupervised manner knowledge class labels data naive bayes classifier access data 
suggests type approach offers promising framework building accurate case classifiers left topic 
research supported national technology agency academy finland 

aha 
study instance algorithms supervised learning tasks mathematical empirical psychological observations 
phd thesis university california irvine 

aha editor 
lazy learning 
kluwer academic publishers dordrecht 
reprinted artificial intelligence review 

atkeson moore schaal 
locally weighted learning 
aha pages 

berger 
statistical decision theory bayesian analysis 
springer verlag new york 
unsupervised bayesian distance measure 
blake keogh merz 
uci repository machine learning databases 
url www ics uci edu mlearn mlrepository html 

castillo 
expert systems probabilistic network models 
monographs computer science 
springer verlag new york ny 

chatfield collins 
multivariate analysis 
chapman hall new york 

cooper herskovits 
bayesian method induction probabilistic networks data 
machine learning 

friedman geiger goldszmidt 
bayesian network classifiers 
machine learning 

gelman carlin stern rubin 
bayesian data analysis 
chapman hall 

heckerman geiger chickering 
learning bayesian networks combination knowledge statistical data 
machine learning september 

heckerman meek 
models selection criteria regression classification 
geiger shenoy editors uncertainty intelligence pages 
morgan kaufmann publishers san mateo ca 

jensen 
bayesian networks 
ucl press london 

kohonen 
self organizing maps 
springer verlag berlin 

kolodner 
case reasoning 
morgan kaufmann publishers san mateo 

kontkanen myllymaki tirri 
bayesian networks visualizing high dimensional data 
intelligent data analysis 
appear 

kontkanen myllymaki tirri 
software bayesian classification feature selection 
agrawal stolorz piatetsky shapiro editors proceedings fourth international conference knowledge discovery data mining kdd pages 
aaai press menlo park 

kontkanen myllymaki tirri 
bayes optimal instancebased learning 
rouveirol editors machine learning ecml proceedings th european conference volume lecture notes artificial intelligence pages 
springer verlag 

kontkanen myllymaki tirri 
bayesian case matching 
smyth cunningham editors advances case reasoning proceedings th european workshop ewcbr volume lecture notes artificial intelligence pages 
springer verlag 

kontkanen myllymaki tirri 
supervised selection bayesian networks 
laskey prade editors proceedings th international conference uncertainty artificial intelligence uai pages 
morgan kaufmann publishers 

kontkanen myllymaki tirri grunwald 
predictive distributions bayesian networks 
statistics computing 

moore 
acquisition dynamic control knowledge robotic manipulator 
seventh international machine learning workshop 
morgan kaufmann 

neapolitan 
probabilistic reasoning expert systems 
john wiley sons new york ny 

pearl 
probabilistic reasoning intelligent systems networks plausible inference 
morgan kaufmann publishers san mateo ca 
petri kontkanen 
stanfill waltz 
memory reasoning 
communications acm 

tirri kontkanen myllymaki 
probabilistic instance learning 
saitta editor machine learning proceedings thirteenth international conference icml pages 
morgan kaufmann publishers 
