wake sleep algorithm unsupervised neural networks geoffrey hinton peter dayan brendan frey radford neal department computer science university toronto king college road toronto canada th november describe unsupervised learning algorithm multilayer network stochastic neurons 
bottom recognition connections convert input representations successive hidden layers top generative connections reconstruct representation layer representation layer 
wake phase neurons driven recognition connections generative connections adapted increase probability reconstruct correct activity vector layer 
sleep phase neurons driven generative connections recognition connections adapted increase probability produce correct activity vector layer 
supervised learning algorithms multilayer neural networks face problems require teacher specify desired output network require method communicating error information connections 
wake sleep algorithm problems 
teaching signal matched goal required force hidden units extract underlying structure 
wake sleep algorithm goal learn representations economical describe allow input reconstructed accurately 
input vector communicated receiver sending hidden representation sending difference input vector top reconstruction hidden representation 
aim learning minimize description length total number bits required communicate input vectors way 
communication takes place minimizing description length required forces network learn economical representations capture underlying regularities data 
neural network quite different sets connections 
bottom recognition connections convert input vector representation layers hidden units 
top generative connections reconstruct approximation input vector underlying representation 
training algorithm sets connections different types stochastic neuron simplicity stochastic binary units states 
state unit probability exp gammab gamma uv bias unit uv weight connection unit units driven generative weights times recognition weights equation cases 
wake phase units driven bottom recognition weights producing representation input vector hidden layer representation representation second hidden layer 
layers representation combined called total representation input binary state hidden unit total representation ff ff total representation communicate input vector receiver 
shannon coding theorem requires gamma log bits communicate event probability distribution agreed sender receiver 
assume receiver knows top generative weights create agreed probability distributions required communication 
activity unit top hidden layer communicated distribution ff gamma ff obtained applying eq 
single generative bias weight unit activities units lower layer communicated distribution ff gamma ff obtained applying eq 
communicated activities layer ff generative weights kj description length binary state unit ff gammas ff log ff gamma gamma ff log gamma ff description length input vector total representation ff simply cost describing hidden states hidden layers plus cost describing input vector hidden states ff ff djff jffl ff jff index layers hidden units index input units states hidden units stochastic input vector represented way 
recognition weights determine conditional probability distribution total representations 
recognition weights fixed simple line method modifying generative weights minimize expected cost ff ffjd ff describing input vector stochastically chosen total representation 
recognition weights choose total representation generative weight adjusted proportion derivative equation purely local delta rule deltaw kj ffls ff ff gamma ff ffl learning rate 
call wake phase learning algorithm 
units driven recognition weights generative weights learn phase 
learning layer total representation better reconstructing activities layer 
obvious recognition weights adjusted maximize probability picking ff minimizes ff 
incorrect 
alternative ways describing input vector possible design stochastic coding scheme takes advantage entropy alternative descriptions 
cost ff ffjd ff gamma gamma ff ffjd log ffjd second term entropy distribution recognition weights assign various alternative representations 
example alternative representations costs bits combined cost bits provided alternatives equal probability 
precisely analagous way energies alternative states physical system combined yield helmholtz free energy system 
physics minimized probabilities alternatives exponentially related costs boltzmann distribution temperature ffjd exp gammac ff fi exp gammac fi adjusting recognition weights focus probability lowest cost representation try recognition distribution similar possible boltzmann distribution posterior distribution representations data network generative model 
exponentially expensive compute exactly simple way getting approximately correct target states hidden units order train distribution produced bottom recognition weights 
turn recognition weights drive units network generative weights starting topmost hidden layer working way input units 
units stochastic repeating process typically produces different fantasy vectors input units 
provide unbiased sample network generative model world 
having produced fantasy adjust recognition weights maximize log probability recovering hidden activities caused fantasy deltaw jk ffls fl fl gamma fl fl specifies states hidden units input units particular fantasy fl probability unit turned recognition weights operating binary activities fl layer 
call sleep phase algorithm 
wake phase uses locally available information 
potential drawback sleep phase recognition weights recovering true causes training data sleep phase optimizes recognition weights fantasy data 
early learning quite different distribution training data 
distribution produced recognition weights factorial distribution hidden layer recognition weights produce stochastic states units hidden layer conditionally independent states layer 
natural factorial distributions neural net allows distribution alternative hidden representations specified numbers gamma 
simplification typically impossible distribution exactly match posterior distribution equation 
impossible example capture explaining away effects activity vector layer economically explained activating unit unit layer activating 
restriction factorial distribution potentially serious limitation 
reason fatal flaw wake phase algorithm adapts generative weights close limiting loss caused inability model non factorial distributions 
see effect occurs helpful rewrite equation different form ff ffjd ff gamma gamma ff ffjd log ffjd ff ffjd log ffjd ffjd terms equation exactly gamma log current generative model 
term negative kullback leibler divergence amount description length exceeds gamma log 
generative models assign equal probability minimizing equation respect generative weights tend favor model posterior distribution similar 
available space generative models wake phase seeks give rise posterior distributions approximately factorial 
making approximations algorithm evaluated performance 
shows learn correct multilayer generative model simple toy problem 
learning kullback leibler divergence equation bits indicates term forced solution generative model perfectly factorial posterior 
algorithm works realistic task identifying highly variable handwritten digits seeing different digit networks provides economical description data 
shows learned digit model generated network similar real data 
widely unsupervised training algorithms neural networks principal components analysis competitive learning called vector quantization clustering 
viewed special cases minimum description length approach hidden layer unnecessary distinguish recognition generative weights 
learning schemes proposed separate feedforward feedback weights 
contrast adaptive resonance theory counter streams model kawato algorithm wake sleep algorithm treats problem unsupervised learning statistical fitting generative model accurately captures structure input examples 
kawato model couched terms forward inverse models alternative way look generative recognition models 
wake sleep algorithm closest spirit barlow ideas invertible factorial representations mumford proposals mapping grenander generative model approach brain 
curious coincidence idea perceptual system uses generative models advocated helmholtz call neural network fits generative model data minimizing free energy eq 
helmholtz machine 
rissanen stochastic complexity statistical inquiry world scientific singapore 
description length viewed upper bound negative log probability data network generative model approach closely related maximum likelihood methods fitting models data 
number bits required specify generative weights included description length currently ignore 
ge hinton rs zemel advances neural information processing systems jd cowan tesauro alspector editors morgan kaufmann san mateo 
unbiased estimate exact gradient easy obtain noise estimate increases size network 
alternatively mean field approximation stochastic recognition model error derivatives computed backpropagation process dayan ge hinton rm neal rs zemel neural computation submitted 
performs stochastic steepest descent kullback leibler divergences jffl fl log fl fl gamma fl log gamma fl gamma fl cost function equation contains terms interchanged leading approximation error equal asymmetry kullback leibler divergences 
principal components analysis hidden representation vector linear function input vector aim minimize squared reconstruction error 
description length perspective cost describing hidden ignored cost describing reconstruction errors needs minimized 
errors coded zero mean gaussian cost describing proportional squared values 
competitive learning hidden unit weight vector similar input vector activated 
reconstruction just weight vector winning hidden unit minimizing squared distance input vector weight vector winning hidden unit minimizes description length reconstruction error 
carpenter grossberg computer vision graphics image processing 
ullman large scale theories cortex koch davis editors mit press cambridge ma 
kawato network 
mumford large scale theories cortex koch davis editors mit press cambridge ma 
mi jordan de rumelhart cognitive science 
hb barlow neural computation 
grenander lectures pattern theory ii iii pattern analysis pattern synthesis regular structures springer verlag berlin 
learning rates generative recognition weights input units weights 
generative biases hidden layer started gamma weights started 
final asymmetric divergence network generative model real data bits 
penalty term eq 
bits 
training data examples digit cedar cdrom available postal service office advanced technology 
starting input layer network architecture 
weights started learning rate connections 
training involved sweeps examples 
testing net run times estimate expected description length image 
research supported canadian federal nserc iris ontario 
fellow 
captions generative model theta images 
top level decides vertical horizontal bars 
level decides possible bar chosen orientation image 
sample images produced model ambiguous white images removed 
neural net input units units hidden layer hidden unit second hidden layer trained theta random examples produced generative model 
training probability distribution produced sleep phase exactly correct 
generative weights units hidden layer 
positive weights white negative weights black area proportional magnitude 
largest weight shown 
generative bias unit shown top right block generative weight single unit layer shown top left 
rightmost block shows generative biases input units 
encourage easily interpretable solution generative weights input units constrained positive 
allowed go negative algorithm finds solutions produce correct distribution complicated way requires units second hidden layer 
handwritten digits normalized quantized produce theta binary images 
examples digit shown left 
separate network trained digit class network shown right 
variations digit class modelled quite 
error rate new test images classified choosing network minimized description length image 
data nearest neighbor classification gave errors backpropagation training single supervised net output units hidden layer gave minimum errors test data optimize number hidden units training time amount weight decay 

