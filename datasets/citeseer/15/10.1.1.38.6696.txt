hidden state reinforcement learning instance state identification andrew mccallum department computer science university rochester rochester ny usa mccallum cs rochester edu www cs rochester edu mccallum real robots real sensors omniscient 
robot course action depends information hidden sensors problems occlusion restricted range bounded field view limited attention say robot suffers hidden state problem 
state identification techniques history information uncover hidden state 
previous approaches encoding history include finite state machines recurrent neural networks genetic programming indexed memory :10.1.1.56.7115
chief disadvantage techniques long training time 
presents instance state identification new approach reinforcement learning state identification learns fewer training steps 
noting learning history learning continuous spaces share property knowing granularity state space approach applies instance memory learning history sequences recording instances continuous geometrical space record instances action percept reward sequence space 
implementation approach called nearest sequence memory learns order magnitude fewer steps previous approaches 
embedded agents perceive environment 
robotics seen surge interest reactive robots agents current sensor values choose action tasks get complex finding tasks solved perception action mappings 
sensors inherently limited world state information relevant choosing action hidden limitations 
reasons important features hidden robot perception sensors noise limited range limited field view occlusions hide areas sensing limited funds space prevent equipping robot desired sensors power supply robot sensors time robot limited computational resources turning raw sensor data usable percepts 
situations immediate perception supply information short term memory past perceptions actions provide information needed 
consider examples ffl space station repair robot sees needs replacement part thrusts station supply bin get new part 
bin occlusions limited range prevent robot seeing distant faulty part 
percepts available position supply bin robot know part needs 
short term memory robot remember part faulty pick correct part 
ffl hospital delivery robot making way patient room blood lab 
travel robot limited sensors able distinguish different hallway intersections turn right turn left 
confusion robot reliably deliver sample 
short term memory robot distinguish intersections remembering features previous locations 
instance robot may know turn right upcoming intersection remembers just passed elevator doors elevator doors left turn intersection 
unique infrared transceivers installed intersections hospital avoid need memory installation may possible important perceptual limitation listed limited funds 
hardware solution uses transceivers considerably expensive software solution uses short term memory 
ffl robot driver uses vision system navigate traffic 
order overcome limited computational power extracting usable perceptual information raw image data perceptual limitation list driver active vision system movable binocular cameras containing fovea 
high resolution fovea necessary recognizing objects cameras fine grained resolution broad field view vision system far pixels limited computational power handle 
driver pass car highway close preceding car blind spot clear cars 
unfortunately addressing computational limitation limitation limited field view 
driver see car front blind spot time 
close car ahead clear blind spot tell robot pull passing lane matter robot redirects active vision system immediate perception include inputs conjunction driver decide pass immediate perception 
short term memory driver look car ahead remember close passing look blind spot clear way elements relevant conjunction decision passing 
principles propose middle path purely reactive agents suffer hidden state problems traditional planning agents require infeasible complete world models 
agent represent world state current perceptions agent require full predictive model entire environment 
described addresses issue hidden state short term memory conjunction principles ffl agents learn tasks 
prefer robots learn behaviors hard coded programming details hand tedious may know environment ahead time want robot adapt behavior environment changes 
ffl agents learn trials possible 
learning experience expensive terms wall clock time dangerous terms potential damage robot surroundings 
experience relatively expensive storage computation 
furthermore expense computation storage decrease advances computing hardware continue cost experience 
ffl agents able handle noisy perceptions actions 
ffl strict optimality necessary 
aim reasonably behavior complexity task 
cases worth running algorithm take times trials learn performance order gain twentieth increase performance 
example changing environment especially true 
introduces new family methods reinforcement learning short term memory 
implementation approach learns order magnitude fewer steps previous approaches 
suggests methods family perform better 
ii background related reinforcement learning agent suffers hidden state time agent state representation missing information needed determine correct action 
formally say reinforcement learning agent suffers hidden state problem agent state representation non markovian respect actions utility 
assumes reader familiar principles reinforcement learning developed field machine learning 
algorithm previous reinforcement learning especially learning 
review reinforcement learning basics included introductions reinforcement learning may 
hidden state problem arises case perceptual aliasing mapping states world sensations agent oneto 
agent perceptual system produces outputs world states different actions required agent state representation consists percepts agent fail choose correct actions 
note agent representation consists percepts agent maintains internal state possible agent suffer hidden state problem 
possibility occurs agent maintains amount internal state internal state disambiguate aliased world states task 
stateless fixed memory agents approaches hidden state problem attempt disambiguate aliased states 
simplest approach simply ignore hidden state problem apply traditional reinforcement learning methods aliased states 
experience shown certain non markovian environments approach cases ignoring hidden state results complete failure see examples explanations :10.1.1.54.2637
careful solution hidden state problem avoid passing perceptually aliased states 
approach taken whitehead lion algorithm 
agent finds state delivers inconsistent reward sets state utility low policy visit 
success algorithm depends deterministic world existence path goal consists states 
solutions avoid aliased states best non markovian state representation evading disasters result ignoring hidden state altogether 
involve learning deterministic policies execute incorrect actions aliased states learning stochastic policies aliased states may execute incorrect actions probability 
approaches depend path states limitations faced aliased states requiring different actions probability executing optimal sequence actions falls faced potentially harmful results incorrect actions deterministically incorrect probabilistically incorrect action choice may prove dangerous faced performance critical tasks inefficiency proportional amount aliasing may unacceptable 
agents learn memory robust solution hidden state problem augment agent state representation line disambiguate aliased states 
state identification techniques uncover hidden state information agent internal state space markovian 
transformation imperfect state information model perfect state information model formalized decision control literature involves adding previous percepts actions definition agent internal state 
augmenting agent percepts history information shortterm memory past perceptions actions rewards agent distinguish perceptually aliased states reliably choose correct actions 
example tasks section illustrated memory disambiguate hidden state robot knows replacement part pick remembers seeing faulty part robot knows turn right left intersection remembers having passed elevator door robot knows pull passing lane remembers having looked front 
memory disambiguate aliased situations allows robot efficiently perform task independent number aliased states path 
allows robot choose correct actions reliably opposed making probabilistic guesses faced ambiguous sensations 
problem represent memory agent learn remember forget 
agent begins knowing perform task hand agent begins knowing memory required 
predefined fixed memory representations constant sized perception windows formally known order markov models undesirable 
length window needed exponentially increase number agent internal states policy stored learned length memory needed agent reverts disadvantages hidden state 
agent designer understands task know maximal memory requirements agent disadvantage constant sized windows tasks different amounts memory needed different steps task 
emphasize way learning remember departure machine learning multi step tasks 
memory learning agents learn mapping states actions difficult learn required state space 
confronted hidden state agent longer rely perception completely define agent internal state space 
internal states agent depend current perception variable learned amount memory past perceptions actions 
related agents learn memory reinforcement learning algorithms techniques adapt memory representation line 
perceptual distinctions approach utile distinction memory splitting states finite state machine doing line analysis statistics gathered steps :10.1.1.54.2637
recurrent training recurrent neural networks 
indexed memory uses genetic programming evolve agents load store instructions register bank 
chief disadvantage techniques require large number steps training 
describing approaches detail consider algorithms related 
environment learning approach implemented toto uses memory impressively fast learns single presentations 
technique designed specifically mobile robots inside rooms corridors 
algorithms depends dead reckoning disambiguate states tied navigation 
navigational tasks global state directly correlated geographical position exactly dead reckoning provides 
meaningful analogy global state calculating dead reckoning general non navigational tasks apparent 
learning classifier system implemented genetic algorithm 
dorigo colombetti learns proper sequences maintaining internal state indicating subtask agent performing 
transition signals produced trainer environment prompt agent switch subtask 
authors modified include percepts indications sensor value changed steps parameter algorithm set robot designer 
variation constant sized perception window approach memory simplification agent remembers value change sensor values steps 
finite state machines perceptual distinctions approach pda utile distinction memory udm learn partially observable markov decision processes state splitting strategies :10.1.1.54.2637
probabilistic finite state machines fsm represent memory certain states aren reached passing certain states previously 
occupation current state effectively give agent memory perceptions actions associated available incoming transitions current state exclusive transitions allow agent distinguish states perception associated states identical 
agent represent arbitrarily long memories nesting contingent transitions 
research studied calculate optimal policy partially observable markov decision process 
pda udm hand emphasize learning finite state machine simple heuristic calculating policy 
major difference pda udm udm splits states mention related associated reinforcement learning provide examples learning finite state machines 
map learning described learns finite state machines 
applicable hidden state requires unique landmark world state perception landmarks allowed noisy handle stochastically unreliable actions 
diversity inference procedure finite automata learns ingenious representation called update graph efficiently capture regularities structure environment 
diversity approach finds set canonical tests repeatable cycles environment 
rivest schapire algorithm depends completely deterministic environment 
neural network implementation approach accommodate noisy sensations disadvantages 
statistics show doing help predict reward words udm creates memory needed task hand 
finite state machine approaches advantage established formal study finite state machines hidden markov models partially observable markov decision processes 
disadvantage limited representational power 
instance agent needs remember bit information remember bit long time entire state space covered intervening time duplicated 
second disadvantage udm pda require inordinate number steps learn tasks demonstrated 
expectation maximization em algorithm learner alternates gathering statistics current model changing current model statistics 
agent begins random minimal fsm model alternately performs trials consisting steps performs state splits joins statistics trial 
large number steps trial required experience statistically significant 
case em algorithms statistics current flawed model requirement splits detected splits 
result splits trial trials required complete learning steps trial learning time suffers dramatically 
recurrent neural networks recurrent algorithm trains recurrent neural network containing fixed number hidden units 
network holds memory backward looping connections hidden units feed input layer current inputs partially determined networks context units turn determined previous inputs 
repeated looping acti vation recurrent connections maintain memory steps 
advantage recurrent neural networks memory representational efficiency greater finite state machines efficiently memorize bit steps instance 
potential handle continuous inputs 
disadvantage recurrent finite state machines state splitting rules memory capacity fixed learning begins number hidden units fixed 
recurrent takes steps learn 
neural networks general known quick convergence convergence recurrent neural networks typically slower feed forward networks 
genetic algorithms genetic programming indexed memory uses genetic programming evolve programs apply load store instructions fixed sized register bank 
registers hold values retrieval obviously supply agent memory 
chief advantage genetic programming indexed memory great representational power 
fact teller point emphasizing class potentially evolved programs turing complete 
unfortunately advantage causes algorithm worst disadvantage pays flexibility terms learning steps required 
genetic programming indexed memory take extremely large number steps learn 
example teller demonstration algorithm grid world box pushing task population contains individuals population evolved generations generation individuals evaluated test cases steps total theta steps 
priority find new alternative learns fewer steps understanding may sacrifice flexibility representation 
agents afford take number learning steps required multiple line state splitting tests number training presentations necessary recurrent neural network converge number evaluations needed evolve population solutions 
extreme flexibility memory representation necessary 
iii instance state identification advocates new solution hidden state problem term instance state identification 
approach inspired successes instance called memory methods learning continuous perception spaces 
methods success physical robots 
term memory introduces unfortunate conflict vocabulary 
memory refer short term memory past perceptions actions disambiguate hidden state refers methods store raw previous experiences directly nearest neighbor locally weighted regression 
avoid confusion phrase instance hope carries meaning 
application instance learning short term memory state identification driven important insight learning continuous spaces learning hidden state crucial feature common learning knowing final granularity agent state space 
learns regions continuous input space represented uniformly areas finely divided states 
learns perceptions represented uniformly uniquely identify course action need memory perceptions divided states detailed history distinguish perceptually aliased world states 
approach works continuous geometrical input space second works action percept reward sequence space history space 
large continuous regions correspond specified small memories small continuous regions correspond specified large memories 
furthermore learning continuous spaces sequence spaces lot gain instance methods 
situations state space granularity unknown especially useful memorize raw previous experiences 
agent incorporates experience merely averaging current flawed state space granularity bound attribute experience wrong states experience attributed wrong state turns garbage wasted 
faced evolving state space keeping raw previous experience path commitment cautious losing information 
keeping records raw previous experience incur certain expenses terms storage computation time expenses justified 
instance compare time required perform multiplications time takes robot move hallway 
learning trials expensive computation expensive ways wall clock time power consumed danger robot danger surroundings example 
techniques dyna experi analogy continuous spaces memory spaces provides perspective implementing memory constant sized perception windows perception windows undesirable precisely reason constant sized discretizations continuous spaces undesirable 
discretization coarsegrained agent fail discretization fine grained number states exponentially needed different granularity needed different regions state space 
ence replay transitional proximity learning asynchronous dynamic programming examples efforts substitute world experience storage computation 
iv nearest sequence memory possible instance techniques choose wanted keep application simple 
mind initial algorithm nearest neighbor 
call nearest sequence memory nsm 
bears emphasizing algorithm straightforward simple naive combination instance methods history sequences think sophisticated instance methods try 
surprising result simple technique works 
depicts analogy nearest neighbor continuous geometric space nearest neighbor sequence space 
application nearest neighbor consists parts recording experience distance metric find neighbors current query point extracting output values neighbors 
apply parts action percept reward sequences reinforcement learning learning follows 
step agent world records action perception reward adding new state single long chain states 
state chain contains snapshot agent immediate experience moment agent experiences laid time connected history chain 

agent choose action finds states considered similar looking state chain states histories learning geometric space learning sequence space nearest neighbor nearest neighbor action percept reward match length comparing nearest neighbor continuous space sequence space 
database instances point want query 
case query point measure neighborhood relations indicated gray cross nearest neighbors indicated gray shadows 
geometric space shown top instances black dots indicating vector values multi dimensional space 
neighborhood metric defined euclidean distance 
sequence space shown bottom instances circles connected arrows indicating action percept reward snapshot time snapshots laid time sequence order 
numbers circles indicate action percept reward triples 
neighborhood metric determined sequence match length number preceding states match states preceding query point 
neighborhood function agent finds situations past similar current situation similar includes current percepts percepts leading current situation 
agent internal state space variable length short term memory choosing instances extract expected reward values tends prefer instances better matching history 
similar current situation 
longer state string previous experiences matches agent experiences state represents agent 

states agent calculates values averaging expected reward values associated nearest states action 
agent chooses action highest value 
regular learning update rule update states voted chosen action 
choosing represent short term memory linear trace simple established technique 
order markov chains representation fixed sized time windows lin albus examples order markov chains 
nearest sequence memory uses linear trace represent memory differs fixed sized window techniques provides variable memory length nearest neighbor nsm represent varying resolution different regions state space 
details algorithm interaction agent environment described actions percepts rewards 
finite set possible actions finite set possible percepts observations scalar range possible rewards 
time step agent executes action result receives new percept reward say percept results action order consistent models active perception percept function world state previous action 
agent active perception execute perceptual actions redirect agent focus attention 
note atypical grouping action percept reward time 
formulations action time index percept reward preceded gamma gamma gamma 
explanation nsm simpler giving action time index percept reward follow gamma gamma 
grouping 
subscripts symbols indicate time step associated specific actions percepts states 
subscript refers current time step subscripts refer arbitrary time steps 
may add subtract constants indicate offsets time step indicated reinforcement learning goal agent time step choose action maximizes expected discounted sum reward called return written flr fl fl just instance algorithms nearest sequence memory records raw experiences 
action environmental state time captured state data point written recorded state available information associated time action resulting percept resulting reward associated state slot hold single expected discounted reward estimate denoted 
value associated action action 
notation indicate expected reward value associated individual instance notation refers average values extracted neighboring instances closest query point 
nearest sequence memory algorithm consists steps 
find nearest neighbor similar states possible action 
state currently chain query point measure distances 
neighborhood metric defined string match length number preceding experience records match experience records preceding query point state 
metric defined recursively 
higher values indicate closer neighbors notation gamma indicates action associated state preceding state chain 
gamma gamma gamma gamma gamma gamma gamma gamma considering possible actions turn find nearest neighbors give vote 
ties resolved preferring states chain 
max ja equation current state expression indicates state gets vote neighborhood metric current state compared neighborhood similarity values states outgoing action 

determine value action averaging values voting states states action executed 
ja 
select action maximum value random exploration 
exploration probability randomly chosen argmax 
execute action chosen step record resulting experience 
creating new state representing current state environment storing action percept reward triple associated increment time counter 
create record agent limit storage computational load limiting number instances maintains 
allowing number instances grow indefinitely number experience steps agent choose keep reasonably large number 
reaches instances step discard oldest instance add new 
choice balanced desire limit storage computation hand versus need agent remember experiences different parts environment visits 
provides way handle changing environment 

update values vote 
perform dynamic programming step standard learning rule update states voted chosen action 
note involves performing steps get values needed calculating utility agent current state 
fi learning rate 
max ja gamma gamma flu note update 
helps agent estimate accurate average reward stationary world biased average rewards strength bias dictated learning rate fi 
agent augments step learning passing discounted reward backwards recorded history chain long propagation increases states values 
practice occur 
process described kind conservative td 
nearest sequence memory performs kinds learning 
instancebased methods learns populating input space raw experience 
instance methods perform dynamic programming values raw experience 
usually instance methods function approximators required outputs values provided raw experience supervised learners 
nearest sequence memory required output values values provided raw experience related raw rewards dynamic programming nearest sequence memory reinforcement learner 
kinds learning need occur 
instance agent adding states chain continue execute steps world dynamic programming implemented learning 
section discusses distinction 
vi experimental results section shows experimental results comparing performance nearest sequence memory algorithms 
performance demonstrated tasks chosen algorithms designers 
case nearest sequence memory learns task roughly order magnitude fewer steps 
nearest sequence memory learns policies quickly learn optimal policies 
section discuss policies optimal nearest sequence memory improved 
perceptual distinctions approach perceptual distinctions approach demonstrated space ship docking application hidden state 
task difficult noisy sensors unreliable actions 
sensors returned incorrect values percent time 
various actions failed percent time failed resulted random states 
perceptual distinctions approach learning rate fi temporal discount factor fl expectation maximization em cycle steps exploration probability 
nearest sequence memory learning rate fi temporal discount factor fl maximum instance chain length exploration probability number neighbors 
higher chosen experiment noisy environment 
graph performance learning appears 
performance indicated measure agent trying maximize sum discounted reward utility nsm results averaged runs 
perceptual distinctions approach takes steps learn task 
nearest sequence memory learns policy steps policy quite optimal 
nsm learn task fewer steps perceptual distinctions approach learns task computation 
baum welch procedure integral part perceptual distinctions approach baum welch takes time space jsj jsj number states model jaj number actions number steps em cycle 
nsm takes time maximum instance chain length 
algorithms 
utility steps performance learning nearest sequence memory perceptual distinctions approach comparing perceptual distinctions approach nearest sequence memory chrisman docking task 
spaceship docking task involves noisy sensors noisy actions 
sensors returned incorrect values time 
actions failed time failed resulted random states 
value plotted utility sum discounted rewards states agent policy visits 
utile distinction memory utile distinction memory demonstrated local perception mazes :10.1.1.54.2637
reinforcement learning maze domains agent perceive global row column numbers indications barrier immediately adjacent right left 
maze positions perceived identical including positions agent execute different actions 
time agent reaches goal reset random location maze 
utile distinction memory learning rate fi temporal discount factor fl expectation maximization cycle steps 
nearest sequence memory learning rate fi temporal discount factor fl maximum chain length exploration probability number neighbors 
graph number steps required learning appears 
results averaged runs 
mazes nearest sequence memory learns task th time required utile distinction memory nearest sequence memory learns mazes utile distinction memory solve 
udm principle capability learn multistep memories practice difficult udm discover 
nsm solves multi step memory tasks problem 
perceptual distinctions approach udm uses baum welch procedure 
udm worse computational complexity nsm 
recurrent recurrent demonstrated robot cup retrieval task 
environment deterministic task difficult nested levels hidden state providing reward task completely deterministic environment agent handle higher learning rate stochastic environment 
number steps steps learn task udm nsm udm nsm number steps udm nsm udm nsm number steps number steps steps learn task steps learn task steps learn task comparing utile distinction memory nearest sequence memory maze task 
agent perceive global row column numbers indications barrier immediately adjacent north south east west 
positions agent execute different actions perceived identical 
goal square indicated 
bar graphs show number training steps required learn perfect policy starting points 
results averaged runs 
finished 
lin tried different neural network algorithms task window recurrent recurrent model recurrent learned task fewest trials 
recurrent temporal discount fl trial time steps 
td experience replay experience recording playback technique developed lin speeding learning 
nearest sequence memory trial time steps learning rate fi temporal discount factor fl maximum instance chain length exploration probability number neighbors 
graph performance learning appears 
nearest sequence memory learns performance trials recurrent takes trials reach equivalent performance 
parallel processing implementation recurrent take computation time number layers net 
parallel processing implementation nsm take computation time 
note nsm lin experience reply mechanism requires agent store chain raw previous experience 
steps trials steps trial learning nearest sequence memory recurrent performance comparison recurrent nearest sequence memory lin cup task 
robot cup retrieval task difficult nested levels hidden state provides reward task completely finished 
vertical axis shows number steps required complete cup task twice possible starting points 
double task completions single trial 
horizontal axis measures number trials learning 
learning curves show mean results runs 
vii discussion reason better performance nearest sequence memory offers improved line performance fewer training steps predecessors 
improvement dramatic believe chief reason lies inherent advantage instancebased state identification described section iii restated paragraph 
key idea instance state identification recognition recording raw experience particularly advantageous learning partition state space case agent trying determine history significant uncovering hidden state 
instance technique agent simply averages new experiences current flawed state space model experiences applied wrong states reinterpreted agent modifies state space boundaries 
furthermore data interpreted context flawed state space biased inappropriate way simply recorded kept uncommitted open easy reinterpretation light data 
scenario especially clear finite state machine approaches experiences averaged current fsm state splitting session agent re gathers experience scratch statistics learning derived current flawed fsm raw data obtain statistics thrown away kept reinterpretation light new state space topology 
statements apply recurrent neural network case agent internal state space defined weights connections memory providing hidden units 
areas improvement experimental results bode instance state identification 
nearest sequence memory simple simplistic implementation works sophisticated approaches may better 
subsections discuss ideas improvement 
better distance metric agent sophisticated neighborhood distance metric exact string match length 
new metric account distances different percepts considering exact matches 
new metric handle continuous valued inputs 
prior experience showing improvements strictly necessary physical robots 
sequence matching algorithm implemented toto small finite set percepts completely intolerant noise 
robot rooms hallways avoiding obstacles learned map environment map reliably travel arbitrary goal locations 
toto intolerant sequence matching algorithm overcame quite noisy sensors intermediate perceptual mechanism translated continuous high dimensional noisy sensor values elements useful percept set 
little domain knowledge averaging time robot intermediate layer produced noise percepts suitable intolerant string matching algorithm 
robot nearest sequence memory precisely mechanism 
furthermore nsm somewhat tolerant noisy percepts actions nsm continue intermediate perceptual mechanism unreliable 
finite state machines nearest sequence memory directly represent cycles environment 
agent chosen learned action primitives encapsulate loops 
approach variable length fuzzy sequence matching help 
structure noise distinction nearest sequence memory demonstrably solves tasks involve noisy sensation action handle noise better technique explicitly separating noise structure 
nearest neighbor explicitly discriminate structure noise 
current query point neighbors wildly varying output values way know variations due noise case averaged due fine grained structure underlying function case closest averaged 
nearest sequence memory built nearest neighbor suffers inability methodically separate history differences significant predicting reward history differences 
believe single important reason nearest sequence memory find optimal policies 
interesting manifestation noise structure confusion occasionally nearest sequence memory exhibits behavior displayed learning animals 
certain circumstances agent finds success round route tends route repeatedly 
exploratory steps uncover efficient path reward nearest sequence memory switch better path 
instance particular learning run space ship docking task agent successfully making redundant degree turns 
agent chose behavior consistently trials random exploration discovered extra turns unnecessary agent began docking extra dance 
specification thinking detail relevant really extra dance noise agent interprets structure 
nearest sequence memory exhibits behavior depends densely state space reward covered instances discovers reward 
occasional tendency extra attention details imply nearest sequence memory respond teacher agent learn efficiently single presentation presumably teacher demonstrate task extra 
progress addresses structure noise issue combining instancebased state identification structure noise separation method utile distinction memory :10.1.1.54.2637
algorithm called utile suffix memory uses tree structured representation related prediction suffix trees parti game algorithm variable resolution dynamic programming 
preliminary results promising 
see details 
separating instance learning dynamic programming pointed section details algorithm nearest sequence memory performs kinds learning instance population state space raw experience dynamic programming calculate expected reward 
currently kinds learning intertwined believe algorithm perform better cleaner separation 
instance component extract relevant state space dynamic programming performed extracted states 
progress mentioned preceding section scheme 
reducing storage computation instance limiting technique discussed section nearest sequence memory computationally efficient competitors 
may want find ways reducing storage computational requirements 
alternative keeping instances agent simply adding states chain thought sufficiently covered possibly significant sequences 
instance chain remain unchanged agent continued learn values instances 
learning complete may want compile chain smaller structure 
related discusses technique compiling example sequences hidden markov models 
previous discusses application technique building probabilistic finite state machines include notion actions 
acknowledgments benefited discussions colleagues including dana ballard andrew moore jeff schneider jonas karlsson 
am grateful dana ballard jonas karlsson virginia de sa ieee reviewers making helpful comments earlier drafts 
material supported nsf 
iri nih phs rr 
list figures comparing nearest neighbor continuous space sequence space 
database instances point want query 
case query point measure neighborhood relations indicated gray cross nearest neighbors indicated gray shadows 
geometric space shown top instances black dots indicating vector values multi dimensional space 
neighborhood metric defined euclidean distance 
sequence space shown bottom instances circles connected arrows indicating action percept reward snapshot time snapshots laid time sequence order 
numbers circles indicate action percept reward triples 
neighborhood metric determined sequence match length number preceding states match states preceding query point 
neighborhood function agent finds situations past similar current situation similar includes current percepts percepts leading current situation 
agent internal state space variable length short term memory choosing instances extract expected reward values tends prefer instances better matching history 
comparing perceptual distinctions approach nearest sequence memory chrisman docking task 
spaceship docking task involves noisy sensors noisy actions 
sensors returned incorrect values time 
actions failed time failed resulted random states 
value plotted utility sum discounted rewards states agent policy visits 
comparing utile distinction memory nearest sequence memory maze task 
agent perceive global row column numbers indications barrier immediately adjacent north south east west 
positions agent execute different actions perceived identical 
goal square indicated 
bar graphs show number training steps required learn perfect policy starting points 
results averaged runs 
performance comparison recurrent nearest sequence memory lin cup task 
robot cup retrieval task difficult nested levels hidden state provides reward task completely finished 
vertical axis shows number steps required complete cup task twice possible starting points 
double task completions single trial 
horizontal axis measures number trials learning 
learning curves show mean results runs 
albus outline theory intelligence ieee trans 
systems man cybernetics vol 
pp 

atkeson memory approaches approximating continuous functions nonlinear modeling forecasting casdagli eubank eds pp 
addison wesley 
ballard brown principles animate vision computer vision graphics image processing image understanding vol 
pp 

barto bradtke singh real time learning control asynchronous dynamic programming technical report university massachusetts amherst ma 
barto sutton watkins learning sequential decision making learning computational neuroscience moore eds mit press cambridge ma 
coins tech report dept computer information sciences university massachusetts amherst ma 
barto sutton anderson neuron elements solve difficult learning control problems ieee trans 
systems man cybernetics vol 
pp 

bertsekas programming stochastic control 
academic press 
bertsekas shreve stochastic optimal control 
academic press 
booker goldberg holland classifier systems genetic algorithms artificial intelligence vol 
pp 

cassandra kaelbling littman acting optimally partially observable stochastic domains proceedings twelfth national conference artificial intelligence seattle wa 
chapman kaelbling learning delayed reinforcement complex domain twelfth international joint conference artificial intelligence 
chrisman reinforcement learning perceptual aliasing perceptual distinctions approach tenth national conference artificial intelligence pp 

colombetti dorigo training agents perform sequential behavior adaptive behavior vol 
pp 

dean angluin engelson kaelbling maron inferring finite automata stochastic output functions application map learning tenth national conference artificial intelligence pp 

dempster laird rubin maximum likelihood incomplete data em algorithm journal royal statistical society series vol 
pp 

dorigo learning control real robot distributed classifier systems technical report politecnico di milano italy 
machine learning press 
dorigo colombetti robot shaping developing autonomous agents learning artificial intelligence vol 
pp 

duda hart pattern classification scene analysis 
john wiley sons new york ny 
marquis conditioning learning 
crofts second edition edition 
edited gregory kimble 
holland adaptation natural artificial systems 
university michigan press ann arbor mi 
jaakkola singh jordan reinforcement learning algorithm partially observable markov decision problems advances neural information processing systems 
morgan kaufmann 
koza genetic programming programming computers means natural selection 
mit press cambridge ma 
lin self reinforcement learning planning teaching proceedings eighth international workshop machine learning 

lin reinforcement learning robots neural networks 
phd dissertation carnegie mellon school computer science january 

lin mitchell reinforcement learning hidden states proceedings second international conference simulation adaptive behavior animals animats pp 

littman memoryless policies theoretical limitations practical results proceedings third international conference simulation adaptive behavior animals animats 
mataric distributed model mobile robot navigation master thesis massachusetts institute technology 
technical report ai tr 
mccallum results utile distinction memory reinforcement learning technical report university rochester computer science dept 
mccallum transitional proximity faster reinforcement learning proceedings ninth international machine learning conference 
morgan kaufmann publishers 
mccallum learning incomplete selective perception technical report university rochester computer science dept march 
phd thesis proposal 
mccallum overcoming incomplete perception utile distinction memory proceedings tenth international machine learning conference :10.1.1.54.2637
morgan kaufmann publishers 
mccallum utile suffix memory reinforcement learning hidden state technical report university rochester computer science dept december 
mccallum instance utile distinctions reinforcement learning proceedings twelfth international machine learning conference 
morgan kaufmann publishers 
moore variable resolution dynamic programming efficiently learning action maps multivariate real valued state spaces proceedings eighth international workshop machine learning pp 

moore efficient memory learning robot control 
phd dissertation university cambridge november 
moore parti game algorithm variable resolution reinforcement learning multidimensional state spaces nips pp 

morgan kaufmann 
mozer bachrach discovering structure reactive environment exploration neural computation vol 
pp 

pryor don shoot dog 
bantam books new york ny 
rabiner tutorial hidden markov models selected applications speech recognition proceedings ieee vol 
february 
selective perception robot driving 
phd dissertation school computer science carnegie mellon university 
tr cmu cs 
rivest schapire diversity inference finite automata proceedings eighth annual symposium foundations science pp 

rivest schapire new approach unsupervised learning deterministic environments proceedings fourth international workshop machine learning langley ed pp 
irvine california 
ron singer tishby learning probabilistic automata variable memory length proceedings computational learning theory 
morgan kaufmann publishers 
schaal atkeson robot juggling implementation memory learning control systems magazine february 
schneider high dimension action spaces robot skill learning twelfth national conference artificial intelligence aaai 
singh jaakkola jordan model free reinforcement learning non markovian decision problems proceedings eleventh international machine learning conference 
morgan kaufmann publishers 
stolcke omohundro hidden markov model induction bayesian model merging advances neural information processing systems november 
sutton integrating architectures learning planning reacting approximating dynamic programming proceedings seventh international conference machine learning austin texas morgan kaufmann 
teller evolution mental models advances genetic programming kinnear ed ch 
mit press 
watkins learning delayed rewards 
phd dissertation cambridge university 
wettschereck dietterich locally adaptive nearest neighbor algorithms advances neural information processing systems 
morgan kaufmann publishers 
whitehead reinforcement learning adaptive control perception action 
phd dissertation department computer science university rochester 
whitehead ballard learning perceive act trial error machine learning vol 
pp 


