parallelizing multiprocessor scheduling problem ahmad yu kwong kwok department computer science hong kong university science technology clear water bay hong kong department electrical electronic engineering university hong kong road hong kong mail cs ust hk eee hku hk revised april existing heuristics scheduling node edge weighted directed task graph multiple processors produce satisfactory solutions incur high time complexities tend exacerbate realistic environments relaxed assumptions 
consequently heuristics scale handle problems moderate sizes 
natural approach reducing complexity aiming similar potentially better solution parallelize scheduling algorithm 
done partitioning task graphs concurrently generating partial schedules partitioned parts concatenated obtain final schedule 
problem nontrivial exists dependencies nodes task graph preserved generating valid schedule 
time clock scheduling global processors executing parallel scheduling algorithm making inherent parallelism invisible 
introduce parallel algorithm guided systematic partitioning task graph perform scheduling multiple processors 
algorithm schedules tasks messages suitable graphs arbitrary computation communication costs applicable systems arbitrary network topologies homogeneous heterogeneous processors 
implemented algorithm intel paragon compared closely related algorithms 
experimental results indicate algorithm yields higher quality solutions order magnitude smaller scheduling times 
algorithm exhibits interesting trade solution quality speedup scaling problem size 
keywords parallel processing resource management processor allocation scheduling task graphs parallel algorithms message passing multiprocessors 
parallel program modeled directed acyclic graph dag nodes represent tasks edges represent communication costs dependencies tasks goal scheduling algorithm minimize execution time properly allocating sequencing tasks processors precedence constraints preserved 
dag scheduled line structure program terms inter task dependencies task execution communication costs known program execution 
scheduling problem intractable severe restrictions imposed task graph machine model 
simplifying assumptions task graph common structure task graph restricted tree fork join arbitrary nodes graph computation costs communication costs edges zero 
likewise assumptions machine model common number available processors unlimited processors fully connected network links contention free processors homogeneous processing speeds 
problem complex presence assumptions polynomial time algorithms optimal solutions known cases task graph tree unit node weights exist multiple fully connected homogeneous processors ii task graph arbitrarily structured unit node weights machine consists exactly homogeneous processors iii graph interval ordered 
communication cost edges task graph ignored cases restricted unit cost third case 
problem np complete arbitrary communication costs taken account 
due intractability problem heuristics devised obtaining suboptimal solutions affordable amount computation time 
heuristics produce high quality solutions time complexities undesirably high 
furthermore heuristics designed relaxed assumptions tend incur higher time complexities 
heuristics small task graphs scale problem size 
solution quality applicability usually conflict goal reducing time complexity 
reduce time complexity compromising solution quality natural approach parallelize scheduling algorithm 
extra advantage parallel machine algorithm schedule programs short time 
great advances networking communication tools heterogeneous environments parallel machines networks workstations increasingly popular 
environment parallel machine execute fast algorithm schedule tasks multiple heterogeneous workstations 
tackling general dag scheduling problem parallel part unexplored approach despite attempts devising parallel heuristics class problems job shop scheduling problem jobs independent 
known dag scheduling algorithms sequential nature 
reason task ordering technique scheduling widely reckoned inherently sequential time clock ordering tasks global 
parallelism scheduling algorithm invisible 
propose parallel algorithm called pbsa parallel bubble scheduling allocation produces high quality scheduling solutions making specific simplifying assumptions mentioned 
novelty algorithm lies systematic partitioning task graph guides concurrent generation sub schedules 
algorithm works regular irregular graph structures arbitrary communication computation costs handles arbitrarily connected network target processors suitable homogenous heterogeneous processor systems 
remaining organized follows 
section brief overview various approaches proposed dag scheduling problem 
section proposed algorithm discuss design principles 
section includes scheduling examples illustrating operation algorithm 
experimental results section conclude final remarks section 
scheduling problem related static scheduling represent parallel program directed acyclic graph dag known task graph macro dataflow graph 
dag vis set nodes representing tasks set directed edges representing communication message 
edges dag directed capture precedence constraints tasks 
cost node ni denoted wn represents computation costs task cost edge ni denoted cn represents communication cost message 
source node edge called parent node destination node called child node 
node parent called entry node node child called exit node 
node start execution gathered messages parent nodes 
shows example dag subsequent discussion 
assume target platform distributed memory multiprocessor system processor local memory constituting processing element pe communicate pes message passing 
communication cost nodes assigned processor assumed zero 
objective scheduling minimize schedule length defined maximum finish time nodes properly assigning tasks processors precedence constraints preserved 
classify existing algorithms proposed context categories bounded number processors scheduling algorithm schedules dag limited number processors directly 
processors assumed fully connected regard link contention scheduling messages 
unbounded number clusters unc scheduling unc algorithm schedules dag unbounded number clusters 
clusters generated algorithms may mapped processors separate mapping algorithm 
algorithms assume processors fully connected 
arbitrary processor network apn scheduling apn algorithm performs scheduling mapping architecture processors connected network topology 
apn algorithm explicitly schedules communication messages task graph 
network channels care link contention factor 
proposed algorithm belongs class 
task duplication tdb scheduling tdb algorithms duplicates tasks order reduce communication overhead 
duplication classes algorithms 
depicts chronological summary various algorithms reported literature categorization classification method 
summary includes complexities algorithms terms number nodes edges task graph number processors 
classification helps making comparison algorithms class clearly algorithms belonging different classes directly compared 
survey means complete extensive taxonomy general scheduling problem proposed complete overview literature scope 
reported dag scheduling algorithms categorized scheme 
purpose compare proposed algorithm apn algorithms bu dls mh 
tackling general scheduling problem parallel relatively unexplored approach hitherto techniques suggested restricted forms scheduling problem 
proposed paradigm called parallel dynamic interaction pdi developing parallel search algorithms np hard optimization problems 
applied pdi method job shop scheduling problem set independent jobs scheduled homogeneous machines 
de suggested parallel simulated annealing parallel tabu search algorithms task allocation problem task interaction graph tig representing communicating processes distributed systems mapped homogeneous processors 
tig different dag undirected graph precedence constraints tasks 
study proposed parallel genetic algorithm scheduling 
dag scheduling heuristics proposed algorithm explain sequential operation proposed pbsa algorithm discuss graph partitioning parallelization issues 
table summarizes definitions notations 
scheduling serially unc apn tdb bu ov dls lee ez oee sarkar md ov wu gajski dsc yang dcp ov kwok ahmad mcp ov wu gajski ov gill ove baxter patel hwang ish ov lewis lc ove kim browne ov adam ky ov kim yi mh el lewis ov lewis py ov papadimitriou yannakakis ov chen ov chung ranka ov colin ov ahmad kwok ove dag scheduling heuristics 
call processors execute pbsa algorithm physical processing elements ppes order distinguish target processing elements task graph scheduled 
sequential version special case single execute pbsa algorithm 
algorithms starts scheduling nodes tpe 
improves schedule migrating nodes 
refer serialized algorithm simply bsa algorithm 
determine accurate scheduling order arrange nodes order called cpn dominant sequence 
sequence determined follows task graph set nodes edges forming path entry node exit node sum computation communication costs maximum 
set nodes called critical path cp task graph sum computational costs nodes cp provides loose lower bound schedule length graph 
cp easily depth search multiple cps select maximum sum computation costs 
cp nodes cpns important nodes finish times determine final schedule length considered scheduling early possible scheduling process 
cpn start execution parent nodes finished execution 
cpn scheduled parent nodes scheduled 
parent node cpn need cpn 
call node branch node defined node cpn path reaching cpn 
addition cpns class nodes called branch node 
node cpn 
classification nodes connected graph divided categories 
categorization relative importance nodes order cpns 
cpn dominant sequence precedence constraints nodes preserved manner reaching cpn inserted cpn appended sequence topological order parent inserted child 
table definitions notations 
notation definition ni node parallel program task graph ni computation cost node ni called node weight cij communication cost directed edge node ni nj called edge weight total number nodes task graph total number edges task graph tpe processing element pe target system task graph scheduled number processing elements target multiprocessor system physical processing element executing pbsa algorithm number physical processing elements executing pbsa algorithm cp critical path task graph cpn critical path node branch node branch node dat ni possible data available time node ni st start time node ft ni finish time node ni vip ni parent node ni message arrives pivot tpe processor target system nodes migrate adjacent processors proc ni processor accommodating node ni ccr communication computation ratio average edge weight divided average node weight rpn remote parent node local graph partition slave pbsa program earliest possible start time rpn latest possible start time rpn est estimated start time rpn estimated target processor rpn may scheduled cpn dominant sequence constructed procedure called serial injection assigns entire cpn dominant sequence single processor called pivot processor largest number incident links 
serial injection procedure outlined 
serial injection pivot processor initialize cpn dominant sequence empty list tasks 
entry cpn node sequence 
set position 
cpn 
cpns included cpn dominant sequence parent nodes sequence put position sequence increment position 
parent node sequence largest nx nx nx ny nx ties broken choosing parent smaller level 
ny parent nodes sequence put ny position sequence increment position 
recursively include ancestor nodes sequence nodes larger value level considered 
repeat step parent nodes nx sequence 
put nx sequence position 
endif nx cpn 
endwhile append sequence decreasing order level 
inject cpn dominant sequence pivot processor 
scheduling tasks messages employ incremental adaptive approach 
serial injection process nodes incrementally migrating adjacent processors breadth order 
course node migration messages incrementally routed scheduled suitable time slots optimized route 
node migrate start time reduced migration destination processor migration valid choice specified underlying routing scheme 
candidate node migration node data arrival time dat defined time message parent nodes finishes delivery earlier current start time pivot processor 
goal migration schedule node earlier time slot adjacent processors allows largest reduction start time node 
determine largest start time reduction need compute dat start time node adjacent processor 
node scheduled processor processor idle time slot starts node dat large accommodate node 
procedure outlines computation start time node processor 
computation st precondition nodes nq nq scheduled processor 
check exists st max ft qk dat ni wn st ft exists compute max ft dat ni smallest satisfying inequality return value start time processor return procedure states determine start time node processor examine idle time slot node processor 
check overlapping portion idle time slot time period node start execution large accommodate node 
idle time slot start time node earliest proceed try idle time slot 
dat node processor constrained finish times parent nodes message arrival times 
node consideration parent node scheduled 
level node length sum computation communication costs longest path node exit node 
level node length longest path entry node node excluding cost node processor message arrival time parent node simply finish time processor intra processor communication time ignored 
hand parent node scheduled processor message arrival time depends message routed scheduled links 
schedule message link search suitable idle time slot link accommodate message 
message scheduled link link idle time slot source node finish time large accommodate message 
procedure outlines scheduling message ex ni link computation mst precondition messages em scheduled link 
check exists mst ek max mft ek ft ni proc ni cij mst em mft eo 
exists compute smallest satisfying inequality return value start time denoted return procedure determines start time message link similar manner procedure determining start time node processor 
dat node processor simply largest value message arrival times 
parent node corresponds largest value message arrival time called important parent vip 
sequential bsa algorithm outlined 
procedure build processor list constructs list processors breadth order pivot processor 
procedure serial injection constructs cpn dominant sequence nodes injects nodes pivot processor 
bsa algorithm load processor topology input task graph pivot tpe processor highest degree max mft er ft ni proc ni ex mst ex processor list build processor list pivot tpe serial injection pivot tpe processor list empty pivot tpe processor processor list ni pivot tpe st ni pivot tpe dat ni pivot tpe proc vip ni pivot tpe determine dat st ni adjacent processor pe exists pe st ni pe st ni pivot tpe allow ni migrate pivot tpe pe update start times nodes messages st ni pe st ni pivot tpe proc vip ni pe allow ni migrate pivot tpe pe update start times nodes messages procedure build processor list takes op time involves breadth 
degree pe defined number incident edges processor network graph 
traversal processor graph 
procedure serial injection takes oe time cpn dominant sequence constructed oe time 
dominant step loop step step 
loop takes oe time compute st dat values node adjacent processor 
migration done takes oe time 
ov nodes pivot tpe op adjacent processors iteration loop takes time 
bsa algorithm takes op time 
target processors heterogeneous decision migration taken determined finish times nodes start times 
heterogeneous processors execution time task varies different processors task start time distinct processors finish times different 
pivot processor cp length shortest order minimize finish times cpns exploiting heterogeneity processors 
ev scheduling parallel parallelize algorithm partitioning dag multiple parts 
simple approach partition dag horizontally vertically 
horizontal partitioning means dividing dag layers nodes paths dag 
contrast vertical partitioning means dividing dag paths 
illustrates simple techniques 
partitioned pieces scheduled independently resultant schedules combined 
efficiency horizontal vertical partitioning depends graph structure 
pbsa algorithm partition cpn dominant sequence vertical horizontal combination 
number partitions equal number ppes available 
independently schedules nodes belonging partition sequential bsa algorithm 
sub schedules partitions generated concatenated form final complete schedule 
horizontal partitioning vertical partitioning simple strategies partitioning task graph 
due dependencies nodes adjacent partitions ppes share global information scheduling process 
specifically attempts schedule node partition know finish time parent node partition called remote parent node rpn determine earliest starttime 
straightforward approach ppes communicate exchange information start times scheduling process 
approach efficient local schedules ppes need revised resulting excessive overhead due communication ppes limits speedup 
approach estimated information inter communication minimized 
estimates definitions 
definition earliest possible start time node largest sum computation costs entry node node including node 
definition latest possible start time node sum computation costs node serial injection ordering node including node 
start time node bounded due precedence constraints bounded node start time node dag serialized serial injection process 
represent optimistic estimate pessimistic estimate start time rpn respectively 
rpn scheduled start time extremes 
crux pick accurate estimate rpn start time values extremes 
approach rpn cpn take optimistic estimate start time take conservative estimate 
specifically estimated start time rpn taken cpn taken parameter called importance factor indicates timeliness scheduling rpn larger value implies rpn estimated start time closer smaller value implies estimated start time closer 
case positive real number determined heuristically 
section experimental results effect schedule lengths 
addition estimated start time rpn need know tpe rpn scheduled 
essential determining dat node scheduled order select suitable tpe node 
estimate manner rpn cpn assume scheduled tpe highest level cpn local partition randomly pick tpe rpn scheduled 
call tpe rpn estimated tpe 
pbsa algorithm designated master ppes slaves 
master responsible pre scheduling post scheduling 
includes serial injection task graph partitioning concatenation including resolving conflicts sub schedules 
slave procedure pbsa outlined 
procedure pbsa slave receive target processor network pbsa master 
receive graph partition information ests pbsa master 
apply serial bsa algorithm graph partition 
rpn est determining dat node scheduled local partition 
send resulting sub schedule pbsa master 
derive time complexity suppose nodes local partition pbsa slave 
step pbsa slave dominant step complexity pbsa slave op number edges local partition 
concatenating sub schedules master procedure concatenates sub schedules 
concatenation process involves matching adjacent sub schedules 
obviously exhaustive matching feasible 
reduce time complexity sub schedules concatenation employ twophase method 
objective phase minimize start time important node sub schedule 
node cpn 
second phase rearranging exit nodes nodes successor partition sub schedule start earlier 
rearrangement potentially important node sub schedule start earlier 
phase sub schedule earliest node determined 
call node leader node tpe leader node scheduled leader tpe 
leader node succeeding nodes leader tpe concatenated tpe preceding sub schedule start time leader node scheduled early possible 
tpe called image leader tpe 
neighboring leader tpe concatenated corresponding neighboring leader tpe image 
done breadth manner 
concatenation process nodes may need move start original scheduled times accommodation inter partition communication messages 
addition corresponding schedule communication messages may need adjusted 
second phase sub schedule concatenated preceding sub schedule exit nodes sub schedule examined re scheduling 
specifically exit node re scheduled tpe allows minimum start time 
procedure performing phase concatenation process called concat schedules outlined 
procedure concat schedules pair adjacent sub schedules determine earliest node sub schedule 
call leader node 
call tpe leader tpe 
concatenate nodes scheduled tpe accommodating leader node tpe sub schedule leader node start early possible 
concatenate nodes sub schedule breadth order neighbors leader tpe 
re schedule exit nodes sub schedule start early possible 
walk concatenated schedule resolve conflict actual start times estimated start times 
derive time complexity suppose nodes subschedule 
step step take om time steps take om time time complexity concat schedules number sub schedules 
number ppes final schedule generated concatenating sub schedules procedure concat schedules valid schedule 
formalized theorem lemma final schedule produced concat schedules preserves precedence constraints 
proof clearly precedence constraints sub schedule preserved slave opm program 
hand inter partition precedence constraints preserved cpn dominant sequence maintains precedence constraints 
step procedure concat schedules resolves potential conflict pairs adjacent pushing nodes succeeding sub schedules 
theorem proved 
qed procedure concat schedules master procedure pbsa algorithm outlined 
procedure pbsa master load processor network topology input task graph 
serial injection partition task graph equal sized sets number node ppes available 
determine ests partitions 
broadcast processor network topology pbsa slave 
send particular graph partition corresponding ests pbsa slave 
wait pbsa slave finish 
concat schedules ppes maximum size partition dominant steps pbsa master steps 
describe step takes op step takes time complexity pbsa master time complexity analyze theoretical speedup denoted pbsa algorithm respect serial bsa algorithm start expression opm pbsa pbsa second term denominator smaller term ignore get approximate theoretical speedup pbsa serial bsa speedup grows square number ppes superlinear 
superlinear speedup mainly due fact pbsa algorithm estimates allowing spend time scheduling inter partition edges 
scheduling examples ev 
dropping ceiling operators pbsa ev ev 
ep pbsa op section examples demonstrate operation proposed algorithm task graph shown 
describe schedules generated serial bsa algorithm pbsa algorithm ppes ring homogeneous target processors 
cpn dominant sequence constructed follows 
critical path task graph cpn dominant sequence follows see cpns marked asterisk 
node level level cpn dominant order levels levels cpn dominant sequence nodes 
pe pe pe pe pe pe pe pe pe pe pe pe intermediate schedule produced bsa serial injection schedule length total comm 
cost intermediate schedule migrate neighboring processors schedule length total comm 
cost intermediate schedule migrates pe schedule length total comm 
cost final schedule migrates pe schedule length total comm 
cost 
pe pe pe pe serial bsa algorithm allocates entire cpn dominant sequence pivot processor pe 
phase migrate scheduled finish earliest possible times 
migrates pe start time improve 
similarly migrates neighboring processor pe migrates pe 
shows intermediate schedule migrations 
migrates pe see 
cpn migrates pe vip scheduled 
migration allows move 
resulting schedule shown final schedule nodes migrate improve start times 
final schedule length 
example executed mh dls bu algorithms 
schedule length mh dls algorithms bu algorithm 
rpn est pe pe pe pe execute pbsa algorithm ppes task graph 
cpn dominant sequence task graph partitioned sets see 
shows estimated parameters remote parent nodes 
partitioning value importance factor taken nodes equal respective 
occupy earliest possible positions cpn dominant sequence 
ests nodes equal independent ests correct schedules shown 
chosen pe 
equal sum computation costs hand equal sum computation costs list nodes est cpn randomly st partition nd partition rd partition calculation calculation estimated values 
pe pe pe pe pe schedule partition schedule second partition schedule third partition 
chosen pe 
parallel execution pbsa slave results sub schedules shown 
clarity partition shown sub schedules 
concatenation sub schedules see partitions concatenated directly need resolving conflicts 
accurate estimations start times 
concatenation second third partitions needs explanation 
leader task partition concat schedules procedure minimize start time 
accomplish appended tpe tpe 
turn scheduled tpe 
appended tpe final walk schedule done minimization start times 
generated schedule similar bsa algorithm schedule length 
difference scheduling scheduling affect final schedule length 
value est rpn estimated scheduling processor turns decision 
need wait data start earlier 
note total communication costs incurred combined schedule larger bsa algorithm 
reason bsa algorithm allow nodes migrate target processors start times improve 
slave program pbsa algorithm global knowledge intermediate state schedule attempts locally schedule node start earliest possible time resulting higher utilization communication links 
pe pe pe pe pe pe pe sub schedules concatenation process combined final schedule generated pbsa algorithm schedule length total comm 
costs incurred 
results section performance results pbsa algorithm compare mh dls bu algorithms 
mh dls bu algorithms sequential compare performance serialized bsa algorithm 
furthermore compare solution quality efficiency pbsa bsa algorithms observe trade solution quality running time 
workload testing algorithms generated suites task graphs regular graphs irregular graphs 
regular graphs represent parallel applications including gaussian elimination cholesky factorization fft 
exists unique critical path gaussian elimination cholesky factorization graphs 
lengths paths gaussian elimination graph critical path 
fft graphs dense graphs contain edges nodes 
furthermore paths fft graph equal length critical paths 
includes miniature examples regular graphs 
applications operate matrices number nodes edges task graphs depends size data matrix 
number nodes task graph application roughly dimension matrix 
numbers nodes pe pe pe pe applications 
experiments varied increments number nodes graphs range approximately 
suite irregular task graphs consists graphs randomly generated structures 
type graph chose values ccr 
value ccr equal represents computation intensive task graph coarse granularity value represents communication intensive task graph fine granularity value represents graph computation communication just balanced 
regular graphs generated weights nodes edges average value ccr corresponded 
generated irregular graphs follows randomly selected computation cost node graph uniform distribution mean equal minimum maximum 
node chose random number indicating number children uniform distribution mean equal connectivity graph increases size graph 
randomly selected communication cost edge uniform distribution mean equal times specified value ccr 
sizes random graphs range nodes increments 
target system topologies node hypercube mesh node fully connected network node random topology 
assume target systems composed homogeneous processors 
stated results pbsa algorithm generated importance factor estimating start times equal 
implemented scheduling algorithms intel paragon xp sequential algorithms mh dls bu bsa executed single processor xp paragon 
pbsa algorithm processors 
schedule lengths speedups experiment compared schedules produced bsa algorithm pbsa algorithm paragon processors mh dls bu algorithms types task graphs various sizes target topologies 
graph size processor network topology significant impact relative schedule lengths value ccr affect schedule length 
shows impact ccr ratios schedule length generated mh dls bu pbsa algorithms bsa algorithm 
point curves average miniature examples gaussian elimination task graph cholesky factorization task graph fft task graph 
schedule length ratio schedule length ratio mh bsa dls bsa bu bsa pbsa bsa ccr mh bsa dls bsa bu bsa pbsa bsa ccr schedule length ratios graphs topologies 
results observations bsa algorithm outperforms mh dls bu algorithms ratios greater margin improvement different algorithm largest improvement bu algorithm random graphs minimum improvement mh algorithm margin improvement larger value ccr large pbsa algorithm ppes outperforms mh dls bu algorithms cases gaussian elimination cholesky factorization graphs pbsa algorithm performed comparably dls algorithm smaller values ccr 
fft graphs pbsa algorithm performed worse dls mh algorithm 
pbsa algorithm outperformed bu algorithm regular graphs fft graphs ccr equal 
observations explained reasons 
cpn dominant sequence captures relatively better tasks ordering scheduling important node scheduled earlier time 
closer look scheduling traces bu algorithm reveals inferior performance primarily due strategy evenly schedule length ratio mh bsa dls bsa bu bsa pbsa bsa ccr gaussian elimination graphs cholesky factorization graphs schedule length ratio mh bsa dls bsa bu bsa pbsa bsa ccr fft graphs random graphs average ratios schedule lengths generated mh dls bu pbsa ppes algorithms bsa algorithm types graphs ccr 
distributing tasks processors 
hand mh dls algorithms perform relatively better bu minimize start times nodes 
incremental message scheduling strategy bsa algorithm major reason better performance 
bu dls mh algorithms scheduling messages relies information stored infrequently updated routing tables 
inaccuracy tables inevitably leads inefficient utilization communication links links contended messages links idle 
turn delays start times nodes 
longer schedule length produced pbsa ppes compared sequential counterpart bsa due reasons inaccuracies incurred due estimation start times ii procedure merging partial schedules cause additional performance degradation 
observing scheduling traces adverse effect inaccurate estimation profound task graph contains multiple critical paths 
random graphs general structures pbsa algorithm yields better performance 
table provides scheduling times seconds serial algorithms single node paragon values taken average target topologies ccrs 
running times pbsa algorithm ppes paragon included comparison 
observe running times sequential algorithms approach thousands seconds number nodes 
dls algorithm takes significantly longer time algorithms 
instance schedule node random graph pbsa algorithm takes minutes dls algorithm takes hours 
results indicate mh faster dls bsa faster dls 
bu algorithm fastest sequential algorithms 
contrast running times pbsa algorithm nearly orders magnitude sequential algorithms demonstrating superlinear speedup 
pbsa algorithm useful generating schedules large task graphs 
investigate effects parallelization applied pbsa algorithm types graphs processors paragon 
computed schedule length ratios respect serial bsa algorithm 
results summarized 
effect task graph size graph type target topology insignificant show average schedule length ratios graph types graph sizes topologies point curves representing average ratios topologies graphs 
performance degradation percentage pbsa respect bsa ranges 
cases performance degradation 
noted earlier degradation pbsa performance due inaccuracy estimation sub schedules concatenation 
effects aggravate increase number processors 
amount degradation smaller compared improvements bsa yielded mh dls bu 
observe average schedule length ratios implying performance pbsa better algorithms 
interesting observation average schedule length ratios pbsa mh dls bu smaller ccr larger contrast observe ratios increase ccrs 
observation implies compared pbsa algorithm performance mh dls bu table average running times mh dls bu bsa pbsa ppes algorithms various task graphs ccrs target topologies 
graph type graph sizes cholesky factorization gaussian elimination fft random matrix nodes matrix nodes matrix nodes matrix nodes matrix nodes matrix nodes matrix nodes matrix nodes matrix nodes matrix nodes matrix nodes matrix nodes matrix nodes matrix nodes matrix nodes matrix nodes matrix nodes matrix nodes matrix nodes matrix nodes matrix nodes matrix nodes matrix nodes matrix nodes matrix nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes algorithms sensitive value ccr 
calculated speedups dividing running times mh dls bu serial bsa algorithms pbsa algorithm 
shows speedup pbsa processors various sizes task graphs regular task graphs cholesky factorization gaussian elimination fft random graphs 
results indicate parallelization strategy pbsa positive negative effects 
negative effects mean potential inaccurate decisions scheduling cause longer schedule length 
positive side algorithm faster workload reduced 
shown section theoretical speedup pbsa respect serial bsa op superlinear 
plots indicate parallel pbsa run processors times faster serial bsa 
ppes speedup appears increase linearly 
ppes speedup close 
observed speedup agrees predicted superlinear speedup 
scalability pbsa algorithm mh dls bu bsa pbsa ppes investigate scalability pbsa algorithm larger number ppes 
results section indicated accuracy start times estimation critically affects quality final schedule 
appears accuracy deteriorates increasing algorithm pbsa mh ccr ccr ccr ccr ccr ccr number ppes number partitions increases 
applied pbsa algorithm node random graphs ccr processors paragon noted average absolute percentage error ests estimated start times schedule length ratios pbsa bsa 
absolute percentage error est defined follows est st 
st pbsa bu pbsa bsa average schedule length ratios pbsa algorithm mh algorithm dls algorithm bu algorithm bsa algorithm graphs topologies 
speedup pbsa mh pbsa dls pbsa bu pbsa bsa number ppes speedup pbsa dls pbsa mh pbsa dls pbsa bu pbsa bsa number ppes regular graphs random graphs average speedups pbsa respect sequential algorithms regular graphs gaussian elimination cholesky factorization fft random graphs 
ccr ccr ccr ccr ccr ccr percentage measured rpn average computed 
indicates percentage error est percent small values ccr ppes 
percentage error larger ccr equal 
reason range probable st rpn larger communication costs larger 
percentage error percent cases schedule lengths degrade quite rapidly ppes seen 
degradation occurs concatenation process cases enhances adverse effect inaccuracy start times estimation 
schedule lengths factor generated bsa algorithm 
percentage error ests ccr ccr ccr number ppes results imply pbsa algorithm reasonably scalable solution quality speedup 
furthermore exhibits trade performance scheduling time providing user choice faster version loss performance slower version better performance 
comparison different partitioning strategies section experimental results illustrate efficacy cpn dominant sequence partitioning strategy parallelization pbsa algorithm 
compared scheme simple graph theoretic methods level sequence method task graph partitioned number horizontal layers see manner 
depth search graph traversed entry nodes exit nodes level number entry node set 
node having parent node level level number assigned nodes layer sorted descending order number children 
sequence nodes constructed partitioned number ppes available 
random topological sequence method nodes task graph topologically sorted simple depth search regard node edge weights 
resulting topological list nodes randomly perturbed swapping randomly selected pairs independent nodes precedence constraints preserved 
example task graph shown suppose initial topological list swapped 
list nodes obtained partitioned number ppes 
level strategy variant horizontal partitioning dag 
random schedule length ratio pbsa bsa ccr ccr ccr number ppes average absolute percentage error estimated start times average schedule length ratios pbsa bsa node random graphs 
topological strategy arbitrary combination horizontal vertical partitioning 
note purely vertical partitioning strategy concatenation scheme 
compare different partitioning strategies modified serial injection process pbsa master procedure implement versions pbsa algorithm original pbsa ii pbsa level pbsa level partitioning iii pbsa random pbsa random topological partitioning 
applied versions node random graphs compared average schedule lengths 
includes results experiments 
apparent results cpn dominant sequence partitioning considerably effective graph theoretic methods 
number ppes effects start times estimation technique study effect importance factor performance pbsa algorithm set previous results varied increments value ran pbsa algorithm node random graphs values ccr 
shows plots ratios average schedule lengths graphs generated pbsa bsa observe ccr small noticeable effect 
smaller value ccr incurs communication values differ 
ccr moderate curves tend convex smaller value worse performance pbsa 
optimal values lie 
main reason small estimations bias accurate moderate ccr 
value ccr large curves convex 
performance highly sensitive extreme values case small value better estimations bias accurate large ccr communication weights large nodes tend scheduled late 
results setting reasonable compromise handling general task graphs 
schedule length parallelization multiprocessor scheduling algorithm natural approach reducing time complexity algorithm 
novel parallel algorithm provide scalable schedule useful scheduling large task graphs virtually impossible schedule sequential algorithms 
proposed pbsa cpn pbsa level pbsa random performance different task graph partitioning strategies 
algorithm outperforms known algorithms reported literature requiring significantly shorter running times 
number avenues extending research 
pbsa algorithm yields considerable speedup sequential bsa algorithm performance degradation observed 
primarily due inaccuracy estimating start times parent nodes belong current partition 
new heuristics needed improving accuracy estimation 
acknowledgments referees constructive comments greatly improved quality 
particularly extended referee insightful suggestions helped better presentation experimental results 
research supported hong kong research council contract numbers 
preliminary version portions international parallel processing symposium 
ccr ppes ppes ppes ppes adam chandy dickson comparison list scheduling parallel processing systems communications acm vol 
dec pp 

ahmad 
kwok exploiting task duplication parallel program scheduling ieee transactions parallel distributed systems vol 
pp 
sept 
ali el time complexity scheduling interval orders communication polynomial parallel processing letters vol 
pp 

ppes ppes ppes ppes ccr effect parameter schedule length 
ccr ppes ppes ppes ppes baxter patel algorithm heuristic static task allocation algorithm proceedings international conference parallel processing vol 
ii aug pp 

taxonomy scheduling general purpose distributed computing systems ieee transactions software engineering vol 
feb pp 

chung ranka application performance analysis compile time optimization approach list scheduling algorithms distributed memory multiprocessors proceedings supercomputing nov pp 

chen shirazi marquis performance evaluation novel scheduling method linear clustering task duplication proceedings international conference parallel distributed systems dec pp 

colin scheduling small computation delays task duplication operations research pp 

de del analysis parallel heuristics task allocation multicomputers computing archiv fur informatik und vol 
pp 

el lewis scheduling parallel programs arbitrary target machines journal parallel distributed computing vol 
june pp 

garey johnson computers intractability guide theory np completeness freeman 
hwang chow anger lee scheduling precedence graphs systems interprocessor communication times siam journal computing vol 
apr pp 

kim browne general approach mapping parallel computation multiprocessor architectures proceedings international conference parallel processing vol 
ii aug pp 

kim yi pass scheduling algorithm parallel programs parallel computing pp 

lewis grain size determination parallel processing ieee software jan pp 


kwok ahmad dynamic critical path scheduling effective technique allocating task graphs multiprocessors ieee transactions parallel distributed systems vol 
may pp 

efficient scheduling arbitrary task graphs multiprocessors parallel genetic algorithm journal parallel distributed computing vol 
nov pp 

lee hurson feng vertically layered allocation scheme data flow systems journal parallel distributed computing vol 
pp 

gill automatic determination grain size efficient parallel processing communications acm vol 
sept pp 

bottom approach task scheduling distributed memory multiprocessor proceedings international conference parallel processing vol 
ii aug pp 


lieu wei task clustering scheduling distributed memory parallel architectures ieee transactions parallel distributed systems vol 
jan pp 

papadimitriou yannakakis architecture independent analysis parallel algorithms siam journal computing vol 
apr pp 

inherently parallel method heuristic problem solving part general framework ieee transactions parallel distributed systems vol 
oct pp 

ramamoorthy chandy gonzalez optimal scheduling strategies multiprocessor system ieee transactions computers vol 
feb pp 

sarkar partitioning scheduling parallel programs multiprocessors mit press cambridge ma 
shirazi wang pathak analysis evaluation heuristic methods static scheduling journal parallel distributed computing pp 

lee compile time scheduling heuristic interconnection constrained heterogeneous processor architectures ieee transactions parallel distributed systems vol 
feb pp 


wu gajski programming aid message passing systems ieee transactions parallel distributed systems vol 
july pp 

yang dsc scheduling parallel tasks unbounded number processors ieee transactions parallel distributed systems vol 
sep pp 


