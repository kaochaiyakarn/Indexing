solve automatically selection problem solving methods eugene fink computer science department carnegie mellon university pittsburgh pa eugene cs cmu edu www cs cmu edu eugene ideas past experience 
polya solve 
choice appropriate problem solving method available methods crucial skill experts areas 
describe technique automatic selection methods statistical analysis past performances 
formalize statistical problem involved selecting efficient problem solving method derive solution problem describe method selection algorithm 
algorithm chooses available methods decides abandon chosen method proves take time 
give empirical results technique selecting search engines prodigy planning system 
choice appropriate problem solving method main themes polya famous book solve polya 
polya showed selection effective approach problem crucial skill mathematician 
psychologists accumulated evidence confirms polya pioneering insight performance experts depends ability choose right approach problem newell simon 
purpose research automate selection problem solving method 
research motivated prodigy planning system includes search engines veloso stone 
need provide mechanism deciding search engine appropriate problem 
second programs real world run forever need means decide interrupt unsuccessful search 
researchers long realized importance automatic evaluation selection search algorithms developed techniques various special cases problem 
particular horvitz described framework evaluating algorithms trade offs computation cost solution quality framework automatic selection sorting algorithm horvitz 
breese horvitz designed decision theoretic algorithm evaluates different methods belief network inference selects optimal method breese horvitz 
hansson mayer russell applied related evaluation selection techniques problem choosing promising branches search space 
copyright american association artificial intelligence www aaai org 
rights reserved 
russell subramanian parr formalized general problem selecting alternative problem solving methods dynamic programming solve special cases problem russell 
minton developed inductive learning system configures programs selecting alternative search strategies minton 
hansen zilberstein studied trade offs running time simple time algorithms designed dynamic programming technique deciding terminate search hansen zilberstein 
mouaddib zilberstein developed similar technique hierarchical knowledge algorithms mouaddib zilberstein 
previous results applicable problem selecting prodigy search algorithms developed techniques rely analysis sufficiently large sample past performance data 
apply prodigy new domain new heuristics control rules usually little prior data 
acquiring data impractical experimentation expensive solving problem 
develop novel selection technique best available data provide accurate estimate 
combine exploitation past data exploration new alternatives enables collect additional performance data system solves problems 
consider task setting time bound chosen problem solving method 
previous results deciding time terminating time algorithms applicable task problem solvers time behavior satisfy assumptions past studies 
provide statistical technique selecting time bounds 
demonstrate determining appropriate bound crucial efficient problem solving choosing right method 
techniques aimed selecting method time bound solving problem 
provide means switching method revising selected bound search solution 
developing means important open problem 
selection prodigy algorithms provided motivation developed technique rely specific properties prodigy system 
selection algorithm applicable choosing multiple problem solving methods ai system 
equally effective small large scale domains 
selection takes little computation usually negligible compared problem solving time 
formalize statistical problem estimating expected performance method past experience section 
derive solution problem section selecting method time bound section 
describe measure problem complexity improve performance estimate section 
note need perfect estimate need accuracy sufficient selecting right method setting close optimal time bound 
give empirical results technique select prodigy planning engines 
show chooses appropriate engine time bound 
time making selection orders magnitude smaller prodigy planning time section 
generality statistical technique applicable practical problems outside artificial intelligence 
illustrate applying learning algorithm decide long wait phone hanging answer 
motivating example suppose prodigy construct plans transporting packages different locations city veloso 
consider planning methods 
control rules designed veloso perez guide prodigy search transportation domain 
method applies selected planning operators early possible call apply 
second method uses control rules special rule delays operator application forces emphasis backward search veloso stone call delay 
distinction apply delay similar planners implemented veloso stone third method alpine knoblock combination apply abstraction generator determines relative importance elements planning domain 
alpine ignores important elements builds solution outline refines solution care initially ignored details 
experiments demonstrated delaying operator execution improves efficiency domains slows prodigy stone abstraction gives drastic time savings worsens performance knoblock bacchus yang 
reliable way select efficient method domain empirical comparison 
application method problem gives outcomes may solve problem may terminate failure exhausting available search space finding solution may interrupt reaches pre set time bound termination 
table give results solving transportation problems methods 
denote successes failures hitting time bound note data illustrating selection problem purpose general comparison planners 
relative performance may different domains 
time sec outcome apply delay alpine packs table performance apply delay alpine transportation problems 
glance data reveals apply performance domain probably best 
statistical analysis confirm intuitive 
show choose time bound selected method 
may evaluate performance dimensions percentage solved problems average success time average time case failure interrupt 
compare different methods need specify utility function takes account dimensions 
assume pay running time get certain reward solving problem 
method solves problem gain gamma time 
particular time gain negative better trying solve problem 
method fails hits time bound gain 
need estimate expected gain candidate methods time bounds select method bound maximize expectation gives statistical problem 
problem suppose method solved problems failed problems interrupted hitting time bound problems 
success times failure times fm interrupt times reward solving new problem time bound estimate expected gain reward time bound determine standard deviation estimate 
stationarity assumption valiant assume past problems new problem drawn randomly population probability distribution 
assume method performance improve time 
statistical foundations derive solution statistical problem 
assume convenience success failure interrupt times sorted increasing order fm consider case time larger lowest past bounds number success times larger similarly number failures estimate expected gain averaging gains obtained past reward time bound method solve problems earning gains gammas gammas gammas failure times resulting negative gains gammaf gammaf gammaf remaining gammac gammad cases hit time bound time earning gammab 
expected gain equal mean gains gamma gamma gamma gamma gamma computed mean random selection problems may different mean problem population 
estimate standard deviation expected gain formula deviation sample mean sqr gamma sum gamma sum gamma gamma gamma gamma gamma sqr gamma gamma gamma show dependency expected gain time bound methods 
give dependency different values reward dash dot lines dashed lines solid lines 
dotted lines show standard deviation reward lower line deviation estimate upper line deviation 
consider case larger past interrupt times example suppose interrupted alpine problem seconds problem seconds obtaining data shown table need estimate gain 
apply time bound delay time bound alpine time bound dependency expected gain time bound rewards dash dot lines dashed lines solid lines 
dotted lines show standard deviation gain estimate reward 
alpine time delta delta delta weight time delta delta delta weight time delta delta delta table distributing chances interrupt times larger time outcomes 
directly gain estimate time bound cause method run old bounds 
re distribute corresponding probabilities outcomes 
interrupted method past succeeded failed larger time hit larger time bound 
may estimate expected outcome data past problem method ran remove sample distribute chance occur higher time outcomes 
example table problems larger times 
remove sample data increase weights larger time outcomes see table 
distribute weight times 
example problems larger times 
distribute weight problems increasing weight table 
repeat process denote resulting weights weights success failure interrupt times larger weight denote obtained gammae weighted times 
compute expected gain gamma gamma gamma gamma gamma gamma wb similarly weights estimating standard deviation expected gain sqr gamma sum gamma gamma sum gamma gamma gamma gammac gammad gammae wb sqr gamma gammac gammad gammae wb application formulas data table alpine reward time bound gives expected gain standard deviation 
assumed derivation execution cost proportional running time 
may readily extend results monotone dependency time cost replacing terms gamma gammaf gammab complex functions 
note past rewards statistical estimate 
reward may differ rewards earned sample problems 
may extend results situations reward function solution quality constant works function change problem problem 
replace terms gamma gamma reward corresponding problem 
resulting expression combines estimate expected reward expected running time 
full fink describe efficient algorithm computes gain estimates estimate deviations multiple values time bound algorithm determines weights finds gain estimates pass sorted list success failure interrupt times time bound values 
time bounds sample problems time complexity 
complexity pre sorting lists log practice takes time rest computation 
implemented algorithm common lisp tested sun 
running time delta delta gamma seconds 
selection method time bound describe statistical estimate choose problem solving methods determine appropriate time bounds 
provide heuristics combining exploitation past experience exploration new alternatives 
basic technique estimate gain number time bounds available method select method time bound maximal gain 
example reward transportation domain best choice apply time bound gives expected gain 
choice corresponds maximum dashed lines 
expected gain time bounds negative better solving problem 
example available method delay reward see dash dot line skip problem 
method past success times candidate time bounds 
compute expected gain bounds 
computed gain time bound get smaller gain closest lower success time extending time bound increase number successes past problems 
describe technique incremental learning performance available methods 
assume past experience accumulate performance data solve problems 
new problem statistical analysis select method time bound 
applying selected method add result performance data 
need choose method time bound past experience 
need deviate maximal expectation selection order explore new opportunities 
selection maximizes expected gain stuck method yielded success set time bound higher success time 
constructed statistical model combining exploration exploitation 
provide solution proved selecting prodigy planners 
consider selection time bound fixed method show select method 
previous data method performance set time bound equal reward 
suppose accumulated data method performance enable determine bound maximal expected gain 
encourage exploration select largest bound expected gain different maximum 
denote maximal expected gain max standard deviation oe max suppose expected gain bound deviation oe 
expected difference gain maximal gain max gamma estimates normally distributed standard deviation expected difference oe max oe estimate deviation approximation distribution small samples may student normal max independent computed data 
say different maximal gain ratio expected difference deviation bounded constant 
set constant tends give results max gammag oe max oe select largest time bound gain estimate satisfies condition 
results selection strategy 
ran methods transportation problems table order 
horizontal axes show problem number vertical axes running time 
dotted lines show selected time bounds dashed lines mark time bounds give maximal gain estimates 
solid lines show running time touch dotted lines methods hit time bound 
successfully solved problems marked circles failures shown pluses 
apply total gain average problem 
maximal gain time bound problems gain problem 
incremental learning yielded near maximal gain spite initial ignorance 
apply estimate maximal gain bound solving problems 
differs bound table bounds ensure near maximal gain prevents sufficient exploration 
delay total gain problem 
data table find optimal bound solved problems bound earn problem 
incremental learning gain thirds gain obtained advance knowledge 
alpine total gain problem 
estimate table gives bound result earning problem 
apply delay alpine eventually optimal bound 
note choice time bound converged optimal experiments 
additional tests shown insufficient exploration prevents finding optimal bound half cases fink 
tried encourage exploration increasing bound max gammag oe max oe 
selected bound converges optimal performance worsens due larger time losses unsolved problems 
describe incremental learning select problem solving method 
past data apply problem number delay problem number alpine problem number results incremental learning time bound running times solid lines time bounds dotted lines maximal gain bounds dashed lines 
successes marked circles failures pluses 
problem number results incremental selection method time bound transportation problems 
graph shows running times solid line successes failures 
rows symbols solid line show selection apply delay alpine 
method select unknown method encouraging exploration 
past data methods select time bound method 
method selected bound find probability best methods 
statistical test estimate probability method better 
compute probability method best product probabilities outperforms individual methods 
computation approximation probabilities multiply independent 
weighted random selection methods chance selecting method equal probability best methods 
strategy leads frequent application methods perform encourages exploratory poor performers 
show results selection strategy transportation domain reward 
experiment problems table additional transportation problems 
horizontal axis shows problem number vertical axis running time 
rows symbols curve show selection planner circle apply cross delay asterisk alpine 
total gain problem 
selection converges apply time bound optimal set problems 
selection problems earn problem 
problem sizes considered task finding problem solving method time bound problems domain 
estimate sizes problems improve performance adjusting time bound apply delay alpine dependency success time problem size 
top graphs show regression polynomial dependency bottom graphs exponential dependency 
problem size 
define problem size easily computable positive value correlates problem complexity larger value longer usually takes solve problem 
finding accurate measure complexity difficult task domains provide rough complexity estimate 
transportation domain estimate complexity number packages delivered 
rightmost column table show number packages problem 
regression find dependency sizes sample problems times solve 
separate regressions success times failure times 
assume dependency time size polynomial exponential 
polynomial logarithm time depends linearly logarithm size exponential dependency time logarithm depends linearly size 
linear square regression find polynomial exponential dependencies 
give results regressing success times transportation problems table 
top graphs show polynomial dependency success time problem size bottom graphs exponential dependency 
horizontal axes show problem sizes number packages vertical axes running time 
circles show sizes times problem instances solid lines regression results 
evaluate regression test 
value problem size scaling success times failure time delay package problem 
ratio estimated slope regression line standard deviation slope estimate 
test converts value probability regression better ignoring sizes simply mean time probability called value 
regression gives fit sample data large small 
give values corresponding intervals value 
regression probability smaller certain bound 
experiments set bound problem sizes 
value customary early detection dependency sizes times important efficiency establishing high certainty dependency 
select polynomial exponential regression value prefer regression larger example polynomial regression wins methods 
note square regression related test quite strong assumptions nature distribution 
problems fixed size distribution time logarithms normal time distributed log normally 
second problem sizes standard deviation distribution 
regression usually provides approximation dependency size time assumptions satisfied 
problem size estimating gain scaling times sample problems size 
illustrate regression scale delay times package success package success package failure estimating gain package problem package size marked vertical dotted line 
slope success regression scaling success times slope failure regression scaling failures 
slope scaling interrupt depend algorithm succeed fail interrupt know outcome occur 
simple heuristic choosing success failure slope smaller sample problems time computing polynomial exponential regression slope selecting regressions scaling sample times delta delta gamma seconds 
incremental learning package time bound packages time bound packages time bound dependency apply expected gain time bound rewards dash dot lines dashed lines solid lines 
dotted lines show standard deviation gain estimate reward 
time bound implemented procedure incrementally updates slope value adding new problem sample 
procedure reduces amortized time statistical computation delta delta gamma seconds problem 
scaling sample times size technique section compute gain estimate standard deviation 
difference reduce second term denominator deviation formula success failure regressions reduce number degrees freedom sample 
compute deviation follows delta gammae gamma show dependency expected gain time bound apply package package package problems 
problem sizes incremental selection experiments section gain problem learning bound apply vs gain obtained sizes learning delay vs sizes learning alpine vs experiment vs 
average running time regression scaling seconds problem 
time statistical computation problem sizes smaller resulting gain increase 
results show problem sizes increases gain 
conducted additional experiments artificially generated data fink demonstrated running times better correlate problem sizes gain increase significant 
empirical examples demonstrated effectiveness statistical selection simple transportation domain 
give results domains 
consider extension transportation domain airplanes carry packages cities vans local delivery cities veloso 
table give performance apply de time sec outcome apply delay alpine packs table performance extended transportation domain 
lay alpine problems 
results incremental learning time bound apply delay alpine reward top graphs legend 
obtained results problem sizes 
apply learning gives gain problem eventually selects bound 
optimal bound set problems 
optimal bound problems earn problem 
estimate problem sizes number packages delivered sizes learning time bound apply gain problem 
delay gains problem chooses bound learning 
actual optimal bound delay problems give problem 
problem size incremental learning gives problem 
alpine gains problem chooses bound 
optimal bound alpine give problem gain 
problem sizes gives problem gain 
alpine outperforms apply delay uses abstraction separates problem city air apply problem number delay problem number alpine problem number incremental selection method problem number incremental learning time bounds top graphs selection method bottom graph extended transportation domain 
tion problem city deliveries 
bottom graph shows results incremental selection method problem sizes legend 
problems table problems 
method converges choice alpine time bound gives gain problem 
optimal choice set problems alpine time bound yield problem 
incremental selection experiment problem sizes gives problem gain 
apply technique bound selection calling friend phone 
determine seconds rings wait answer hanging 
reward reaching party may determined time willing wait order talk opposed hanging calling 
table give times calls rounded seconds success occurred party answered phone 
reply answering machine considered failure 
graph left shows dependency expected gain time bound rewards calls different people home numbers measured time ring skipping static silence connection delays 
time time time table waiting times seconds phone call experiments 
dash dot line dashed line solid line 
optimal bound rewards rings optimal bound rings 
graph right shows results selecting bounds incrementally reward 
learned bound converges optimal bound 
average gain obtained learning call 
optimal bound calls earn call 
experiments prodigy domains phone call domain show learning procedure usually finds appropriate bound yields near maximal gain 
full fink series experiments artificially generated time values normal uniform log normal log uniform distributions 
learning gives results distributions 
open problems stated task selecting problem solving methods statistical problem derived approximate solution demonstrated experimentally effectiveness selecting appropriate method time bound 
full fink describe similarity problems improve accuracy method selection 
designed module uses similarity choose relevant data library past problem solving episodes enables selection algorithm adjust method bound selection specific features problem 
ai planning provided motivation generality statistical model applicable wide range real life situations outside ai 
main limitation applicability stems restrictions reward function 
plan test effectiveness time bound dependency gains time bound problem number learning time bound dependency expected gain time bound phone call domain rewards dash line dashed line solid line results incremental learning time bound 
statistical selection algorithm ai systems method selection tasks outside ai 
model flexible need provide mechanism switching method revising time bound process search solution 
plan study possibility running competing problem solving methods parallel machines require extension statistical model 
open problem consider possible dependencies reward solution quality enhance model account dependencies 
need allow interleaving promising methods selecting new methods original selection failed 
multi method strategy effective sticking method 
am grateful valuable insights 
helped construct statistical model estimating performance problem solving methods provided ideas 
owe manuela veloso martha pollack henry rowley anonymous reviewers valuable comments 
research sponsored wright laboratory aeronautical systems center air force materiel command usaf defense advanced research project agency darpa number 
bacchus yang 
expected value hierarchical problem solving 
proceedings tenth national conference artificial intelligence 
breese horvitz 
ideal reformulation belief networks 
proceedings sixth conference uncertainty artificial intelligence 
fink 
statistical selection problem solving methods 
technical report cmu cs department computer science carnegie mellon university 
hansen zilberstein 
monitoring progress anytime problem solving 
proceedings fourteenth national conference artificial intelligence 
hansson mayer 
heuristic search evidential reasoning 
proceedings fifth workshop uncertainty artificial intelligence 
horvitz 
reasoning varying uncertain resource constraints 
proceedings seventh national conference artificial intelligence 
knoblock 
automatically generating abstractions problem solving 
ph dissertation school computer science carnegie mellon university 
technical report cmu cs 
knoblock 
automatically generating abstractions planning 
artificial intelligence 
minton 
automatically configuring constraint satisfaction programs case study 
constraints international journal 
mouaddib zilberstein 
knowledge anytime computation 
proceedings international joint conference artificial intelligence 
newell simon 
human problem solving 
englewood cliffs nj prentice hall 
perez 
learning search control knowledge improve plan quality 
ph dissertation school computer science carnegie mellon university 
technical report cmu cs 
polya 
solve 
garden city ny doubleday second edition 
russell subramanian parr 
provably bounded optimal agents 
proceedings thirteenth international joint conference artificial intelligence 
russell 
fine grained decision theoretic search control 
proceedings sixth conference uncertainty artificial intelligence 
stone veloso blythe 
need different domain independent heuristics 
proceedings second international conference ai planning systems 
valiant 
theory learnable 
communications acm 
veloso stone 
flecs planning flexible commitment strategy 
journal artificial intelligence research 
veloso 
planning learning analogical reasoning 
springer verlag 
