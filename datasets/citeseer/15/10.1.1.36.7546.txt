raw compiler project agarwal amarasinghe barua frank lee sarkar taylor mit laboratory computer science cambridge ma cag www lcs mit edu raw compilers today capable inferring detailed program parallelism analyzing program behavior 
traditional interface compiler processor defined instruction set architecture isa unable communicate compiler knowledge processor 
approach taken modern processors incorporate purely run time algorithms hardware perform analyses optimizations detection instruction level parallelism 
complex hardware implementations exploit small fraction parallelism information available compiler 
raw architecture developed mit aims maximally utilize compiler fully exposing hardware delegating hardware control completely software system 
raw microprocessor set simple risc processor tiles interconnected high speed mesh network provide hardware implementations complex algorithms conventional microprocessors 
compiler run time software system fully orchestrate raw hardware resources implement run time analyses optimizations tailored need individual application 
novel approach provides opportunities challenges raw compiler run time system 
advent risc revolutionized microprocessor making simpler 
simplicity achieved mainly loading task complex instructions hardware compiler resulted cost effective high performance processor 
subsequent generations microprocessors failed take heed lesson hardware simplicity 
focused fully hardware approaches optimizations resulting complex hardware implementations algorithms branch prediction instruction level parallelism detection register renaming order execution 
microprocessors clearly benefit compiler optimizations take full advantage compiler 
performing analysis compile time simplify eliminate complex algorithms hardware 
furthermore hardware approaches heavy resource time constraints compiler analysis rigorous leading increase effectiveness optimizations 
course compiler analysis completely eliminate run time analysis 
run time analysis necessary information required perform optimization available compile time 
run time analysis optimization need implemented hardware 
run time software system utilized cases 
hard coded custom logic software system adapt individual requirements program readily accept changes updates algorithms 
numerous attempts compiler run time software technology eliminate complex algorithms hardware 
prominent example approach vliw processor 
approaches concentrated loading individual algorithms optimizations hardware software 
developing novel approach fully exposes low level details hardware architecture compiler 
call machines approach raw architectures minimal set mechanisms hardware 
mechanisms fully exposed software allowing efficient implementations high level application programs 
corollary raw philosophy keeping hardware simple software system bear responsibilities 
raw software system includes run time system compiler 
run time system manages dynamic mechanisms historically handled hardware including branch prediction caching speculative execution 
compiler faces issues resource allocation inter tile exploitation fine grained parallelism communication scheduling configurable logic 
remaining sections organized follows 
section describes raw architecture 
sections discuss issues opportunities challenges developing raw run time system raw compiler respectively 
section details current status suif raw compiler 
close project status section 
regs alu cl switch pc pc raw raw microprocessor composition 
typical raw system include raw microprocessor coupled chip stream io devices 
raw architecture raw microprocessor chip comprises set replicated tiles 
tile contains simple risc processor portion memory instructions data configurable logic programmable switch 
simple replicated tile raw machine comprises interconnected set tiles 
tile contains simple risc pipeline interconnected tiles pipelined point point network 
large number distributed registers eliminates small register name space problem allowing exploitation ilp greater degree 
sram memory distributed tiles eliminates memory bandwidth bottleneck provides significantly lower latency memory module 
distributed architecture allows multiple high bandwidth paths external packaging technology permit 
amount memory chosen roughly balance areas devoted processing memory match memory access time processor clock 
current raw processor bind specialized logic structures register renaming logic dynamic instruction issue logic hardware 
focuses keeping tile small maximize number tiles fit chip increasing amount parallelism exploit clock speed achieve 
example single transistor die available years carry tiles including equivalent cpu floating point unit configurable logic bytes sram bytes dram 
significantly higher switching speeds internal chip compared alu mem regs mem regs alu mem regs alu switch switch switch raw mem regs alu alu alu superscalar alu mem regs mem regs alu mem regs alu switch switch switch multiprocessor raw microprocessors versus superscalar processors multiprocessors 
raw microprocessor distributes register file memory ports communicates alus switched point point interconnect 
contrast superscalar contains single register file memory port communicates alus global bus 
multiprocessors communicate coarser grain memory subsystem 
chip communication dram latencies allow software replace specialized hardware functionality 
depicted raw architecture viewed microprocessor replaces bus architecture superscalar processors switched interconnect uses software implement operations register renaming instruction scheduling dependency checking 
reducing amount hardware support operations counter current market trends approach available chip area memory compute logic results faster clock reduces verification complexity chip 
taken benefits software synthesis complex operations competitive hardware application performance 
programmable integrated interconnect shown raw machine uses switched interconnect buses 
switch integrated directly processor pipeline support single cycle send receive operations 
processor communicates switch distinct opcodes distinguish accesses static dynamic network ports 
signal raw processor travels single tile width clock cycle 
tight integration switch processor short wire lengths permit inter tile communication occur nearly speed register read 
chip interconnect allows channels hundreds wires tens vlsi switches pad limited rarely dominated internal switch area 
switch multiplexes logically distinct networks static dynamic set physical wires 
static network routing decisions statically compile time run time switches blindly follow routing instructions generated compiler 
dynamic network routing decisions network run time similar routing done 
configurability tile raw architecture contains configurable logic 
configurable logic comprised small array byte wide alus registers sufficient routing resources connect configurations commonly appear general purpose computing 
small amount bit level control logic support special bit level conditional operations 
raw processor coarser traditional fpga computer evolved structures better suited random hardware glue logic datapath oriented computation 
compared fpga computer raw configurable logic smaller configuration state better propagation delays common operations lower routing requirements smaller area alu operation 
achieve level fine grained parallelism fpga computer attain parallelism exists application 
traditional fpga computers leverage bit level configurability trade precision parallelism 
raw explores idea restrictions 
programming algorithmic analysis issues worth effort provide complete precision flexibility 
byte granular configurable logic allows implementation operations sun vis hp max intel mmx extensions 
run time system achieving top performance programs data dependent branching patterns pointer memory operations requires mechanisms analyze react program behavior run time 
modern superscalar processor operations provided hardware units including caches branch prediction buffers register renaming logic instruction scheduling units 
raw processor functions provided run time system 
example raw caching software 
run time system manages memory hierarchy checking memory access providing current mapping requested address 
systems software manage memory system compiler responsibilities 
insert code perform checks memory optimize away checks statically determined redundant 
inevitably baseline cost software caching higher hardware caching 
software caching handle sophisticated algorithms conveniently built hardware provides opportunity customize algorithms 
features lead higher hit rates turn mitigate possibly completely recover software overhead 
exciting potential exploiting inter tile parallelism scalability software caching 
similar support run time system compiler required provide mechanisms speculative execution 
speculative execution increases available parallelism programs dynamic dependence branching patterns 
additional costs performing services branch prediction checking dependences speculatively issued loads need reduced 
cost reduction come directly compiler elimination unnecessary operations come indirectly algorithms provide superior support 
additional benefit moving mechanisms software hardware eliminated permitting room chip additional parallelism better clock cycle times 
run time system provide support performance monitoring debugging 
requested features breakpoint may change program running run time system needs support dynamic code scheduling 
run time system inserts code needed user need pay additional overhead features currently 
performance raw system depend number factors 
support dynamic behaviors moved run time system program execute instructions raw system hardware support 
hand simpler raw hardware execute faster clock rate explore available parallelism 
performance involve balance factors depend compiler ability eliminate optimize run time mechanisms 
raw compiler raw compiler faces unique opportunities challenges 
compilation issues raw include resource allocation exploitation fine grained parallelism communication scheduling configurable logic code generation 
resource allocation executing program raw processor run time system program execution mapped collection identical tiles 
fixing allocation hardware tiles allocated application specific manner perform tasks 
resources dedicated performance critical highly parts run time system 
program execution divided coarse grain parallel regions 
parallel region executed multiple tiles behave single logical processor 
number tiles allocated region depends availability fine grain parallelism region 
tiles region communicate static communication regions communicate run time system dynamic messages 
size number regions allocated decided compiler nature parallelism available application 
example small number large regions high level fine grained parallelism known compiler large number smaller regions coarse grained parallelism available 
raw compiler needs perform program analysis obtain run time resource requirements availability fine grain coarse grain parallelism 
exploitation fine grain parallelism raw flexibility explore forms parallelism 
support simd mimd programming models conventional multiprocessors 
fact low latency network allows attain speedup bigger set applications 
interestingly raw introduces new paradigm exploiting parallelism 
due cost communication raw profitably exploit fine grained parallelism tiles 
particular multiple tiles exploit ilp single original instruction stream 
raw approach exploiting ilp differs superscalar vliw 
contrast superscalar raw exploits ilp complex space consuming hardware logic 
raw software approach suffer limitations small hardware instruction window 
vliw processors raw processing units instruction stream 
approach flexible require instructions proceed lock steps 
hand distributed structure raw exploitation locality important issue 
raw offers opportunities new optimization techniques 
example pertains register spilling 
conventional architectures registers spill memory 
raw provides cheaper alternative spilling registers neighboring tile 
effect raw maintains double advantage available register resources modern architectures 
single tile afford registers modern processor area basis adjacent tiles pool share registers opportunity profitable availability cheap communication 
communication scheduling static network raw compiler novel traditional compilers 
traditional compilers schedule single type event instructions single dimension time raw compiler hand faces generalized problem schedule instructions communication events events scheduled temporally spatially 
compilation code static network provides challenges perspective correctness performance 
respect correctness switch code 
respect performance compiler carefully orchestrate communication minimize network stalls 
issues complicated presence dynamic events branches cash misses dynamic messages 
correct code generated static network face compile time uncertainties 
statically ordering communication way deadlock free compiler spatial dimension vliw superscalar processors small size 
guarantee resultant code deadlock free compile time estimates event latencies cycle accurate 
static network leveraged ways 
general case provide low latency communication needed exploit profitably fine grain parallelism multiple tiles 
approach compiler takes instruction stream input partitions stream multiple instruction streams maps stream tile instruction schedules stream schedules communication streams partitioning scheduling framework compiler builds reported 
addition compiler turn certain loops inter tile pipelines leverage low cost communication achieve maximum throughputs 
static dynamic networks application need consider issues arise due interaction 
ensure compiler resultant programs correct deadlock free 
addition timing optimizations beneficial due performance differences static dynamic networks 
timing static messages highly predictable delays due dynamic messages large highly unpredictable 
programs networks benefit optimizations attempt minimize time wasted multiple nodes blocked static messages due large delay dynamic message single node 
optimization applicable cases continue send static messages containing variables stored locally remote locations local processor blocked dynamic network 
optimization possible static messages depend information carried dynamic messages 
example variables processor acts passive home location active generator new values may safely sent remote processors local processor blocked dynamic network 
configurable logic presence raw configurable logic provides opportunities optimization 
simplest manifestation raw configurable support ability perform operations 
model mirrors simd vector processors 
existing vectorizing compiler leveraged provide automatic compilation operations 
compiler useful current commercial processors extensions vis mmx 
additionally configurable logic replace frequently executed patterns program codes 
particular configurable logic helpful reducing overhead run time system 
instance run time system require additional parallel checks done support memory dependence checking speculative execution 
parallel structures configured accelerate operations making overhead doing software operations similar hardware overhead 
structures longer needed logic note compiler needs perform tasks order list tasks required independent 
compiler input program configurable logic description regs alu cl switch pc pc static data processor code switch code components code emitted raw compiler relations architecture 
reconfigured perform useful common operations 
configurable logic accelerate signal processing algorithms medium sized compute intensive cores 
data pipelined configured logic mimics program graph attain parallelism superscalar attain due limitation instruction sequencing resources 
reconfigurable logic tiles chained provide support extremely long pipelines 
sort structure built tiles 
reconfigurable logic require sequencing memory resources tile 
provides improved computation density suitable stream multimedia applications 
difficult tasks compiler identify regions code suited configurable implementation 
exploring possibility tree pattern matching techniques assist identifying regions 
regions identified hardware compilation algorithms employed compile critical program patterns configurable hardware 
code generation raw compiler hardware omniscient code generation phase involved traditional compiler 
additional traditional task data partitioning tile code generation raw compiler generate code static switches emit bit sequences programming configurable logic 
components opportunity developing new optimization techniques 
see compiler outputs relate architecture 
compiler implementation suif compiler infrastructure implement raw compiler 
suif features ideal framework raw compiler project 
provides single tool compiler development program analysis resource allocation switch tile code generation performed 
plan leverage aggressive analyses data flow analysis pointer alias analyses available suif infrastructure 
development new techniques unique architecture requires perform lot prototyping objective suif designed handle 
availability traditional optimizations passes strong front ends popular languages fortran help evaluate raw architecture wide range benchmarks real programs 
short term goal develop compiler path take subset sequential programs map multi tile code runnable raw simulator 
compiler currently handles static programs arbitrary control flow forbids memory disambiguated compile time 
optimizations 
compiler path handle simple programs follows 
stress base approach programs expect classes special programs regular programs handled special cases optional optimization 
expect approach provide correct fall back implementation programs 
suif cfg library divide program basic blocks 
basic blocks merged superblocks contain sufficient parallelism profitably parallelized 
series compiler passes performed individual superblocks 
renaming pass renames variables memory storage locations accessed superblock single assignment renaming serves purposes 
exposes inherent parallelism superblock simplifies task switch code generation 
renaming pass partitioner partitions superblock multiple instruction streams executed parallel 
number streams generated depends amount parallelism superblock limited maximum number processors system 
communication instructions inserted resultant streams preserve semantics original superblock 
partitioning placer maps stream physical processor 
backend mips architecture translates instruction stream suif mips assembly run raw tiles 
code sequences instruction scheduled account inter tile communication latencies 
switch codes corresponding tile codes generated 
compiler passes turn superblock parallel instruction streams raw 
final pass uses information control flow graph set instruction streams processor 
final result mimd parallel program static communication semantically equivalent original program 
renaming performed incurs overhead copy overhead result renaming array variables 
current research embarked developing complete compiler run time system raw architecture 
describes opportunities challenges developing software 
currently initial compiler generate unoptimized code small set programs 
extending compiler ability handle wide range programs capacity perform raw specific aggressive optimization 
goal achieve performance comparable provided scaling existing architecture achieve performance orders magnitude better applications compiler discover exploit fine grained parallelism data access patterns features 
acknowledgments research funded darpa contract dabt nsf pyi award 
aho ganapathi tjiang 
code generation tree matching dynamic programming 
acm toplas october 
auslander philipose chambers eggers bershad 
fast effective dynamic compilation 
proc 
pldi may 
babb frank lee barua taylor kim agarwal 
raw benchmark suite computation structures general purpose computing 
ieee symposium field programmable custom computing machines napa valley ca apr 
babb tessier agarwal 
virtual wires overcoming pin limitations logic emulators 
proceedings ieee workshop fpga custom computing machines pages napa ca april 
ieee 
mit lcs tm january 
cmelik keppel 
shade fast instruction set simulator execution profiling 
proc 
acm sigmetrics conference pages may 
engler 
vcode retargetable extensible fast dynamic code generation system 
proc 
pldi pages may 
suif parallelizing optimizing research compiler 
sigplan notices december 
fisher 
long instruction word architectures eli 
proc 
th annual symposium computer architecture pages june 
hall amarasinghe murphy liao lam 
detecting coarse grain parallelism interprocedural parallelizing compiler 
proceedings supercomputing san diego ca dec 
proebsting 
simple efficient burs table generation 
proceedings acm sigplan conference programming language design implementation san california june 
sarkar 
partitioning scheduling parallel programs multiprocessors 
pitman london mit press cambridge massachusetts 
series research monographs parallel distributed computing 
scales gharachorloo thekkath 
low overhead software approach supporting fine grain shared memory 
proceedings seventh international conference architectural support programming languages operating systems pages cambridge massachusetts october 
lebeck reinhardt larus wood 
fine grain access control distributed shared memory 
proc 
asplos vi pages oct 
wahbe lucco anderson graham 
efficient software fault isolation 
proc 
fourteenth acm symposium operating system principles pages dec 
wahbe lucco graham 
practical data breakpoints design implementation 
proc 
pldi pages june 
yang 
dsc scheduling parallel tasks unbounded number processors 
ieee transactions parallel distributed systems 

