lazy bayesian rules lazy semi naive bayesian learning technique competitive boosting decision trees zheng geoffrey webb kai ming ting school computing mathematics deakin university victoria australia webb deakin edu au lbr lazy semi naive bayesian classifier learning technique designed alleviate attribute interdependence problem naive bayesian classification 
classify test example creates conjunctive rule selects appropriate subset training examples induces local naive bayesian classifier subset 
lbr significantly improve performance naive bayesian classifier 
bias variance analysis lbr reveals significantly reduces bias naive bayesian classification cost slight increase variance 
interesting compare lazy technique boosting bagging known state art non lazy learning techniques 
empirical comparison lbr boosting decision trees discrete valued data shows lbr average significantly lower variance higher bias 
result interaction effects average prediction error lbr range learning tasks level directly comparable boosting 
lbr provides competitive discrete valued learning technique error minimization primary concern 
efficient single classifier applied classify cases typical incremental learning scenario 
commonly studied classifier learning techniques simple computationally efficient naive bayesian classifier duda hart kononenko langley sage shown domains surprisingly accurate compared alternatives including decision tree learning rule learning neural networks instancebased learning cestnik kononenko bratko kononenko domingos pazzani 
naive bayesian classification bayes theorem assumption attributes mutually independent class 
technique predicts test example vector attribute values hv delta delta delta vn belongs class maximizes posterior probability jv jc prior probability class jc conditional probability occurs class examples 
prior probability conditional probabilities usually directly estimated frequency counts training set 
attribute independence assumption application bayes theorem classification practical domains 
domingos pazzani argue naive bayesian classifier optimal independence assumption violated long ranks conditional probabilities classes example correct 
previous research shown semi naive techniques bayesian networks explicitly adjust naive strategy allow violations independence assumption improve prediction accuracy naive bayesian classifier domains friedman goldszmidt kohavi kononenko langley langley sage pazzani sahami singh provan webb pazzani 
suggests ranks conditional probabilities frequently correct 
tackle problem naive bayesian tree learner nbtree kohavi combines naive bayesian classification decision tree learning breiman friedman olshen stone quinlan 
uses tree structure split instance space sub spaces defined paths tree 
generates naive bayesian classifier sub space 
decision nodes trees contain ate tests conventional decision trees 
leaf naive bayesian tree contains local naive bayesian classifier consider attributes involved tests path leading leaf classify examples reach leaf 
shown nbtree frequently achieves higher accuracy naive bayesian classifier decision tree learner kohavi 
nbtree alleviate attribute interdependence problem naive bayesian classification extent nbtree suffers replication fragmentation problems pagallo haussler small disjunct problem holte acker porter ting due tree structure 
solve problems improve performance naive bayesian classification previously proposed lazy bayesian rule lbr learning technique zheng webb 
lbr thought applying lazy learning techniques aha naive bayesian tree induction 
classification time test example lbr builds appropriate rule conjunction conditions antecedent local naive bayesian classifier consequent 
experimental results zheng webb shown lbr average accurate naive bayesian classifier quinlan implementation zheng webb nbtree kohavi constructive bayesian classifier eliminates attributes constructs new attributes cartesian products existing nominal attributes pazzani selective naive bayesian classifier langley sage lazy decision tree learning algorithm friedman kohavi yun 
comparisons directly related algorithms reveal performance interesting hold new contender state theart 
studies shown committee learning techniques notably boosting bagging achieve low prediction error especially decision tree learning bauer kohavi breiman freund schapire quinlan schapire freund bartlett lee 
techniques generate multiple models form committee repeated application single base learning algorithm 
classification time committee members vote final decision 
compare lbr boosting bagging decision trees previous research showed achieve high average accuracy bauer kohavi quinlan 
interesting note boosting naive bayes effective ting zheng boosting requires base learner sensitive variations training set naive bayes stable training samples resulting low variance high bias 
section describes algorithms lbr adaboost bagging 
section contains empirical studies 
carry extensions previous analysis lbr zheng webb compare lbr adaboost bagging 
show lbr significantly reduce bias naive bayesian classification cost slight increase variance 
furthermore lbr average significantly lower variance higher bias adaboost decision trees 
result interaction effects average prediction error lbr range learning tasks level directly comparable adaboost 
section summarizes findings 
lbr adaboost bagging algorithms describe lbr algorithm subsection 
sub sections short descriptions adaboost bagging respectively 
readers referred detailed descriptions algorithms freund schapire breiman 
empirical studies involve algorithms naive bayesian classifier nb decision tree learner 
nb described briefly section 
detail available duda hart kononenko langley sage 
implementation nb probability attribute value conditional class estimated training set cestnik 
probability class estimated laplace estimate cestnik 
assume readers familiar widely 
complete description quinlan 
lbr lbr uses lazy learning alleviate attribute interdependence problem naive bayesian classification avoid replication fragmentation small disjunct problems nbtree kohavi may suffer 
retains training examples classification time 
classifying test example lbr generates rule called bayesian rule appropriate test example 
contrasts creating model training time nbtree single tree average appropriate examples 
antecedent bayesian rule conjunction attribute value pairs conditions form attribute value 
current version lbr deal directly discrete valued attributes 
numeric attributes discretized pre process 
consequent bayesian rule local naive bayesian classifier created local training examples satisfy antecedent rule 
table lazy bayesian rule learning algorithm lbr att test input att set attributes set training cases described att classes test test case described att output predicted class test nb classifier trained att errors errors estimated cv cond true repeat best number cases attribute att value test missing subset cases nb classifier trained att gamma fag subset errors estimated cv subset portion errors gamma subset best significantly lower errors best best best best cond cond best best subset corresponding best att att gamma fa best errors errors estimated cv best classify test return class local naive bayesian classifier uses attributes appear rule antecedent 
table outlines lbr algorithm 
generation bayesian rule attribute value pairs utilize attribute values test example considered 
objective grow antecedent bayesian rule ultimately decreases errors local naive bayesian classifier consequent rule 
antecedent bayesian rule defines sub space instance space test example belongs selecting subset available training instances 
instances sub space attribute values specified antecedent 
consequence attributes removed local naive bayesian classifier 
test example lbr uses greedy search generate bayesian rule antecedent matches test example 
growth rule starts special bayesian rule antecedent true 
rule local naive bayesian classifier trained entire training set attributes 
classifier identical conventional naive bayesian classifier 
step greedy search lbr tries select add antecedent current bayesian rule best attribute value pair value test example 
search attributes ignored included antecedent current rule value test example missing 
objective determine including attribute value pair bayesian rule significantly improve estimated accuracy 
search method similar forward selection relevant attribute subset selection john kohavi pfleger 
utility adding possible va current rule evaluated follows 
subset examples subset satisfies identified current local training set train temporary naive bayesian classifier attributes occur antecedent current bayesian rule 
error classifier subset estimated fold cross validation cv 
estimate cv estimate errors local naive bayesian classifier current bayesian rule gamma subset assess value adding va current bayesian rule 
measure lower estimated errors local naive bayesian classifier significance level better tailed pairwise sign test attribute value pair candidate condition added current bayesian rule 
sign test control likelihood adding conditions reduce error chance 
evaluating possible conditions candidate condition lowest error estimate added antecedent current bayesian rule 
training cases satisfy antecedent rule discarded process repeated 
continues candidate conditions 
happens damaging attribute inter dependencies exist local naive bayesian classifier local training set small cv cv errors reliable estimates true errors re substitution errors breiman friedman olshen stone cv naive bayesian classifier computationally efficient 
evaluate utility attribute value pair local training set different attributevalue pairs cover different subsets estimated errors different attribute value pairs corresponding subsets training examples subset comparable 
reduce instance sub space specializing antecedent bayesian rule 
cases growing bayesian rule significantly reduce errors 
local naive bayesian classifier bayesian rule classify test example consideration 
adaboost boosting freund schapire general framework improving base learning algorithm 
training sequentially builds different classifiers form committee adaptively modifying distribution training set performance previously created classifiers 
objective generation classifier concentrate training examples misclassified previous classifiers 
implementation adaboost uses quinlan base learner 
training set consisting instances specified number trials adaboost builds pruned trees trials repeatedly invoking 
denotes weight instance trial trial instance weight trial decision tree built distribution training error ffl calculated summing weights instances misclassifies dividing trial weight set ffl incorrectly classifies gamma ffl 
gamma set gamma solve numerical underflow problem bauer kohavi 
weights renormalized sum example bauer kohavi ffl greater re initialized allow boosting process continue 
webb forthcoming reweight examples continue boosting ffl reaches zero 
allows possibility committee member zero error derived enabling vote large finite weight giving committee members casting vote case draw committee members zero resubstitution error 
employ webb forthcoming reweighting scheme contexts setting random values continuous poisson distribution generated gammalog random random min max returns random integer value min max inclusive 
values assigned random process vector weights normalized sum size training set 
boosting process continues build tree 
note tree ffl discarded trial repeated re initialized instance weights tree ffl accepted committee 
classification stage test example decision trees committee perform weighted voting prediction 
vote decision tree worth log fi units fi ffl gamma ffl 
implementation adaboost similar bauer kohavi 
difference bauer kohavi halt induction treat having infinite weight ffl bootstrap sampling method re initialize weights ffl implementation sets fi gamma ffl re initializes weights continuous poisson distribution situations 
continuous poisson distribution chosen experiments show weight re initialization method small advantage bootstrap sampling method 
bagging boosting bagging breiman generates committees changing distribution training set 
bagging change instance distribution stochastic 
primary idea generate committee classifiers induced bootstrap sample original training set 
committee size training set consisting instances bagging generates bootstrap samples created uniformly sampling instances replacement 
applies build decision tree bootstrap sample 
decision trees committee vote equal weight classify test example 
evaluation section analyze behavior lbr bias variance decomposition gain insight improves performance base learning algorithm 
compare lbr boosting bagging 
section describes bias variance decomposition 
section illustrates experimental domains methods 
subsections results analysis lbr comparison boosting bagging 
section gives comparison terms compute time 
bias variance bias variance analyses provide useful insights generalization performance learning algo rithm 
bias measure error due central tendency variance measure error due disagreements classifiers formed algorithm distribution training sets 
definitions bias variance study adopt kohavi wolpert definitions decompose error intrinsic noise optimal bayes error squared bias variance 
due difficulty estimating intrinsic noise practical experiments real domains follow kohavi wolpert strategy generating bias term includes intrinsic noise squared bias 
values calculated directly performance classifier test case experiments described 
experimental domains methods study uses natural domains previous studies lbr zheng webb 
cover wide variety domains uci machine learning repository blake keogh merz 
test suite includes domains domingos pazzani studying naive bayesian classification toe domain 
domain chosen previous research contains inter dependent attributes target concept known 
employ bias variance estimation process developed webb forthcoming 
domain fold cross validations breiman friedman olshen stone carried algorithm 
algorithms run default option settings training test set partitions domain 
error rate test set reported 
error bias variance rate reported subsections average runs algorithm 
current implementation lbr nb deal discrete valued attributes numeric attributes discretized pre process experiments entropy discretization method fayyad irani learning algorithms 
words algorithms compared performance learning discrete valued data 
pair training set test set training set test set discretized cut points training set 
bias variance analysis lbr lbr designed improve performance naive bayesian classifier nb 
bias variance decomposition lbr performance provides interesting table error rates lbr nb adaboost bagging domain lbr nb adaboost bagging annealing audiology breast kr kp credit 
glass heart hepatitis horse hypo iris labor led liver 
pima postop 
tumor promoters solar sonar soybean splice tictactoe wine zoology mean mean mean geometric mean error ratios 
insights achieves effect 
table presents error rates lbr nb results adaboost bagging discussed section 
penultimate row shows geometric mean error ratio nb lbr 
geometric arithmetic mean appropriate averaging ratio values 
row table shows numbers wins ties losses error rates nb lbr domains significance level pairwise sign test win tie loss outcome 
consider comparison significant sign test reveals probability observed result extreme obtained chance 
illustrate impact bias variance applying lbr semi naive bayesian approach place nb naive bayesian approach presents bar chart shows domain decrease error due bias reduction increase error due bias variance comparison lbr nb increased variance 
summarizes result domains displaying average error algorithms decomposed average bias variance 
apparent lbr outperforms nb terms average error rate error ratio record 
relative error reduction lbr nb 
sign test shows lbr obtains lower error rates significantly reverse comparison nb 
apparent effect lbr substantially reduce bias naive bayes 
shows lbr obtain higher bias nb experimental domains 
lbr reduces variance nb 
reason introducing rules reduces instance space number training instances 
increase variance 
bias term strongly dominates error nb bias reduction usually outweighs variance increase 
accounts lbr lower error domains 
comparing lbr adaboost bagging turn comparison lbr boosting bagging decision trees 
base learning algorithm committee techniques performance considered 
adaboost bagging trees error rate bias variance decomposition experiments shown increasing committee size usually improves performance especially adaboost schapire freund bartlett lee 
results adaboost bagging provided table 
relative error reduction lbr 
sign test fails show lbr significantly better lbr lower error rates twice reverse 
lbr lower bias variance average 
note despite discretization numeric attributes results adaboost bagging reported comparable better earlier published results bauer kohavi breiman quinlan higher value base learning algorithm adaboost reduces bias variance 
bagging bias reduces variance 
note repeated experiments committees size results favorable boosting bagging results reported 
example average error rate adaboost trees substantially higher adaboost trees 
trees average error bagging compared trees 
comparing lbr adaboost bagging observations 
ffl lbr lower average error adaboost bagging 
terms geometric mean error ratio adaboost lower error lbr 
geometric mean accuracy ratio calculated shows adaboost lower accuracy lbr 
seemingly incompatible outcome results adaboost tendency better performance error low accuracy high lbr tends perform reverse context 
adaboost wins domains loses domains draws domain 
indicates lbr competitive adaboost 
geometric mean error ratio indicates bagging higher average error lbr 
bagging wins domains loses domains 
ffl lbr lower average variance adaboost bagging compared adaboost bagging 
frequency lbr achieves lower variance significant level respect adaboost bagging 
ffl lbr higher average bias adaboost versus lower average bias bagging versus 
frequency lbr higher bias significant level respect adaboost bagging 
deal directly numeric attributes impede carry discretization pre process running 
applies adaboost bagging 
additional experiment exactly experimental method adaboost bagging run discretization 
average error rates adaboost bagging discretization domains training test set partitions respectively 
slightly lower corresponding values discretization 
accuracy discretization worse lbr geometric mean error ratio win tie loss record 
adaboost discretization shows slight advantage lbr 
win tie loss record adaboost discretization lbr domains 
difference significant level tailed pairwise sign test 
geometric mean error ratio adaboost discretization lbr 
big advantage adaboost geometric mean accuracy ratio adaboost discretization lbr slight advantage adaboost 
discussed context adaboost discretization results adaboost tendency better performance error low accuracy high lbr tendency perform reverse context 
geometric mean error ratio win tie loss record bagging discretization lbr respectively suggesting bagging discretization advantage lbr average 
hand results suggest performance lbr improved carrying discretization generation bayesian rules pre processing employing techniques handling numeric values proposed john langley 
computation time section discuss lbr computational profile compare computation time adaboost decision trees 
adaboost bagging similar execution times result omitted 
lazy learners different computation time profiles eager learners generate explicit model training time 
consider total computation time learning training set classifying objects test set computation time eager learner strongly dominated requirements learning model training set 
computation associated applying model classify test objects usually relatively trivial 
particular computational profile relatively insensitive number objects classified 
total compute time differ little irrespective objects classified 
contrast lazy learners delay computation classification time amount computation dependent number objects classified 
major computational overhead lbr evaluation potential addition condition antecedent rule constructed 
requires fold cross validation resulting local naive bayesian classifier 
fold cross validation naive bayesian classifier implemented efficiently computational requirements classifying single test case reasonable 
lazy learning requires process repeated test example cumulative computation large test set substantial 
computational overhead substantially reduced caching useful information 
current implementation lbr evaluation function values attribute value pairs examined retained test example 
avoids re calculation evaluation function values attribute value pairs classifying test examples appear test set reducing entire execution time 
experiment shows caching information reduces execution time lbr average datasets experiment 
evaluation attribute value pairs different test examples repeated including repeated generation identical rules different test examples 
lbr efficient caching information local classifiers indices training examples different stages growth rules 
course increase memory requirements 
table shows execution time seconds lbr adaboost running sun ultrasparc computer 
adaboost uses time lbr domains 
terms mean execution time domains lbr time slower adaboost dominated domains adaboost faster lbr 
domains large numbers test instances contribute long execution lbr 
example splice junction domain test set size current experiment 
change experiment fold cross validation fold cross validation test set size reduced execution time lbr reduces seconds seconds adaboost execution time increases seconds seconds 
suggests test set size great effect execution time lbr little effect execution time adaboost 
interpreting results table noted relate contexts created threefold cross validation thirds available data training third testing 
results better favor lbr proportion data testing decreased better favor adaboost proportion increased 
table compute time seconds lbr adaboost domain adaboost lbr annealing audiology breast kr kp credit 
glass heart hepatitis horse hypo iris labor led liver 
pima postop 
tumor promoters solar sonar soybean splice tictactoe wine zoology mean mean evaluates lazy bayesian rule learning algorithm 
algorithm seeks overcome attribute interdependence problem naive bayesian classifier forming customized naive bayesian classifier case classified 
customized naive bayesian classifier formed selecting subset available training examples appear relevant classifying current test example 
practice builds accurate naive bayesian classifier test example naive bayesian classifier trained training examples 
shows lazy bayesian rule induction algorithm effective technique improving naive bayesian classifier 
show effect achieved significantly substantially reducing bias naive bayesian classification achieved cost slight increase variance 
current implementation constraints notably accepts discrete attributes restricted antecedent conditions form attribute value suggest room improve performance basic lazy bayesian rule induction algorithm described 
promising direction research 
show prediction error lbr discrete valued data competitive state ofthe art non lazy techniques 
due lazy learning computation profile technique dominated number cases classified 
consequence efficient small number test cases training set case contexts incremental learning traditionally employed 
acknowledgments authors grateful ross quinlan providing 
anonymous reviewers valuable comments 
aha 
ed 
lazy learning 
dordrecht kluwer academic 
bauer kohavi 
empirical comparison voting classification algorithms bagging boosting variants 
appear machine learning 
blake keogh merz 
uci repository machine learning databases www ics uci edu mlearn mlrepository html 
irvine ca university california department information computer science 
cestnik kononenko bratko 
assistant knowledge elicitation tool sophisticated users 
proc 
nd european working session learning uk sigma press 
breiman friedman olshen stone 
classification regression trees 
wadsworth belmont 
breiman 
bagging predictors 
machine learning 
cestnik 
estimating probabilities crucial task machine learning 
proc 
european conf 
artificial intelligence 
domingos pazzani 
independence conditions optimality simple bayesian classifier 
proc 
th intl 
conf 
machine learning pp 

san francisco ca morgan kaufmann 
duda hart 
pattern classification scene analysis 
new york john wiley 
fayyad irani 
multi interval discretization continuous valued attributes classification learning 
proc 
th intl 
joint conf 
artificial intelligence san mateo ca morgan kaufmann 
freund schapire 
decisiontheoretic generalization line learning application boosting 
journal computer system sciences 
friedman kohavi yun 
lazy decision trees 
proc 
th natl 
conf 
artificial intelligence menlo park ca aaai press 
friedman goldszmidt 
building classifiers bayesian networks 
proc 
th natl 
conf 
artificial intelligence menlo park ca aaai press 
holte acker porter 
concept learning problem small disjuncts 
proc 
th intl 
joint conf 
artificial intelligence san mateo ca morgan kaufmann 
john kohavi pfleger 
irrelevant features subset selection problem 
proc 
th intl 
conf 
machine learning san mateo ca morgan kaufmann 
john langley 
estimating continuous distributions bayesian classifiers 
proc 
th conf 
uncertainty artificial intelligence san mateo ca morgan kaufmann 
kohavi 
scaling accuracy naivebayes classifiers decision tree hybrid 
proc 
nd intl 
conf 
knowledge discovery data mining menlo park ca aaai press 
kohavi wolpert 
bias plus variance decomposition zero loss functions 
proc 
th intl 
conf 
machine learning morgan kaufmann 
kononenko 
comparison inductive naive bayesian learning approaches automatic knowledge acquisition 
wielinga 
eds current trends knowledge acquisition 
amsterdam ios press 
kononenko 
semi naive bayesian classifier 
proc 
european conf 
artificial intelligence 
langley 
induction recursive bayesian classifiers 
proc 
european conf 
machine learning berlin springer verlag 
langley sage 
induction selective bayesian classifiers 
proc 
th conf 
uncertainty artificial intelligence seattle wa morgan kaufmann 
pagallo haussler 
boolean feature dis empirical learning 
machine learning 
pazzani 
constructive inductive cartesian product attributes liu motoda 
eds 
feature extraction construction selection data mining perspective boston ma kluwer academic 
quinlan 
programs machine learning san mateo ca morgan kaufmann 
quinlan 
bagging boosting 
proc 
th natl 
conf 
artificial intelligence menlo park ca aaai press 
sahami 
learning limited dependence bayesian classifiers 
proc 
nd intl 
conf 
knowledge discovery data mining menlo park ca aaai press 
schapire freund bartlett lee 
boosting margin new explanation effectiveness voting methods 
proc 
th intl 
conf 
machine learning morgan kaufmann 
singh provan 
efficient learning selective bayesian network classifiers 
proc 
th intl 
conf 
machine learning san francisco ca morgan kaufmann 
ting 
problem small disjuncts remedy decision trees 
proc 
th canadian conf 
artificial intelligence canadian soc 
comp 
studies intelligence 
ting zheng 
improving performance boosting naive bayesian classification 
proc 
rd pacific asia conf 
knowledge discovery data mining berlin springer verlag 
webb 
forthcoming technique combining boosting bagging 
accepted publication machine learning 
webb pazzani 
adjusted probability naive bayesian induction 
proc 
th australian joint conf 
artificial intelligence berlin springerverlag 
zheng webb lazy bayesian rules deakin university computing technical report tr available www cm deakin edu 
au papers lbr tr ps gz 
