scalable concurrent priority queue algorithms nir shavit october addresses problem designing bounded range priority queues queues support fixed range priorities 
bounded range priority queues fundamental design modern multiprocessor algorithms application level lowest levels operating system kernel 
available priority queue literature directed existing small scale machines chose evaluate algorithms broader concurrency scale simulated node shared memory multiprocessor architecture similar mit alewife 
empirical evidence suggests priority queue algorithms currently available literature scale 
findings simple new algorithms provide true scalability concurrency range 
priority queues fundamental class data structures design modern multiprocessor algorithms 
uses range application level lowest levels operating system kernel 
effective concurrent priority queue algorithms currently available literature ingenious heap implementations support unbounded range allowable priorities 
unfortunately show scalability limited 
may part reason highly concurrent applications designed sequential priority queue implementations alternatively avoiding priorities altogether 
hard quantify reason believe availability effective concurrent priority queue implementations ultimately simplify application design making prioritization possible having take steps limit concurrent access priority queue 
addresses problem designing scalable priority queue structures 
concentrate design bounded range priority queues queues support fixed range priorities example operating systems schedulers 
concentrating queues fixed range priorities offer new algorithmic approaches avoid tightly synchronized structures priority heaps search trees skip lists :10.1.1.15.9072
began research evaluating performance representative bounded range priority queue implementations algorithms literature context shared memory multiprocessor architecture minimal hardware support 
common synchronization primitives register compare swap representative algorithms described section included hunt variant skip lists pugh list bins tree bins structures mcs locks mellor crummey scott 
load linked store conditional pair full empty bits evaluated structures running series simple synthetic benchmarks accepted proteus simulator brewer proteus simulate processor ccnuma multiprocessor similar alewife agarwal realistic memory bandwidth latency 
real node machine note previous research della shown appropriate scaling proteus simulates node alewife machine accurately kinds data structures tested 
empirical evidence section indicates concurrent heap hunt skip lists pugh performance problems low concurrency levels 
hand simple list bins tree bins structures mcs locks perform exceptionally low concurrency levels 
unfortunately algorithms point concurrency scale contention high performance degrades algorithm unusable 
findings section simple new algorithms 
variants list bins tree bins structures new forms combining funnel coordination mechanism shavit place mcs locks bins tree nodes 
combining funnels randomized variant software combining trees support efficient parallel fetch increment operations 
new versions combining funnel structures allow support novel bounded fetch decrement operation 
structures trees counting networks provide efficient implementations fetch increment operations readily transformed new bounded fetch increment required priority queues 
research lim agarwal della shavit karlin shown key delivering performance wide range concurrency levels ability data structure adapt load encountered 
adaption techniques lim agarwal centralized form coordination replaces entire data structure say mcs combining tree order handle higher respectively lower load 
approach avoid replacing complete structure require centralized opposed distributed algorithmic solution strong coordination 
achieve adaptability construct parts data structures potential hot spots internal nodes bins localized adaptive mechanism combining funnels 
section describe combining funnels detail outline new variant funnels constructions 
final part section compares performance new adaptive algorithms methods 
potential algorithms provide scalable performance concurrency range 
concurrent priority queue implementations evaluating performance priority queue alternatives testing implementations various published methods methods consider natural choices 
implementation structures optimized best ability maintaining characteristics original 
comparing algorithms alewife designed scale hundreds processors largest machine currently available nodes 
keep mind general algorithms designed allow unbounded range priorities 
chose funnels alternately adaptive mechanisms local level 
peak performance high load optimal combining tree efficient combining funnel 
widely varying families hope minimize role actual implementation properties algorithms 
section describes priority queue algorithms studied 
provide pseudo code defines algorithm behavior illustrates important implementation issues 
discussion different consistency conditions apply priority queues appears appendix heap priority queue implemented array single mcs lock entire data structure shown appendix algorithm supports arbitrary priorities linearizable 
representative class centralized lock algorithms 
priority queue implementation hunt 
algorithm see appendix 
single lock protects variable holding size heap 
processors acquire order operations previous data structure held duration operation 
heap size updated lock element heap acquired lock released 
order increase parallelism insertions traverse heap bottom deletions proceed topdown insertions employ novel bit reversal technique allows series insertion operations proceed heap independently getting way 
implementation code authors ftp site optimized proteus 
algorithm supports arbitrary priorities linearizable 
representative class algorithms rao kumar yan zhang centralized locking sophisticated terms minimizing number duration lock access traversing shared heap structure 
bin counter algorithms priority queue algorithms sequel shared data structures call counter bin 
counter shared object holds integer supports fetch increment fai fetch decrement fad operations 
increment decrement operations may optionally bounded meaning update value counter specified bound 
bin called bag pool object holds arbitrary elements supports insertion specified element bin insert emptiness test bin empty removal unspecified element bin delete 
simple code implementing data structures appears 
code bin insert bin delete uses locks explicitly implementation fai uses operator stress implement locks execute operations hardware implement combining funnels 
skiplist priority queue algorithm pugh skip list structure optimized fixed set priorities seen appendix pre allocated links link contains bin stores items link priority 
insert element priority processor adds item bin th link 
link currently threaded skip list inserts pugh concurrent skip list insertion algorithm 
deletion follow ideas johnson setting aside special delete bin 
delete operations attempt remove items bin 
processor find bin empty bin skip list sets delete buffer point 
lowers contention deletion phase 
fetch increment int fai counter atomically old val val return old bounded fetch decrement int counter int bound atomically old val old bound val return old bin insert bin acquire lock size maxsize elems size release lock bin empty bin return size bin delete bin acquire lock bin empty elems size release lock return right code shared counter operations left simple bin implementation 
skip lists shown complexity better empirical performance search tree methods algorithm represent performance delivered class search tree priority queue algorithms 
shown algorithm maintains array bins 
insert item priority processor simply adds th bin 
delete min operation scans bins smallest largest priority attempts delete element non empty bins encounters 
scan stops element 
bins implemented data structure linearizable 
algorithm binary trees counters illustrated 
tree leaves th leaf holds items priority gamma shared counters bin bins insert bin insert bins pri delete min bin empty bins bin delete bins null return return code simple bounded range priority queue algorithm linear layout 
leaves tree root insert leaves pri bin insert bin root parent fai counter deletemin root leaf counter bin delete bin return code simple bounded range priority queue algorithm binary tree layout showing associated counter operations 
tree internal nodes count total number items leaves subtree rooted node left lower priority child 
delete min operations start root descend smallest priority non empty leaf examining counter node 
go right zero decrement go left 
decision implemented bounded fetch decrement operation 
processors overtake insertions proceed top manner incorrect executions occur 
insertions item priority traverse tree bottom th leaf 
ascending left child parent counter incremented fetch increment operation 
section combining funnel implementations algorithms 
new funnel algorithms counter bin implementations provided scalable sources contention simultaneously accessed processors 
create priority queues replace simple data structures potential trouble spots combining funnel implementations 
specifically employ combining funnel counters inner tree nodes combining funnel stacks implement bins algorithms 
combining funnel basics section outlines key elements combining funnel data structure 
interested reader find details 
funnel composed typically small number combining layers implemented arrays memory 
processors accessing serial object locate combine 
processors pass layer read pid processor id randomly chosen array element write place 
attempt collide processor id read 
successful collision allows processor exchange information update operations 
example assume processors attempt access bin object concurrently operations bin insert bin insert respectively 
processors pass bin example processors going funnel 
left see go layer collides wait advance second layer collide 
right side see combining trees dynamically formed collisions waiting processor child advancing processor 
combining layers reads id manages collide updates operation bin insert fa bg changes operation wait inserted 
doing operation considered parent 
parent completes operations informs children results 
processor continue layer exists 
suppose collides processor doing bin insert child 
operation bin insert fa cg changes operation wait inserted 
operation complete informs turn inform passing funnel layers processor said exit funnel may try apply updated operation object example bin 
illustrates process 
scenario just suppose operation bin delete collides wishes remove item wants add natural return item satisfying requests needing access bin 
process called elimination allows operation reversing semantics eliminated early stages funnel traversal 
elimination improves performance allows operations complete sooner reduces contention 
funnels support adaption 
having processor decide locally combining layers traverse applying operation object trade overhead going layers increased chances collision 
high load extra overhead layers justified low load contention better simply apply operation done 
priority queues funnels order construct combining funnel variants need provide implementations basic primitives funnels 
shavit contains detailed pseudo code constructing combining funnel counter supports fetch increment operation implementation fai needed 
section novel construction implementing operation directly implemented trees counting networks 
primitives provide needed counter support 
implement bins stack construction 
bin insert push bin delete pop bin empty reads current value stack pointer compares zero 
funnels implement priority queues support quiescent consistency 
chose stacks place bins funnel implementation simple linearizable supports elimination 
cause unfairness starvation items equal priority insertions occlude earlier ones 
matters application dependent 
fifo queues give elimination hybrid data structure supports elimination funnel queues items internally fifo order 
returning see requires bins implement funnel straightforward manner 
point stressed delete min operation queries bin see empty trying remove value 
crucial performance testing emptiness faster requires read going funnel trying remove element 
previous case replace bin operations stack operations 
calls fai replaced funnel implementation counters top levels tree counters mcs locks 
require funnels deeper levels contention traffic lower top 
experiments show cost performance avoided making cut decision funnels tree 
bottom automatically shrunk adapting prefer low overhead fewer combining layers 
simulation taken long chose approach details full 
scalable bounded counter algorithm funnels appendix awe pseudo code bounded fetch increment decrement counter combining funnels 
note priority queue scheme get counter supports operations unbounded increment bounded decrement returns indication counter decremented 
sake completeness provide implementation supports bounded fetch decrement bound pseudo code definition appears analogous bounded fetch increment bound 
fetch add operation possible construct type bounded counter directly gottlieb shown 
example bounded fetch done applying fetch add operation value returned greater bound applying fetch add operation returning bound 
approach potentially require fetch operations case require traversals combining funnel show avoid extra overhead incorporating bounds checking operation directly funnel algorithm 
approach allows integrate elimination seamlessly algorithm 
seen performance gain elimination high 
blush adding bounds check trivial departure standard combining fetch add operation appears combine usual perform bounds check root combining structure 
unfortunately approach wrong due subtle fact bounded processors fetch add elimination decrement operations fetch add elimination comparison latencies combining funnel fetch add bounded decrement 
equal number increment decrement operations varying degrees concurrency elimination bounded version substantially efficient left 
processors varying distribution operations right eliminations rarer eventually fetch add faster incur overhead 
operations commutative 
example consider counter value operations bounded fetch decrement bound fetch increment applied 
decrement applied operations return counter value changes 
operations applied opposite order increment returns decrement counter ends value 
order apply operations important problems arise trying apply entire tree operations determining processor return value parallel 
way overcome difficulties forcing trees homogeneous contain kind operation 
operations need know determine final value counter 
performance tests performed simulated processor distributed shared memory multiprocessor similar mit alewife machine agarwal simulator proteus multiprocessor simulator developed brewer 
benchmarks processors alternate performing local accessing priority queue 
accessing queue processors choose insert random value apply delete min operation result unbiased coin flip 
experiments show local kept small constant queue initially empty 
experiment measured latency amount time cycles takes average access object 
series experiments keeps concurrency constant changing number priorities varies concurrency keeping number priorities constant 
ran set preliminary benchmarks processors queue priorities find set funnel parameters layer width depth funnel delay times minimized latency 
set parameters funnels counters stacks 
experiments show low concurrency levels simple algorithms best 
total number priorities determines priorities large number priorities 
concurrency increases version dated february 
processors skiplist processors latency different priority queue implementations priorities low concurrency levels 
right close bottom part graph left 
contention begins undermine effectiveness implementations 
processors priorities linear arrangement combining funnel stacks effective priority queue 
supporting priorities favors binary tree layout method choice priorities high concurrency levels 
experimental results graphs compare latency operations low concurrency levels processors priorities 
unbounded algorithms display linear increase latency concurrency rises algorithm hunt performing slightly better 
results expected match 
queue contains relatively items time fact algorithm allows parallelism access queue little benefit processors spend time trying acquire initial lock 
skiplist algorithm slightly better due efficient insertion method delete buffer 
right hand side allows closer look algorithms lower latency 
roughly equal slopes method low overhead especially insertion clear leader 
times slower due overhead funnels versus mcs locks see details 
tree methods nearly latency slower methods slower 
contain serial bottleneck faster performs fai root acquire lock initial critical section 
low concurrency levels overhead critical performance factor 
number processors increases effects contention serialization parallelism play increasing role 
evident graph compares promising methods levels concurrency processors priorities 
slowest method high concurrency levels contention root increases time perform counter operations 
fastest till processors performs relatively processors 
surprising till notice insertions independent low overhead deletions perform reads attempt lock promising bins 
processors contention individual bins play major role 
algorithm processors latency scalable priority queue implementations priorities top high concurrency levels 
ins 
del ins 
del ins 
del ins 
del latencies thousands cycles insert delete min bounded range priority queue implementations 
number priorities number processors 
avoids contention increases parallelism price higher overhead pays processors 
method traverses funnels starts paying earlier processors performance leader 
concurrency increases parallelism inherent funnels ability avoid contention cause gap methods widen processors method times faster times faster 
offers break latency figures cost insert delete min 
generally increasing number priorities increases size data structure 
may mean required update data structure increasing latency conversely processor spread memory contention reducing latency 
example processors dominant factor going priorities increases latency 
change processors contention important reduces latency 
funnel methods susceptible contention added overhead funnels support priorities dominant factor determining performance 
tree methods insert cheaper delete min average insertions update half counters path root 
graphs demonstrate behavior different algorithms range priorities goes processors 
interplay increased affects delete min decreased contention benefits insert explains shaped curve 
slows linearly added priority overhead new funnel outweighs saving contention 
algorithm shows constant latency processors priorities priorities latency scalable priority queue implementations varying number priorities processors left processors right 
graph level concurrency latency determined exclusively time pass root 
growth latency logarithmic deeper nodes traffic higher ones 
funnels shrink mcs locks 
graphs show high concurrency levels method consistently works better nearly priorities 
faced small range priorities consider method 
cases see funnel methods offer best scalable solution implementing parallel bounded range priority queues 
believe results promising 
show simple trivial algorithms quite practice 
importantly show performance degrades due appearance hot spots possible remedy implementation general technique combining funnels 
indication combining funnels approach effective embeddable technique 
borne research real applications large scale machines mean parallel data structures constructed understood techniques today contention reducing parallelization methods massage trouble spots 
acknowledgments colleague dan numerous comments suggestion tree priority queue structure 
wish anonymous referees 
agarwal johnson kubiatowicz kurihara lim maa 
mit alewife machine large scale distributed memory multiprocessor 
scalable shared memory multiprocessors kluwer academic publishers 
mit technical report mit lcs tm june 
martin sadler dias snir 
sp system architecture 
ibm systems journal 
aspnes herlihy shavit 
counting networks 
journal acm vol 
september pp 


lr algorithm concurrent operations priority queues 
proceedings nd ieee symposium parallel distributed processing pp 

kahan tera computer 
personal communication march 
biswas browne 
simultaneous update priority structures proceedings international conference parallel processing august pp 

biswas browne 
data structures parallel resource management 
ieee transactions software engineering pp 

larsen 
chromatic priority queues 
technical report department mathematics computer science university pp may 
brewer dellarocas 
proteus user documentation 
mit technology square cambridge ma edition december 
brewer dellarocas weihl 
proteus high performance parallel architecture simulator 
mit technical report mit lcs tr september 
della 
reactive trees 
master thesis massachusetts institute technology 
della shavit 
reactive trees 
proceedings th annual symposium parallel algorithms architectures spaa june 
goodman vernon 
efficient synchronization primitives large scale cache coherent multiprocessors 
proceedings third international conference architectural support programming languages operating systems asplos pages april 
gottlieb grishman kruskal mcauliffe rudolph snir 
nyu designing mimd parallel computer 
ieee transactions computers february 
gottlieb rudolph 
basic techniques efficient coordination large numbers cooperating sequential processors 
acm transactions programming languages systems april 
huang 
evaluation concurrent priority queue algorithms 
technical report massachusetts institute technology mit lcs mit lcs tr may 
hunt michael parthasarathy scott 
efficient algorithm concurrent priority queue heaps 
information processing letters november 
johnson 
highly concurrent priority queue link tree 
technical report university florida 
august 
karlin li manasse owicki 
empirical studies competitive spinning shared memory multiprocessor 
th acm symposium operating system principles sosp pp 
october 
kruskal rudolph snir 
efficient synchronization multiprocessors shared memory 
fifth acm sigact sigops symposium principles distributed computing august 
herlihy wing 
linearizability correctness condition concurrent objects 
acm transactions programming languages systems july 
lim agarwal 
reactive synchronization algorithms multiprocessors 
sixth international conference architectural support programming languages operating systems asplos vi pp 

lim agarwal 
waiting algorithms synchronization large scale multiprocessors 
acm transactions computer systems august 
mellor crummey scott 
algorithms scalable synchronization shared memory multiprocessors 
acm transactions computer systems feb 
pfister norton 
hot spot contention combining multistage interconnection networks 
ieee transactions computers november 
rudolph 
personal communication regarding start ng system 
csg www lcs mit edu start mit 
pugh 
skip lists probabilistic alternative balanced trees 
communications acm june 
pugh 
concurrent maintenance skip lists 
technical report institute advanced computer studies department computer science university maryland college park cs tr 
rao kumar 
concurrent access priority queues 
ieee transactions computers december 

personal communication june 
shavit 
elimination trees construction pools stacks proceedings th annual symposium parallel algorithms architectures spaa pages july 
shavit 
trees 
acm transactions computer systems november 
shavit 
combining funnels 
proceedings seventeenth annual acm symposium principles distributed computing pages puerto mexico june th july nd 
kahan 
processor management tera mta computer system 
available ftp www tera com www archives library processor html 
yew lawrie 
distributing hot spot addressing large scale multiprocessors 
ieee transactions computers april 
yan zhang 
lock bypassing efficient algorithm concurrently accessing priority heaps 
acm journal experimental algorithmics vol 

www acm org 
distributed coordinated data structures 
ph dissertation tel aviv university march 
appendix pseudo code priority queue implementations way sure trees homogeneous forcing processors funnel layer roots trees size 
processors enter layer funnel roots trees size 
processors operations perform fai combine root tree size advances funnel layer 
operations reversing fai eliminate exit funnel 
processors advance funnel layer collided combined processor root tree size operation 
colliding trees size opposing operations yields elimination 
collisions trees different sizes allowed 
scheme keeps processors roots trees size th layer sure collisions trees size rare 
case elimination subtle interaction bounds checking accounting effects simple assuming operations trees interleaved counter moves away original position 
walk code 
assume processor public data accessed pointer obj encapsulates data functions specific object 
fields sum current sum operations combined 
location address funnel layer number funnel processor currently traversing 
value set null processor funnel unavailable collisions 
result set result operation known 
holds values elim count indicates operation eliminated part way acquired obj 
second operation return value 
adaption factor indication load modify width funnel layers response load variations 
fields obj current value counter 
levels number funnel layers 
attempts number collision attempts processor try accessing 
width array holding respective width values funnel layers 
layer dimensional array holding elements funnel layer 
spin array holding respective amounts time processor delay layer 
lines set data structures operation 
lines contain collision code 
processor picks random layer location swaps pointer written attempts collide locking lines 
collision succeeds added list operation lines elimination occurs short cuts get counter value lines 
discovers collided lines goes wait value line 
obj attempts collision attempts tries perform bounded decrement operation central counter compare swap operation line int obj sum total initially depth location obj funnel obj mainloop obj attempts obj levels wid obj width random wid swap obj layer read partner continue cas location obj 
empty attempt cas location obj 
empty collision sum sum opposites 
val obj short cut val bot val counter result elim val result elim val eliminate break mainloop sum sum combine location obj advance layer append list children add child location obj obj spin delay combine location obj 
break mainloop cas location obj empty val obj calc new value new val sum central new bot new bot cas obj val new result count val 
updated central break mainloop distribute results location obj 
result empty wait result event val result foreach list children iterate children event elim eliminated result elim val get result result count val total total sum code bounded fetch decrement 

succeeds moves distribution phase 
distribution phase begins code waits result lines arrives iterates children handing value setting result field lines 
distributing values important know elimination occurred distribute value operations tree due interleaved ordering operations dec 
way adaption fits funnel framework altering width funnel layers 
line processor picks random location current layer entire width obj width fraction adaption factor width 
changing value adaption factor processors purely local level determine size funnel 
decision actual load encountered measured ratio successful collisions collision attempts passes loop lines 
see details 
appendix priority queues parallel setting sequential setting priority queue data structure supports operations insert inserts element queue certain priority pri ordered set delete min removes returns element priority smallest currently queue 
priority queue said bounded range set possible priorities elements finite 
case map priorities set integers say queue range notice definition refers smallest priority element currently queue 
definition precise concurrent setting multiple operations may overlap time clear element really queue 
standard consistency conditions capture behavior queues parallel environment giving meaning notion smallest priority element currently queue 
condition due herlihy wing linearizability 
data structure said linearizable concurrent execution associate operation single time point actual execution interval operation said take place associated execution valid sequential execution 
put differently compressing operations single point time place single time line resulting execution conform object sequential specification 
linearizability priority queue means order insert delete min operations consistently real time order occured set minimal priorities order defined 
consistency condition semantically weaker allows computational efficiency aspnes quiescent consistency 
object said quiescent time operations began completed operation begins object consistent operation performed compressed single point time latest quiescent point began earliest quiescent point ends meets object sequential specification 
main difference quiescent consistency linearizability quiescent consistency require preservation real time order operation completes operation start operation overlaps may reordered priority guarantees consistent priority queues quite strong 
assume priority queue quiescent time contains set elements define infinite executions point fact quiescent point may appear time execution imposes strong restrictions allowable implementations 
maxsize int insert acquire propagate heap standard heap algorithm release delete min acquire save propagate heap standard heap algorithm release return save insert acquire bit reverse acquire lock release propagate heap local locks node delete min acquire acquire lock save bit reverse acquire lock release propagate heap local locks node return save code heap priority queue single lock left 
code priority queue algorithm algorithm hunt right 
error handling shown 
skiplist head skiplist link bin insert bin insert link pri bin link pri threaded insert link pri skip list pugh algorithm link pri threaded delete min bin delete null acquired head delete element skip list pugh algorithm bin release null skiplist code bounded range priority queue algorithm skip lists 
error handling shown 
operation acquired block merely attempts acquire lock returns indication success 
different priority 
delete min operations performed time returned elements exactly elements smallest priority denote min 
exactly linearizable priority queue 
occur operations insert new set elements elements returned delete min operations set min min 
words sequence deletes new elements enter queue set returned minimal values may include minimum elements joint set new old enqueued values exact timeline dequeue enqueue operations ordered 
theoretically means new items smaller priority old minimum item enqueued may deleted place remain queue 
real world sytems problem considered advantage newly arrived smaller priority items serviced ahead older low priority ones 
elements priority formal definitions somewhat complex capture uncertainty choosing elements priority 
define fx pg largest priority smallest priority case delete min operations performed priority queue intervening insertion operations elements returned set subset size gamma elements priority 

