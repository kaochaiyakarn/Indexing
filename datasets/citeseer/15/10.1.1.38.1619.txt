tree decomposition approach parallel query optimization thomas hpc graduate fellow srivastava bhaskar li computer science department university minnesota electrical engineering computer science building union street minneapolis minnesota approach transforming relational join tree detailed execution plan resource allocation information execution parallel machine 
approach starts transforming query tree generated sequential optimizer operator tree partitioned forest linear chains pipelined operators 
algorithm scheduling chains conformance precedence ordering parallel machine 
aim scheduling achieve objective minimum response time maximum speedup provide set experiments demonstrate quality allocation plans generated method 
major benefit method uses inter operator intra operator pipelining methods parallelizing relational query operators simultaneously 
experiments show heuristic scheduling method creates schedules close optimal easy compute 
propose new way calculate speedup takes account amount done 

principal reasons popularity relational databases separation query specification execution 
user usually need knowledge system details specifies query logic language predicates required answer satisfy 
declarative query large usually exponential number ways executing supported part army research office contract number university minnesota army high performance computing research center 
supported part national science foundation 
iri 
leave university china 
having different cost 
selecting best cost plan executing query problem query optimization 
success building query optimizers relational databases important reason popularity 
np hard problem optimal solution small problems 
led substantial research improved heuristics years ono 
focus relational query optimization far finding plans involve amount uniprocessor time taken proportional done 
case multiprocessors efficient uniprocessor solutions sequential dependencies making difficult parallelize 
reduce query response time determine solution takes parallel time may 
leads new query processing algorithms query optimization techniques 
specifically query plan able take advantage different kinds including intra operator parallelism operator select join sort evaluated multiple processors inter operator parallelism operators evaluated independently parallel pipelining pairs operators evaluated parallel producer consumer relationship 
substantial amount research done develop accurate cost models sequential environment mack continues done cost models exist parallel environment 
quote dewitt gray necessary optimizer technology exists accurate cost models developed validated 
needed area dewi 
lu proposed cost model response time query plan considered 
having pipelining successive levels plan tree simplifies calculation considerably 
query execution modeled data flow manner flow rate expression average response time derived 
model uses bottom approach response time tree rooted operator estimated sum time taken operator maximum response times children 
effect response time due pipelined execution operators modeled 
gang proposed similar approach operator tree level 
proposal comprehensive date provides details cost formulae operators rules combination 
effect resource contention modeled multiplicative factor stretches response time 
careful allocation resources maximize utilization crucial parallel processing systems 
parallel query optimization needs done resources processors disks communication channels memory 
fullest generality independently allocated 
architectural characteristics may constrain problem distributed memory architecture processor memory assignment equivalent goes 
disk allocation primarily intermediate results usually guided data declustering approach 
processor assignment considered lu hong gang turek chen hua memory assignment considered 
memory continues critical resource performance database operations parallel environment schn careful modeling contention important 
step memory allocation reported gang critical resource guide search heuristic shown 
presents phase optimization approach phases phase minimization phase relational calculus algebra query translated procedural plan query tree transformations applied obtain plan heuristically minimal sequential execution time 
exactly sequential optimizers phase approach uses standard sequential optimizer 
phase ii plan parallelization resource allocation phase decides amount type intra operator inter operator pipelining parallelism 
resource allocation processor memory decisions 
decisions disks guided relations stored declustered li 
aim phase minimize objective function response time consider response time parallel execution time optimization objective 
standard sequential query optimization technology phase discusses phase ii 
allow join tree produced sequential optimizer linear bushy focus scheduling join tree inter operator intra operator pipelining effectively 
goals obey precedence constraints operations may started operations lower join tree completed ii allocate processors avoid idling possible called system fragmentation chen iii allocate memory maximize effectiveness 
goals increase difficulty scheduling chen 
step transform join tree basic form called operator tree 
joins replaced constituent operations 
example sort merge join replaced sort left input sort right input merge 
second step decide intermediate results materialized pipelined 
characteristics operators sorts completed merge heuristics keep possible sequential optimizer generates linear join trees 
case intra operator parallelism inter operator parallelism 
clearer section 
pipelines short processors may starve 
third step heuristic processor allocation method shelf scheduling turek 
original heuristic scheduling set independent tasks generalized handle set dependent ones 
approach finding critical path precedence graph operator tree 
task placed order shelves 
remaining tasks assigned existing shelves rules 
experimental evaluation shows method compares favorably plan exhaustive search 
studies buffer management sequential chou parallel schn environments shown query execution performance highly sensitive amount memory available 
nested loops join algorithm instance goes quadratic linear complexity memory hold smaller relation 
non shared memory parallel machines increasing degree intra operator parallelism operation net effect reducing complexity aggregate memory increased 
interesting implication algorithm may sublinear speedup behavior certain threshold speedup suddenly jump 
observed experiments 
threshold information resource allocation interesting research direction 
main contributions method decomposing query plans component tasks easily computed method scheduling tasks query plan parallel execution schedule inter operator intra operator pipeline parallelism model shows possible achieve super linear speedup available memory increases additional processors organized follows section describes concepts notation proposed approach 
section presents method decomposing query schedulable components section gives description scheduling heuristics 
section presents results experimental evaluation method section provides 

query plan representation focus optimizing select project join spj queries parallel execution 
section examine represent query plans 

sequential plan representation join tree describe notation representing sequential query plans 
captures join ordering type algorithm 
shows example plan illustrating properties 
sequential optimizer typically heuristic rules generate plan output form graph 
plans developed optimizer starting point parallel optimization process 
way principal heuristic optimization process 
nest loop hash hash hash example query plan representation showing join ordering algorithm join 
parallel plan representation operator tree step method derive operator tree join tree supplied sequential optimizer 
derivation accomplished expanding node join tree atomic sub operations 
example sort merge join relations decomposed separate steps sort ii sort iii merge results sort operations 
similarly hash join nested loop join decomposed primitive components 
identified primitive atomic components 
scan operation scans entire relation select non indexed project redistribute 
sort sorts input 
hash builds hash table input 
probe uses input probe hash table hits output 
merge merges sorted inputs 
nested loops compares tuple relation tuple second relation 
additional criteria need specified tell tuples match equality attribute components combined form composite operators 
example sorts merge put create sort merge join 
shows graphical representation primitive operations listed composite operations implement hash join sort merge join 
primitives composite sub trees scan sort hash probe merge nested loops hash probe sort sort merge primitive graph components dashed lines indicate mandatory materialized edges note operators dashed lines leading 
indicate materialization points 
algorithms effectively pipelined 
example hash table building phase hash join fully completed prior probe phase 
mark output hash operation materialized 
mandatory materialization points constrain search space need consider pipelining operations materialized edge 
shows join graph conversion operator tree 
join operator broken constituent primitive parts 
nest loop probe hash merge sort sort probe hash probe hash example join tree operator tree 
optimization process overview shows architecture optimizer 
query graph converted select project join tree 
parameters database catalog architectural parameters describing current hardware configuration optimization criteria select heuristically optimal query tree 
phase optimization process standard sequential optimization techniques 
query tree transformed operator tree transformations described earlier algorithms selected operation 
scans inserted data redistribution select project operations 
operator tree derived edge labeled materialized pipelined 
materialized label indicates child operator completes entire result materialized parent operator begins 
pipelined label means operators execute producer consumer pair 
materialized edges required input operation complete prior start operation 
example hash join algorithms require hash table completely built prior commencement probing 
case input probe operation output hash operation materialized 
search high variability pipelines select operator returns portion relation may produce tuples long duration followed burst production range query convert materialized edges 
materialization situations high variability lead lot processor idling consumer side pipeline ii difficult estimate proper buffer size pipeline 
break labeled operator tree materialized edge 
gives forest tree components 
component pipeline segment scheduled methods described section 
query generator sql query tree join graph join tree joint tree operator tree operator tree forest schedulable components resource allocation schedulable components query optimizer plan database parameters optimization criteria architecture parameters intermediate cost analysis heuristics cost model simulator performance architecture optimizer 
tree decomposition method tree decomposition method uses steps transform join tree forest schedulable components 
steps follows 
transform join tree operator tree 
label edges operator tree materialized pipelined 
decompose operator tree schedulable components details steps follow subsequent sections example continuing example query joins relations non equijoin 
gives query graph example query 
example query graph 
step transform join tree operator tree transform join tree operator tree 
joins atomic actions composed parts may scheduled separately 
possible join algorithms choose situations 
transformation rules 
scan distribute operator inserted base relation leaves tree join direct parent 
scan operator capable doing select project redistribute operation combination 

scan distribute operator inserted output join input join parent 

join exploded component parts determined algorithm chosen 
algorithm choice turn dependent available access paths availability different access methods relation determine join algorithms 
equijoin non equijoin non require algorithm examines possibility nested loops 
absolute relative input relation sizes sizes input relations intermediate results joins query estimated various formulae chen 
prediction available memory rough idea amount available memory give ability reject algorithms nested loops require large amounts memory competitive 
sorted output certain query predicates require sorting incoming relation prior application 
example sort merge join require input relations sorted prior join step 
choose join algorithm accommodate sorted order able eliminate separate sort step parent operation 
cost estimate algorithm parameterized cost model obtainable literature determine algorithm choice particular location graph 
section give examples parameterized cost models algorithms 
join join join join join original join tree output sequential optimizer hash join sort merge hash join join hash join scan scan scan scan scan scan scan scan scan scan output step operator tree literature exists deciding best algorithm set join conditions sequential parallel schn environments 
scope develop new methods selection 
observe decision locally regard joins graph exception sorting criteria may required join graph 
figures show join tree resulting operator tree respectively 

step label edges operator tree materialized pipelined step decide edges operator graph represent result materialization pipelining 
sources information decision probe merge probe join probe scan scan scan hash hash scan scan scan scan scan sort sort hash scan scan operator tree materialized edges labeled shown dashed lines probe merge probe join probe scan scan scan hash hash scan scan scan scan scan sort sort hash scan scan 






components decomposition mandatory materializations operators algorithms implement operators require materialized input easily produce materialized output 
primitive composite components shown show edges predetermined materialized 
high variability pipelines pipelined edges connect high variability producers consumers require large amounts memory handle worst case average remains unutilized 
edges candidates materialization 
pipeline length pipeline chains long may need broken eliminate high propagation delays 
furthermore high predicate selectivity initial operators may drastically reduce offered pipeline stages leading inefficient resource utilization 
shows labeled edges example tree edge labeling step 
broken lines indicate materialized edges solid lines indicate pipelined edges 

step decompose operator tree forest schedulable components operator tree decomposed forest components breaking materialized edge 
resulting components single operation pipelined chain operators 
shows components result example tree 
forest components created resource processor memory allocation decisions 
section presents heuristics allocating resources components scheduling components execution 

resource allocation section describe cost model trees cost descriptor node expressed function cost children parameters 
cost model lends naturally bottom evaluation 
gang describe cost models type 
query sum effort spent processors working 
response time parallel time evaluate particular query 
cost model enhancement gang described detail 

model calculate amount needed compute operator amount memory available execution 
timing parameters taken chen added facilitate calculation done 
parameters described table specific values evaluation shown table 
parameters size left input size right input expected size output operator respectively 
unary operators inp represent size input 
read time read single tuple write time output single tuple 
input pipelined read recv read read similarly output pipelined write send write write parameters done operator follows scan select inp read inp write read inp write inp redistribute inp read write inp part hash inp read write inp hash insert probe assume uniform distribution tuples join attribute probing 
read read parameters describing reads left right child follow parameterization read 
sufficient memory hold entire hash table read read write hash comp build algorithm defaults writing portion fit memory disk partitioning split table schn hybrid hash algorithm 
portion memory joined buffer full read disk probe hash table 
results extra disk read write tuple match portion hash table fits memory 
number tuples written re read expressed number tuples minus tuples partition 
probing cost hash comp tuple 
full formula read read write hash comp read write build term description left right children operator tree node root current node consideration tree number processors amount memory single processor number nodes precedence graph speedup factor single operator estimate di estimate node precedence graph estimate di response time estimate node precedence graph size left child input operator tuples size right child input operator tuples expected size operation output tuples number tuples disk page selectivity select operations sync sets tl tf read time read single tuple write time write single tuple read cost reading tuple disk write cost writing tuple disk recv cost receiving tuple pipeline send cost sending tuple pipeline hash cost applying hash function insert cost tuple insertion hash table comp cost tuple comparison build cost building result tuple part cost applying partition function sort factor sort multiplied table cost model notation sort inp inp read write inp inp inp sort log 
inp external sort resulting disk read write inp inp inp inp inp read write sort log log log merge estimated multiplying join selectivity times 
join selectivity sequential optimizer phase determine join order available phase ii estimate 
read read parameters describing reads left right child follow parameterization read 
read read write comp build max gb nested loops estimated merge case 
read read parameters describing reads left right child follow parameterization read 
sufficient memory hold entire left relation read read write comp build outer relation completely reread inner relation read write read read comp build write 
scheduling components units scheduling tree components constructed previous phase 
schedule follow precedence indicated original tree step developing high level schedule perform topological sort knuth components form precedence graph 
component ends materialized edge connecting component rule children component finished prior component starting 
consider component schedulable task label tasks shows precedence graph components example graph 
order schedule components idea shelf strategy turek way performing orthogonal rectangle packing 
treat tasks malleable rectangles height rectangle non decreasing function rectangle width 
width represents number processors assigned task height represents time task take number processors 
turek gives general solution case tasks independent number tasks larger number processors 
generalized approach model notion precedence 
turek shelf scheduling algorithm compute optimal number shelves pack shelf optimal manner 
show single shelf optimal independent tasks 
order capture constraints precedence algorithm chooses length critical path precedence graph number shelves component critical path executed 
shelf executed separately order 
processors available shelf 
pack shelves algorithm 
component compute initial uniprocessor estimate estimate bg 
find critical path precedence graph length critical path determining number shelves 
assign node critical path separate shelf order leaf shelf root shelf 
mark remaining nodes unassigned 
assign remaining non critical path nodes shelves rules leaf node unassigned 
max task estimate estimate max di shelf lowest total estimate satisfies precedence constraints task max assign max mark max assigned 
shelf greedy processor assignment algorithm turek 
examine step scheduling algorithm detail means continuing example step compute initial estimates task appropriate estimation functions section get estimate done task 
estimate amount done single processor 
example graph estimates hold 
precedence graph aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa critical path step find critical path precedence graph critical path longest terms number nodes root leaf path tree 
easily breadth type level assignment algorithm 
shows critical path precedence graph example step assign critical path nodes shelves shows initial set node shelf assignments critical path tasks estimates shelf parentheses 
critical path nodes assigned respective shelves marked assigned 
initial shelf setup final task shelf assignments step assign remaining nodes critical path assigned shelves remaining nodes need assigned shelves 
node assigned lowest tree 
assigned precedence shelves 
shelf parent located shelf 
heuristic assign shelf lowest total weight shelf 
node assigned 
nodes level node higher estimate assigned 
node assigned shelf lower shelf parent located 
case node assigned shelf 
node assigned shelf lowest estimate shelves node assigned 
node assigned 
additional constraints assigned shelf children node parent 
shelf meets constraint shelf node assigned 
shows final shelf assignments estimates shelves 
processors final schedule example query step processor assignment shelves shelf processors execute tasks assigned shelf 
clearly want tasks finish nearly time reduce processor idling 
optimal greedy algorithm turek assign processors accomplished 
algorithm works assigning processor task 
greedy step repeated remaining processor find estimate di maximal 
assign processor task tasks set tasks assigned shelf procs number processors assigned task point algorithm 
procs estimate estimated time execute task procs processors 
iterative processor allocation algorithm follows tasks procs tasks procs procs tasks best estimate estimate procs procs best best phase times component maximal estimate di operations connected pipeline edges 
case assign processor operations pipeline chain 
choose greedy method processor assigned operations turn response time chain calculated 
processor assigned operation chain causes largest reduction execution time entire chain 
shows completed schedule 

performance evaluation plan generator implemented ideas described previous sections 
evaluated plans produced generator cost model discussed section 
section describe experimental setup results obtained detailed example 

experimental setup table shows various timing parameter values experiment 
parameter value meaning read sec cost reading tuple disk write sec cost writing tuple disk recv sec cost receiving tuple pipeline send sec cost sending tuple pipeline hash sec cost applying hash function insert sec cost tuple insertion hash table comp sec cost tuple comparison build sec cost building result tuple part sec cost applying partition function sort sec factor sort multiplied factor speedup calculation number tuples disk block materialization takes percentage available memory materialization takes place memory pipeline cost multiplied times number hops pipeline table parameters experiment pairs relations sizes mb mb mb mb mb tuples experimental database 
join selectivity ranged study behavior tree decomposition scheme varied parameters sample queries 
number relations varied 
memory size processor ranged mb mb mb mb 
number processors 
additional parameters added experimental runs 
cost pipeline hop 
multiplied number segments pipeline chain estimate extra costs pipeline propagation 
purpose penalize pipelines slightly overhead plus account processor idling consumer side due selectivity producer side 
percentage memory materialization occurs requires percentage memory materialization occur memory disk activity 
materialized items communicated set processors expensive disk write read avoided small intermediate results 
response times estimated formulae shown appendix parameter setting runs 
values reported average values metrics plan quality ratio execution time estimates obtained tree decomposition scheme lower bound obtained exhaustive search 
metric measures effectiveness proposed method 
speedup ratio time execution query plan takes processor memory time execution takes processors memory processor 
heuristic search count vs optimal search count compare number alternative plans considered proposed heuristic method vs number plans considered exhaustive search 
results discussion plan quality shown plan quality tree decomposition method averages times optimal value exhaustive search 
observe number processors larger number tasks available executed tasks shelf plan quality degrades somewhat 
plan quality worst large number processors small number relations 
quality improves number relations number tasks decomposed operator tree participating query increase 
noticed plan quality consistently better complicated queries relations participating 
believe stems nature complex queries complex queries constraints freedom move tasks shelves 
means proposed method come schedule closer arrived exhaustive search 
plan quality ratio worse situations complexity processors brought bear problem 
processors affect relative placement tasks shelves number processors assignment tasks shelf 
assigning processors task reduces time time reduced linearly due non linear scalability algorithms may possible come better plan examining different inter shelf task arrangements exhaustive method 
plan quality vs number processor mb processor number processors plan quality vs memory processors memory processor mb mb mb mb plan quality vs number processors memory speedup vs number processors mb processor number processors speedup vs memory processors memory processor mb mb mb mb speedup number processors memory processor speedup shows speedup differing numbers processors fixed amount memory processor 
interesting note doubling processors holding amount memory processor constant resulted doubling speedup 
super linear speedup result combination increased processing power decreased 
processing power increased adding processors simultaneously workload decreased processors constant amount memory processor aggregate memory available 
done memory disk resulting reduced disk load 
disk relatively expensive compared done memory see decrease workload communications workload increased additional processors 
shows speedup number processors held constant amount memory processor varied mb processor mb processor 
cases speedup curve initially rises peaks falls 
divide curve regions 
region curve rises indicating memory results speedup 
region increasing memory effect parallel system increasing memory amount uniprocessor memory increased amount buffering effect subsequent reduction higher parallel system 
second region curve speedup begins fall 
occurs parallel version reaches point adding additional memory little effect problem fits main memory 
adding memory sequential version impact respect reducing disk 
expect see third region flat speedup curve sequential version memory profitably 
number relations search type processors processors processors processors heuristic exhaustive heuristic exhaustive heuristic exhaustive table number alternatives tested method relations processors search space heuristic generally search space optimal parallel query plan large searched exhaustively 
order measure reduced space searched compared number tree alternatives examined heuristic tree decomposition method vs number alternatives examined exhaustive search method 
table shows drastic reduction space searched heuristic method 
queries involving joins relations reduction search space quite large 

phase approach parallel query optimization transforming relational join tree detailed parallel execution plan 
method yields resource allocation information execution relational database query parallel machine takes advantage inter operator intra operator pipelining methods parallelizing execution 
decisions aim optimizing criteria response time speedup summary proposed method decomposing query plan schedulable component tasks shown schedule tasks precedence ordering easily computable heuristic method shown method comprehensive schedules inter operator pipeline parallelism demonstrated experiment quality method heuristics contained shown possible achieve super linear speedup memory increases due additional processors applied problem plans include validating cost model 
validated cost model allow possibly incorporate tree decomposition method optimizer makespan final schedule cost particular join tree configuration 

chen chen lo yu young segmented right deep trees execution pipelined hash joins proceedings th vldb conference vancouver canada 
chen chen yu wu scheduling processor allocation parallel execution multi join queries proceedings data engineering 
chou 
chou dewitt katz klug design implementation wisconsin storage system software practice experience vol 
october 
dewi dewitt gray parallel database systems database processing passing fad acm sigmod record vol 
december 
gang ganguly hasan krishnamurthy query optimization parallel execution proceedings acm sigmod international conference management data san diego ca june 
hong hong exploiting inter operation parallelism xprs proceedings acm sigmod international conference management data san diego june 
ioannidis kang randomized algorithms optimizing large join queries proc 
intl 
conf 
mgmt 
data atlantic city nj may 
hua hua lo young including load balancing issue optimization multi way join queries shared database computers proceedings nd international conference parallel distributed information systems san diego january 
jarke koch query optimization database systems acm computing surveys vol 
pp 
june 
knuth knuth fundamental algorithms volume art computer programming 
addison wesley 
li li srivastava rotem cmd multidimensional declustering method parallel database systems proceedings th vldb conference 
lipton naughton schneider practical selectivity estimation adaptive sampling proc 
intl 
conf 
mgmt 
data atlantic city nj may 
lu lu 
shan 
tan optimization multi way join queries parallel execution proceedings th vldb conference barcelona spain august 
mack lohman optimizer validation performance evaluation local queries proc 
acm sigmod intl 
conf 
mgmt 
data washington may 
ono ono lohman measuring complexity join enumeration query optimization proc 
th large database conference brisbane australia august 
schn schneider dewitt performance evaluation parallel join algorithms shared multiprocessor environment proceedings acm sigmod conference 
schn schneider dewitt tradeoffs processing complex join queries hashing multiprocessor database machines proc 
th large database conference august brisbane australia 
selinger access path selection relational database management system proceedings acm sigmod intl 
conf 
mgmt 
data 
srivastava optimizing multi join queries parallel relational databases proceedings nd international conference parallel distributed information systems san diego january 
swami gupta optimization large join queries proc 
acm sigmod intl conf 
mgmt 
data chicago il june 
turek turek wolf yu scheduling parallelizable tasks putting shelf acm sigmetrics june 
wilschut apers dataflow query execution parallel main memory environment proceedings st international conference parallel distributed information systems miami december 
wong decomposition strategy processing relational queries acm transactions database systems vol 
september 
zait parallel query processing dbs proceedings nd international conference parallel distributed information systems san diego january 
appendix response time model section notation gang describe rules combining costs nodes operator tree 
model estimate operator rules combine operator estimates calculate estimate response time query plan exhaustive heuristic methods 
tf tl cost descriptor node operator tree 
tf time block output generated tl time block output generated 
nodes operator tree execution times respectively 

estimates response time parallel execution 
max 
estimates response time sequential execution followed 

estimates response time residual query executed pipelined fashion 
estimated tl tl 
shows operations timeline format 
pipeline producer consumer descriptors pf pl cf cl operator descriptor pipeline tf pf cf tl pf cf pl pf cl cf 
materialized execution subtree descriptor tf tl computed operation sync sets tf tl sync tf tl tl tl 
operation sets time tuple production equal time tuple production indicates tuples output operator may tuple output 
time max tl tl time line diagram cost descriptor operations cost operator tree computed recursively formulae cost descriptors root 
tree response time root subtree response time root individual operator costs derived descriptors operator 
amount done operator calculated timing parameters chen 
account resource contention factors speedup processor modeled 
method calculates speedup number processors assigned task cost descriptors derived estimate time output block 
operator tf tl cost descriptor tl tf bw min estimate amount time fill block finish whichever comes 
tf set tl output materialized 

