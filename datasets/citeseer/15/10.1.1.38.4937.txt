concept decompositions large sparse text data clustering dhillon cs utexas edu department computer science university texas austin tx usa modha almaden ibm com ibm almaden research center harry road san jose ca usa original version february revised version july 
unlabeled document collections increasingly common available mining data sets represents major contemporary challenge 
words features text documents represented high dimensional sparse vectors dimensions sparsity typical 
study certain spherical means algorithm clustering document vectors 
algorithm outputs disjoint clusters concept vector centroid cluster normalized unit euclidean norm 
contribution empirically demonstrate owing high dimensionality sparsity text data clusters produced algorithm certain fractal self similar behavior 
second contribution introduce concept decompositions approximate matrix document vectors decompositions obtained squares approximation linear subspace spanned concept vectors 
empirically establish approximation errors concept decompositions close best possible truncated singular value decompositions 
third contribution show concept vectors localized word space sparse tend orthonormality 
contrast singular vectors global word space dense 
observe surprising fact linear subspaces spanned concept vectors leading singular vectors quite close sense small principal angles 
concept vectors produced spherical means algorithm constitute powerful sparse localized basis text data sets 
keywords concept vectors fractals high dimensional data information retrieval means algorithm leastsquares principal angles principal component analysis self similarity singular value decomposition sparsity vector space models text mining 
large sets text documents increasingly common 
example world contains nearly pages growing rapidly www alexa com ibm patent server consists patents www patents ibm com databases contain documents www com 
furthermore immense amount text data exists private corporate intranets archives media companies scientific technical publishing houses 
context applying machine learning statistical algorithms clustering classification principal component analysis discriminant analysis text data sets great practical interest 
focus clustering text data sets 
clustering discover latent concepts sets unstructured text documents summarize label collections 
clustering inherently useful organizing searching large text collections example automatically building ontology yahoo 
www yahoo com 
furthermore clustering useful compactly summarizing disambiguating navigating results retrieved search engine altavista www altavista com 
conceptual structure generated clustering akin kluwer academic publishers 
printed netherlands 
final tex dhillon modha table contents front books inverted index altavista akin indices back books provide complementary information navigating large body information 
clustering useful personalized information delivery providing setup routing new information arriving new scientific publications 
experiments describing certain syntactic clustering web applications see broder 
clustering visualizing navigating collections documents dhillon 
various classical clustering algorithms means algorithm variants hierarchical agglomerative clustering graph theoretic methods explored text mining literature detailed reviews see rasmussen willet 
flurry activity area see boley cutting hearst pedersen sahami silverstein silverstein pedersen vaithyanathan dom zamir etzioni :10.1.1.54.3039
starting point applying clustering algorithms unstructured text data create vector space model text data salton mcgill 
basic idea extract unique content bearing words set documents treat words features represent document vector certain weighted word frequencies feature space 
observe may regard vector space model text data set word document matrix rows words columns document vectors 
typically large number words exist moderately sized set documents words common 
document vectors highdimensional 
typically documents contain fewer words comparison total number words entire document collection 
document vectors sparse 
understanding exploiting structure statistics vector space models major contemporary scientific technological challenge 
shall assume document vectors normalized unit norm thought points high dimensional unit sphere 
normalization mitigates effect differing lengths documents singhal 
natural measure similarity vectors inner product known cosine similarity salton mcgill 
variant known euclidean means algorithm duda hart hartigan uses cosine similarity rasmussen 
shall show algorithm partitions highdimensional unit sphere collection great shall refer algorithm spherical means algorithm 
algorithm computes disjoint partitioning document vectors partition computes centroid normalized unit euclidean norm 
shall demonstrate normalized centroids contain valuable semantic information clusters refer concept vectors 
spherical means algorithm number advantages computational perspective exploit sparsity text data efficiently parallelized dhillon modha converges quickly local maxima 
furthermore statistical perspective algorithm generates concept vectors serve model may classify documents 
focus study structure clusters produced spherical means algorithm applied text data sets aim gaining novel final tex concept decompositions insights distribution sparse text data high dimensional spaces 
structural insights key step second focus explore intimate connections clustering spherical means algorithm problem matrix approximation word document matrices 
generally speaking matrix approximations attempt retain signal vector space models discarding noise 
extremely useful improving performance information retrieval systems 
furthermore matrix approximations practice feature selection dimensionality reduction prior building learning model classifier 
search retrieval context deerwester berry proposed latent semantic indexing lsi uses truncated singular value decomposition svd principal component analysis discover latent relationships correlated words documents 
truncated svd popular studied matrix approximation scheme golub van loan 
earlier leary peleg image compression developed memory efficient matrix approximation scheme known decomposition 
gallant caid implicit matrix approximation scheme context vectors 
papadimitriou proposed computationally efficient matrix approximations random projections 
isbell viola independent component analysis identifying directions representing sets highly correlated words directions implicit matrix approximation scheme 
title suggests main goal derive new matrix approximation scheme clustering 
briefly summarize main contributions section empirically examine average intra inter cluster structure partitions produced spherical means algorithm 
find clusters certain fractal self similar behavior commonly low dimensional data sets 
observations important proposed statistical model text data consistent empirical constraints 
aside claiming breakthrough point discovery fractal nature ethernet traffic greatly impacted design control analysis high speed networks leland 
section propose new matrix approximation scheme concept decomposition solves squares problem clustering computes squares approximation linear subspace spanned concept vectors 
empirically establish surprising fact approximation power measured frobenius norm concept decompositions comparable best possible approximations truncated svds golub van loan 
important advantage concept decompositions computationally efficient require memory truncated svds 
section show concept vectors localized word space sparse tend orthonormality 
contrast singular vectors obtained svd global word space dense 
observe surprising fact subspaces spanned concept vectors leading singular vectors quite close sense small principal angles golub final tex dhillon modha 
sparsity concept vectors important speaks economy parsimony model constituted 
sparsity crucial computational memory efficiency spherical means algorithm 
concept vectors produced spherical means algorithm constitute powerful sparse localized basis text data sets 
preliminary versions irregular conference held berkeley ca www nersc gov conferences irregular program html siam annual meeting held atlanta ga www siam org meetings ms htm 
due space considerations removed experimental results complete details appear ibm technical report dhillon modha 
probabilistic latent semantic analysis plsa hofmann views word matrix occurrence table describing probability word related document approximates matrix aspect model saul pereira 
similar spirit concept decompositions distinct plsa 
framework geometric concerned orthonormal projections plsa probabilistic concerned statistical kullback leibler projections 
examine nature large sparse high dimensional text data find certain fractal behavior see sections 
hand plsa uses classical multinomial model describe statistical structure cluster 
employ spherical means algorithm followed squares approximation step plsa employs em type algorithm 
mining extremely large text data sets speed essence spherical means squares generically faster corresponding em type algorithm 
explore document clustering show matrix approximation power concept decompositions close truncated svds compare contrast concept vectors singular vectors 
word notation small bold letters denote column vectors capital bold letters denote matrices script bold letters denote linear subspaces 
kxk denote norm vector denote usual inner product dot product vectors kxk denote frobenius norm matrix 

vector space models text section briefly review represent set unstructured text documents vector space model 
basic idea represent document vector certain weighted word frequencies 
addition introduce text data sets results 

parsing preprocessing 
ignoring case extract unique words entire set documents 

eliminate non content bearing stopwords sample lists stopwords see frakes baeza yates chapter 

document count number occurrences word 
final tex concept decompositions 
heuristic information theoretic criteria eliminate non content bearing highfrequency low frequency words salton mcgill 
words stopwords known function words 
eliminating function words removes little information speeding computation 
general criteria pruning function words ad hoc purpose word help discriminating cluster neighbors function word 
see figures number clusters small large fraction words treated function words selected function words independently number clusters 

elimination suppose unique words remain 
assign unique identifier remaining word unique identifier document 
steps outline simple preprocessing scheme 
addition may extract word phrases new york may reduce word root stem eliminating plurals tenses prefixes suffixes frakes baeza yates chapter 
point passing efficient implementation scheme lexical analyzers fast scalable hash tables appropriate data structures 

vector space model preprocessing scheme yields number occurrences word document say ji number documents contain word say counts create document vectors 
follows 
set th component document vector product terms ji ji ji term weighting component depends ji global weighting component depends normalization component intuitively ji captures relative importance word document captures importance word entire set documents 
objective weighting schemes enhance discrimination various document vectors enhance retrieval effectiveness salton buckley 
schemes selecting term global normalization components example presents schemes respectively term global normalization components total choices 
extensive set popular schemes denoted txn tfn known respectively normalized term frequency normalized term frequency inverse document frequency 
schemes emphasize words higher frequencies ji ji txn scheme uses tfn scheme emphasizes words low collection frequency uses log 
schemes document vector normalized unit norm ji final tex dhillon modha intuitively effect normalization retain direction document vectors 
ensures documents dealing subject matter similar words differing length lead similar document vectors 
comparative study various document length normalization schemes see singhal 
introduce sets text documents classic nsf 
example classic obtained classic data set containing documents merging popular medline cisi cranfield sets 
medline consists abstracts medical journals cisi consists abstracts information retrieval papers cranfield consists abstracts aeronautical systems papers ftp ftp cs cornell edu pub smart 
preprocessed classic collection proceeding section 
removing common stopwords collection contained unique words eliminated low frequency words appearing documents roughly documents high frequency words appearing documents roughly documents 
left words high dimensional feature space 
created document vectors txn scheme 
document vector dimension average document vector contained nonzero components sparse 
example nsf obtained nsf data set downloading abstracts awarded national science foundation march august www nsf gov included subjects astronomy population studies undergraduate education materials mathematics biology health oceanography computer science chemistry 
preprocessed nsf collection proceeding section 
removing common stopwords collection contained unique words eliminated low frequency words appearing documents roughly documents high frequency words appearing documents roughly documents 
left words 
created document vectors tfn scheme 
document vector dimension average document vector contained nonzero components roughly sparse 
stress addition txn tfn weighting schemes conducted extensive experiments number different schemes 
furthermore experimented various cut thresholds 
cases essence empirical results remained 

spherical means algorithm section study partition high dimensional sparse text data sets classic nsf disjoint conceptual categories 
briefly formalize spherical means clustering algorithm 
empirically study structure clusters produced algorithm 
final tex concept decompositions 
cosine similarity follows document vectors 
points unit sphere furthermore weighting schemes components document vectors nonnegative document vectors fact nonnegative orthant vectors inner product natural measure similarity 
unit vectors denote angle kxk cos inner product known cosine similarity 
cosine similarity easy interpret simple compute sparse vectors widely text mining information retrieval frakes baeza yates salton mcgill 

concept vectors suppose document vectors denote partitioning document vectors disjoint clusters fx fixed mean vector centroid document vectors contained cluster number document vectors note mean vector need unit norm capture direction writing corresponding concept vector km concept vector important property 
unit vector cauchy schwarz inequality concept vector may thought vector closest cosine similarity average sense document vectors cluster 
objective function motivated measure coherence quality cluster final tex dhillon modha observe document vectors cluster identical average coherence cluster highest possible value 
hand document vectors cluster vary widely average coherence small close 
kc km kc km rewriting yields remarkably simple intuition quality cluster measured norm sum document vectors cluster 
measure quality partitioning fp objective function fp intuitively objective function measures combined coherence clusters 
objective function proposed studied theoretically context market segmentation problems kleinberg 

spherical means seek partitioning document vectors disjoint clusters maximizes objective function seek solution maximization problem fp arg max fp fp finding optimal solution maximization problem np complete kleinberg theorem see garey 
discuss approximation algorithm spherical means algorithm effective efficient iterative heuristic 

start arbitrary partitioning document vectors fp fc denote concept vectors associated partitioning 
set index iteration 
document vector find concept vector closest cosine similarity compute new partitioning fp induced old concept vectors fc fx words set document vectors closest concept vector happens document vector simultaneously closest final tex concept decompositions concept vector randomly assigned clusters 
clusters defined known voronoi dirichlet partitions 

compute new concept vectors corresponding partitioning computed km denotes centroid mean document vectors cluster 
stopping criterion met set set exit 
increment go step 
example stopping criterion fp fp suitably chosen 
words change objective function iteration algorithm certain threshold 
establish spherical means algorithm outlined decreases value objective function 
lemma fp fp proof fp fp inequality follows second inequality follows 
final tex dhillon modha intuitively proof says spherical means algorithm exploits duality concept vectors partitioning concept vectors fc induce partitioning fp turn implies better concept vectors fc corollary limit exists lim 
fp proof lemma sequence numbers fp increasing 
furthermore follows triangle inequality fp km increasing sequence numbers bounded constant 
sequence converges limit exists 
corollary says spherical means algorithm iterated indefinitely value objective function eventually converge 
important realize corollary imply underlying partitioning fp converges 
refer reader interested general convergence results pollard sabin gray 
spherical means algorithm gradient ascent schemes prone local 
algorithm yielded reasonable results experimental results reported 
key algorithm careful selection starting partitioning fp example may randomly assign document clusters may compute concept vector entire document collection obtain starting concept vectors randomly perturbing vector voronoi partitioning corresponding set concept vectors 
furthermore try initial partitionings select best terms largest objective function trials 
tried exactly initial partitioning strategy 

experimental results undertake empirical study spherical means algorithm example persuade reader algorithm produces meaningful clusters 
example confusion matrix clustered classic data set clusters 
confusion matrix shows clusters produced algorithm reasonably identified medline cisi cranfield data sets respectively 
final tex concept decompositions number iterations objective function value number iterations 
value objective function versus number means iterations classic left panel nsf right panel data sets 
medline cisi cranfield conclude table algorithm discovers class structure underlying classic data set 
see top words describing clusters example clustered nsf data set clusters 
see top words corresponding clusters 
top words corresponding remaining clusters see dhillon modha 
words constitute anecdotal evidence coherence clusters produced algorithm 
empirically validate lemma corollary 
example objective function decreases clustered classic data set clusters 
clustering left panel plot value objective function versus number means iterations 
seen fixed number iterations increases value objective decreases fact quickly converges 
furthermore see larger number clusters larger value objective function 
example objective function decreases repeat example nsf data set right panel 
low dimensions may picture cluster data points nice round cloud points mean center 
show intuitions directly carry high dimensional sparse text data sets 
specifically examine intra inter cluster structure produced spherical means algorithm 
final tex dhillon modha 
average intra cluster estimated probability density functions clusters nsf data set 
suppose clustered document vectors clusters fp fc fn respectively denote corresponding concept vectors number document vectors 
obtain insight structure cluster distribution cosine similarities cluster 
follows 
compute numbers usually clusters obtain insight average intracluster structure computing numbers plotting estimated probability density function pdf numbers 
recall pdf compactly completely captures entire statistics underlying set data points 
pdf nonnegative function unit area 
estimate pdf set dimensional data points unweighted mixture histogram method rissanen 
example average intra cluster structure clustered nsf data set clusters 
plot estimated pdfs numbers clusterings 
note range axis plotting cosine similarities 
majority mass estimated pdf lies interval follows virtually document vectors close corresponding concept vector consequently concept vectors hardly pictured surrounded data points fact large empty space document vectors corresponding concept vectors 
observe increased mass estimated pdfs progressively shifts final tex concept decompositions 
average inter cluster estimated probability density functions clusters nsf data set 
right increasing clusters coherent document vectors progressively closer corresponding concept vectors 
increases empty space concept vectors progressively shrinks 
spite behavior examples show algorithm tend find meaningful clusters 
reconcile facts studying average intercluster structure computing cosine similarity document vector concept vectors corresponding clusters contain document 
compute numbers plot estimated pdf numbers 
example average inter cluster structure clusterings example plot estimated pdfs numbers corresponding respectively 
comparing inter cluster estimated pdf intracluster estimated pdf see corresponding peaks separated assigns mass assigns mass 
document vectors cluster close corresponding concept vector document vectors outside cluster 
sense relative difference possible meaningfully cluster sparse data 
comparing intra inter cluster estimated pdfs figures see increases overlap corresponding intra cluster estimated pdfs steadily decreases 
furthermore increased mass estimated inter cluster pdfs progressively shifts left 
final tex dhillon modha examples show clusters high dimensional sparse data properties commonly observed low dimensional data sets 
light examples follows modeling multidimensional distributions document vectors cluster daunting problem example gaussian distribution mean hardly tenable model 
examples illustrate average intra inter cluster structures exhibit behavior various scalings resolutions number clusters 
essential difference structures progressive movement intracluster pdfs corresponding movement inter cluster pdfs 
believe facts point certain fractal self similar nature high dimensional sparse text data needs studied 

comparison euclidean means algorithms point difference objective function traditional objective function duda hart hartigan kx mean vector associated cluster objective function measures sum squared euclidean distances document vectors closest mean vectors 
objective function minimized known euclidean means algorithm duda hart 
euclidean means algorithm similar spherical means algorithm 
obtained replacing partitioning fx kx kx computing mean vector step concept vector km contrast cluster boundaries corresponding objective functions 
boundary clusters associated spherical means algorithm say locus points satisfying locus hyperplane passing origin 
intersection hyperplane unit sphere great 
spherical means algorithm partitions unit sphere collection great 
similarly boundary clusters associated euclidean means algorithm say locus points satisfying locus hyperplane intersection unit sphere general great 
cosine similarity measure closeness final tex concept decompositions natural partition document vectors great objective function observe write objective function equivalently fp kx fp follows maximization problem equivalent minimization problem fp arg min fp fp section show naturally thought matrix approximation problem 

matrix approximations clustering document vectors define word document matrix matrix columns document vectors 
spherical means algorithm designed cluster document vectors disjoint partitions 
explicitly designed approximate word document matrix 
surprising natural connection clustering matrix approximation explore section 

clustering matrix approximation partitioning fp document vectors fx clusters approximate document vector closest concept vector 
words document vector cluster approximate concept vector define matrix approximation fp th column concept vector closest document vector natural question effective matrix approximating matrix 
measure error approximating squared frobenius norm difference matrix kx matrix frobenius norm golub van loan defined kak final tex dhillon modha observe write matrix approximation error kx kx fp follows maximizing objective function thought constrained matrix approximation problem 
matrix approximation rank particular matrix approximation corresponding final partitioning fp rank subsection compare approximation power best rank approximation word document matrix 
matrix approximation singular value decompositions singular value decomposition svd widely studied information retrieval text mining example latent semantic indexing berry deerwester 
denote rank word document matrix golub van loan define svd usv orthogonal matrix left singular vectors orthogonal matrix right singular vectors diagonal matrix positive singular values arranged decreasing order magnitude 
note write xx vs columns define orthonormal eigenvectors associated nonzero eigenvalues xx respectively diagonal elements nonnegative square roots nonzero eigenvalues xx write statistical terminology columns known principal components obtained deleting columns respectively obtained deleting rows columns matrix known truncated svd matrix rank equal written thought squares approximation matrix column space matrix example singular values left panel plot largest singular values classic word document matrix 
precisely speaking obtain principal components subtract mean document vector 
final tex concept decompositions index singular values index 
largest singular values classic left panel nsf right panel word document matrices 
example singular values right panel plot largest singular values nsf word document matrix 
mention experiments report results svds including examples michael berry www netlib org index html 
seen singular values decrease continuously obvious sudden drop 
natural cut point practice various information retrieval experiments truncation value normally 
known result golub van loan establishes truncated svd best rank approximation squared frobenius norm 
truncated svds baseline rank matrix approximations measured 
lemma matrix rank holds kx kx yk lemma holds arbitrary matrices rank equal particular holds matrix approximation corresponding final partitioning fp produced spherical means algorithm kx empirically validate examine lower bound 
example comparing left panel compare errors approximating classic word document matrix truncated svds matrices various values seen fixed approximation error truncated svd significantly lower final tex dhillon modha number vectors best svd clustering approximation error number vectors clustering best svd 
comparing approximation errors kx kx classic left panel nsf right panel data sets various values example comparing right panel repeat example nsf data set 
observe nsf data set right panel give results singular vectors 
trying compute singular vectors subroutine ran memory workstation ibm rs mbytes memory 
hand easily compute concept vectors nsf data set 
follows matrix approximation bad 
hindsight expected uses naive strategy approximating document vector closest concept vector 
significant room seeking better rank matrix approximations 

concept decompositions show approximating document vector linear combination concept vectors possible obtain significantly better matrix approximations 
fp denote partitioning document vectors fx clusters 
fc denote corresponding concept vectors 
define concept matrix matrix th column matrix concept vector assuming linear independence concept vectors follows concept matrix rank partitioning document vectors define corresponding concept decomposition word document matrix squares approximation column space concept matrix write concept decomposition matrix final tex concept decompositions matrix determined solving squares problem arg min kx zk known closed form solution exists squares problem equation intuitively pleasing constitute efficient numerically stable way compute matrix computationally qr decomposition concept matrix golub van loan 
lemma establishes concept decomposition better matrix approximation lemma kx kx proof matrix approximation rank inequality follows lemma 
observe may write ji matrix ji document vector cluster ji 
inequality follows 
lemma holds partitioning particular holds concept decomposition corresponding final partitioning fp produced spherical means algorithm 
show approximations turn quite powerful introduce random matrix approximations 
denote matrix entries randomly generated uniform distribution 
experiments rand function matlab 
assuming columns linearly independent write squares approximation column space example comparing left panel compare errors approximating classic word document matrix truncated svds matrices random matrix approximations various values seen approximation errors attained concept decompositions kx quite close attained optimal kx comparison random matrix approximations worse 
final tex dhillon modha number vectors best svd random concept decompositions number vectors best svd concept decompositions random 
comparing approximation errors kx kx kx classic left panel nsf right panel data sets various values example comparing right panel repeat example nsf data set 

concept vectors singular vectors comparison section demonstrated concept vectors may obtain matrix approximations concept decompositions comparable quality svd 
section compare contrast basis sets concept vectors singular vectors columns 

concept vectors local sparse fp denote partitioning document vectors disjoint clusters 
associate word cluster document cluster follows 
word contained weight word larger weight word concept vector precisely define fw jg examples show word clusters labeling cluster document vectors 
example word clusters clustering classic data set example right hand side list top words corresponding word clusters 
clear word cluster essentially localized underlying concepts medline cisi cranfield 
contrast seen top words leading singular vectors distributed underlying concepts 
final tex concept decompositions example word clusters example clustered nsf data set clusters 
display concept vectors display leading singular vectors 
concept vectors singular vectors annotated top words 
plots remaining concept vectors singular vectors see dhillon modha 
define total order words follows 
order words increasing order respective weight impose arbitrary order word clusters 
example order word clusters words precede words 
total orders illustrate locality concept vectors 
example locality clustering classic data set clusters example plot concept vectors 
plots total order words described boundaries word clusters evident ordering words word cluster 
shows weight concept vector concentrated localized corresponding word cluster 
analogously plot leading singular vectors classic word document matrix total order words 
contrast concept vectors singular vectors distribute weight word clusters 
concept vectors localized singular vectors global nature 
intuitively speaking concept vectors compared wavelets singular vectors compared fourier series 
observe concept vectors nonnegative singular vectors assume positive negative values 
example locality repeat example nsf data set clusters figures 
leave task comparing contrasting figures reader exercise 
example sparsity left panel plot ratio number nonzero entries concept vectors total number entries various values see increases concept vectors progressively sparser 
example concept vectors roughly sparse 
contrast singular vectors virtually completely dense 
intuitively number clusters increases fewer document vectors cluster average sense 
recall document vector sparse consequently concept vectors sparse 
write average inner product concept vectors fc average inner product takes value value corresponds orthonormal concept vectors value corresponds identical concept vectors 
final tex dhillon modha patients cells treatment normal growth blood children disease cell human library research libraries retrieval data systems science scientific book study pressure boundary theory layer mach method supersonic heat shock effects 
concept vectors corresponding clustering classic data set clusters 
concept vector top words corresponding weights shown right 
final tex concept decompositions pressure boundary library layer theory method mach data analysis shock library libraries boundary research layer pressure systems data retrieval science library boundary layer retrieval libraries patients data scientific systems cells 
leading singular vectors classic word document matrix 
singular vector top words corresponding weights shown right 
final tex dhillon modha number clusters number clusters 
sparsity nsf concept matrix left panel average inner product concept vectors nsf data set right panel 
example orthonormality nsf data set right panel plot average inner product versus number clusters number clusters increases see average inner product concept vectors progressively moves 
concept vectors tend orthonormality 
contrast singular vectors orthonormal 
far contrasted concept vectors singular vectors 
subsection shows surprising fact subspaces spanned vectors fact quite close 

principal angles comparing concept singular subspaces concept vectors fc corresponding final partitioning produced means algorithm define corresponding concept subspace define singular subspace dimensional linear subspace spanned leading singular vectors closeness subspaces measured quantities known principal angles golub golub van loan 
intuitively principal angles generalize notion angle lines higher dimensional subspaces subspaces assume dim dim principal angles recursively defined 
cosq max max final tex concept decompositions subject constraints kfk kgk 

vectors 

called principal vectors pair subspaces 
intuitively angle closest unit vectors angle closest unit vectors respectively orthogonal 
write average cosine principal angles subspaces cosq see golub algorithm compute principal angles 
example principal angles table compare singular subspace various concept subspaces classic data set 
cosq cosq cosq observe increases cosines principal angles tend 
fact singular subspace essentially completely contained concept subspace subspaces virtually share common dimensional subspace spanned leading principal vectors 
example principal angles table compare singular subspace various concept subspaces nsf data set 
cosq cosq cosq cosq cosq cosq cosq cosq cosq cosq increases cosines principal angles tend 
interested comparing individual singular subspaces individual concept subspaces interested comparing sequence singular subspaces fs sequence concept subspaces fc hard directly compare sequences example compare fixed singular subspace various concept subspaces 
results comparing fixed concept subspace various singular subspaces see dhillon modha 
example comparing singular subspace various concept subspaces nsf data set plot average cosine principal angles left panel final tex dhillon modha number clusters number principal angles 
average cosine left panel cosines right panel principal angles various concept subspaces nsf data set 
number clusters number principal angles 
average cosine left panel cosines panel principal angles various concept subspaces nsf data set 
cosines principal angles right panel singular subspace various concept subspaces 
note average cosine greater left panel cosines greater right panel 
furnish similar plots singular subspace various concept subspaces 
closing individually concept vectors different singular vectors concept subspaces turn quite close singular subspaces 
result surprising especially high dimensions 

studied vector space models large document collections 
models high dimensional sparse unique computational statistical challenges commonly encountered low dimensional dense data 
final tex concept decompositions solar galaxies observations ast stars waves telescope materials phase properties optical devices films magnetic ocean ice climate mantle sea water carbon chemistry reactions molecules organic nmr compounds chemical 
concept vectors corresponding clustering nsf data set clusters 
concept vector top words corresponding weights shown right 
final tex dhillon modha chemistry materials undergraduate design computer molecular courses faculty courses education college teachers mathematics undergraduate chemistry ocean species ice materials climate nmr theory chemistry equations geometry nmr molecules differential 
leading singular vectors nsf word document matrix 
singular vector top words corresponding weights shown right 
final tex concept decompositions clustering invaluable tool organize vector space model associated document collection 
fast spherical means clustering algorithm produce meaningful clusters descriptive labels see figures 
geometrically spherical means clustering algorithm partitions high dimensional space voronoi dirichlet regions separated hyperplanes passing origin 
cluster associate concept vector provides compact summary cluster 
spherical means algorithm seeks high coherence clusters 
average cluster coherence quite low high dimensional space large void surrounding concept vector see example 
behavior uncommon lowdimensional dense data sets example think distribution gaussian cloud mean 
ties document vectors nearest concept vectors weak natural question clustering data possible 
reconcile paradox observing document vectors close nearest concept vector absolute sense relative distances concept vectors 
furthermore average intra inter cluster structures similar various resolutions 
essential difference progressive separation inter cluster structure 
prompts obvious analogy fractals mandelbrot 
proposed statistical model text data consistent fractal behavior 
fact meaningful seek maximum entropy distributions subject empirical constraints 
evidence self similarity provided concept vector plots figures demonstrate word counts outside cluster general distribution 
known word count distributions text collections obey certain zipf law zipf 
results suggest possibility zipf law may hold recursive fashion cluster collection sub cluster cluster 
main findings concept decompositions derived concept vectors basic task matrix approximation 
surprisingly approximation power concept decompositions comparable truncated svds 
furthermore subspaces spanned concept vectors quite close subspaces singular vectors figures 
svd computation known large time memory requirements 
faster concept decompositions profitably replace svds applications dimensionality reduction feature selection improved information retrieval 
spite fact concept decompositions truncated svds possess similar approximation power constituent concept vectors singular vectors quite 
particular concept vectors localized word space singular vectors global nature figures 
locality concept vectors extremely useful labeling human intelligible fashion latent concepts discovered algorithm example see words right hand side panels figures 
furthermore singular vectors concept vectors sparse constitute compact description data 
concept vectors constitute powerful sparse localized basis text data sets 
resist temptation compare concept vectors wavelets singular vectors fourier series 
final tex dhillon modha berry dumais brien linear algebra intelligent information retrieval 
siam review 
golub numerical methods computing angles linear subspaces 
mathematics computation 
boley gini gross 
han hastings karypis kumar mobasher moore document categorization query generation world wide web webace 
ai review 
broder glassman manasse zweig syntactic clustering web 
technical report digital systems research center 
caid system method context vector generation retrieval 
patent 
cutting karger pedersen tukey scatter gather cluster approach browsing large document collections 
proc 
acm sigir 
deerwester dumais furnas landauer harshman indexing latent semantic analysis 
journal american society information science 
dhillon modha concept decompositions large sparse text data clustering 
technical report rj ibm almaden research center 
dhillon modha parallel data clustering algorithm distributed memory multiprocessors 
zaki ho eds large scale parallel data mining lecture notes artificial intelligence volume 
springer verlag new york pp 

large scale parallel kdd systems workshop san diego ca 
dhillon modha visualizing class structure multidimensional data 
ed proceedings th symposium interface computing science statistics vol 

minneapolis mn pp 

duda hart pattern classification scene analysis 
wiley 
frakes baeza yates information retrieval data structures algorithms 
prentice hall englewood cliffs new jersey 
gallant methods generating revising context vectors plurality word stems 
patent 
garey johnson complexity generalized lloyd max problem 
ieee trans 
inform 
theory 
golub van loan matrix computations 
baltimore md usa johns hopkins university press 
hartigan clustering algorithms 
wiley 
hearst pedersen reexamining cluster hypothesis scatter gather retrieval results 
proc 
acm sigir 
hofmann probabilistic latent semantic indexing 
proc 
acm sigir 
isbell viola restructuring sparse high dimensional data effective retrieval 
advances neural information processing vol 

kleinberg papadimitriou raghavan microeconomic view data mining 
data mining knowledge discovery 
limited memory matrix methods applications 
ph thesis applied mathematics program university maryland college park 
leland taqqu willinger wilson self similar nature ethernet traffic 
ieee acm transactions networking 
mandelbrot fractal geometry nature 
freeman 
leary peleg digital image compression outer product expansion 
ieee trans 
communications 
papadimitriou raghavan tamaki vempala latent semantic indexing probabilistic analysis 
proc 
seventeenth acm sigact sigmod sigart symp 
principles database systems seattle washington 
pp 

pollard quantization method means 
ieee trans 
inform 
theory 
rasmussen clustering algorithms 
frakes baeza yates eds information retrieval data structures algorithms 
pp 

rissanen speed yu density estimation stochastic complexity 
ieee trans 
inform 
theory 
final tex concept decompositions sabin gray global convergence empirical consistency generalized lloyd algorithm 
ieee trans 
inform 
theory 
sahami sonia service organizing networked information autonomously 
proc 
acm digital libraries 
salton buckley term weighting approaches automatic text retrieval 
inform 
proc 
management pp 

salton mcgill modern retrieval 
mcgraw hill book 
saul pereira aggregate mixed order markov models statistical language processing 
proc 
nd int 
conf 
empirical methods natural language processing 
silverstein projections efficient document clustering 
proc 
acm sigir 
silverstein pedersen constant time clustering arbitrary corpus subsets 
proc 
acm sigir 
singhal buckley mitra salton pivoted document length normalization 
proc 
acm sigir 
vaithyanathan dom model selection unsupervised learning applications document clustering 
proc 
th int 
machine learning conf bled slovenia 
willet trends hierarchic document clustering critical review 
inform 
proc 
management pp 

zamir etzioni web document clustering feasibility demonstration 
proc 
acm sigir 
zipf human behavior principle effort 
addison wesley reading ma 
final tex 
