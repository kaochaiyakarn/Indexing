efficient backprop yann lecun leon bottou orr klaus robert muller image processing research department labs research schulz drive red bank nj usa willamette university state street salem usa gmd berlin germany research att com willamette edu klaus gmd de originally published orr muller neural networks tricks trade springer 

convergence back propagation learning analyzed explain common phenomenon observed practitioners 
undesirable behaviors backprop avoided tricks rarely exposed serious technical publications 
gives tricks offers explanations 
authors suggested second order optimization methods advantageous neural net training 
shown classical second order methods impractical large neural networks 
methods proposed limitations 
backpropagation popular neural network learning algorithm conceptually simple computationally efficient works 
getting art science 
designing training network backprop requires making seemingly arbitrary choices number types nodes layers learning rates training test sets forth 
choices critical foolproof recipe deciding largely problem data dependent 
heuristics underlying theory help guide practitioner better choices 
section introduce standard backpropagation discuss number simple heuristics tricks improving performance 
discuss issues convergence 
describe classical second order non linear optimization techniques show application neural network training limited despite claims contrary literature 
second order methods accelerate learning certain cases 
learning generalization approaches automatic machine learning successful approaches categorized gradient learning methods 
learning machine represented computes function th input pattern represents collection adjustable parameters system 
cost function measures discrepancy correct desired output pattern output produced system 
average cost function train average errors set input output pairs called training set simplest setting learning problem consists finding value minimizes train 
practice performance system training set little interest 
relevant measure error rate system field practice 
performance estimated measuring accuracy set samples disjoint training set called test set 
commonly cost function mean squared error gamma train cost function learning machine parameters output ep error desired output dp input 
zp fig 

gradient learning machine 
chapter focused strategies improving process minimizing cost function 
strategies conjunction methods maximizing network ability generalize predict correct targets patterns learning system previously seen detail 
understand generalization consider backpropagation works 
start set samples input output pair function learned 
measurement process noisy may errors samples 
imagine collected multiple sets samples set look little different noise different points sampled 
data sets result networks minima slightly different true function 
chapter concentrate improving process finding minimum particular set examples 
generalization techniques try correct errors introduced network result choice dataset 
important 
theoretical efforts analyzed process learning minimizing error training set process called empirical risk minimization 
theoretical analyses decomposing generalization error terms bias variance see 
bias measure network output averaged possible data sets differs desired function 
variance measure network output varies datasets 
early training bias large network output far desired function 
variance small data little influence 
late training bias small network learned underlying function 
trained long network learned noise specific dataset 
referred overtraining 
case variance large noise varies datasets 
shown minimum total error occur sum bias variance minimal 
number techniques early stopping regularization maximizing generalization ability network backprop 
idea chapter minimization strategies cost function tricks associated increasing speed quality minimization 
clear choice model model selection architecture cost function crucial obtaining network generalizes 
keep mind wrong model class proper model selection done superb minimization clearly help 
fact existence overtraining led authors suggest inaccurate minimization algorithms better ones 
standard backpropagation tricks analyses primarily context classical multi layer feed forward neural networks apply gradient learning methods 
simplest form multilayer learning machine trained gradient learning simply stack modules implements function xn fn wn xn gamma xn vector representing output module wn vector tunable parameters module subset xn gamma module input vector previous module output vector 
input module input pattern partial derivative respect xn known partial derivatives respect wn xn gamma computed backward recurrence wn wn xn gamma xn xn gamma wn xn gamma xn wn xn gamma jacobian respect evaluated point wn xn gamma wn xn gamma jacobian respect jacobian vector function matrix containing partial derivatives outputs respect inputs 
equations applied modules reverse order layer layer partial derivatives cost function respect parameters computed 
way computing gradients known back propagation 
traditional multi layer neural networks special case system modules alternated layers matrix multiplications weights component wise sigmoid functions units yn gamma xn wn matrix number columns dimension xn gamma number rows dimension xn vector function applies sigmoid function component input 
yn vector weighted sums total inputs layer applying chain rule equation classical backpropagation equations obtained ij gamma gamma ik equations written matrix form yn xn wn xn gamma yn xn gamma yn simplest learning minimization procedure setting gradient descent algorithm iteratively adjusted follows gamma gamma simplest case scalar constant 
sophisticated procedures variable methods takes form diagonal matrix estimate inverse hessian matrix cost function second derivative matrix newton quasi newton methods described chapter 
proper choice important discussed length 
practical tricks backpropagation slow particularly multilayered networks cost surface typically non quadratic non convex high dimensional local minima flat regions 
formula guarantee network converge solution convergence swift convergence occurs 
section discuss number tricks greatly improve chances finding solution decreasing convergence time orders magnitude 
detailed theoretical justifications sections 
stochastic versus batch learning 
iteration equation requires complete pass entire dataset order compute average true gradient 
referred batch learning entire batch data considered weights updated 
alternatively stochastic online learning single example fz chosen randomly training set iteration estimate true gradient computed error example weights updated gamma estimate gradient noisy weights may move precisely gradient iteration 
shall see noise iteration advantageous 
stochastic learning generally preferred method basic backpropagation reasons advantages stochastic learning 
stochastic learning usually faster batch learning 

stochastic learning results better solutions 

stochastic learning tracking changes 
stochastic learning faster batch learning particularly large redundant datasets 
reason simple show 
consider simple case training set size inadvertently composed identical copies set samples 
averaging gradient patterns gives exact result computing gradient just 
batch gradient descent wasteful recomputes quantity times parameter update 
hand stochastic gradient see full epoch iterations long training set 
practice examples rarely appear dataset usually clusters patterns similar 
example phoneme classification patterns phoneme ae hopefully contain information 
redundancy batch learning slower line 
stochastic learning results better solutions noise updates 
nonlinear networks usually multiple local minima differing depths 
goal training locate minima 
batch learning discover minimum basin weights initially placed 
stochastic learning noise updates result weights jumping basin possibly deeper local minimum 
demonstrated certain simplified cases 
stochastic learning useful function modeled changing time quite common scenario industrial applications data distribution changes gradually time due wear tear machines 
learning machine detect follow change impossible learn data properly large generalization errors result 
batch learning changes go undetected obtain bad results average rules line learning operated properly see section track changes yield approximation results 
despite advantages stochastic learning reasons consider batch learning advantages batch learning 
conditions convergence understood 

acceleration techniques conjugate gradient operate batch learning 

theoretical analysis weight dynamics convergence rates simpler 
advantages stem noise stochastic learning advantageous 
noise critical finding better local minima prevents full convergence minimum 
converging exact minimum convergence stalls due weight fluctuations 
size fluctuations depend degree noise stochastic updates 
variance fluctuations local minimum proportional learning rate 
order reduce fluctuations decrease anneal learning rate adaptive batch size 
theory shown optimal annealing schedule learning rate form number patterns constant 
practice may fast method remove noise mini batches start small batch size increase size training proceeds 
mller discusses method doing orr discusses linear problems 
deciding rate increase batch size inputs include small batches difficult determining proper learning rate 
effectively size learning rate stochastic learning corresponds respective size mini batch 
note problem removing noise data may critical thinks generalization 
overtraining may occur long noise regime reached 
advantage batch training able second order methods speed learning process 
second order methods speed learning estimating just gradient curvature cost surface 
curvature estimate approximate location actual minimum 
despite advantages batch updates stochastic learning preferred method particularly dealing large data sets simply faster 
shuffling examples networks learn fastest unexpected sample 
advisable choose sample iteration unfamiliar system 
note applies stochastic learning order input presentation irrelevant batch course simple way know inputs information rich simple trick crudely implements idea simply choose successive examples different classes training examples belonging class contain similar information 
heuristic judging new information training example contains examine error network output target value input 
large error indicates input learned network contains lot new information 
sense input frequently 
course large mean relative training examples 
network trains relative errors change frequency presentation particular input pattern 
method modifies probability appearance pattern called emphasizing scheme 
choose examples maximum information content 
shuffle training set successive training examples rarely belong class 

input examples produce large error frequently examples produce small error 
order gradients summed batch may affected roundoff error significant range gradient values 
careful perturbing normal frequencies input examples changes relative importance network places different examples 
may may desirable 
example technique applied data containing outliers disastrous outliers produce large errors frequently 
hand technique particularly beneficial boosting performance infrequently occurring inputs phoneme recognition normalizing inputs convergence usually faster average input variable training set close zero 
see consider extreme case inputs positive 
weights particular node weight layer updated amount proportional ffix ffi scalar error node input vector see equations 
components input vector positive updates weights feed node sign sign ffi 
result weights decrease increase input pattern 
weight vector change direction inefficient slow 
example inputs positive 
general shift average input away zero bias updates particular direction slow learning 
shift inputs average training set close zero 
heuristic applied layers means want average outputs node close zero outputs inputs layer 
problem addressed coordinating inputs transformed choice sigmoidal activation function 
discuss input transformation 
discussion sigmoid follows 
convergence faster inputs shifted described scaled covariance number training examples covariance th input variable th component th training example 
scaling speeds learning helps balance rate weights connected input nodes learn 
value covariance matched sigmoid 
sigmoid covariance choice 
exception scaling covariances value occurs known inputs significance 
case beneficial scale significant inputs visible learning process 
transforming inputs 
average input variable training set close zero 

scale input variables covariances 

input variables uncorrelated possible 
tricks shifting scaling inputs quite simple implement 
trick quite effective difficult implement decorrelate inputs 
consider simple network 
inputs uncorrelated possible solve value minimizes error concern vice versa 
words variables independent system equations diagonal 
correlated inputs solve simultaneously harder problem 
principal component analysis known karhunen loeve expansion remove linear correlations inputs 
inputs linearly dependent extreme case correlation may produce degeneracies may slow learning 
consider case input twice input 
network output constant lines gamma constant 
gradient zero directions see 
moving lines absolutely effect learning 
trying solve effectively problem 
ideally want remove inputs decrease size network 
shows entire process transforming inputs 
steps shift inputs mean zero decorrelate inputs equalize covariances 
lines constant fig 

linearly dependent inputs 
mean cancellation covariance equalization fig 

transformation inputs 
sigmoid nonlinear activation functions give neural networks nonlinear capabilities 
common forms activation function sigmoid monotonically increasing function asymptotes finite value sigma approached 
common examples standard logistic function gammax hyperbolic tangent tanh shown 
sigmoids symmetric origin see preferred reason inputs normalized produce outputs inputs layer average close zero 
contrast say logistic function outputs positive mean positive 
sigmoids 
symmetric sigmoids hyperbolic tangent converge faster standard logistic function 

recommended sigmoid tanh gamma delta tanh function computationally expensive approximation ratio polynomials 

helpful add small linear term tanh ax avoid flat spots 
fig 

recommended standard logistic function gammax 
hyperbolic tangent tanh gamma delta constants recommended sigmoid chosen transformed inputs see previous discussion variance outputs close effective gain sigmoid roughly useful range 
particular sigmoid properties sigma sigma second derivative maximum effective gain close 
potential problems symmetric sigmoids error surface flat near origin 
reason avoid initializing small weights 
saturation sigmoids error surface flat far origin 
adding small linear term sigmoid help avoid flat regions 
choosing target values classification problems target values typically binary 
common wisdom suggest target values set value sigmoid asymptotes 
drawbacks 
instabilities result 
training process try drive output close possible target values achieved asymptotically 
result weights output hidden driven larger larger values sigmoid derivative close zero 
large weights increase gradients gradients multiplied exponentially small sigmoid derivative twisting term added sigmoid producing weight update close zero 
result weights may stuck 
second outputs saturate network gives indication confidence level 
input pattern falls near decision boundary output class uncertain 
ideally reflected network output value possible target values near asymptote 
large weights tend force outputs tails sigmoid regardless uncertainty 
network may predict wrong class giving indication low confidence result 
large weights saturate nodes impossible differentiate typical examples 
solution problems set target values range sigmoid asymptotic values 
care taken insure node restricted linear part sigmoid 
setting target values point maximum second derivative sigmoid best way take advantage nonlinearity saturating sigmoid 
reason sigmoid choice 
maximum second derivative sigma correspond binary target values typical classification problems 
targets choose target values point maximum second derivative sigmoid avoid saturating output units 
twisting term small linear term added node output tanh ax 
initializing weights starting values weights significant effect training process 
weights chosen randomly way sigmoid primarily activated linear region 
weights large sigmoid saturate resulting small gradients learning slow 
weights small gradients small 
intermediate weights range sigmoid linear region advantage gradients large learning proceed network learn linear part mapping difficult nonlinear part 
achieving requires coordination training set normalization choice sigmoid choice weight initialization 
start requiring distribution outputs node standard deviation oe approximately 
achieved input layer normalizing training set described earlier 
obtain standard deviation close output hidden layer just need recommended sigmoid requirement input sigmoid standard deviation oe 
assuming inputs unit uncorrelated variance standard deviation units weighted sum oe ij insure oe approximately weights randomly drawn distribution mean zero standard deviation oe gamma number inputs unit 
initializing weights assuming 
training set normalized 
sigmoid weights randomly drawn distribution uniform mean zero standard deviation oe gamma fan number connections feeding node 
choosing learning rates principled method described section estimating ideal learning rate schemes empirical proposed literature automatically adjust learning rate 
schemes decrease learning rate weight vector oscillates increase weight vector follows relatively steady direction 
main problem methods appropriate stochastic gradient line learning weight vector fluctuates time 
choosing single global learning rate clear picking different learning rate weight improve convergence 
way doing computing second derivatives described section 
main philosophy sure weights network converge roughly speed 
depending curvature error surface weights may require small learning rate order avoid divergence may require large learning rate order converge reasonable speed 
learning rates lower layers generally larger higher layers see 
corrects fact neural net architectures second derivative cost function respect weights lower layers generally smaller higher layers 
rationale heuristics discussed detail sections suggestions choose actual value learning rate different weights see section 
shared weights time delay neural networks tdnn convolutional networks learning rate proportional square root number connections sharing weight know gradients sum independent terms 
equalize learning speeds give weight learning rate learning rates proportional square root number inputs unit weights lower layers typically larger higher layers tricks improving convergence include momentum momentum deltaw deltaw increase speed cost surface highly size steps directions high curvature yielding larger effective learning rate directions low curvature denotes strength momentum term 
claimed momentum generally helps batch mode stochastic mode systematic study known authors 
adaptive learning rates authors including sompolinsky darken moody sutton murata proposed rules automatically adapting learning rates see 
rules control speed convergence increasing decreasing learning rate error 
assume facts learning rate adaptation algorithm smallest eigenvalue hessian see eq sufficiently smaller second smallest eigenvalue large number iterations parameter vector approach minimum direction minimum eigenvector hessian see eq 
conditions evolution estimated parameter thought onedimensional process minimum eigenvector approximated large number iterations see kh ik denotes norm 
adopt projection hv kh ik approximated minimum eigenvector dimensional measure distance minimum 
distance control learning rate details see gamma gamma ffi ffi ffi ffj gamma ffi controls leak size average ff fi constants auxiliary variable calculate leaky average gradient note set rules easy compute straightforward implement 
simply keep track additional vector eq averaged gradient norm vector controls size learning rate see eq 
algorithm follows simple intuition far away minimum large distance proceeds big steps close minimum learning rate theoretical details see 
radial basis functions vs sigmoid units systems nodes dot products sigmoids types units layers 
common alternative radial basis function rbf network see rbf networks dot product weight input vector replaced euclidean distance fig 

convergence flow 
final stage learning average flow approximately dimensional minimum approximation minimum eigenvalue direction hessian 
input weight sigmoid replaced exponential 
output activity computed output exp gamma oe kx gamma oe mean standard deviation th gaussian 
units replace coexist standard units usually trained combination gradient descent output units unsupervised clustering determining means widths rbf units 
sigmoidal units cover entire space single rbf unit covers small local region input space 
advantage learning faster 
rbf units may form better set basis functions model input space sigmoid units highly problem dependent 
negative side locality property rbfs may disadvantage particularly high dimensional spaces may units needed cover spaces 
rbfs appropriate low dimensional upper layers sigmoids high dimensional lower layers 
convergence gradient descent little theory section examine theory tricks earlier 
dimension update equation gradient descent written gamma de dw know value affects convergence learning speed 
illustrates learning behavior different sizes weight starts vicinity local minimum 
dimension easy define optimal learning rate opt learning rate move weight minimum wmin precisely step see 
smaller opt stepsize smaller convergence take multiple timesteps 
opt opt weight oscillate wmin eventually converge 
twice size opt stepsize large weight ends farther wmin 
divergence results 
opt min opt min opt min opt min opt dw de dw min min de dw ii fig 

gradient descent different learning rates 
optimal value learning rate opt consider case dimension 
assuming approximated quadratic function opt derived expanding taylor series current weight gamma de dw gamma dw shorthand de wc dw de dw fi fi wc quadratic second order derivative constant higher order terms vanish 
differentiating sides respect gives de dw de dw gamma dw setting wmin noting de wmin dw left rearranging wmin gamma dw gamma de dw comparing update equation find reach minimum step opt dw gamma easier way obtain result illustrated ii 
bottom graph plots gradient function quadratic gradient simply straight line value zero minimum wc current weight simply slope line computed standard slope formula gamma gamma wmin solving wmin gives equation 
learning rate gives fastest convergence opt largest learning rate causing divergence see jmax opt exactly quadratic higher order terms equation precisely zero approximation 
case may take multiple iterations locate minimum opt convergence quite fast 
multiple dimensions determining opt bit difficult right side matrix gamma called hessian components ij equal total number weights 
measure curvature dimensions lines constant quadratic cost oval shape shown 
eigenvectors point directions major minor axes 
eigenvalues measure steepness corresponding 
example 
mean square lms algorithm single layer linear network error function jd gamma eigenvectors min min fig 

lines constant number training vectors 
hessian case turns covariance matrix inputs pt eigenvalue measure covariance spread inputs corresponding shown 
fig 

lms algorithm eigenvectors eigenvalues measure spread inputs input space 
scalar learning rate problematic multiple dimensions 
want large convergence fast shallow directions small eigenvalues large weights diverge steep directions large eigenvalues 
see specifically expand time minimum wmin gamma wmin wmin gamma wmin differentiating result update equation gives gamma gamma jh wmin gamma wmin subtracting wmin sides gives gamma wmin gamma jh wmin gamma wmin gamma jh wmin matrix transformation shrinks vector eigenvalues magnitude update equation converge 
help choosing learning rates 
ideally want different learning rates different 
simple lined coordinate axes weights 
case weights uncoupled assign weight learning rate corresponding eigenvalue 
weights coupled rotate diagonal coordinate axes line see 
purpose diagonalizing hessian discussed earlier 
theta rotation matrix theta diagonal theta theta cost function written wmin theta gamma wmin theta theta wmin theta theta gamma wmin making change coordinates theta gamma wmin simplifies equation transformed update equation gamma note gamma diagonal diagonal components gamma equation converge gamma constrained single scalar learning rate weights require max order avoid divergence max largest eigenvalue fastest convergence opt max min lot smaller max convergence slow min direction 
fact convergence time proportional condition number max min desirable small eigenvalue spread possible 
rotated aligned coordinate axes consists independent dimensional equations 
choose learning rate weight independent 
see optimal rate th weight opt examples linear network displays set examples drawn gaussian distributed classes centered 
eigenvalues covariance matrix 
train single layer linear network inputs output weights bias see lms algorithm batch mode 
displays weight trajectory error learning learning rates 
note learning rate see eq 
jmax max cause divergences evident 
fig 

simple linear network 
fig 

classes drawn gaussian distributions centered 
shows example stochastic batch mode learning 
learning rate 
see trajectory noisier batch mode estimate gradient iteration 
cost plotted function epoch 
epoch simply defined input presentations stochastic learning weight space log mse db epochs weight space log mse db epochs fig 

weight trajectory error curve learning 
corresponds weight updates 
batch epoch corresponds weight update 
weight space log mse db epochs batch fig 

weight trajectory error curve stochastic learning 
log mse db weight space epochs fig 

weight trajectories errors network trained stochastic learning 
multilayer network shows architecture simple multilayer network 
input hidden output node 
weights biases 
activation function tanh 
training set contains examples classes 
classes gaussian distributed standard deviation 
class mean class mean 
target values class class 
shows stochastic trajectory example 
fig 

minimal multilayer network 
input transformations error surface transformations revisited results previous section justify tricks discussed earlier 
subtract means input variables reason trick nonzero mean input variables creates large eigenvalue 
means condition number large cost surface steep directions shallow convergence slow 
solution simply preprocess inputs subtracting means 
single linear neuron eigenvectors hessian means subtracted point principal axes cloud training vectors recall 
inputs large variation spread different directions input space large condition number slow learning 
recommend normalize variances input variables 
input variables correlated error surface spherical possibly reduce eccentricity 
correlated input variables usually cause eigenvectors rotated away coordinate axes versus weight updates decoupled 
decoupled weights learning rate weight method optimal trick decorrelate input variables 
suppose input variables neuron decorrelated hessian neuron diagonal eigenvalues point coordinate axes 
case gradient best descent direction seen fig 
point arrow shows gradient point minimum 
assign weight learning rate equal inverse corresponding eigenvalue descent direction direction arrow points directly minimum separate learning rate weight 
classical second order optimization methods briefly introduce newton conjugate gradient levenberg marquardt quasi newton bfgs method see 
newton algorithm get understanding newton method recapitulate results section 
assuming quadratic loss function see eq depicted ii compute weight update lines eq deltaw gamma jh gamma chosen range practice perfectly quadratic 
equation information hessian taken account 
error function quadratic step sufficient converge 
usually energy surface minimum ellipsoid extreme shell depending conditioning hessian 
whitening transform known signal processing literature change ellipsoid shape spherical shape theta see eq 
inverse hessian eq basically spheres error surface locally 
approaches shown equivalent newton algorithm untransformed weight space usual gradient descent whitened coordinate system see 
summarizing newton algorithm converges step error function quadratic gradient descent invariant respect linear transformations input vectors 
means convergence time affected shifts scaling rotation input vectors 
main drawbacks theta hessian matrix stored inverted takes iterations impractical variables 
error function general non quadratic guarantee convergence 
hessian positive definite zero negative eigenvalues error surface flat directions curved downward newton algorithm diverge hessian positive definite 
course hessian matrix multilayer networks general positive definite 
reasons newton algorithm original form usable general neural network learning 
gives insights developing sophisticated algorithms discussed 
ql network input output network input output newton algorithm 
gradient descent fig 

sketch whitening properties newton algorithm 
conjugate gradient important properties conjugate gradient optimization method doesn hessian explicitly attempts find descent directions try minimally spoil result achieved previous iterations uses line search importantly works batch learning 
third property shown 
assume pick descent direction gradient minimize line direction line search 
subsequently try find direction gradient change direction merely length conjugate direction moving direction spoil result previous iteration 
evolution descent directions ae iteration ae fi ae gamma choice fi done fletcher reeves descent direction gradients conjugate direction fig 

sketch conjugate gradient directions error surface 
fi re re re gamma re gamma polak fi re gamma re gamma re re gamma re gamma directions ae ae gamma defined conjugate ae ae gamma conjugate directions orthogonal directions space identity hessian matrix see 
important convergence choices fig 

sketch conjugate gradient directions error surface 
line search procedure 
perfectly quadratic function variables convergence steps proved 
non quadratic functions polak choice robust 
conjugate gradient viewed smart choice choosing momentum term known neural network training 
applied large success multi layer network training problems moderate sized low redundancy data 
typical applications range function approximation robotic control time series prediction real valued problems high accuracy wanted 
clearly large redundant classification problems stochastic backpropagation faster 
attempts define main disadvantage conjugate gradient methods remains batch method partly due precision requirements line search procedure 
quasi newton bfgs quasi newton bfgs method iteratively computes estimate inverse hessian algorithm requires line search works batch learning 
positive definite estimate inverse hessian done directly requiring matrix inversion gradient information 
algorithmically described follows positive definite matrix chosen search direction set ae re line search performed ae gives update parameters time gamma gamma ae estimate inverse hessian updated 
compared newton algorithm quasi newton approach needs gradient information 
successful quasi newton algorithm fletcher bfgs method 
update rule estimate inverse hessian gamma oe oe ffi oe ffiffi ffi oe gamma ffioe ffi oe abbreviations theta vectors oe re gamma re gamma ffi gamma gamma mentioned complexity required store theta matrix algorithm practical small networks non redundant training sets 
variants exist aim reduce storage requirements see 
gauss newton levenberg marquardt gauss newton levenberg marquardt algorithm square jacobi approximation mainly designed batch learning complexity important mean squared error loss functions 
gauss newton algorithm newton algorithm hessian approximated square jacobian see section discussion deltaw gamma re levenberg marquardt method gauss newton regularization parameter prevents blowing eigenvalues small deltaw gamma re denotes unity matrix 
gauss newton method valid quadratic cost functions similar procedure works cost called natural gradient see 
tricks compute hessian information multilayer networks discuss techniques aimed computing full partial hessian information finite difference method square jacobian approximation gauss newton levenberg marquardt algorithm computation diagonal hessian obtaining product hessian vector computing hessian 
semi analytical techniques allow computation full hessian omitted complicated require forward backward propagation steps 
finite difference write th line hessian re re ffioe gamma re ffi oe vector zeros th position 
implemented simple recipe compute total gradient multiple forward backward propagation steps 
add ffi th parameter compute gradient subtract results divide ffi 
due numerical errors computation scheme resulting hessian perfectly symmetric 
case symmetrized described 
square jacobian approximation gauss newton levenberg marquardt algorithms assuming mean squared cost function gamma gamma gradient gamma gamma hessian follows gamma simplifying approximation hessian square jacobian positive semi definite matrix dimension theta second term eq dropped 
equivalent assuming network linear function parameters readily implemented th column jacobian training patterns forward propagate set activity output units th output backpropagation step taken gradient accumulated 
backpropagating second derivatives consider multi layer system functional blocks inputs outputs parameters form 
assume knew theta matrix 
straight forward compute matrix drop second term eq resulting estimate hessian positive semi definite 
reduction achieved ignore diagonal terms similar derivation done obtain times matrix backpropagating diagonal hessian neural nets backpropagation procedures computing diagonal hessian known 
assumed layer network functional form jx see sigmoidal network 
gauss newton approximation dropping term contain obtain ki ki gaussian nonlinearity shown rbf networks obtain ki gamma ki gamma ki cost computing diagonal second derivatives running equations layer essentially regular pass gradient square weights weighted sums 
technique applied optimal brain damage pruning procedure see 
wx fig 

backpropagating diagonal hessian sigmoids left rbfs right 
computing product hessian vector methods hessian hessian exclusively products vector 
interestingly way computing products going trouble computing hessian 
finite difference method fulfill task arbitrary vector psi psi ff ff psi gamma gradient computations point ff psi respectively readily computed backprop ff small constant 
method applied compute principal eigenvector eigenvalue power method 
iterating setting psi psi psi vector psi converge largest eigenvector psi corresponding eigenvalue 
see accurate method finite differences similar complexity 
analysis hessian multi layer networks interesting understand tricks shown previously influence hessian hessian change architecture details implementation 
typically eigenvalue distribution hessian looks sketched small eigenvalues medium ones large ones 
argue large eigenvalues cause trouble training process non zero mean inputs neuron states wide variations second derivatives layer layer correlation state variables 
exemplify show eigenvalue distribution network trained ocr data 
clearly wide spread eigenvalues see observe ratio eleventh eigenvalue 
long tail eigenvalue distribution see painful ratio largest smallest eigenvalue gives conditioning learning problem 
large ratio corresponds big difference axis ellipsoidal shaped error function larger ratio find shell shaped minima extremely steep small axis flat long axis 
general characteristic hessian multi layer networks spread layers 
roughly sketch shape hessian varies flat layer quite steep eigenvalue order log eigenvalue ratio st th eigenvalues fig 

eigenvalue spectrum layer shared weights network theta theta theta trained handwritten digits 
layer 
affects learning speed provide ingredient explain slow learning lower layers fast oscillating learning layer 
trick compensate different scale learning inverse diagonal hessian control learning rate see section 
applying second order methods multilayer networks concentrate section tailor second order techniques training large networks repeat pessimistic facts applying classical second order methods 
techniques full hessian information gauss newton levenberg marquardt bfgs apply small networks trained batch mode small networks ones need speeding 
second order methods conjugate gradient bfgs 
require line search stochastic mode 
tricks discussed previously apply batch learning 
experience know carefully tuned stochastic gradient descent hard beat large classification problems 
smaller problems require accurate real valued outputs function approximation control problems see conjugate gradient polak eigenvalue magnitude number eigenvalues big fig 

eigenvalue spectrum layer shared weights network theta theta theta trained handwritten digits 
fig 

multilayered architecture second derivative smaller lower layers 
eq offers best combination speed reliability simplicity 
attempts mini batches applying conjugate gradient large redundant problems 
variant conjugate gradient optimization called scaled cg interesting line search procedure replaced levenberg marquardt type algorithm 
stochastic diagonal levenberg marquardt method obtain stochastic version levenberg marquardt algorithm idea compute diagonal hessian running estimate second derivative respect parameter 
instantaneous second derivative obtained backpropagation shown formulas section 
soon running estimates compute individual learning rates parameter ki ffl ki ffl denotes global learning rate ki running estimate diagonal second derivative respect ki parameter prevent ki blowing case second derivative small optimization moves flat parts error function 
running estimate computed ki new gamma fl ki old fl ki fl small constant determines amount memory 
second derivatives computed prior training subset training set 
change slowly need reestimated epochs 
note additional cost regular backpropagation negligible convergence rule thumb times faster carefully tuned stochastic gradient algorithm 
see convergence stochastic diagonal levenberg marquardt method toy example different sets learning rates 
obviously experiment shown contains fewer fluctuations due smaller learning rates 
computing principal eigenvalue vector hessian give tricks computing principal eigenvalue vector hessian having compute hessian 
remember section introduced method approximate smallest eigenvector hessian having compute hessian averaging see 
weight space log mse db hessian largest eigenvalue max max epochs learning rates maximum admissible learning rate batch fig 

stochastic diagonal levenberg marquardt algorithm 
data set gaussians examples 
network linear unit inputs output parameters weights bias 
weight space log mse db hessian largest eigenvalue max max epochs learning rates maximum admissible learning rate batch fig 

stochastic diagonal levenberg marquardt algorithm 
data set gaussians examples 
network linear unit inputs output parameters weights bias 
power method repeat result discussion section starting random initial vector psi iteration psi new psi old psi old eventually converge principal eigenvector vector principal eigenspace psi old converge corresponding eigenvalue 
taylor expansion method fact small perturbations gradient lead principal eigenvector psi new ff ff psi old psi old gamma ff small constant 
iteration procedure requires forward backward propagation steps pattern training set 
online computation psi rule running average obtain largest eigenvalue average hessian fast psi new gamma fl psi ff ff psi old psi old gamma summarize eigenvalue vector computations 
random vector chosen initialization psi 
input pattern desired output forward backward propagation step performed gradients stored 
ff psi old psi old added current weight vector 
forward backward propagation step performed perturbed weight vector gradients stored 
difference ff gamma computed running average eigenvector updated 
loop reasonably stable result obtained psi 
optimal learning rate opt psi see evolution eigenvalue function number pattern presentations neural network handwritten character recognition task 
practice adapt leak size running average order get fewer fluctuations indicated 
see fewer pattern presentations correct order magnitude eigenvalue learning rate reached 
experiments observe fluctuations average hessian training small 
number pattern presentations eigenvalue fig 

evolution eigenvalue function number pattern presentations shared weight network layers connections free parameters 
training set consists handwritten digits 
start initial conditions perform fixed number epochs learning rates computed multiplying predicted learning rate predefined constant 
choosing constant predicted optimal rate gives residual errors close error achieved best choice constant 
words predicted optimal rate optimal 
learning rate predicted optimal learning rate error epoch epochs epochs epochs epochs fig 

mean squared error function ratio learning rate predicted optimal learning rate fully connected network theta theta 
training set consists handwritten digits 
learning rate predicted optimal learning rate error epoch epochs epochs epochs epochs fig 

mean squared error function ratio learning rate predicted optimal learning rate shared weight network layers theta theta theta theta theta local connections free parameters shared weights 
training set consists handwritten digits 
discussion recommendations mentioned practitioner facing multi layer neural net training problem go steps shuffle examples center input variables subtracting mean normalize input variable standard deviation possible decorrelate input variables 
pick network sigmoid function shown set target values range sigmoid typically 
initialize weights random values prescribed 
preferred method training network picked follows training set large samples redundant task classification stochastic gradient careful tuning stochastic diagonal levenberg marquardt method 
training set large task regression conjugate gradient 
classical second order methods impractical useful cases 
non linear dynamics stochastic gradient descent multi layer neural networks particularly pertains generalization far understood 
theoretical systematic experimental needed 

gratefully acknowledge mutual exchange daad nsf 

amari 
neural learning structured parameter spaces natural riemannian gradient 
michael mozer michael jordan thomas petsche editors advances neural information processing systems volume page 
mit press 

amari 
natural gradient works efficiently learning 
neural computation 

battiti 
second order methods learning steepest descent newton method 
neural computation 

becker lecun 
improving convergence learning second oder ds 
david touretzky geoffrey hinton terrence sejnowski editors proceedings connectionist models summer school pages 
lawrence erlbaum associates 

bishop 
neural networks pattern recognition 
clarendon press oxford 

bottou 
online algorithms stochastic approximations 
david saad editor online learning neural networks workshop newton institute cambridge 
newton institute series cambridge university press 

broomhead lowe 
multivariable function interpolation adaptive networks 
complex systems 

buntine weigend 
computing second order derivatives feedforward networks review 
ieee transactions neural networks 
appear 

darken moody 
note learning rate schedules stochastic optimization 
lippmann moody touretzky editors advances neural information processing systems volume pages 
morgan kaufmann san mateo ca 

kung 
principal component neural networks 
wiley new york 

fletcher 
practical methods optimization chapter polynomial time algorithms pages 
john wiley sons new york second edition 

geman bienenstock doursat 
neural networks bias variance dilemma 
neural computation 

goldstein 
mean square optimality continuous time robbins monro procedure 
technical report dept mathematics university southern california la 

golub van loan 
matrix computations nd ed 
johns hopkins university press baltimore 

heskes kappen 
line learning processes artificial neural networks 
editor mathematical approaches neural networks volume pages 
elsevier amsterdam 

robert jacobs 
increased rates convergence learning rate adaptation 
neural networks 

kramer sangiovanni vincentelli 
efficient parallel learning algorithms neural networks 
touretzky editor advances neural information processing systems 
proceedings conference pages san mateo ca 
morgan kaufmann 

lecun 
de apprentissage connectionist learning models phd thesis universit curie paris vi 

lecun 
generalization network design strategies 
pfeifer fogelman steels editors connectionism perspective amsterdam 
elsevier 
proceedings international conference connectionism perspective university zurich 

october 

lecun boser denker henderson howard hubbard jackel 
handwritten digit recognition backpropagation network 
editor advances neural information processing systems vol 
san mateo ca 
morgan kaufman 

lecun denker solla 
optimal brain damage 
editor advances neural information processing systems vol 
pages 

lecun solla 
second order properties error surfaces 
advances neural information processing systems vol 
san mateo ca 
morgan kaufmann 

lecun simard pearlmutter 
automatic learning rate maximization line estimation hessian eigenvectors 
giles hanson cowan editors advances neural information processing systems vol 
san mateo ca 
morgan kaufmann 

mller 
scaled conjugate gradient algorithm fast supervised learning 
neural networks 

mller 
supervised learning large redundant training sets 
international journal neural systems 

moody darken 
fast learning networks locally tuned processing units 
neural computation 

murata 
japanese 
phd thesis university tokyo 

murata 
muller amari 
adaptive line learning changing environments 
michael mozer michael jordan thomas petsche editors advances neural information processing systems volume page 
mit press 

oppenheim schafer 
digital signal processing 
prentice hall englewood cliffs 

orr 
dynamics algorithms stochastic learning 
phd thesis oregon graduate institute 

orr 
removing noise line search adaptive batch sizes 
michael mozer michael jordan thomas petsche editors advances neural information processing systems volume page 
mit press 

orr 
regularization selection radial basis function centers 
neural computation 

pearlmutter 
fast exact multiplication hessian 
neural computation 

press flannery teukolsky vetterling 
numerical art scientific programming 
cambridge university press cambridge england 

saad editor 
online learning neural networks workshop newton institute 
newton institute series cambridge university press cambridge 

saad solla 
exact solution line learning multilayer neural networks 
physical review letters 

sompolinsky seung 
line learning dichotomies algorithms learning curves 

oh kwon cho editors neural networks statistical mechanics perspective pages 
singapore world scientific 

sutton 
adapting bias gradient descent incremental version delta 
william swartout editor proceedings th national conference artificial intelligence pages san jose ca july 
mit press 

van der 
minimisation methods training feed forward networks 
neural networks 

vapnik 
nature statistical learning theory 
springer verlag new york 

vapnik 
statistical learning theory wiley new york 

waibel hinton shikano lang 
phoneme recognition time delay neural networks 
ieee transactions acoustics speech signal processing assp 

heskes 
stochastic dynamics learning momentum neural networks 
journal physics 

yang amari 
efficiency robustness natural gradient descent learning rule 
michael jordan michael kearns sara solla editors advances neural information processing systems volume 
mit press 
