survey pomdp solution techniques kevin murphy september goals ai design agent interact environment maximize reward function 
control theory addresses problem uses slightly di erent language agent controller environment plant maximizing reward minimizing cost 
control theory mainly concerned tasks continuous spaces designing guided missile intercept airplane minimum expected time see sg ai mainly concerned tasks discrete spaces designing program play bridge maximize chance winning 
ai control theory common dw problems designing mobile robot perform household chores require techniques elds 
designing agents act uncertainty convenient model environment pomdp partially observable markov decision process pronounced pom dp 
discrete time step environment assumed state agent performs action control environment stochastically changes new state agent doesn see environment state receives observation stochastic function 
pomdp reduces mdp 
addition agent receives special observation signal called reward pomdp characterized state transition function jx observation function jx reward function jx 
goal agent learn policy maps observation history trajectory action maximize quality value 
case stochastic policy returns probability distribution actions 
value policy de ned ways 
way expected discounted nite sum rewards discount factor ensures sum nite 
expectation taken starting states stochastic transitions emissions environment stochastic decisions agent 
special case agent trying get speci absorbing goal state classical ai planning 
case give reward step goal state set cost policy expected time reach goal 
non periodic tasks expected reward time step better measure lim agent nite memory capacity compress unbounded history internal state illustrated 
agent learn policy maps internal agent states actions 
addition way updating internal state new observation share andrew moore view proper word agent words travel secret double assume actions take unit discrete time unspeci ed time scale 
allow actions take variable lengths time semi markov model see sps 
environment agent agent interacting environment 
arrives 
categorize agents controllers axes go details 
agent know environment model 
learn 
kind internal state agent maintain 
de nes reactive memoryless agent 
nite xed length window past observations 
sux tree de nes variable length markov model 
recurrent neural network 
jh belief state 
requires knowing environment model 
nite state machine 
policy represented learned 
learn reinforcement learning technique sb 
note may environment markov continuous valued may need function approximators represent pomdp known convert belief state mdp see section compute 
optimal approach model known computationally intractable 
consider approximating belief state 
pomdp known solve underlying completely observed mdp basis various heuristics see section 
ability compute parameters policy perform local search best controller see section 
learning agents reactive agents simplest agent maintain internal state purely reactive memoryless 
equivalently de ne environment jaj discrete actions jy discrete observations jaj jy reactive policies 
lit showed problem nding optimal deterministic reactive policy discrete pomdp general intractable certain cases solved branch bound search 
ls loc sarsa sb learn deterministic reactive policy function 
see pm analysis happens rl techniques applied non markovian models 
sjj showed deterministic reactive policy arbitrarily worse stochastic reactive policy jsj ws suggest gradient ascent method searching reactive stochastic policies discrete pomdps similar methods discussed section 
reactive policies plagued perceptual aliasing little sensory data problem states may appear fact di erent 
underlying states require di erent actions lead loss performance 
consider agents just current observation 
finite history windows straightforward observations input policy 
discrete observations histories arranged sux tree leaf action performed 
utile sux memory algorithm mcc uses sux tree variable depth leaves variable length markov model rst 
tree algorithm mcc slight extension observation treated vector di erent branches created depending value speci components history 
allows policy ignore irrelevant features input problem sensory data 
result tree structured function tree grown state leaf tree markov wrt reward 
combines hierarchical controller formulated ham par nearest sequence memory mcc approach 
nsm utile sux memory xed length history 
enables level hierarchy maintain perceptual history de ne state space 
recurrent neural nets problem nite history important observations past forgotten 
approach recurrent neural network maintain state lm 
belief state controllers environment model known optimal approach agent compute sucient statistic jh internal state called belief information state updated online bayes rule called state estimator se xjy yjx xja heuristics underlying mdp compute underlying mdp combine belief state various heuristic ways 
examples cas 
compute state arg max de ne mdp mls approximation de ne mdp mdp approximation 
entropy belief state threshold heuristics act reduce entropy dual mode control 
gib gin world best computer bridge program uses similar heuristic mdp belief state estimated sampling card distributions player consistent underlying mdp solved distribution double dummy program 
problem techniques assume uncertainty vanish underlying mdp fully observed 
control theory called certainty equivalence see sg example roughly mls heuristic applied missile guidance problem belief state approximated particle ltering 
techniques perform information gathering actions 
partially overcome doing deeper lookahead see section 
belief state mdp discrete pomdp induces mdp states belief states 
transition function mdp ja ja yja jb se yja xjx yja just reciprocal normalizing factor se equation 
reward function compute value function belief state mdp de ne controller arg max satis es jb yja se belief state mdp consider options compute exact value function exact belief state 
compute approximate value function exact belief state 
compute exact value function approximate belief state 
compute approximate value function approximate belief state 
approximations necessary computing intractable computing doubly exponential horizon time computing exponential number discrete state variables 
discuss combinations 
exact exact states belief state mdp continuous valued value function turns piecewise linear convex plc 
basis exact algorithms witness algorithm computing see cas review 
simple explanation algorithms see www cs brown edu research ai pomdp tutorial 
unfortunately exact computation intractable current computing power solve pomdps dozen states 
bp discuss way exploit structure pomdp form dbn compute exact tree structured value functions policies 
approximate exact attempts compute approximate value functions belief state mdp just treat regular continuous state mdp 
see hau review 
mention just 
pr proposed smooth di erentiable approximation plc value function called de ned vector size jbj 
free parameters number vectors vectors degree smoothing 
discretizing grid interpolation see bra 
exact approximate computing inecient 
possible approximations boyen koller bk algorithm bk requires model speci ed dbn particle ltering pf ddfg :10.1.1.119.6111
pomdp goal state estimation merely right side decision boundary policy 
pb suggest way modify sampling techniques take account speedup runtime performance 
approximate approximate bk algorithm approximate belief state tracking dbn feed marginals various approximators including neural network mixed results 
thr uses pf approximate belief state tracking continuous state model uses nearest neighbor function approximator distance metric kl distance clouds points smoothed gaussian kernel 
horizon control kmn propose building lookahead tree depth horizon control 
prove node sucient sample number states independent complexity underlying mdp 
extended belief state mdps sampling observations states 
complexity jsj 
number samples jaj number actions jsj size state space 
jsj reduced pomdp factored dbn approximate belief state updating bk algorithm bk error bounds combination approximations derived ms :10.1.1.119.6111
unfortunately method relies heavily discounting keep lookahead depth tractable 
lookahead controller just described learn policy ective time 
furthermore estimates values leaves due discounting 
alternative admissible heuristic lower bound value leaves mdp update values intermediate nodes real time dynamic programming bbs 
approach taken gb discretizes represented table 
unfortunately horizon control works discrete discretized actions special cases linear quadratic gaussian 
direct policy search widely approach solving mdps indirect computing possibly rst solving case known model greedy policy arg max 
problems applied pomdps bellman equation hold pomdp markov wrt agent state space may possible nd stable stochastic policies better deterministic ones pomdps 
learning fail converge stable policy presence function approximation mdps 
may hard compute max continuous actions 
direct policy search su er problems 
finite state controllers discrete pomdps optimal controller nite number states 
way compute fsm solve belief state mdp convert resulting policy graph fsm 
direct way search directly space fsms 
pomdp unknown discusses gradient ascent 
discusses slight variation state controller represented factored form terms memory bits individually addressed 
pomdp known discusses branch bound gradient ascent techniques han discusses policy iteration assumes ability compute belief states 
neural network controllers popular formulation stochastic reactive policy discrete actions uses softmax function follows feature vector state action weights policy 
policy learned gradient ascent 
approach generalizes multi layer perceptrons 
estimating model kmn suggest generative model pomdp build set trajectory trees size evaluate log rmax user speci able bound error estimate 
main result trees need built evaluate vc dimension policy class sample complexity independent underlying pomdp 
nj strengthens result building trees size linear allowing continuous actions 
way compute enumerate policies discrete state action spaces nelder mead simplex algorithm nd policy 
estimating model controller di erentiable function try gradient ascent learn 
key problem reliably estimating gradient main approaches online ine 
online scenario pomdp unknown agent estimates interacting world single sample path 
rst methods called reinforce wil 
main problem approach high variance estimate 
overcome cost slight bias introducing pseudo discount factor bb mar 
alternative way reduce variance actor critic algorithms sb kk 
authors shown mdp stationary distribution markov chain induced 
agent learn value function policy simultaneously long learns value function faster critic td squares temporal di erence learning boy actor gradient ascent 
importantly kt independently proved equation holds represented function approximator provided linear features policy 
gradient ascent procedure achieve local optimum presence function approximation contrast pure critic methods 
vaps algorithm bm similar 
estimating model ine learning requires access generative model sophisticated techniques conjugate gradient ascent 
completely di erent technique 
note avg 
approximate inference compute bk pf take derivatives 
bk algorithm analytically pf importance sampling 
evolutionary techniques obviously evolutionary algorithms search space policies see msg details 
environment model easiest case agent knows underlying state space environment form functions jx jx jx 
functions usually assumed simple parametric forms 
control theory transition observation functions assumed linear gaussian cost function quadratic 
ai transition observation functions assumed discrete multinomial distributions reward function function discrete state action 
discrete state models may represented factored form dbn bdh 
case parameters assumed known 
slightly realistic case parameters unknown 
example known state space consists positions velocities plane missile objects subject newton laws certain parameters mass type plane noise variances unknown 
furthermore parameters may change time 
kind problem studied adaptive control theory 
hardest case agent knows environment 
agent try learn model controversial topic ai 
control theory widely accepted learning model environment useful called system identi cation 
model assumed linear established techniques people typically neural networks 
ai literature learning discrete state models environment 
utile distinction memory algorithm mcc combines em algorithm state splitting approach learn hmm 
sal learns sigmoidal fully interconnected dbn dynamic bayesian network mean eld inference online gradient ascent 
applies discrete driving task nds advantages compared learning hmm 
authors learn models maximize likelihood observations necessarily related performance mccallum state splitting criterion utile splits state markov respect reward 
prepared stronger assumptions kind environment agent embedded specialized representations get better performance 
example mobile robot dimensional space real simulated bene ability represent map world 
robot designed deliver mail bene ability represent appearance typical locations people 

bb baxter bartlett 
direct gradient reinforcement learning gradient estimation algorithms 
technical report dept comp 
sci australian natl 
univ 
bbs barto bradtke singh 
learning act real time programming 
arti cial intelligence journal 
bdh boutilier dean hanks 
decision theoretic planning structural assumptions computational leverage 
ai research 
bk boyen koller :10.1.1.119.6111
tractable inference complex stochastic processes 
uai 
bm baird moore 
gradient descent general reinforcement learning 
nips 
boy boyan 
squares temporal di erence learning 
intl 
conf 
machine learning 
bp boutilier poole 
computing optimal policies partially observable decision processes compact representations 
aaai 
bra brafman 
heuristic variable grid solution method pomdps 
aaai 
baxter weaver bartlett 
direct gradient reinforcement learning ii 
gradient ascent algorithms experiments 
technical report dept comp 
sci australian natl 
univ 
cas cassandra 
exact approximate algorithms partially observable markov decision processes 
phd thesis brown 
ddfg doucet de freitas gordon 
sequential monte carlo methods practice 
springer verlag 
forthcoming 
dw thomas dean michael wellman 
planning control 
morgan kaufmann 
gb ge ner bonet 
solving large pomdps real time dynamic programming 
fall aaai symp 
pomdps 
gin ginsberg 
gib steps expert level bridge playing program 
ijcai 
han hansen 
solving pomdps searching policy space 
uai 
hau hauskrecht 
value function approximations partially observable markov decision processes 
ai research 
hernandez mahadevan 
hierarchical memory reinforcement learning 
nips 
jsj jaakkola singh jordan 
reinforcement learning algorithm partially observable markov decision problems 
nips 
kk kimura kobayashi 
analysis actor critic algorithms eligibility traces reinforcement learning imperfect value functions 
intl 
conf 
machine learning 
kaelbling littman cassandra 
planning acting partially observable stochastic domains 
arti cial intelligence 
kmn kearns mansour ng 
approximate planning large pomdps reusable trajectories 
nips 
kmn kearns mansour ng 
sparse sampling algorithm near optimal planning large markov decision processes 
ijcai 
kt 
actor critic algorithms 
nips 
lit littman 
memoryless policies theoretical limitations practical results 
proc 
conf 
simulation adaptive behavior 
lm 
lin mitchell 
memory approaches learning non markovian domains 
technical report cmu cs cmu 
loc 
ect eligibility traces nding optimal memoryless policies partially observable markov decision processes 
nips 
ls singh 
eligibility traces nd best memoryless policy partially observable markov decision processes 
intl 
conf 
machine learning 
mar 
simulation optimization markov decision processes 
phd thesis mit lids 
mcc mccallum 
reinforcement learning selective perception hidden state 
phd thesis univ rochester 
meuleau 
kim kaelbling cassandra 
solving pomdps searching space nite policies 
uai 
meuleau peshkin 
kim kaelbling 
learning nite state partially observable environments 
uai 
ms mcallester singh 
approximate planning factored pomdps belief state simpli cation 
uai 
msg moriarty schultz grefenstette 
evolutionary algorithms reinforcement learning 
ai research 
nj ng jordan 
pegasus policy search method large mdps pomdps 
uai 
ng parr koller 
policy search density estimation 
nips 
par parr 
hierarchical control learning markov decision processes 
phd thesis berkeley 
pb poupart boutilier 
value directed belief state approximation pomdps 
uai 
pm 
analysis direct reinforcement learning non markovian domains 
intl 
conf 
machine learning 
peshkin meuleau kaelbling 
learning policies external memory 
intl 
conf 
machine learning 
pr parr russell 
approximating optimal policies partially observable stochastic domains 
ijcai 
rodriguez parr koller 
reinforcement learning approximate belief states 
nips 
rst dana ron yoram singer naftali tishby 
power amnesia learning probabilistic automata variable memory length 
machine learning 
sal sallans 
learning factored representations partially observable markov decision processes 
nips 
sb sutton barto 
learning 
mit press 
sg salmond gordon 
particles mixtures tracking guidance 
doucet de freitas gordon editors sequential monte carlo methods practice 
springer verlag 
sjj singh jaakkola jordan 
learning state estimation partially observable markov decision processes 
intl 
conf 
machine learning 
sutton mcallester singh mansour 
policy gradient methods reinforcement learning function approximation 
nips 
sps sutton precup singh 
mdps semi mdps framework temporal abstraction reinforcement learning 
arti cial intelligence 
thr thrun 
monte carlo pomdps 
nips 
wil williams 
simple statistical gradient algorithms connectionist reinforcement learning 
machine learning 
ws williams singh 
experimental results learning stochastic memoryless policies partially observable markov decision processes 
nips 

