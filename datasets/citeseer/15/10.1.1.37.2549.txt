fast parallel sparse matrix vector multiplication roman stefan institute scientific computing eth rich integrated systems laboratory eth rich sparse matrix vector product important computational kernel runs computers super scalar risc processors 
analyse performance sparse matrix vector product symmetric matrices originating fem describe techniques lead fast implementation 
shown optimisations incorporated efficient parallel implementation messagepassing 
conduct numerical experiments different machines show optimisations speed sparse matrix vector multiplication substantially 
key words sparse matrices matrix vector multiplication source code optimisation parallel linear algebra performance analysis sparse matrix vector product focus large symmetric sparse matrices fit memory cache 
matrices stored symmetric sparse skyline format sss ideas applied general sparse matrices stored formats 
sss format extension commonly compressed sparse row format csr 
matrices sss format strictly lower triangle stored csr format arrays ia ja va 
double precision array va length nnz contains non zero values strictly lower triangle stored row row 
email inf ethz ch email iis ee ethz ch preprint submitted parallel computing june integer array ja length nnz contains column indices nonzero elements stored array va integer array ia length contains pointers row arrays va ja 
diagonal stored separately double precision array da length order matrix nnz number non zero values strictly lower triangle 
algorithm shows matrix vector multiplication code ax matrix stored sss format 
accesses matrix data structure stride loop access pattern irregular depends sparsity structure algorithm loop rows lower triangle xi load ia ia loop nonzero elems row ja load column index va load matrix element xi da xi compute upper bound performance sparse matrix vector product architecture profound knowledge design processor memory subsystem required 
shown optimal cache performance daxpy routine computed hp pa architecture 
computation needs know cache line size cache penalty outstanding memory requests processor supports 
compute optimal cache performance sparse matrix vector product manner difficult performance depends sparsity pattern matrix 
straight forward approach portable gives comparable results 
compute ratio number floating point operations number bytes memory traffic 
best case scenario read memory kept cache get flops byte test matrices described tab 

approximation assume large write back cache double precision arithmetic 
multiply memory bandwidth system get upper bound performance sparse matrix vector product 
determine memory bandwidth benchmarking highly optimised computational kernels tuned hardware 
benchmark vendor measured max 
measured system processor bandwidth perf 
perf 
mbytes mflops mflops intel linux pc mhz pentium iii sun enterprise mhz ultra sparc dec workstation mhz alpha hp class mhz pa hp class mhz pa ibm sp mhz power sc intel paragon mhz xp table machines numerical experiments measured memory bandwidth predicted maximal performance ropt measured performance alg 

ropt 
numbers column measured matrix cav see tab 

intel paragon eth rich discontinued time conducted experiments don results available machine 
supplied blas daxpy routines similar ratio read write operations sparse matrix vector product 
gives realistic results peak memory bandwidth reported vendor 
tab 
observe measured performance sparse multiplication code far optimal performance limited memory bandwidth 
compiler unable generate efficient code mainly data dependencies irregular loops 
additional cache misses generated accesses degrade performance 
hand upper bound computed ibm sp unrealistic memory cache small kbytes keep vectors cache 
numerical experiments published standard vendor supplied compilers safe optimisations turned 
intel linux pc gnu compiler benchmarks 
design fast sparse matrix vector product processor applied techniques improve implementation alg 

software pipelining software pipelining source code way processor pipelines better filled able load data registers earlier data prefetching reduce data dependencies innermost loop iteration 
increases instruction level parallelism 
compilers unable perform optimisations satisfactory way 
compilers information necessary move load instructions safely 
due lack type information generate code conservatively 
compilers trouble unrolling complex loops revisiting alg 
vendor supplied compilers able unroll inner loop alg 
optimized version alg 

data loaded memory loop iteration needed processor better overlap computation memory transfers 
algorithm initialization mult diagonal element xi di da ia yi di xi iteration prefetch data ja va yj prefetch iteration ja va calc prefetched data yj xi prefetch iteration yj rename prefetched data iteration prefetch yj xi yi register blocking reduce number memory accesses register blocking splitting matrix sum matrices 
am consisting small dense blocks fixed size :10.1.1.31.7599
multiplying matrix consisting small dense blocks code load fewer indices needed block 
multiplying dense block elements loaded reused times 
approach store matrices contains small dense blocks equal size contains remaining non zero elements 
approach authors store matrix small dense blocks expense having store zero entries explicitly 
store matrix ai small dense blocks data structure original matrix sss format exception store block coordinate pair just value 
build data structure linear time greedy algorithm scans matrix row row 
register blocking implemented ways multiplying matrix matrix multiply matrix ai vector sum results 
multiplying row row multiply ai time row row 
variant optionally store nonzero elements sequence accessed store dense blocks different size row storing matrices ai separately 
implemented variants 
superior 
optimal routine chosen depending matrix machine 
matrix reordering cuthill mckee reordering matrix reduce bandwidth 
smaller bandwidth matrix vector multiplication vector elements accessed particular matrix row accessed row 
matrix reordering reduce cache misses accesses generate importantly lowers number messages sent parallel implementation see section :10.1.1.31.7599
name cav cav size matrix nonzeros nonzeros row storage mbytes mbytes properties symmetric symmetric table matrices numerical experiments 
matrices originate fem code solves maxwell equations 
amount dense blocks contained matrices listed fig 

mflops sec 
performance software pipelined code matrix cav straight forward implementation software pipelined code linux pc sun dec alpha hp class hp class ibm sp mflops sec 
performance software pipelined code matrix cav straight forward implementation software pipelined code linux pc sun dec alpha hp class hp class ibm sp fig 

performance software pipelined code straight forward code matrix cav matrix cav 
experiments carried single processor 
numerical experiments numerical experiments matrices listed tab 

matrices originate fem code design particle accelerator cavities essentially solves maxwell equations 
matrices large amount small dense blocks node variables located grid point fem mesh 
exact amount dense blocks listed fig 

experiments carried different machines listed tab 

benchmark optimisations separately figs 
measure best performance applying optimisations fig 

fig 
shows performance software pipelined code comparison original code alg 

benefit substantial platforms 
improvement ranges intel linux pc ibm sp matrix cav 
mflops sec 
performance register blocked codes matrix cav linux pc sun dec alpha hp class hp class ibm sp mflops sec 
performance register blocked codes matrix cav linux pc sun dec alpha hp class hp class ibm sp fig 

performance codes register blocking 
fig 
shows results matrix cav fig 
shows results matrix cav 
darker bars bottom show performance code including blocked portion unblocked portion 
bars including top parts light gray represent performance code portion multiplies blocked part matrix 
number brackets percentage non zero elements stored dense blocks size 
experiments carried single processor 
mflops sec 
performance codes reordered matrices matrix cav original cuthill mckee reverse cuthill mckee random linux pc sun dec alpha hp class hp class ibm sp mflops sec 
performance codes reordered matrices matrix cav original cuthill mckee reverse cuthill mckee random linux pc sun dec alpha hp class hp class ibm sp fig 

performance codes working matrices different orderings 
fig 
shows results matrix cav fig 
shows results matrix cav 
experiments carried single processor 
fig 
shows impact block size performance register blocked code 
matrix cav maximal improvement intel linux pc 
platforms improvement lies 
seen lighter colored bars fig 
performance code substantially higher matrices consisting solely small dense blocks 
fig 
shows performance code multiplying matrices different orderings 
compared original ordering performance increased substantially cuthill mckee type reorderings 
experiments random ordering indicate performance depends heavily matrix ordering 
cases original ordering suited matrix vector multiplication case improvement cuthill mckee mflops sec 
straight forward implementation fastest code platform performance best code matrix cav linux pc sun dec alpha hp class hp class ibm sp mflops sec 
straight forward implementation fastest code platform performance best code matrix cav linux pc sun dec alpha hp class hp class ibm sp fig 

performance best code code 
fig 
shows results matrix cav fig 
shows results matrix cav 
architecture code performs best compare code 
labels top bars show block sizes yield best performance register blocking 
means matrix splitted matrices containing blocks containing remaining blocks third containing remaining elements 
type reorderings substantial :10.1.1.31.7599
platform choose fastest code takes discussed optimisations account compare corresponding version 
results shown fig 

sp achieve improvement intel linux pc get improvement hp class sun enterprise server hp class dec alpha workstation get performance increases 
parallelisation message passing parallel implementation parallel implementation distribute lower triangular part matrix block rows see fig 

balance load assign number nonzeros processor 
distribution vectors corresponds distribution matrix rows 
preprocessing step processor collects necessary information actual matrix vector multiplication 
done way storage format matrix implies processor needs elements local parts vector processor purpose smallest block containing needed elements determined 
fig 

data distribution parallel implementation 
shows matrix distributed processors non banded banded matrix 
matrix symmetric lower triangle stored 
vectors depicted right show local parts vector parts vector known processor multiplication local part matrix 
informations exchanged 
actual matrix vector multiplication processor receives portion vector processor sends processor vector 
due symmetry matrix 
consequence information exchanged vector preprocessing step 
actual parallel matrix vector code implemented slightly different routines latency hiding exchange parts vector multiply local part matrix exchange parts vector form resulting vector 
latency hiding exchange parts vector time multiply local block column upper triangle 
send vector processors 
arrival remote parts vector local lower triangle multiplied 
arrival vectors form resulting vector 
latency hiding exchange parts vector time multiply local diagonal block matrix 
arrival remote parts vector multiply remaining local part matrix exchange parts vector form resulting vector 
routine reasonable choice machines support latency hiding 
routine disadvantage exploit symmetry matrix matrix read memory twice 
routine disadvantage diagonal block matrix stored separately implementation efficient 
done preprocessing step 
parallel algorithm benefits matrix reordering done optimisation serial code 
seen fig 
number messages reduced smaller matrix bandwidth 
fig 
shows worst case speedup routine routine routine speedups matrix cav hp class number processors speedup routine routine routine speedups matrix cav hp class number processors fig 

speedups parallel matrix vector multiplication code hp class speedups reported matrices cav cav cuthill mckee ordering 
processor needs local vector parts processors multiplication 
fig 
processor needs local parts neighbour processor 
results fig 
show crucial matrix reordering parallel numerical experiments carried parallel experiments platforms hp class hp class intel paragon intel pentium iii beowulf cluster ibm sp 
hp class hp exemplar spp hp class hp exemplar shared memory machines processors interconnection network 
intel paragon processors distributed memory arranged grid 
intel beowulf cluster consists dual cpu pentium iii processors 
computing nodes grouped frames nodes 
ethernet network connects computing nodes 
ibm sp distributed memory machine processors connected multistage network 
software pipelining optimisation described section incorporated parallel version 
entirely possible implement register blocking parallel version mainly limited time available 
mentioned matrices reordered cuthill mckee algorithm 
fig 
shows measured speedups hp class 
smaller matrix cav get super linear speedup due cache effects 
results hp class shown fig 

platform get higher super speedup routine routine routine speedups matrix cav hp class number processors speedup routine routine routine speedups matrix cav hp class number processors fig 

speedups parallel matrix vector multiplication code hp class speedups reported matrices cav cav cuthill mckee ordering 
note scaling diagrams high speedups reached cav 
speedup routine routine routine speedups matrix cav intel paragon number processors speedup routine routine routine speedups matrix cav intel paragon number processors fig 

speedups parallel matrix vector multiplication code intel paragon 
matrices cav cav reordered cuthill mckee algorithm 
linear speedup matrix cav 
matrix speedup processors 
speedups matrix cav higher compared hp class 
speedup processors 
fig 
shows measured speedups intel paragon 
code scales especially matrix cav 
apart super linear speedups hp class hp class machine gives best speedups fast network compared performance processors 
results intel beowulf cluster depicted fig 

numerical experiments cpu node 
cpu measured speedups matrix cav comparable speedups hp class 
irregularities processors show case processors communicate processors located frame 
fig 
shows speedup speedups matrix cav intel beowulf cluster routine routine routine number processors speedup speedups matrix cav intel beowulf cluster routine routine routine number processors fig 

speedups parallel matrix vector multiplication code intel beowulf cluster speedups reported matrices cav cav cuthill mckee ordering 
speedup routine routine routine speedups matrix cav ibm sp number processors speedup routine routine routine speedups matrix cav ibm sp number processors fig 

speedups parallel matrix vector multiplication code ibm sp 
speedups reported matrices cav cav cuthill mckee ordering 
measured speedups ibm sp 
code scale intel paragon sp faster processors slower interconnection network 
fig 
shows influence reordering performance 
matrices left original ordering performance unacceptably low 
small number processors number messages low performance worse 
results conducted ibm sp behavior observed platforms 
speedup speedups matrix cav ibm sp routine cuthill mckee routine original routine original routine original number processors speedup speedups matrix cav ibm sp routine cuthill mckee routine original routine original routine original number processors fig 

speedups parallel matrix vector multiplication code ibm sp matrix reordering 
speedups reported matrices cav cav original ordering cuthill mckee ordering 
reordered matrices routine shown gives best results case 
computed upper bound performance sparse matrix vector product showed straight forward implementations perform poorly 
techniques improved performance 
message passing implementation benefits optimisations scales reasonably 
think go automatic generation sparse matrix vector multiplication codes optimised matrix target architecture 
approach successfully applied applications fft dense blas 
acknowledgments peter oscar improvements corrections draft 
rolf explanatory discussions subject software pipelining 
saad basic tool kit sparse matrix computations tech 
rep research institute advanced computer science nasa ames research center field ca 
advanced optimization pa processors presentation hiper conference zurich 
toledo improving memory system performance sparse matrix vector multiplication proceedings eighth siam conference parallel processing scientific computing siam :10.1.1.31.7599
im yelick optimizing sparse matrix vector multiplication smps ninth siam conference parallel processing scientific computing siam march 
george liu computer solution large sparse positive definite systems prentice hall englewood cliffs nj 
comparison solvers large eigenvalue problems occuring design resonant cavities numerical linear algebra applications 
saad portable library distributed memory sparse iterative solvers tech 
rep msi 
johnson fftw adaptive software architecture fft icassp 
whaley dongarra automatically tuned linear algebra software atlas sc proceedings 

