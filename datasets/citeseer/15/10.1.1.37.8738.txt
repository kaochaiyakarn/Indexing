journal artificial intelligence research submitted published model inductive bias learning jonathan baxter jonathan baxter anu edu au research school information sciences engineering australian national university canberra australia major problem machine learning inductive bias choose learner hypothesis space large contain solution problem learnt small ensure reliable generalization reasonably sized training sets 
typically bias supplied hand skill insights experts 
model automatically learning bias investigated 
central assumption model learner embedded environment related learning tasks 
environment learner sample multiple tasks search hypothesis space contains solutions problems environment 
certain restrictions set hypothesis spaces available learner show hypothesis space performs sufficiently large number training tasks perform learning novel tasks environment 
explicit bounds derived demonstrating learning multiple tasks environment related tasks potentially give better generalization learning single task 

hardest problem machine learning task initial choice hypothesis space large contain solution problem hand small ensure generalization small number examples mitchell 
suitable bias actual learning task straightforward 
existing methods bias generally require input human expert form heuristics domain knowledge example selection appropriate set features 
despite successes methods clearly limited accuracy reliability expert knowledge extent knowledge transferred learner 
natural search methods automatically learning bias 
introduce analyze formal model bias learning builds pac model machine learning variants vapnik valiant blumer ehrenfeucht haussler warmuth haussler 
models typically take general form learner supplied hypothesis space training data drawn independently underlying distribution information contained learner goal select hypothesis minimizing measure expected loss respect example case squared loss 
models learner bias represented choice contain solution problem regardless data learner receives learn 
course best way bias learner supply containing just single optimal hypothesis 
finding hypothesis precisely original learning problem ai access foundation morgan kaufmann publishers 
rights reserved 
baxter pac model distinction bias learning ordinary learning 
put differently pac model model process inductive bias simply takes hypothesis space proceeds 
overcome problem assume faced just single learning task learner embedded environment related learning tasks 
learner supplied family hypothesis spaces goal find bias hypothesis space appropriate entire environment 
simple example problem handwritten character recognition 
preprocessing stage identifies removes small rotations dilations translations image character advantageous recognizing characters 
set individual character recognition problems viewed environment learning problems set problems form distinguish characters distinguish characters preprocessor represents bias appropriate problems environment 
currently unknown biases appropriate environment 
able learn automatically 
examples learning problems viewed belonging environments related problems 
example individual face recognition problem belongs essentially infinite set related learning problems individual face recognition problems set individual spoken word recognition problems forms large environment set fingerprint recognition problems printed chinese japanese character recognition problems stock price prediction problems 
medical diagnostic prognostic problems multitude diseases predicted pathology tests constitute environment related learning problems 
cases environments normally modeled treated single multiple category learning problems 
example recognizing group faces normally viewed single learning problem multiple class labels face group multiple individual learning problems 
reliable classifier individual face group constructed easily combined produce classifier group 
furthermore viewing faces environment related learning problems results show bias learnt learning novel faces claim traditional approach 
point goes heart model concerned adjusting learner bias performs better fixed set learning problems 
process fact just ordinary learning richer hypothesis space components labelled bias able varied 
suppose learner faced potentially infinite stream tasks adjusting bias subset tasks improves learning performance unseen tasks 
bias appropriate problems environment learnt sampling tasks 
single task learnt bias extracted specific task 
rest general theory bias learning developed idea learning multiple related tasks 
loosely speaking formal results stated section main theory learning multiple related tasks reduces sampling burden required generalization number examples required task basis 
model inductive bias learning bias learnt sufficiently training tasks learning novel tasks drawn environment 
second point shows form meta generalization possible bias learning 
ordinarily say learner generalizes seeing sufficiently training examples produces hypothesis high probability perform examples task 
bias learner generalizes seeing sufficiently training tasks produces hypothesis space high probability contains solutions novel tasks 
term process learning learn thrun pratt 
main theorems stated agnostic setting necessarily contain hypothesis space solutions problems environment give improved bounds realizable case 
sample complexity bounds appearing results stated terms combinatorial parameters related complexity set hypothesis spaces available bias learner 
boolean learning problems pattern classification parameters bias learning analogue vapnik chervonenkis dimension vapnik blumer 
application general theory problem learning appropriate set features environment related tasks formulated bias learning problem 
case continuous neural network features able prove upper bounds number training tasks number examples training task required ensure set features works training tasks high probability novel tasks drawn environment 
upper bound number tasks scales measure complexity possible feature sets available learner upper bound number examples task scales number examples required learn task true set features correct bias known number tasks 
case see number related tasks learnt increases number examples required task generalization decays minimum possible 
boolean neural network feature maps able show matching lower bound number examples required task form 
related large body previous algorithmic experimental machine learning statistics literature addressing problems inductive bias learning improving generalization multiple task learning 
approaches seen special cases closely aligned model described orthogonal 
completely exhaustive section overview main contributions 
see thrun pratt chapter comprehensive treatment 
hierarchical bayes 
earliest approaches bias learning come hierarchical bayesian methods statistics berger gelman carlin stern 
contrast bayesian methodology takes essentially empirical process approach modeling problem bias learning 
model mixture hierarchical bayesian information theoretic ideas baxter similar 
empirical study showing utility hierarchical bayes approach domain containing large number related tasks heskes 
baxter early machine learning 
rendell variable bias management system introduced mechanism selecting different learning algorithms tackling new learning problem 
shift better bias utgoff early scheme adjusting bias primarily focussed searching bias applicable large problem domains 
environment related tasks may interpreted environment analogous tasks sense task arrived analogy sufficiently tasks 
early discussion analogy context see russell particular observation analogous problems sampling burden task reduced 
metric approaches 
metric nearest neighbour classification vector quantization determine nearest code book vector represents form inductive bias 
model extra assumptions tasks environment specifically marginal input space distributions identical differ conditional probabilities assign class labels shown optimal metric distance measure vector quantization neighbour classification baxter baxter bartlett 
metric learnt sampling subset tasks environment distance measure learning novel tasks drawn environment 
bounds number tasks examples task required ensure performance novel tasks baxter bartlett experiment metric successfully trained examples subset japanese characters fixed distance measure learning unseen characters 
similar approach described thrun mitchell thrun neural network output trained match labels novel task simultaneously forced match gradient derivative information generated distance metric trained previous related tasks 
performance novel tasks improved substantially derivative information 
note adaptive metric techniques machine learning focus exclusively adjusting metric fixed set problems learning metric suitable learning novel related tasks bias learning 
feature learning learning internal representations 
adaptive metric techniques approaches feature learning focus adapting features fixed task learning features novel tasks 
cases features learnt subset tasks explicit aim novel tasks intrator edelman low dimensional representation learnt set multiple related image recognition tasks successfully learn novel tasks kind 
experiments reported baxter chapter baxter baxter bartlett nature 
bias learning inductive logic programming ilp 
predicate invention refers pro cess ilp new predicates thought useful classification task hand added learner domain knowledge 
new predicates background domain knowledge learning novel tasks predicate invention may viewed form model inductive bias learning inductive bias learning 
preliminary results approach chess domain reported khan muggleton 
improving performance fixed task 
multi task learning caruana trains extra neural network outputs match related tasks order improve generalization performance fixed task 
approach explicitly identify extra bias generated related tasks way learn novel tasks example exploiting bias provided set related tasks improve generalization performance 
similar approaches include suddarth suddarth holden abu mostafa 
bias computational complexity 
consider inductive bias sample complexity perspective learnt bias decrease number examples required novel tasks generalization 
natural alternative line enquiry computational complexity learning algorithm may improved training related tasks 
early algorithms neural networks vein contained sharkey sharkey pratt 
reinforcement learning 
control tasks appropriately viewed elements sets related tasks learning navigate different goal states learning set complex motor control tasks 
number papers reinforcement learning literature proposed algorithms sharing information related tasks improve average generalization performance tasks singh ring learning bias set tasks improve performance tasks sutton thrun schwartz 
overview section bias learning model formally defined main sample complexity results showing utility learning multiple related tasks feasibility bias learning 
results show sample complexity controlled size certain covering numbers associated set hypothesis spaces available bias learner way sample complexity learning boolean functions controlled vapnik chervonenkis dimension vapnik blumer 
results section upper bounds sample complexity required generalization learning multiple tasks learning inductive bias 
general results section specialized case feature learning neural networks section algorithm training features gradient descent 
special case able show matching lower bounds sample complexity multiple task learning 
section concluding remarks directions research 
proofs quite lengthy moved appendices interrupt flow main text 
tables contain glossary mathematical symbols 
baxter symbol description referenced input space output space distribution learning task loss function hypothesis space hypothesis learning algorithm error hypothesis training set distribution set learning tasks distribution learning tasks family hypothesis spaces environment empirical error training set bias learning algorithm function induced set average set set function probability distributions set pseudo metric loss hypothesis space sample empirical loss pseudo metric covering number capacity covering number capacity sequence hypotheses sequence distributions average loss average loss set feature maps output class composed feature maps hypothesis space associated loss function class associated covering number capacity pseudo metric feature maps covering number model inductive bias learning symbol description referenced covering number capacity neural network hypothesis space restricted vector growth function vapnik chervonenkis dimension restricted matrix restricted matrix growth function dimension function upper dimension function lower dimension function metric optimal performance average set permutations integer pairs permuted empirical metric functions optimal average error 
bias learning model section bias learning model formally introduced 
motivate definitions describe main features ordinary single task supervised learning models 
single task learning computational learning theory models supervised learning usually include ingredients input space output space probability distribution loss function hypothesis space set hypotheses functions example problem learn recognize images mary face neural network set images typically represented subset component pixel intensity set distribution peaked images different faces correct class labels 
learner hypothesis space class neural networks mapping input space loss case discrete loss 
baxter loss function allows unified treatment pattern recognition real valued function learning regression usually goal learner select hypothesis minimum expected loss minimizing course learner know search practice learner samples repeatedly generate training set distribution information contained learner produces hypothesis general learner simply map set training samples hypothesis space stochastic learner treated assuming distribution valued algorithms seek minimize empirical loss defined course intelligent things data simply minimizing empirical error example add regularisation terms avoid fitting 
learner chooses hypothesis uniform bound probability large deviation bound learner gen error function empirical loss training set bound holds depends richness loss convergence controlled vc dimension conditions ensuring convergence understood boolean function learning discrete theorem 
probability distribution suppose generated sampling times probability choice training set satisfy proofs result may vapnik blumer 
reproduced 

vc dimension class boolean functions largest integer exists subset restriction contains boolean functions model inductive bias learning theorem provides conditions deviation small guarantee true error small 
governed choice contains solution small error learner minimizes error training set high probability small 
bad choice mean hope achieving small error 
bias learner model represented choice hypothesis space bias learning model main extra assumption bias learning model introduced learner embedded environment related tasks sample environment generate multiple training sets belonging multiple different tasks 
model ordinary single task learning learning task represented distribution bias learning model environment learning problems represented pair set probability distributions distribution set possible learning problems controls learning problems learner see example learner face recognition environment highly peaked face recognition type problems learner character recognition environment peaked character recognition type problems view environments sets individual classification problems single multiple class classification problems 
recall paragraph previous section learner bias represented choice hypothesis space enable learner learn bias supply family set hypothesis spaces putting formally learning learn bias learning problem consists input space output space separable metric spaces loss function distribution environment hypothesis space family set probability distributions set functions assume loss function range equivalently rescaling assume bounded 

bias governed learner uses hypothesis space 
example circumstances learner may choose full power neural network example early stopping 
simplicity away features algorithm assume uses entire hypothesis space 
domain algebra subsets suitable purposes borel algebra generated topology weak convergence assume separable metric spaces separable metric space metric topology weak convergence parthasarathy problem existence measures see appendix discussion particularly proof part lemma 
baxter define goal bias learner find hypothesis space loss minimizing way small high probability contains solution problem drawn random sense measures appropriate bias embodied environment able find minimizing general learner know directly 
learner sample environment way sample times yield sample times yield resulting training sets henceforth called sample generated process supplied learner 
sequel sample denoted written matrix 
sample simply training sets sampled different learning tasks task selected environmental probability distribution size training set kept primarily facilitate analysis 
information contained learner choose hypothesis space way learner find minimizing empirical loss defined 
note simply average best possible empirical error achievable training set function biased estimate unbiased estimate require choosing minimal average error distributions defined ordinary learning intelligent things training data learner map output minimizing 
denoting set samples general bias takes samples input produces hypothesis spaces model inductive bias learning stated learners 
deterministic bias learner trivial extend results stochastic note concerned sample complexity properties bias learner discuss issues computability searching entire hypothesis spaces family hypothesis spaces extra representational question model bias learning ordinary learning family represented searched defer discussion section main sample complexity results model bias learning introduced 
specific case learning set features suitable environment related learning problems see section 
regardless learner chooses hypothesis space uniform bound probability large deviation compute upper bound bound bias learner generalization error view question generalization bias learning model tasks examples task required ensure close high probability uniformly informally tasks examples task required ensure hypothesis space solutions training tasks contain solutions novel tasks drawn environment 
turns kind uniform convergence bias learning controlled size certain function classes derived hypothesis space family way vc dimension hypothesis space controls uniform convergence case boolean function learning theorem 
size measures auxiliary definitions needed state main theorem introduced subsection 
covering numbers definition 
hypothesis hypothesis space define hypothesis space family define sequence hypotheses define denote define hypothesis space family define baxter part definition hypotheses turned functions mapping composition loss function 
just collection functions original hypotheses come called loss function class 
case interested average loss tasks hypotheses chosen fixed hypothesis space motivates definition collection restriction belong single hypothesis space definition 
define hypothesis space family define size close uniformly size defined terms certain covering numbers need define measure distance elements elements definition 
define controls large sample ensure sequence probability distributions similarly distribution define easily verified pseudo metrics respectively 
definition 
cover set contained note require just measurable functions denote size smallest cover 
define capacity supremum probability measures way place define capacity defined similar supremum sequences probability measures 
pseudo metric metric condition uniform convergence bias learners model inductive bias learning machinery state main theorem 
theorem hypothesis space family required permissible 
discussed detail appendix note weak measure theoretic condition satisfied real world hypothesis space families 
logarithms base theorem 
suppose separable metric spaces probability distribution set distributions suppose sample generated sampling times give sampling times permissible generate hypothesis space family 
number tasks satisfies number examples task satisfies probability sample proof 
see appendix important points note theorem satisfy 
provided capacities finite theorem shows bias learner selects hypothesis spaces bound generalisation error sufficiently large samples bias learner find terms exact value involves finding smallest error hypothesis training sets upper bound example gradient descent error function give upper bound see section brief discussion achieved feature learning setting 

order learn bias sense close uniformly number tasks number examples task sufficiently large 
intuitively reasonable bias learner see sufficiently tasks confident nature environment sufficiently examples task confident nature task 

learner small value learn novel tasks drawn theorem bounding sample complexity required generalisation learning proof similar proof bound theorem 
baxter theorem 
training set generated sampling distribution number training examples satisfies permissible hypothesis space 
probability satisfy capacity appearing equation defined analogous fashion capacities definition just pseudo metric 
important thing note theorem number ex amples required generalisation learning novel tasks proportional logarithm capacity learnt hypothesis space contrast learner bias learning reason select hypothesis space consequently view candidate solution hypothesis hypothesis spaces sample complexity proportional capacity general considerably larger capacity individual learner learnt learn environment learning sense needs far smaller training sets learn novel tasks 

having learnt hypothesis space small value theorem tells probability expected value novel task course rule really bad performance tasks probability generating bad tasks bounded 
particular note just expected value function markov inequality probability 

keeping accuracy confidence parameters fixed note number examples required task generalisation obeys provided increases sublinearly upper bound number examples required task decrease number tasks increases 
shows suitably constructed hypothesis space families possible share information tasks 
discussed theorem 
choosing hypothesis space family model inductive bias learning theorem provides conditions close guarantee small 
governed choice contains hypothesis space small value learner able find minimizing error sample minimizing sufficiently large theorem ensures high probability small 
bad choice mean hope finding learner 
small error 
sense choice represents hyper bias note sample complexity point view optimal hypothesis space family choose containing single minimal hypothesis space contains solutions problems environment set problems high probability 
bias learning choice hypothesis spaces output bias learning algorithm guaranteed hypothesis space environment hypothesis space minimal learning problem environment require smallest possible number examples 
scenario analagous trivial scenario ordinary learning learning algorithm contains single optimal hypothesis problem learnt 
case learning done just bias learning done correct hypothesis space known 
consisting possible func extreme contains single hypothesis space bias learner produce tions bias learning impossible restricted hypothesis space output produce hypothesis space improved sample complexity requirements unseen tasks 
focussing extremes highlights minimal requirements successful bias learning occur hypothesis spaces strictly smaller space functions small skewed contain solutions large majority problems environment 
may simply replaced problem selecting right bias selecting right hypothesis space equally difficult problem selecting right hyper bias right hypothesis space family 
cases selecting right hyper bias far easier selecting right bias 
example section see feature selection problem may viewed bias selection problem 
selecting right features extremely difficult knows little environment intelligent trial error typically best 
bias learning scenario specify set features exist find loosely parameterised set features example neural networks learn features sampling multiple related tasks 
learning multiple tasks may learner interested learning learn just wants learn fixed set tasks environment previous section assume learner starts hypothesis space family receives sample generated distributions time learner simply looking hypotheses average generalization contained hypothesis space error hypotheses minimal 
denoting writing error empirical loss baxter regardless learner chooses prove uniform bound perform probability large deviation training sets high probability perform examples tasks 
theorem 
probability distributions sample generated sampling times permissible hypothesis space family 
number examples task satisfies probability choice recall definition meaning satisfy 
proof 
omitted follow proof bound theorem 
bound theorem virtually identical bound theorem note depends inversely number tasks assuming part max expression dominate 
helps depends rate growth function lemma shows growth small ensure worse learning multiple tasks terms upper bound number examples required task 
lemma 
hypothesis space family model inductive bias learning proof 
denote set functions member hypothesis space 
recall definition lemma appendix right hand inequality follows 
mea inequality probability measure sure obtained copy product ignoring elements product 
cover pick construction lemma establishes inequality 
keeping accuracy parameters fixed plugging see upper bound number examples required task increases number tasks best decreases upper bound provides strong hint learning multiple related tasks advantageous number examples required task basis 
section shown feature learning types behavior possible advantage decrease 
dependence theorems bounds sample complexity scale behavior improved empirical loss guaranteed zero realizable case 
behavior results interested relative deviation empirical true loss absolute deviation 
formal theorems lines stated appendix 
feature learning restricted feature sets nearly ubiquitous method encoding bias areas machine learning statistics including classification regression density estimation 
section show problem choosing set features environment related tasks recast bias learning problem 
explicit bounds calculated general feature classes section 
bounds applied problem learning neural network feature set section 
feature learning model consider quote vapnik classical approach estimating multidimensional functional dependencies belief real life problems exists small number strong features simple functions say linear combinations approximate unknown function 
necessary carefully choose low dimensional feature space regular statistical techniques construct approximation 
baxter general set strong features may viewed function space typically lower dimensional space set feature maps may viewed set features 
mapping input carefully chosen quote 
general simple functions features may represented class functions mapping define hypothesis space hypothesis space family problem carefully choosing right features equivalent bias learning problem find right hypothesis space 
provided learner embedded environment related tasks capacities feature set learnt carefully chosen 
represents important simplification choosing set features difficult part machine learning problem 
section give theorem bounding finite theorem tells theorem specialized neural network classes section 
note forced function class feature maps necessary 
variants results follow obtained allowed vary capacity bounds general feature classes general feature classes 
notationally easier view feature maps mapping map absorb loss function definition viewing denoted follows drop subscript cause confusion 
class belongs denoted previously function definitions define capacity usual way supremum probability measures pulling back define capacity define pseudo metric metric follows easily verified pseudo metric 
note defined supremum integrand measurable 
guaranteed hypothesis space family permissible lemma part 
define smallest cover pseudo metric space capacity respect supremum probability measures state main theorem section 
model inductive bias learning theorem 
hypothesis space family equation 
proof 
see appendix learning neural network features general set features may viewed map typically high dimensional input space smaller dimensional space 
section consider approximating feature map hidden layer neural network input nodes output nodes 
denote set feature maps bounded subset number weights parameters layers 
set feature previous section 
defined output node hidden layer node parameters th feature sigmoid squashing function layer hidden node vector entire feature map computes output hidden node parameters 
assume lipschitz 
weight total number feature parameters argument sake assume simple functions features class previous section squashed affine maps sigmoid function keeping neural network flavor features 
setting feature weights generates hypothesis space bounded subset 
lipschitz exists constant set hypothesis spaces baxter multiple output classes input feature map neural network feature learning 
feature map implemented hidden layers 
output nodes correspond different tasks sample node network computes squashed linear function nodes previous layer 
hypothesis space family 
restrictions output layer weights weights restriction lipschitz squashing function needed obtain finite upper bounds covering numbers theorem 
feature finding set features environment equivalent finding hy space theorem correct set features may learnt finding hypothesis space small error sufficiently large sample specializing squared loss framework empirical loss equation turn means finding set feature map parameters sigmoid function range restrict outputs range 
algorithms finding set features provided squashing function differentiable gradient descent small variation backpropagation compute derivatives find feature weights minimizing local minimum 
extra difficulty ordinary gradient descent appearance definition solution perform gradient descent output parameters node feature weights details see baxter baxter chapter empirical results supporting theoretical results 

model inductive bias learning sample complexity bounds neural network feature learning size ensuring resulting features learning novel tasks environment theorem 
compute logarithm covering numbers theorem 
hypothesis space family form neural network weights mapping feature weights output weights bounded squashing function lipschitz squared loss output space bounded subset exist constants independent recall specialized squared loss 
proof 
see appendix noting neural network hypothesis space family permissible plugging theorem gives theorem 
theorem 
hypothesis space family hypothesis space set squashed linear maps composed neural network feature map 
suppose number features total number feature weights assume feature weights output weights bounded squashing function lipschitz 
sample generated environment probability satisfy discussion baxter 
keeping accuracy confidence parameters fixed upper bound number examples required task behaves learner simply learning fixed tasks learning learn upper bound applies recall theorem 

note away feature map altogether upper bound independent apart important term 
terms upper bound learning tasks just hard learning task 
extreme fix output weights effectively number examples required task decreases range behavior number examples required task possible improvement decrease number tasks increases recall discussion section 

feature map learnt achieved techniques outlined baxter baxter bartlett baxter chapter output weights estimated learn novel task 
keeping accuracy parameters fixed requires examples 
number tasks learnt increases upper bound number examples required task decays minimum possible 
small number strong features assumption correct small 
typically little idea features confident neural network capable implementing feature set need large implying decreases rapidly increasing terms upper bound number examples required task learning small feature sets ideal application bias learning 
upper bound number tasks fare scales comparison traditional multiple class classification special case multi task framework marginal distribution input space task varies tasks conditional distribution output space example multi class problem face recognition number faces recognized marginal distribution simply natural distribution images faces 
case example addition sample th task conditional distribution samples remaining conditional distributions view training sets containing examples large training set multi class problem examples altogether 
bound theorem states proportional total number parameters network result expect haussler 
specialized traditional multiple class single task framework theorem consistent bounds known 
argued problems face recognition really single task multiple class problems 
appropriately viewed 
example classified large margin naive parameter counting improved bartlett 
model inductive bias learning potentially infinite collection distinct binary classification problems 
case goal bias learning find single output network classify subset faces 
learn set features reliably fixed preprocessing distinguishing single face faces 
new thing provided theorem tells provided trained output neural network sufficiently examples sufficiently tasks confident common feature map learnt tasks learning new unseen task provided new task drawn distribution generated training tasks 
addition learning new task requires estimating output node parameters task vastly easier problem estimating parameters entire network sample computational complexity perspective 
high confidence learnt features learning novel tasks drawn environment features candidate study learn nature environment 
claim features learnt small set tasks guarantee generalization novel tasks features implement idiosyncrasies specific tasks invariances apply tasks 
viewed bias feature learning perspective traditional class classification perspective bound number examples required task takes somewhat different meaning 
tells provided large collecting examples large number tasks really need collect examples collect feature map known examples vs examples 
tells burden imposed feature learning negligibly small viewed perspective sampling burden required task 
learning multiple tasks boolean feature maps ignoring accuracy confidence parameters theorem shows number examples required task learning tasks common neural network feature map bounded number features number adjustable parameters feature map 
examples required learn single task true features known shows upper bound number examples required task decays order minimum possible number tasks increases 
suggests learning multiple tasks advantageous truly convincing need prove lower bound form 
proving lower bounds real valued setting complicated fact single example convey infinite amount information typically extra assumptions targets corrupted noise process 
concern complications section restrict attention boolean hypothesis space families meaning hypothesis maps measure error discrete loss 
show sample complexity learning tasks boolean hypothesis space family controlled vc dimension type parameter give nearly matching upper lower bounds involving 
derive bounds hypothesis space family considered previous section lipschitz sigmoid function replaced hard threshold linear threshold networks 
baxter bound number examples required task generalization tasks theorem shows features performing tasks generalize novel tasks number parameters feature map 
feature learning problems quite large recall note section useful know tasks fact necessary restrictions environmental distributions generating tasks 
unfortunately able show lower bound 
empirical evidence suggesting practice upper bound number tasks may weak 
example baxter bartlett reported experiments set neural network features learnt subset japanese characters turned classifying unseen characters features contained parameters 
similar results may intrator edelman experiments reported thrun thrun pratt chapter 
gap experiment theory may just example looseness inherent general bounds may analysis tightened 
particular bound number tasks insensitive size class output functions class section may looseness arisen 
upper lower bounds learning tasks boolean hypothesis space families recall concepts theory boolean function learning 
boolean functions applying functions clearly say class set binary vectors obtainable shatters growth function defined vapnik chervonenkis dimension size largest set shattered important result theory learning boolean functions sauer lemma sauer 
lemma sauer lemma 
boolean function class positive integers generalize concepts learning tasks boolean hypothesis space family 
model inductive bias learning definition 
boolean hypothesis space family 
denote matrices input space define set binary matrices define 
define note define lemma 
say shatters matrix proof 
inequality trivial definitions 
get second term maximum second inequality choose construct matrix rows length shattered clearly shatters term maximum take sequence shattered hypothesis space consisting union hypothesis spaces distribute elements equally rows throw away 
set matrices subset lemma 
size baxter proof 
observe functions sequences obtained choosing functions collection boolean applying examples second examples 
definition result follows lemma applied follows proof theorem particular proof theorem appendix clear may replaced boolean case 
making replacement theorem choices discussion theorem obtain bound probability large deviation empirical true performance boolean setting 
theorem 
probability distributions sample generated sampling times permissible boolean hypothesis space family 
corollary 
conditions theorem number examples task satisfies probability choice proof 
applying theorem require satisfied satisfy lemma 
setting satisfied model inductive bias learning corollary shows algorithm learning tasks hypothesis space family requires examples task ensure high probability average true error hypotheses selects average empirical error sample give theorem showing learning algorithm required produce hypotheses average arbitrary sequence factor number examples equation true error best possible error achievable distributions necessary 
sequence probability distributions define theorem 
boolean hypothesis space family learning algorithm input samples functions 
producing output hypotheses exist distributions random choice proof 
see appendix linear threshold networks contains probability theorems show constants factor sample complexity learning tasks boolean hypothesis space family controlled complexity parameter section derive bounds hypothesis space families constructed thresholded linear combinations boolean feature maps 
specifically assume form squashing function replaced hard threshold don restrict range feature output layer weights 
note case proof theorem carry constants theorem depend lipschitz bound theorem 
hypothesis space family form hard threshold sigmoid function recall parameters input dimension number hidden nodes feature map number features output nodes feature map respectively 
map 
baxter number adjustable parameters feature proof 
recall denote matrix note denotes feature map parameters 
set binary matrices obtainable composing thresholded linear functions elements restriction function applied element row functions may differ rows 
slight abuse notation define fix sauer lemma node hidden layer feature map computes functions input vectors distinct functions input output hidden layer points fixing hidden layer parameters node second layer functions image produced output feature map computes hidden layer 
second hidden layer computes functions output hidden layer points total possible matrix number functions computable row thresholded linear combination output feature map number binary sign assignments obtainable applying linear threshold functions rows convex function substituting shows setting satisfied model inductive bias learning definition observe theorem 
theorem extra restrictions proof 
bound apply lemma 
setting shows layer linear threshold networks input nodes hidden nodes hidden layer hidden nodes second hidden layer output node 
theorem bartlett restrictions stated greater contains choose feature weight assignment feature map identity components input vector insensitive setting components 
generate points image feature map shattered linear threshold output node combining theorem shows examples task suffice learning tasks linear threshold hypothesis space family combining theorem theorem shows learning algorithm fail set tasks 

problem inductive bias broad significance machine learning 
introduced formal model inductive bias learning applies learner able sample multiple related tasks 
proved provided certain covering numbers computed set hypothesis spaces available bias learner finite hypothesis space contains solutions sufficiently training tasks contain solutions novel tasks drawn environment 
specific case learning set features showed number examples required task task training set obeys number baxter features measure complexity feature class 
showed bound essentially tight boolean feature maps constructed linear threshold networks 
addition proved number tasks required ensure performance features novel tasks showed set features may gradient descent 
model represents step formal model hierarchical approaches learning 
modelling learner uncertainty concerning environment probabilistic terms shown learning occur simultaneously base level learn tasks hand meta level learn bias transferred novel tasks 
technical perspective assumption tasks distributed allows performance guarantees proved 
practical perspective problem domains viewed probabilistically distributed sets related tasks 
example speech recognition may decomposed different axes words speakers accents face recognition represents potentially infinite domain related tasks 
medical diagnosis prognosis problems pathology tests example 
domains benefit tackled bias learning approach 
natural avenues enquiry include alternative constructions widely applicable specific example feature learning gradient descent represents just possible way generating searching hypothesis space family interesting investigate alternative methods including decision tree approaches approaches inductive logic programming khan general learning techniques boosting applied bias learning setting 
algorithms automatically determining hypothesis space family model structure fixed apriori represents hyper bias bias learner 
interesting see extent structure learnt 
algorithms automatically determining task relatedness 
ordinary learning usu ally little doubt individual example belongs learning task 
analogous question bias learning individual learning task belongs set related tasks contrast ordinary learning clear cut answer 
examples discussed speech face recognition task relatedness question cases medical problems clear 
grouping large subset tasks related tasks clearly detrimental impact bias learning multi task learning evidence support caruana :10.1.1.145.8948
algorithms automatically determining task relatedness potentially useful avenue research 
context see silver mercer thrun sullivan 
note question task relatedness clearly meaningful relative particular hypothesis space family example possible collections tasks related contains possible hypothesis space 
extended hierarchies 
extension level approach arbitrarily deep hierarchies see langford 
interesting question extent hierarchy inferred data 
somewhat related question automatic induction structure graphical models 
model inductive bias learning supported various times australian postgraduate award shell australia postgraduate fellowship engineering physical sciences research council australian postdoctoral fellowship 
way people contributed helpful comments suggestions improvement including martin anthony peter bartlett rich caruana john langford stuart russell john shawe taylor sebastian thrun anonymous referees 
appendix uniform convergence results theorem provides bound uniform probability large deviation obtain general result follow haussler introduce parameterized class metrics main theorem uniform bound probability large values better bounds realizable case appendix 
lemma 
properties easily established theorem follow corollary 


ease exposition dealing explicitly hypothesis spaces containing functions constructing loss functions mapping loss function general view just function set ignore particular construction terms loss function remainder section stated hypothesis spaces sets functions mapping considerably convenient transpose notation samples writing training sets columns rows 
recalling definition transposition lives new setting 

equation prior discussion definition generalizes quantities definition 
sets functions mapping simply denote map baxter denote set functions 
elements equivalently element writing rows define recall equation 
similarly product probability measure define recall equation 
define recall equation 
class functions mapping necessarily form supremum product probability measures size smallest cover recall definition 
define theorem main result rest uniform convergence results derived 
theorem 
permissible class functions mapping generated independent trials product probability measure immediate corollary 
corollary 
conditions theorem proof theorem proof double symmetrization argument kind chapter pollard 
borrowed ideas proof theorem haussler 
symmetrization extra piece notation bottom half viz model inductive bias learning 
top half lemma symmetrization trick relate probability large deviation empirical estimate loss true loss probability large deviation independent empirical estimates loss 
lemma 
permissible set functions probability measure proof 
note guarantees measurability suprema lemma part 
triangle inequality chebyshev inequality fixed gives result 
substituting expression right hand side second symmetrization baxter second symmetrization trick bounds probability large deviation empirical estimates loss right hand side computing probability large deviation elements randomly permuted second sample 
definition introduces appropriate permutation group purpose 
definition 
integers denote set permutations sequence pairs integers lemma 

permissible set functions mapping statement theorem 
fix cover chosen uniformly random 
proof 
fix done 
choose generality assume form rows loss triangle inequality implies model inductive bias learning construction gives 
bound probability term right hand side 
lemma 
assumption function written form chosen uniformly random 
proof 
simplify notation denote independent random variable pair probability probability zero mean independent random variables bounded ranges ho inequality devroye gy rfi lugosi noting range required 
minimized setting baxter putting fixed lemmas give giving value note simply empirical distribution puts point mass recall definition 
random choice independently identically distributed swaps swaps drawn component drawn distribution 
integrate respect choice write applying lemma expression gives theorem 
proof theorem model inductive bias learning piece notation required proof 
hypothesis space probability measures note estimate indicate sampling process addition sample quence probability measures empirical generated se supplied learner 
notion lemma means probability generating sequence measures environment sample lemma 
holds 
proof 
follows directly triangle inequality treat inequalities lemma separately 
inequality lemma replace supremum inequality supremum lemma 
proof 
suppose equality 
suppose exists baxter satisfy definition property metric exists pick arbitrary assumption compatibility satisfying inequality 
definition ordering reals say 
triangle inequality satisfying inequality 
choosing shows exists identical argument run role interchanged 
cases completes proof lemma 
nature sampling process permissible assumed lemma appendix 
satisfies conditions corollary combining lemma equation substituting corollary gives lemma sample size required ensure holds 
lemma 
inequality note expectation distributed bound left hand side apply corollary replaced replaced replaced respectively replaced replaced note permissible lemma 
model inductive bias learning inequality satisfied 
putting lemma lemma equation proved general version theorem 
theorem 
permissible hypothesis space family erated environment sample gen get theorem observe gives substituting setting maximizing theorem gives theorem 
realizable case theorem sample complexity scales improved requiring require see observe setting theorem treating constant gives corollary 
conditions theorem bounds particularly useful know maximizes 
appendix proof theorem recalling definition form write set written composition function classes note define baxter setting lemmas enable bound lemma 
proof 
fix measure definition proved form minimum size cover measure set algebra minimum size cover definition shown cover note defined measurable measurable 
lemma choose line follows triangle inequality second line follows facts result follows 
cover recalling definition definition lemma 
lemma 
proof 
fix product probability measure covers choose cover result follows 
bounding lemma lemma model inductive bias learning similar techniques prove lemmas shown satisfy equations imply inequality 
bounding wish prove probability measure hypothesis space family form note corresponds algebra arbitrary set induces probability measure note probability measure space defined bounded positive functions probability measures elements corresponding hypothesis spaces measurability 
gives inequality 
guaranteed lemma part ap proof theorem baxter order prove bounds theorem apply theorem neural network hypothesis space family equation 
case structure subset set hidden layer neural networks bounded inputs hidden nodes outputs lipschitz squashing function feature class squashing function weights bounded subset lipschitz restriction bounded restrictions weights ensure lipschitz classes 
exists case 
loss function squared loss 
norm probability measures recall assumed output space marginal distribution probability measures define derived similarly supremum probability measures borel subsets size smallest cover metric 
similarly set supremum probability measures equations imply applying theorem haussler find substituting expressions applying theorem yields theorem 
appendix proof theorem model inductive bias learning proof follows similar argument anthony bartlett ordinary boolean function learning 
need technical lemma 
lemma 
random variable uniformly distributed function mapping valued random variables proof 
denote number occurences random sequence function viewed decision rule observations tries guess probability optimal decision rule bayes estimator 
half probability binomial inequality random variable normal tate inequality tate states combining inequalities completes proof 
distributions th row shattered row set contained note optimal error achieved sequence contains sequence shatters optimal error sample baxter element array 
equal number occurrences select uniformly random sample output learning algorithm generate probability generating configuration process sum possible configurations 
lemma jensen inequality 
valued random variable implies plugging shows sampling model inductive bias learning inequality holds random choice hold specific choice learning algorithm sequence distributions setting ensures assuming equality get solving provided setting subject constraint substituting expressions shows satisfied assuming right hand side approximately maximized point value exceeds obtain dependence theorem observe assumption contains functions exists distributions concentrated generated product distributions learning algorithm chooses wrong hypothesis note baxter choose uniformly random cording lemma shows generate provided combining constraints finishes proof 
appendix measurability sample ac order theorems hold full generality impose constraint called hypothesis space family introduced pollard ordinary hypothesis classes definition similar dudley image admissible dudley 
extending definition cover hypothesis space families 
section assume functions map complete separable metric space denote borel algebra topological space section view set probability measures topological space equipping topology weak convergence 
algebra generated topology 
definitions taken minor modifications pollard 
definition 
set valued functions indexed set exists function definition 
set permissible indexed set 
analytic subset polish space 
function indexing measurable respect product algebra analytic subset polish space simply continuous image borel subset polish space analytic subsets polish space include borel sets 
important projections analytic sets analytic measured complete measure space projections borel sets necessarily borel measured borel measure 
details see dudley section 
lemma 
proof 
omitted 
permissible define hypothesis space families 
permissible 

topological space called polish metrizable complete separable metric space 
model inductive bias learning definition 
hypothesis space family permissible exist sets analytic subsets polish spaces respectively function measurable respect denote analytic subsets pollard appendix 
set measure space analytic subset polish space 
complete contains product algebra facts analytic sets taken projection recall definition definition lemma assume completed respect probability measure respect environmental measure lemma 
permissible hypothesis space family 

permissible 

permissible 
permissible 
measurable 
measurable 
permissible 
proof 
absorbed loss function hypotheses complete fold products simply set follows lemma 
immediate definitions 
permissible proved identical argument measurable suprema section pollard appendix note borel measurable defined borel measurable chapter 
automatically implies 
indexed define measurable function 
shown function appropriate way 
prove theorem measurable defined indexes appropriate way permissible provided contains set projection property analytic 
assumed complete measurable property 
measurable 
important 
property analytic sets measurable function follows 
baxter abu mostafa 

method learning hints 
hanson cowan giles 
eds advances neural information processing systems pp 
san mateo ca 
morgan kaufmann 
anthony bartlett 

neural network learning theoretical foundations 
cambridge university press cambridge uk 
bartlett 

lower bounds vc dimension multi layer threshold networks 
proccedings sixth acm conference computational learning theory pp 
new york 
acm press 
summary appeared neural computation 
bartlett 

sample complexity pattern classification neural networks size weights important size network 
ieee transactions information theory 
baxter 

learning internal representations 
ph thesis department mathematics statistics university south australia 
copy available anu edu au jon papers thesis ps gz 
baxter 

learning internal representations 
proceedings eighth international conference computational learning theory pp 

acm press 
copy available anu edu au jon papers colt ps gz 
baxter 

bayesian information theoretic model learning learn multiple task sampling 
machine learning 
baxter 

canonical distortion measure vector quantization function approximation 
proceedings fourteenth international conference machine learning pp 

morgan kaufmann 
baxter bartlett 

canonical distortion measure feature space nn classification 
advances neural information processing systems pp 

mit press 
berger 

statistical decision theory bayesian analysis 
springer verlag new york 
blumer ehrenfeucht haussler warmuth 

learnability dimension 
journal acm 
caruana 

multitask learning 
machine learning 
devroye gy rfi lugosi 

probabilistic theory pattern recognition 
springer new york 
dudley 

course empirical processes vol 
lecture notes mathematics pp 

springer verlag 
dudley 

real analysis probability 
wadsworth brooks cole california 
model inductive bias learning gelman carlin stern 
eds 

bayesian data analysis 
chapman hall 


history hierarchical bayesian methodology 
bernardo groot lindley smith 
eds bayesian statistics ii 
university press valencia 
haussler 

decision theoretic generalizations pac model neural net learning applications 
information computation 
heskes 

solving huge number similar tasks combination multi task learning hierarchical bayesian approach 
shavlik 
ed proceedings th international conference machine learning icml pp 

morgan kaufmann 
intrator edelman 

low dimensional representation suitable diverse tasks 
connection science 


classical descriptive set theory 
springer verlag new york 
khan muggleton 

repeat learning predicate invention 
page 
ed proceedings th international workshop inductive logic programming ilp lnai pp 

springer verlag 
langford 

staged learning 
tech 
rep cmu school computer science 
www cs cmu edu jcl research staged latest ps 
mitchell 

need biases learning generalisations 
dietterich shavlik 
eds readings machine learning 
morgan kaufmann 
parthasarathy 

measures metric spaces 
academic press london 
pollard 

convergence stochastic processes 
springer verlag new york 
pratt 

discriminability transfer neural networks 
hanson cowan giles 
eds advances neural information processing systems pp 

morgan kaufmann 
rendell 

layered concept learning dynamically variable bias management 
proceedings tenth international joint conference artificial intelligence ijcai pp 

ijcai ring 

continual learning reinforcement environments 
oldenbourg verlag 
russell 

knowledge analogy induction 
morgan kaufmann 
sauer 

density families sets 
journal combinatorial theory 
sharkey sharkey 

adaptive generalisation transfer knowledge 
artificial intelligence review 
baxter silver mercer 

parallel transfer task knowledge dynamic learning rates measure relatedness 
connection science 
singh 

transfer learning composing solutions elemental sequential tasks 
machine learning 


distribution inequalities binomial law 
annals probability 
suddarth holden 

neural systems hints developing complex systems 
international journal man machine studies 
suddarth 

rule injection hints means improving network performance learning time 
proceedings eurasip workshop neural networks portugal 
eurasip 
sutton 

adapting bias gradient descent incremental version delta bar delta 
proceedings tenth national conference artificial intelligence pp 

mit press 
tate 

double inequality normal distribution 
annals mathematical statistics 
thrun 

learning th thing easier learning 
advances neural information processing systems pp 

mit press 
thrun mitchell 

learning thing 
proceedings international joint conference artificial intelligence pp 

morgan kaufmann 
thrun sullivan 

discovering structure multiple learning tasks tc algorithm 
saitta 
ed proceedings th international conference machine learning icml pp 

kaufmann 
thrun pratt 
eds 

learning learn 
kluwer academic 
thrun schwartz 

finding structure reinforcement learning 
tesauro touretzky leen 
eds advances neural information processing systems vol 
pp 

mit press 
utgoff 

shift bias inductive concept learning 
machine learning artificial intelligence approach pp 

morgan kaufmann 
valiant 

theory learnable 
comm 
acm 
vapnik 

estimation dependences empirical data 
springer verlag new york 
vapnik 

nature statistical learning theory 
springer verlag new york 

