shrinking language models robust approximation adam buchsbaum giancarlo jeffery westbrook study problem reducing size language model preserving recognition performance accuracy speed 
successful approach represent language models weighted finite state automata 
analogues classical automata determinization minimization algorithms provide general method produce smaller equivalent 
extend approach introducing notion approximate determinization 
provide algorithm applied language models north american business task achieves size reduction compared previous techniques negligible effects recognition time accuracy 

important goal language model engineering produce small language models guarantee fast accurate automatic speech recognition asr 
practice see tradeoffs size vs accuracy accuracy vs speed 
progress automatic methods reducing sizes language models preserving recognition performance accuracy speed 
de mori kenny discuss size reduction lexical trees order speed asr systems deal unweighted trees 
pereira discuss weighted finite state automata model human language asr systems 
advantage approach classical techniques automata determinization minimization extended produce smaller formally equivalent inputs input string assigned cost input reduced models 
mohri develops ideas reports significant size reductions application asr word lattices 
labs park ave florham park nj alb research att com dipartimento di matematica ed universit di palermo palermo italy math 
supported labs 
labs park ave florham park nj research att com extend mohri approach relaxing requirement output model formally equivalent input 
introduce technique call approximate determinization produces wfa accepts precisely strings language approximates costs 
clearly arbitrarily different costs zero strings produce trivially small language models perform poorly asr systems 
algorithm accepts approximation parameter controls tradeoff output model size recognition performance 
experiments nab language models achieve size reduction compared mohri exact determinization minimization algorithms affecting asr time accuracy 
results invariant different beam widths 
show approximation algorithm preserves best strings asr output 
technique completely general applicable language model 
section describes algorithm 
section reports experimental results showing size reduction preservation best strings 
section reports experimental results showing size reduction word accuracy 
conclude section 

definitions weighted finite automaton wfa tuple ffi set states initial state set labels ffi theta theta 
set transitions set final states 
called lattice 
size jaj jqj 
deterministic sequential wfa transition oe pair oe nondeterministic wfa multiple transitions pair oe differing target state sequence transitions gamma oe induces string oe delta delta delta oe string accepted accepted accepts weight weight set sequences transitions accept string weight min lan appear proc 
icassp may seattle washington fl ieee guage set weighted strings accepted accepted ag wfa represent language models weights interpreted negative log probabilities 
weight assigned string wfa negative log probability string occurring input speech 
definitions algorithms apply arbitrary semirings 

algorithm mohri automatic method reducing size language model classical automata theory 
input wfa normally nondeterministic replaced equivalent deterministic wfa equivalent deterministic wfa minimum size computed 
size reduction comes process determinization 
describe mohri algorithm wfa determinization call observations behavior motivate approximate determinization algorithm wfa ffi mohri generalizes classical power set construction build deterministic wfa follows 
start state forms initial queue pop state theta 
values encode path length information follows 
oe fq set states reachable oe transitions ae min oe ffi fr minimum weights oe transitions plus respective ae min jm fae ae gamma ae new pushed transition oe ae added oe transition state deterministic 
ta set sequences transitions accept string sequence transitions accepts string 
shown min ta fc 
ta set sequences transitions state state induce string sequence transitions induces string ends state shown min ta fc remainder encodes difference weight shortest path state induces weight path inducing remainder state zero 
importance weights wfa determinization studied effects atis word lattices 
tuples states occurred multiple times different remainders remainders tended clustered sense 
corresponding remainders equal differed tuples differences small relative magnitude 
led approximate determinization algorithm generalizes mohri algorithm follows 
addition wfa takes approximation parameter input 
constructing state wfa exists previously constructed state jr gamma delta minfr place 
corresponding zero 
deterministic accepts exactly strings may assign different weights strings 
gives pseudocode sections measure quality consider preserves best strings word lattices 
show affects recognition performance applied language models 

application word lattices section study quality measuring preserves best strings word lattices 
applied various approximation parameters word lattices generated north american business speech recognizer atis grammar 
lattice measured resulting size reduction determined maximum best strings input appeared order best strings output averaged results test set 
depicts resulting size reduction 
axis shows different values axis compares sizes results input word lattices 
small values gave reduction factors better states transitions 
shows large values affect best paths lattices 
axis shows maximum best strings preserved order 

recognition performance section study quality applying different values shrink composed lexicon grammar nab task 
ran 
input ffi 
input approximation parameter 
ffi 
ff gg 

remove element 
fqg 
oe 
fq fq oe ffig 
comment states multiplicity reachable oe transitions 
ae min oe ffi 
ae min jm ae 
ae gamma ae 

ff xm fqg mg 
comment set tuples states identical 
ff xm jx gamma delta gg 

fq 
fq 
ffi ffi oe ae 

pick element 
ffi ffi oe ae 



output ffi 
approximate determinization algorithm 
approximation parameter size vs original lattice average state compression median state compression average transition compression median transition compression size reduction vs approximation 
approximation parameter best strings preserved max median average preservation best strings 
approximation parameter word accuracy vs benchmark asr size vs minimum exact model word accuracy vs correct recognition performance small nab models 
sentences models speech recognizer determined word accuracy result correct answer asr exact models 
figures plot results different nab language models 
middle curves show relative sizes models produced compared respective exact models produced bottom curves show recognition word accuracy degrade appreciably 
equivalent 
top curves show output asr approximate models nearly identical output exact models just substitute mistake 
repeated experiment twelve different beam widths producing similar results time 

experimental evidence shows approximate determinization produces models considerably smaller minimal formally equivalent models negligible effects asr performance 
saving model space reduces memory consumption recognizer line time 
saw effect asr time deserves study 
surmise parts model explored recognition 
bigger models required expect size reduction achieved positive effect caching paging 
approximation parameter word accuracy vs benchmark asr size vs minimum exact model word accuracy vs correct recognition performance large nab models 

berstel 
transduction context free languages volume der mathematik und 
springerverlag 
berstel reutenauer 
rational series languages volume eatcs 
springerverlag 
eilenberg 
automata languages machines volume academic press 
kenny hollan gupta 
admissible heuristics rapid lexical access 
ieee trans 
speech audio proc 
mori 
lexical tree compression 
proc 
nd eurospeech volume pages 
mohri 
finite state transducers language speech processing 
comp 
ling 
pereira riley 
speech recognition composition weighted finite automata 
finitestate language processing 
mit press 
pereira riley sproat 
weighted rational transductions application human language processing 
proc 
arpa hlt pages 
riley hindle pereira 
word speech text system 
proc 
th eurospeech volume pages 

