ijcnn appear 
metrics learn relevance samuel kaski sinkkonen neural networks research centre helsinki university technology box fin hut finland samuel kaski hut fi sinkkonen hut fi introduce algorithm learning local metric continuous input space measures distances terms relevance processing task 
relevance defined local changes discrete auxiliary information may example class data items index performance contextual input 
set neurons learns representations maximize mutual information outputs random variable representing auxiliary information 
implicit knowledge gained relevance transformed new metric input space measures change auxiliary information sense local approximations kullback leibler divergence 
new metric processing algorithms 
especially useful data analysis applications distances interpreted terms local relevance original variables 
success unsupervised learning algorithms including principal components analysis various clustering algorithms self organizing maps depends crucially feature extraction choice relative scaling input variables 
successful feature extraction stages tailored task hand expert knowledge heuristic rules thumb 
implicit auxiliary information available relevance input 
classification samples may known goal discover characteristics classes find suitable features classification 
alternatively example process monitoring indicator performance may associated data vector goal find factors affect performance 
prediction task goal may discover features important successful prediction 
introduce algorithm learns take auxiliary information account 
stage network learns features maximize mutual information features auxiliary information 
mutual information outputs different processing modules previously criterion building representations coherent time space 
approach similar point features 
explicitly transform local metric original input space original proximity relations preserved new distances measure local change auxiliary information 
maximization mutual information produce metric 
auxiliary information chosen indicate relevance goals learning task distances measured relevance goals 
distance signifies equal relevance 
relevance metric algorithms 
algorithms unsupervised desirable properties unsupervised algorithms fast learning generalizability visualization capability preserved 
demonstration self organizing map learning new metric sec 

new metric especially useful data analysis applications easily interpreted terms local relevance original input variables 
networks maximize mutual information consider network neurons 
neuron receives stochastic input activation jth neuron response input denoted 
learning method general sense depend type neurons exact form parameterization 
section detailed learning algorithm derived suitable form parameterization 
components come carefully optimized feature extraction stage noisy may completely irrelevant task hand 
completely unsupervised network unable distinguish relevant irrelevant information 
question addressed section network learn utilize auxiliary information available learning responses neurons reflect auxiliary information possible 
assuming auxiliary information relevant task network intended perform neurons learned extract relevant features 
input network output distribution activity neurons 
activations sum unity distribution may interpreted conditional probability distribution random variable called jx values denoted marginal distribution dx probability density function input 
assume auxiliary information related sample represented discrete valued random variable denoted denote values indices may denote example possible classes data alternative contexts possible outcomes prediction task 
space outcomes continuous may discretized suitably 
note usually distribution jx values associated inputs known 
aim learning optimize parameters neurons information activities neurons mediate maximized maximize mutual information distribution activity neurons distribution auxiliary random variable mutual information log log jx jx dx decomposed joint probability fact auxiliary information directly affect activity neurons 
variables conditionally independent section introduce algorithm maximizes mutual information adjusting parameters network 
maximization mutual information gaussian neurons shown gradient respect parameters neurons derivation omitted log jx dx expression valid kinds parameterized neurons 
section show maximize normalized gaussian neurons priori set common width 
response neuron input kx parameters optimized 
note denote brevity depends 
gaussian neurons gradient rw log jv jv dx straightforward maximize 
integrals jv dx estimated weighted sums data supposedly drawn distribution 
parameters readily optimized general purpose optimization algorithm 
order achieve line learning stochastic approximation maximize 
stochastic approximation responses neurons respectively interpreted densities jx jx discrete random variables conditionally independent expression jx jx jx sampling function stochastic approximation 
leads algorithm step stochastic approximation draw input distinct neurons multinomial distribution fy adapt parameters log jv jv gradually decreasing step size 
estimates jv adapted simultaneously leaky integration rate change larger 
metric measures relevance mutual information measure statistical dependency outputs network auxiliary information outputs form representation input construction intended capture dependency limits allowed parametrization auxiliary information hand selected changes distribution signify relevance 
section aim dependency mediated explicit knowledge judge relevance local changes various directions input space 
assuming selected suitably relevance measured changes estimated distribution function formulated new metric input space 
start making dependency explicit 
mutual information written integral probability jx log jx jx dx const jx exp log jv integral equal average kullback leibler divergence distributions fp jx jx wish simultaneously maximize construct density estimates sense kullback leibler divergence need jx equation density estimate 
measure relevance difference pair input samples difference corresponding jx distance measures useful disadvantage generate topology may different original topology input space 
points originally far away may zero distance density estimates jx identical 
preservation topology input space course important want auxiliary information completely override original identity data 
introduce additional constraint topology input space may change guaranteed measuring distances locally defining non local distances path integrals minimal paths input space 
fixes new metric locally similar original local scaling new distances form dx dx dx 
differences estimates posterior distributions jx measured terms kullback leibler divergence known distance computed locally jx kp jx dx dx dx cjx rx log cjx rx log cjx fisher information matrix 
new metric posterior density fp jx changes evenly directions input space 
new metric visualization unsupervised methods distance measure supervised learning 
note invariance property constructed metric 
distance invariant certain mappings input space 
differentiable mapping cjx cjx ds yj jacobian matrix mapping 
distance dx mapping dx ec dx rx log cjx 
dx distance invariant invertible smooth mappings computed correct posterior probabilities cjx 
reality estimates estimates processing stages insensitive large class topology preserving nonlinear transformation input space 
unsupervised learning relevance metric discuss new relevance metric distance measure subsequent processing 
gaussian neurons discussed sec 
derive explicit metric generated 
similar metrics derived types representations 
gaussian neurons estimate conditional probability distribution jx estimate distance measure get approximation jx squared kullback leibler distances 
formula dg denoted log jv distance measure principle processing task 
metric originally derived differential appropriate methods rely mainly distances close points input space 
demonstrate relevance metric computing self organizing maps new metric 
self organizing map som regular grid units model vector associated unit learning process model vectors modified learn follow distribution input data ordered fashion model vectors close map lattice attain close locations input space 
map grid chosen dimensional resulting map display visualizing various properties input data useful data analysis applications 
som algorithm consists iterative application steps 
winning unit closest current input sample sought winner neighbors map lattice adapted input sample 
carry steps relevance metric 
winner defined unit model vectors adapt time step ci rm ci jx ci called neighborhood function decreasing function distance units map lattice 
height width ci decrease gradually time 
experiments section demonstrate kind metric ensues network learns extract essential characteristics dimensional easily artificial data set 
metric som algorithm visualize class distribution data 
experiment intended serve illustration report results practical data papers 
data sampled spherically symmetric normal distribution cf 
fig 

available auxiliary information consisted class data sample 
classes highly overlapping distributed cylindrical fashion conditional probability distribution jx constant column directed axis 
plane orthogonal axis class concentrated somewhat middle xy plane cf 
inset top left corner fig 

summary distribution data spherically symmetric radial direction xy plane relevant viewpoint auxiliary information 
fig 
shows weight vectors parameters neurons learned stochastic approximation method described sec 

vectors located approximately plane orthogonal axis represent axis 
xy plane neurons center located regular symmetric configuration center able best represent important radial direction 
information distances dg estimated converged network illustrated small line segments fig 
radial direction xy plane dominates 
distances largest class distribution changes rapidly 
direction axis distances practically zero 
locations weight vectors neurons centers gaussian kernels network learned maximize mutual information auxiliary class information 
different cross sections dimensional space shown circles denote weight vectors projected cross sections 
gray levels indicate conditional probability jx class location small line segments dots depict dominant direction relative distances local relevance metric 
bottom right corner depict distribution data curves inset top left corner depict conditional probability distributions classes cross section plane 
distribution classes data map units soms 
class som relevance metric 
class som relevance metric 
class som euclidean metric 
class som euclidean metric 
light shades denote high density dark shades low density respectively 
demonstrate relevance metric preprocessing stage processing 
computed self organizing map original euclidean metric relevance metric visualized class distribution resulting maps 
seen fig 
classes distinctly separated som computed relevance metric border classes represented 
note data analysis application possible som displays visualizing aspects data 
instance distribution original variables visualized reveal variables contribute separation classes locally input space 
discussion shown maximization mutual information auxiliary random variable feature set derived input seen form density estimation 
estimate defines metric input space additionally constrained preserve original topology space local distances 
auxiliary variable chosen changes value signify relevance task hand metric measures relevance 
measure optimal restrictions posed parametrization network 
knowledge principle new 
works aspects resemble approach exist 
amari wu augmented support vector machines making isotropic change metric near class border 
contrast metric non isotropic global 
jaakkola haussler induced distance measure discrete input space generative probability model 
crucial differences external information constrain metric preserve topology 
aware information bottleneck framework tishby 
setup discrete aim finding local metrics 
approach related goal maximize mutual information representation relevance indicator 
unsupervised algorithm learns relevance metric metric output data analysis learning process supervised unsupervised 
topology input space preserved typical unsupervised methods metric local scaling space induced supervised manner 
compared standard separate feature extraction stage change metric defines manifold general projected euclidean space dimension 
dimensionality preserving mapping local properties exists means change metric general operation feature selection dimensionality preserving dimensionality reducing nonlinear mapping 
obvious applications method exploratory data analysis 
metric original input space transformed interpretation discovered relevant factors straightforward 
high dimensional inputs results visualized dimensionality reducing method self organizing map 
forming relevance metric additionally considered kind nonlinear discriminant analysis 
linear discriminant analysis finds linear transformation maximizes class separability 
metric transforms input space locally change class distribution isotropic direction allows inspection class distributions closely 
classical canonical correlation analysis generalized replacing linear combinations nonlinear functions 
possible metric task finding statistical dependencies data sets replacing discrete auxiliary random variable parametrized set features computed auxiliary continuous random variable 
advantage method creates easily interpretable metric 
acknowledgments supported academy finland 
amari wu 
improving support vector machine classifiers modifying kernel functions 
neural networks 
becker 
mutual information maximization models cortical self organization 
network computation neural systems 
becker hinton 
self organizing neural network discovers surfaces random dot stereograms 
nature 
jaakkola haussler 
exploiting generative models discriminative classifiers 
advances neural information processing systems 
morgan kauffmann publishers san mateo ca 
appear 
kohonen 
self organizing maps 
springer berlin 
second extended edition 
kullback 
information theory statistics 
wiley new york 
lai fyfe 
neural implementation canonical correlation analysis 
neural networks 
phillips kay smyth 
discovery structure multi stream networks local processors contextual guidance 
network computation neural systems 
tishby pereira bialek 
information bottleneck method 
th annual allerton conference communication control computing illinois 
appear 
