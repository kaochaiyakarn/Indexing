overview fortran programming system ken kennedy charles koelbel ulrich kremer chau wen tseng tr march center research parallel computation rice university box houston tx fourth workshop languages compilers parallel computing santa clara ca august 
springer verlag 
overview fortran programming system ken kennedy charles koelbel ulrich kremer chau wen tseng department computer science rice university houston tx success large scale parallel architectures limited difficulty developing machineindependent parallel programs 
developed fortran version fortran extended data decomposition specifications provide portable data parallel programming model 
presents design key components fortran programming system prototype compiler environment assist automatic data decomposition 
fortran compiler addresses program partitioning communication generation optimization data decomposition analysis run time support unstructured computations storage management 
fortran programming environment provides static performance estimator automatic data partitioner 
believe fortran programming system significantly ease task writing machine independent data parallel programs 
widely recognized parallel computing represents plausible way continue increase computational power available computational scientists engineers 
widely successful parallel computers easy today vector supercomputers 
major component success research supported center research parallel computation national science foundation science technology center 
vector supercomputers ability write machine independent vectorizable programs 
automatic vectorization compiler technologies possible scientist structure fortran loops understood rules vectorizable style expect resulting program compiled efficient code vector machine 
compare current situation parallel machines 
scientists wishing machine rewrite programs extension fortran explicitly reflects architecture underlying machine messagepassing dialect mimd distributed memory machines vector syntax simd machines explicitly parallel dialect synchronization mimd shared memory machines 
conversion difficult resulting parallel programs machine specific 
scientists discouraged porting programs parallel machines risk losing investment program changes new architecture arrives 
way overcome problem identify data parallel programming style allows efficient compilation fortran programs variety parallel machines 
researchers working area including concluded programming style useful sufficient general 
reason information included program text compiler accurately evaluate alternative translations 
similar reasoning argues cross compilations current parallel extensions fortran 
reasons chosen different approach 
believe selecting data decomposition important intellectual step developing data parallel scientific codes 
current parallel programming languages provide little support data decomposition 
developed enhanced version fortran introduces data decomposition specifications 
call extended language fortran suggests data decomposition distribution 
reasonable data decompositions provided fortran program written data parallel programming style believe advanced compiler technology implement efficiently variety parallel architectures 
developing prototype fortran compiler generate node programs ipsc mimd distributed memory machine 
successful result project go far establishing feasibility machineindependent parallel programming mimd shared memory compiler directly mimd distributed memory implementation 
additional step construction effective fortran compiler simd distributed memory machines 
initiated rice project build compiler existing vectorization technology 
fortran compiler automates time consuming task deriving node programs data decomposition 
remaining components fortran programming system static performance estimator automatic data partitioner support important step developing data parallel program selecting data decomposition 
rest presents data decomposition specifications fortran structure prototype fortran compiler design fortran programming environment 
conclude discussion validation strategy 
fortran data decomposition problem approached considering levels parallelism data parallel applications 
question arrays aligned respect array dimensions 
call problem mapping induced structure underlying computation 
represents minimal requirements reducing data movement program largely independent machine considerations 
alignment arrays program depends natural fine grain parallelism defined individual members data arrays 
second question arrays distributed actual parallel machine 
call machine mapping caused translating problem finite resources machine 
affected topology communication mechanisms size local memory number processors underlying machine 
distribution arrays program depends coarse grain parallelism defined physical parallel machine 
fortran version fortran provides data decomposition specifications levels parallelism decomposition align distribute statements 
decomposition problem index domain require storage 
element decomposition represents unit computation 
decomposition statement declares name dimensionality size decomposition 
align statement map arrays decompositions 
arrays mapped decomposition automatically aligned 
alignment take place dimensions 
alignment arrays decompositions specified placeholders subscript expressions array decomposition 
example real decomposition align declared dimensional decomposition size theta array aligned respect dimensions permuted offsets dimension 
arrays aligned decomposition distribute statement maps decomposition finite resources physical machine 
distributions specified assigning independent attribute dimension decomposition 
predefined attributes block align decomposition real distribute block cyclic distribute fortran data decomposition specifications cyclic block cyclic 
symbol marks dimensions distributed 
choosing distribution decomposition maps arrays aligned decomposition machine 
example decomposition distribute block distribute cyclic distributing decomposition block results column partition arrays aligned distributing cyclic partitions rows round robin fashion processors 
sample data alignment distributions shown 
predefined regular data distributions effectively exploit regular data parallelism 
irregular distributions run time processing required manage irregular data parallelism unstructured computations 
fortran irregular distributions may specified explicit user defined function data array 
example integer map decomposition distribute map elements decomposition mapped processor indicated array map 
fortran supports dynamic data decomposition changing alignment distribution decomposition point program 
note goal designing fortran support general data decompositions possible 
intent provide decompositions powerful express data parallelism scientific programs simple permit compiler produce efficient programs 
fortran language semantics similar sequential fortran 
result quite usable computational scientists 
addition believe phase strategy specifying data decomposition natural conducive writing modular portable code 
fortran bears similarities cm fortran 
complete language described detail 
fortran compiler stated previously major steps writing data parallel program selecting data decomposition derive node programs explicit communications access nonlocal data 
manually inserting communications time consuming tedious non portable error prone step parallel programming 
significant increases source code size common expected 
major advantage programming fortran ability utilize advanced compiler techniques automatically generate node programs explicit communication data decompositions specified program 
prototype compiler developed context parallel programming environment take advantage analysis fortran compiler output transformation capabilities editor 
main goal fortran compiler derive data decomposition parallel node program minimizes load imbalance communication costs 
approach convert fortran programs single program multiple data spmd form explicit message passing executes directly nodes machine 
basic strategy partition program owner computes rule processor performs computation data owns 
relax rule prevents compiler achieving load balance reducing communication costs 
fortran compiler bears similarities arf id superb 
current prototype generates code subset decompositions allowed fortran block distributions 
depicts output livermore loop kernel generated fortran compiler 
program partitioning phase compiler partitions program processors data decomposition 
define iteration set local processor set loop iterations cause access data owned iteration set calculated alignment distribution specified fortran program 
owner computes rule set loop iterations execute union iteration sets left hand sides lhs individual assignment statements loop 
partition computation processors reduce loop bounds processor executes iterations set 
multiple statements loop iteration set individual statement may subset iteration set loop 
statements add guards membership tests iteration set lhs ensure assignments local array elements 
communication computation partitioned fortran compiler introduce communications nonlocal data accesses preserve semantics original program 
requires calculating data sent received processor 
calculate send iteration set right hand side rhs iteration set minus iteration set lhs 
similarly receive iteration set rhs iteration set lhs minus iteration set 
sets represent iterations data sent received fortran compiler summarizes array locations accessed send receive iterations rectangular triangular regions known regular sections generate calls communication primitives 
communication optimization naive approach introducing communication insert send receive operations directly preceding causing nonlocal data access 
generates small messages may prove inefficient due communication overhead 
fortran compiler data dependence information determine communication may inserted outer loop vectorizing messages combining small messages 
algorithm calculate appropriate loop level message described 
major goal fortran compiler aggressively optimize communications 
intend apply techniques proposed li chen recognize regular computation patterns utilize collective communications primitives 
especially important recognize reduction operations 
regular communication patterns plan employ collective communications routines express 
unstructured computations irregular communications incorporate parti primitives saltz 
fortran compiler may utilize data decomposition dependence information guide program transformations improve communication patterns 
considering usefulness transformations particularly loop interchanging strip mining loop distribution loop alignment 
replicating computations dead code elimination applied eliminate communication 
communications may optimized considering interactions loop nests program 
intra interprocedural dataflow analysis array sections show assignment variable live point program intervening assignments variable 
information may eliminate redundant messages 
instance assume messages previous loop nests retrieved nonlocal elements array 
values live messages fetch values succeeding loop nests may eliminated 
data different arrays sent processor may buffered message reduce communication overhead 
owner computes rule provides basic strategy fortran compiler 
may relax rule allowing processors compute values data 
instance suppose multiple rhs assignment statement owned processor owner lhs 
computing result processor owning rhs sending result owner lhs reduce amount data communicated 
optimization simple case owner stores rule proposed 
particular may desirable fortran compiler partition loops processors loop iteration executed single processor parti 
technique may improve communication provide greater control load balance especially irregular computations 
eliminates need individual statement guards simplifies handling control flow loop body 
data decomposition analysis fortran provides dynamic data decomposition permitting align distribute statements inserted point program 
complicates job fortran compiler know decomposition array order generate proper guards communication 
define reaching decompositions set decomposition specifications may reach array aligned decomposition may calculated manner similar reaching definitions 
fortran compiler apply intra interprocedural analysis calculate reaching decompositions distributed array 
multiple decompositions reach procedure node splitting run time techniques may required generate proper code program 
permit modular programming style effects data decomposition specifications limited scope enclosing procedure 
procedures inherit decompositions callers 
semantics require compiler insert calls run time data decomposition routines restore original data decomposition procedure return 
changing data decomposition may expensive calls eliminated possible 
define live decompositions set decomposition specifications may reach array aligned decomposition may calculated manner similar live variables 
reaching decompositions fortran compiler needs intra interprocedural analysis calculate live decompositions decomposition specification 
data decompositions determined live may safely eliminated 
similar analysis may hoist dynamic data decompositions loops 
run time support irregular computations advanced algorithms scientific applications amenable techniques described previous section 
adaptive meshes example poor load balance high communication cost static regular data distributions 
algorithms require dynamic irregular data distributions 
algorithms fast multipole algorithms heavy index arrays compiler analyze 
cases communications analysis performed run time 
fortran project supports dynamic irregular distributions 
inspector executor strategy generate efficient communications adapted parti 
inspector transformation original fortran loop builds list nonlocal elements known set received execution loop 
global transpose operation performed collective communications calculate set data elements sent processor known set 
executor uses computed sets control actual communication 
performance results parti primitives indicate inspector implemented acceptable overhead particularly results saved executions original loop 
storage management guards communication calculated fortran compiler select manage storage nonlocal array received processors 
different storage schemes described ffl overlaps developed expansions local array sections accommodate neighboring nonlocal elements 
useful programs high locality may waste storage nonlocal accesses distant 
ffl buffers designed overcome contiguous nature overlaps 
useful nonlocal area bounded size near local array section 
sets training estimator performance static partitioner data automatic environment fortran user fortran fortran compiler message passing fortran fortran parallel programming system ffl hash tables set accessed nonlocal elements sparse 
case irregular computations 
hash tables provide quick lookup mechanism arbitrary sets nonlocal values 
storage type nonlocal data determined compiler needs analyze space required various storage structures generate code nonlocal data accessed correct location 
storage management parts fortran compiler described detail 
fortran programming environment choosing decomposition fundamental data structures program pivotal step developing data parallel applications 
selected data decomposition usually completely determines parallelism data movement resulting program 
unfortunately existing tools advise programmer making important decision 
evaluate decomposition programmer insert decomposition program text compile run resulting program determine effectiveness 
comparing data decompositions requires implementing running versions program tedious task best 
process prohibitively difficult assistance compiler automatically generate node programs data decomposition 
researchers proposed techniques automatically derive data decompositions simple machine models 
techniques insufficient efficiency data decomposition highly dependent actual node program generated compiler performance parallel machine 
optimal data decompositions may prove inferior compiler generates node programs suboptimal communications poor load balance 
similarly marginal data decompositions may perform compiler able utilize collective communication primitives exploit special hardware parallel machine 
need programming environment helps user understand effect data decomposition program structure efficiency compiler generated code running target machine 
fortran programming system shown provides environment 
main components environment static performance estimator automatic data partitioner 
fortran programming system built top provides program analysis transformation editing capabilities allow users restructure programs data parallel programming style 
zima vienna working similar tool support data decomposition decisions automatic techniques 
gupta banerjee propose automatic data decomposition techniques assumptions proposed distributed memory compiler 
static performance estimator clearly impractical dynamic performance information choose data decompositions programming environment 
static performance estimator needed accurately predict performance fortran program target machine 
required scheme allows compiler assess costs communication routines computations 
static performance estimator fortran programming system caters needs 
performance estimator general theoretical model distributed memory computers 
employs notion training set kernel routines measures cost various computation communication patterns target machine 
results executing training set parallel machine summarized train performance estimator machine 
utilizing training sets performance estimator achieves accuracy portability different machine architectures 
resulting information may fortran compiler guide communication optimizations 
static performance estimator divided parts machine module compiler module 
machine module predicts performance node program containing explicit communications 
uses machine level training set written message passing fortran 
training set contains individual computation communication patterns timed target machine different numbers processors data sizes 
estimate performance node program machine module simply look results computation communication pattern encountered 
compiler module forms second part static performance estimator 
assists user selecting data decompositions statically predicting performance program set data decompositions 
compiler module employs compiler level training set written fortran consists program kernels stencil computations matrix multiplication 
training set converted message passing fortran fortran compiler executed target machine different data decompositions numbers processors array sizes 
estimating performance fortran program requires matching computations program kernels training set 
compiler level training set provides natural way respond changes fortran compiler machine 
simply recompile training set new compiler execute resulting programs reinitialize compiler module performance estimator 
possible incorporate possible computation patterns compiler level training set performance estimator encounter code fragments matched existing kernels 
estimate performance codes compiler module rely machine level training set 
plan incorporate elements fortran compiler performance estimator mimic compilation process 
compiler module convert unrecognized fortran program fragment equivalent node program invoke machine module estimate performance 
note desirable assist automatic data decomposition static performance estimator need predict absolute performance data decomposition 
needs accurately predict performance relative data decompositions 
prototype machine module implemented common class loosely synchronous scientific problems 
predicts performance node program express communication routines different numbers processors data sizes 
prototype performance estimator proved quite precise especially predicting relative performances different data decompositions 
screen snapshot typical performance static performance estimator estimation session shown 
user select program segment loop invoke performance estimator clicking estimate performance button 
prototype responds execution time estimate selected segment target machine estimate communication time represented percentage total execution time 
allows effectiveness data partitioning strategy evaluated part node program 
automatic data partitioner goal automatic data partitioner assist user choosing data decomposition 
utilizes training sets static performance estimator select data partitions efficient compiler parallel machine 
automatic data partitioner may applied entire program specific program fragments 
invoked entire program automatically selects data decompositions user interaction 
believe regular loosely synchronous problems written dataparallel programming style automatic data partitioner determine efficient partitioning scheme user interaction 
alternatively automatic data partitioner may starting point choosing data decomposition 
invoked interac tively specific program segments responds list best decomposition schemes static performance estimates 
user satisfied predicted performance performance estimator locate communication computation intensive program segments 
fortran environment advise user effects program changes choice data decomposition 
analysis performed automatic data partitioner divides program separate computation phases 
intra phase decomposition problem consists determining set data decompositions performance individual phase 
data partitioner tries match phase parts phase computation patterns compiler training set 
match returns set decompositions best measured performance recorded compiler training set 
match data partitioner perform alignment distribution analysis phase 
resulting solution may accurate effects fortran compiler target machine estimated 
alignment analysis prune search space possible arrays alignments selecting alignments minimize data movement 
alignment analysis largely machineindependent performed analyzing array access patterns computations phase 
intend build inter dimensional alignment techniques li chen knobe 
distribution analysis follows alignment analysis 
applies heuristics prune unprofitable choices search space possible distributions 
efficiency data distribution determined machine dependent aspects topology number processors communication costs 
automatic data partitioner uses final set alignments distributions generate set reasonable data decomposition schemes 
worst case set decompositions cross product alignment distribution sets 
static performance estimator invoked select set data decompositions best predicted performance 
computing data decompositions phase automatic data partitioner solve inter phase decomposition problem merging individual data decompositions 
determines profitability redistributing arrays computational phases 
interprocedural analysis merge decomposition schemes computation phases procedure boundaries 
resulting decompositions entire program performance user 
validation strategy plan establish compilation automatic data partitioning schemes fortran achieve acceptable performance variety parallel architectures 
benchmark suite developed geoffrey fox syracuse consists collection fortran programs 
program suite versions original fortran program best hand coded message passing version fortran program nearby fortran program fortran version nearby program fortran version program 
nearby version program utilize basic algorithm message passing program explicit message passing blocking loops program removed 
fortran version program consists nearby version plus appropriate data decomposition specifications 
validate fortran compiler compare running time best hand coded message passing version program output fortran compiler fortran version nearby program 
validate automatic data partitioner generate fortran program nearby fortran program 
result compiled fortran compiler running time compared compiled version hand generated fortran program 
purpose validation program suite provide fair test prototype compiler data partitioner 
expect tools perform high level algorithm changes 
test ability analyze optimize programs machine independent issues structure computation machine dependent issues number interconnection processors parallel machine 
validation strategy test key parts fortran programming system limits machine independent fortran programming model efficiency ability compiler technology effectiveness automatic data partitioning performance estimation techniques 
scientific programmers need simple machineindependent programming model efficiently mapped large scale parallel machines 
believe fortran version fortran enhanced data decompositions provides portable data parallel programming model 
success depend compiler environment support provided fortran programming system 
fortran compiler includes sophisticated intraprocedural interprocedural analyses dynamic data decomposition program transformation communication optimization support regular irregular problems 
significant remains implement optimizations preliminary experiments expect fortran compiler generate efficient code large class dataparallel programs minimal user effort 
fortran environment distinguished ability accurately estimate performance programs collective communication real parallel machines automatically choose data partitions account characteristics compiler generated code underlying machine 
assist user developing efficient fortran programs 
believe fortran programming system powerful useful tool significantly ease task writing portable data parallel programs 
authors wish bala geoffrey fox marina inspiring ideas 
grateful research group providing underlying software infrastructure fortran programming system 

translating control parallelism data parallelism 
proceedings fifth siam conference parallel processing scientific computing houston tx march 
fox kennedy kremer 
interactive environment data partitioning distribution 
proceedings th distributed memory computing conference charleston sc april 
fox kennedy kremer 
static performance estimator guide data partitioning decisions 
proceedings third acm sigplan symposium principles practice parallel programming williamsburg va april 
callahan cooper hood kennedy torczon 
parallel programming environment 
international journal supercomputer applications winter 
callahan kennedy 
compiling programs distributed memory multiprocessors 
journal supercomputing october 
callahan kennedy kremer 
dynamic study vectorization pfc 
technical report tr dept computer science rice university july 
chapman zima 
automatic support data distribution 
proceedings th distributed memory computing conference portland april 
fox kennedy koelbel kremer tseng wu 
fortran language specification 
technical report tr dept computer science rice university december 
fox johnson otto salmon walker 
solving problems concurrent processors volume 
prentice hall englewood cliffs nj 

updating distributed variables local computations 
concurrency practice experi ence september 
gupta banerjee 
automatic data partitioning distributed memory multiprocessors 
proceedings th distributed memory computing conference portland april 
havlak kennedy 
implementation interprocedural bounded regular section analysis 
ieee transactions parallel distributed systems july 
hill 
new tool parallelization 
supercomputing review april 
kennedy tseng 
compiler optimizations fortran mimd distributed memory machines 
proceedings supercomputing albuquerque nm november 
kennedy tseng 
compiler support machine independent parallel programming fortran technical report tr dept computer science rice university january 
appear saltz mehrotra editors compilers runtime software scalable multiprocessors elsevier 
saltz mehrotra 
performance hashed cache data migration schemes multicomputers 
journal parallel distributed computing august 
hudak abraham 
compiler techniques data partitioning sequentially iterated parallel loops 
proceedings acm international conference supercomputing amsterdam netherlands june 
fox flower 
automatic symbolic parallelization system distributed memory parallel computers 
proceedings th distributed memory computing conference charleston sc april 
kennedy tseng 
analysis transformation editor 
proceedings acm international conference supercomputing cologne germany june 
kennedy tseng 
interactive parallel programming editor 
ieee transactions parallel distributed systems july 
knobe steele jr data optimization allocation arrays reduce communication simd machines 
journal parallel distributed computing february 
koelbel mehrotra 
compiling global name space parallel loops distributed execution 
ieee transactions parallel distributed systems october 
li chen 
index domain alignment minimizing cost cross referencing distributed arrays 
frontiers rd symposium frontiers massively parallel computation college park md october 
li chen 
compiling programs massively parallel machines 
ieee transactions parallel distributed systems july 
saltz smith nicol crowley 
principles runtime support parallel processors 
proceedings second international conference supercomputing st malo france july 

parallel languages respond needs scientific programmers ieee computer december 

express user manual 
sadayappan 
methodology parallelizing programs multicomputers complex memory multiprocessors 
proceedings supercomputing reno nv november 
rogers pingali 
process decomposition locality 
proceedings sigplan conference program language design implementation portland june 
snyder socha 
algorithm producing balanced partitionings data arrays 
proceedings th distributed memory computing conference charleston sc april 
thinking machines cambridge ma 
cm fortran manual version edition september 
wolfe 
semi automatic domain decomposition 
proceedings th conference hypercube concurrent computers applications monterey ca march 
wu saltz 
runtime compilation methods multicomputers 
proceedings international conference parallel processing st charles il august 
zima 
bast 
superb tool semi automatic mimd simd parallelization 
parallel computing 

