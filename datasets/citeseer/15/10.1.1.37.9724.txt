clustering large graphs matrices drineas alan frieze ravi kannan santosh vempala vinay consider problem dividing set points euclidean clusters variable fixed minimize sum distance squared point cluster center 
formulation differs ways frequently considered clustering problems literature fixed variable sum squared distances measure argue problem natural contexts 
consider relaxation discrete problem find subspace sum distances squared points minimized 
show relaxation solved singular value decomposition svd linear algebra 
ii solution relaxation get approximation algorithm original problem 
importantly iii argue fact relaxation provides generalized clustering useful right 
iv show svd randomly chosen submatrix suitable probability distribution matrix provides approximation svd matrix yielding fast randomized algorithm 
applied problems large size typically arise modern applications 
consider problem clustering rows theta matrix problem dividing set rows clusters computer science department yale university new haven ct 
email drineas cs yale edu 
department mathematical sciences carnegie mellon university pittsburgh pa 
email alan random math cmu edu 
supported part nsf ccr 
computer science department yale university new haven ct 
email kannan cs yale edu 
department mathematics cambridge ma university california berkeley 
email vempala math mit edu 
indian institute science bangalore india 
email vinay csa ernet 
cluster similar rows 
notion similarity rows discussed detail function length vector difference rows 
equivalently may view problem geometrically points euclidean space wish divide clusters cluster contains points close 
problem includes special case problem clustering vertices directed undirected graph matrix just adjacency matrix graph 
vertices similar lot common neighbors 
notions similarity notions clustering general problems turn np hard 
polynomial time approximation algorithms 
aim deal large matrices upwards rows columns upwards nonzero entries polynomial time bound algorithm value 
formally deal case vary number clusters fixed 
seek linear time algorithms small constants 
show basic singular value decomposition svd linear algebra provides excellent tool 
show svd helps solve approximately clustering problems described 
importantly argue svd directly gives call continuous clustering point belong cluster certain intensity clusters necessarily disjoint 
basic linear algebra show natural properties continuous clustering argue useful problems 
develop linear time randomized algorithm approximate svd procedure feasible large matrices modern applications 
discrete clustering 
consider clustering problem points fa dimensional euclidean space positive integer considered fixed vary 
problem find points fb fa dist minimized 
dist euclidean distance nearest point problem wish minimize sum squared distances nearest cluster center 
call discrete clustering problem dcp 
dcp np hard reduction minimum bisection 
note defines clusters cluster center centroid points easily seen fact set fx point jx gamma bj jx gamma xj gamma xj centroid delta delta delta dcp problem partitioning set points clusters sum variances clusters minimized 
define relaxation call continuous clustering problem ccp problem finding subspace dimension minimizes dist easy see optimal value dcp upper bound optimal value ccp 
set points fa vb vb subspace generated points follow standard linear algebra continuous clustering problem solved polynomial time exactly optimal subspace read singular value decomposition svd matrix containing rows see section 
attempt solve dcp follows solve ccp find subspace project problem solve discrete clustering problem dimensional space fixed 
show dimensional problem solved exactly polynomial time give approximation algorithm dcp 
discuss section approximation algorithm focus 
fast svd 
stated earlier focus twofold 
firstly give fast randomized algorithm finds approximation rank satisfying high probability jja gamma jj min rank jja gamma djj jjajj sum squares entries matrix error parameter 
nearly best rank approximation sense described 
known satisfying ffl read svd polynomial time 
making pass entire matrix algorithm run time ffl maximum number non zero entries column fact replaced average number entries column probability distribution algorithm 
see section 
computing svd show sufficient compute svd matrix consisting ffl randomly picked rows rows randomly picked distribution satisfying conditions see section details 
see prove ffl rows picked holds high probability may pick fewer rows practice check randomized way holds lets time worst case bound problem worst case 
note gives algorithm achieves running time prohibitively large modest values :10.1.1.126.5994:10.1.1.126.5994
follow ideas 
note algorithm running time grow linearly 
clearly approximation form useful approximation small rank large exact algorithms feasible 
examples situations conditions prevail describe briefly typical examples 
latent semantic indexing 
general technique analyzing collection documents assumed related example documents dealing particular subject see 
suppose documents terms occur documents 
model hypothesizes relationships documents small number main unknown topics documents 
aim technique find set topics best describe documents 
part concerns 
topic modeled non negative reals interpretation jth component topic vector gives frequency jth term occurs discussion topic 
model hand easy argue linear algebra line reasoning similar field factor analysis statistics best topics top singular vectors called document term matrix theta matrix ij frequency jth term ith document 
alternatively define ij depending jth term occurs ith document 
second example clustering documents web discuss section 
generalized clustering 
second main focus argue approximately best rank approximation yields generalized clustering generalized clustering differs respects normal clustering cluster subset equivalently components reals ith component gives intensity ith point belongs cluster secondly requirement clusters disjoint discrete clustering replaced requirement vectors corresponding different clusters orthogonal 
see notion clustering quite natural allows desirable features allowed discrete clustering having overlapping clusters realistic practice 
review related literature 
clustering location studied subjects notions measure effectiveness cluster 
discuss relation defined problem considered 
see agarwal sharir bern eppstein everitt bailey surveys related 
traditional clustering problem problem 
defines cluster size maximum pairwise distance points cluster wish minimize maximum cluster size divisions clusters 
problem partition clusters find cluster cluster center median 
optimal solution minimizes sum distances point median cluster 
problem easier constant factor approximation algorithms known dyer frieze hochbaum shmoys 
problem approximation algorithms harder come 
notably arora raghavan rao discovered polynomial time approximation scheme points located euclidean plane 
generally fixed dimensional space note problem number dimensions variable number clusters fixed 
notation 
matrix jjajj denotes sum squares entries denotes th row th column real matrix expressed oe oe oe oe called singular values form orthonormal set vectors oe av oe singular value decomposition linear algebra know matrix producing minimum jja gamma djj matrices rank av implies oe discrete clustering problem show solve dcp time input considered fixed 
set cluster centers defines voronoi diagram cell fx jx gamma jx gamma ig consists points closest point cell polyhedron total number faces gamma delta face set points equidistant points 
seen partition determines best computation centroids move boundary hyperplanes optimal voronoi diagram face passing point face contains points assume points general position simple perturbation argument deals general case 
means face contains affinely independent points lost information side face place points try possibilities face 
leads enumerative procedure solving dcp algorithm dcp dimensions ffl enumerate gamma delta dk sets gamma hyperplanes contains affinely independent points ffl check arrangement defined hyperplanes exactly cells 
ffl td choices cell assign point lying hyperplane ffl defines unique partition find centroid set partition compute fa remarked previously ccp solved linear algebra 
dimensional subspace orthogonal projections theta matrix rows rank jja gamma ajj ja gamma dist solve ccp find vectors svd known minimize jja gamma ajj rank matrices take space vsv spanned singular vectors row space show combining ideas gives approximation dcp 
projection subspace vsv 
optimal solution dcp input algorithm general dcp ffl compute vsv ffl solve dcp input obtain ffl output follows optimal value dcp dcp satisfies dcp ja gamma note optimal solution dcp consists projection points dcp dist dist combining get dcp ja gamma dist dist fa proving get approximation 
generalized clusters svd section argue natural way generalizing clusters leads singular vectors 
introduce typical motivation 
suppose wish analyze structure large portion web 
consider underlying directed graph vertex url edge vertex vertex hypertext link turns quite useful cluster vertices graph 
obviously large graphs arise application traditional heuristics polynomial time ones 
examine particular application detail 
directed graph wish divide vertex set clusters similar vertices 
information graph vertices similar share lot common neighbors 
generally assumption relevant information captured matrix 
dwell modeling part translation real problem matrix assume done 
going back example clustering vertices graph clusters similar vertices examine similarity may precisely defined 
useful purpose think web example edge means thinks important 
intuitively similar vertices reinforce opinions documents important 
clustering means partition node set subsets similar nodes 
partition strict quite common overlapping clusters 
traditional clustering finds subsets going characteristic vectors may view vectors 
strict 
different nodes may belong cluster different intensities 
example set neighbors large nodes subset cluster intuitively include reinforcement intuitively important cluster 
define cluster just reals 
intensity belongs 
assign weight importance cluster crucial quantity regard vector frequency occurrence node neighborhood cluster high values mean high reinforcement 
jx aj measure importance cluster 
note scaled component central definition definition cluster jxj 
weight cluster denoted jx aj 
jxj denotes euclidean length 
reasoning definition euclidean lengths 
exhaustively discuss possible measures look obvious norms possible examples illustrate advantage euclidean norm carry norms 
euclidean norm suppose norm nodes neighborhood set putting zero putting gives weight 
prefer larger clusters greater reinforcement 
easy see restrict jxj choose larger cluster 
similarly maximum weight cluster obviously choice 
easy see norm jx aj highest indegree node desirable 
similar example provided case norm having defined weight cluster want describe decomposition process mentioned earlier successively removes maximum weight cluster graph 
maximum weight cluster cluster 
write scalar orthogonal known basic linear algebra orthogonal jv aj ju aj jw aj larger higher weight require different may arbitrarily close leads correct requirement required orthogonal orthogonality replaces disjointness traditional partition 
crucial definition definition optimal clustering set orthonormal vectors maximum weight cluster subject orthogonal gamma argue directly linear algebra final corresponding removing clusters operation subtracting theta matrix gamma defines residual graph removing clusters 
represents weighted graph edge weights 
intuition clusters large weight residual matrix small norm sum squared entries 
way quantifying linear algebra note matrix rank fact sum squares entries matrices form gamma rank norm norm matrix max juj matrices form gamma rank optimal clustering error matrix small possible natural ways 
defined weights clusters looking degrees 
symmetrically may look degrees 
luckily linear algebra tells elaborate final fact optimal clustering respect indegrees yields optimal clustering respect degrees vice versa 
close section important property optimal clusterings 
discussion reinforcement opinions note reinforcement comes large sets nodes necessarily disjoint edges form aspect clustering svd technique cluster graphs pioneered kleinberg 
see gibson kleinberg raghavan 
kleinberg considered ubiquitous problem glean relevant documents usually large set documents returned standard web search program key word 
intuition define document authority lot documents returned search point hypertext link 
argues idea just take documents pointed lot 
defines dual notion document hub points lot documents 
generally suppose documents returned search engine 
defines theta matrix ij depending ith document points th 
explicitly deal large matrix 
sets find vectors hub weight document weight higher document hub authority weight document normalization jxj jyj argues reproduce argument desirable find max jxj jyj ay maximizing expect hub weights authority weights mutually consistent 
course problem finding singular vectors large examples hundreds judiciously chooses submatrix computes singular vectors 
approach choose submatrix probability distribution 
points especially case key word multiple meanings top singular vectors large singular values interesting 
example key word java top singular vectors put high authority weights documents programming language java set singular vectors put high weights documents island documents coffee interest find largest singular vectors small problem consider 
linear algebra know removing top clusters residual matrix maximum weight cluster oe 
argue submatrices large sums 
context web application means removing top clusters appreciable set hubs authorities 
lemma notation subsets fi fi fi fi fi fi ij fi fi fi fi fi fi oe corollary jsj jt average degree node induced subgraph oe 
oe falls rapidly resid ual graphs large reinforcing subgraphs 
words case large reinforcing subgraphs caused relatively clusters 
fast svd algorithm suppose nonnegative reals summing satisfying cja jjajj constant ability sample columns probabilities fp sampler samples columns probabilities proportional length squared 
may take 
sampler property available idea lengths columns case easy set sampler pass matrix matrix claim making pass entire matrix set data structures sample entries fast time sample fp satisfying :10.1.1.126.5994
suppose ij ij ja ij create log bins pass put lth bin entries gamma ja ij keep track number entries bin 
pretend entries bin equal absolute value easy set sampler details data structures elementary left reader 
algorithm pick certain number columns fp satisfying 
scale columns certain way fp 
fp ensure heavier columns picked scaling looked compensating weighting heavier columns 
course supply proof method works 
algorithm 
ffi 

select independently columns distribution fp column selected include sp columns matrix theta matrix random selection 
find 
takes time time thetar maximum number entries column 
find top eigenvectors theta matrix return sp jsp clusters 

analysis algorithm 
note orthonormal linear algebra jja gamma ajj jjajj gamma ja linear algebra min rank jja gamma djj oe oe oe prove lemma 
positive real pr ja gamma oe jjajj 
get probability gamma ffi algorithm jja gamma ajj min rank jja jjajj proof singular vectors writing aa ss coordinate system coordinate vectors see aa gamma ss entry aa gamma ss aa gamma ss gamma ss jj ja gamma joe gamma ss jj applying hoffman inequality see golub van loan see oe ss gamma oe aa oe gamma oe gamma ss jj adding inequality real get ja gamma oe gamma ss jj re phrasing lemma pr gamma ss jj jjajj cs gives assertion lemma :10.1.1.126.5994
applying get probability gamma ffi gamma ss jj jjajj putting applying cauchy schwartz inequality get second assertion lemma 
doing better worst case 
note prove value job possible actual problem situation may far worst case 
fact practice suffices pick rows smaller worst case bound 
may check resulting approximation sufficiently close randomized fashion sample entries gamma estimate sum squares matrix 
error satisfactory may increase details variance estimates procedure routine left final 
preliminary experimental results 
algorithm asymptotically linear time upper bound bound depends polynomially ffl 
hope practice better theoretically provable bound ffl 
performed limited experiments date dense matrices plan perform experiments larger sparse matrices 
algorithm interest case matrix low rank approximation tried algorithm certain randomly generated dense matrices known singular values 
generated theta random matrices form sigmav random orthogonal matrices sigma diagonal matrix containing specified singular values generate generated perfectly random orthogonal matrices procedure described stewart subsequently performed random walk set orthogonal matrices applying random givens rotations order get new elements set 
experiments varied percentage frobenius norm matrix contained top singular values parameters number top singular values large oe oe oe kak oe singular values words fraction frobenius norm captured singular values 
varied 
set singular values oe oe oe delta delta gamma delta respectively constant th proportional gamma 
singular values set equal 
little calculation shows subject conditions jjajj singular values completely determined 
reason choosing top way want fall steeply computation singular values power method better method easy time wanted occupy constant fraction frobenius norm 
experiments find rows pick random approximation error wanted ensure input matrix rank approximation find jja gamma jj min rank jja gamma djj jjajj note know singular values compute min rank jja gamma djj determine requirement met 
varied number random rows picked requirement met 
results obtained running experiments random matrices generated described number rows note theoretical bound guarantees requirement met order large 
limited experiments promising 
plan experiments larger sparse matrices generated specific singular values 
acknowledgment wish alan edelman stan michael littman prabhakar raghavan ravi comments 
agarwal sharir planar geometric location problems dimacs 
arora raghavan rao approximation schemes medians related problems proceedings th annual acm symposium theory computing 
bern eppstein approximation algorithms geometric problems approximation algorithms hard problems hochbaum ed pws publishing 
berry dumais brien 
linear algebra intelligent information retrieval siam review 
facility location survey applications methods springer 
dyer frieze simple heuristic centre problem operations research letters 
everitt cluster analysis published behalf social science research council heinemann educational books new york press 
frieze kannan vempala fast algorithms finding low rank approximations appear focs golub van loan matrix computations johns hopkins university press london :10.1.1.126.5994
gibson kleinberg raghavan clustering categorical data approach dynamical systems large data bases vldb 
hochbaum shmoys best possible heuristic center problem mathematics operations research 
cluster analysis data analysis north holland 
kleinberg authoritative sources hyperlinked environment proceedings th annual acm siam symposium discrete algorithms 
kleinberg papadimitriou raghavan segmentation problems stoc 
cluster analysis social scientists 
stewart efficient generation random orthogonal matrices application condition estimators siam numerical analysis vol 
june pp 

bailey cluster analysis mcgrawhill 

