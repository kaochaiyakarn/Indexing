ieee learning semantic similarity reusable software components dieter merkl min tjoa kappel department information engineering university vienna wien austria department computer science university linz linz austria email dieter tjoa ifs univie ac ifs uni linz ac properly structured software libraries crucial success software reuse 
specifically structure software library ought reflect functional similarity stored software components order facilitate retrieval process 
propose application artificial neural network technology achieve structured library 
detail utilize artificial neural network adhering unsupervised learning paradigm 
distinctive feature model semantic relationship stored software components geographically explicit 
actual user software library gets notion semantic relationship components terms geographical closeness 

software reuse concerned technological organizational background existing software components build new applications 
software reuse supposed increase productivity software suppliers due saved time component reused quality software due fact component tested different contexts 
software reuse operational actual software developers equipped libraries containing reusable software components 
concerning software libraries stringent requirements 
software libraries provide large number reusable components wide spectrum application domains 
components may reused may easily adapted needs application currently consideration 
software libraries organized way locating appropriate component easy actual users 
particularly library provide assistance user locating components meet specified functionality 
address second requirement structuring contents software libraries way locating needed component facilitated 
elaborate discussion aspects software reuse refer 
rest describe approach relying artificial neural network technology achieve semantically structured software library 
term semantically structured denotes structuring functional similarity stored components 
words components exhibit similar behavior stored near identifiable 
approach artificial neural network adhering unsupervised learning paradigm selforganizing feature map 
architecture neural networks exhibits distinctive feature preserving topological relationship input data 
topological relationship geographically explicit dimensional grid neurons output space 
input data mapped dimensional surface functionally similar components mapped geographically near regions surface 
main reasons choosing artificial neural network 
firstly artificial neural networks robust sense tolerating noisy inexact input data 
property especially important query processing query may regarded inexact representation needed software component 
secondly model serves associative memory 
words artificial neural network capable retrieving software components supplied just description needed component 
thirdly artificial neural network able generalize 
pragmatically speaking artificial neural network learns conceptually relevant features input domain proc 
rd int conference software reuse 
rio de janeiro 
brazil 
nov 
ieee computer society press 

pp 
ieee organizes software library accordingly 
selection self organizing feature map basis library organization motivated unsupervised learning rule additionally ability visualize relationship input data clusters 
general unsupervised learning may described adapting structure artificial neural network enable construction internal models capture regularities input domain 
adaptation exclusively input data additional information example desired result learning process 
benefits learning rule obvious case existing structured software libraries 
similar results achievable artificial neural network architectures 
unique characteristic selforganizing feature maps representation intra inter cluster relationships 
precisely semantic similarity input data corresponds distance software components grid output units 
contrary artificial neural network models behave merely predicates component member certain cluster 
apply model library containing operating system commands examples reusable software components 
operating system commands convenient examples determine efficiency approach reasons 
component description easily obtained accessing textual description various commands operating system manual 
operating system manual regarded real world example software documentation 
behavior various commands known outcome structuring library judged intuitively 
related may divided groups 
group adheres textual description software software manual applies approaches area information retrieval structure software library 
second group relies knowledge technology semantic networks 
approach combining information retrieval techniques build software representation cluster analysis structure software library reported 
detail classification reusable software components performed concept lexical affinities pairs words occur frequently sentence 
order obtain structured library authors hierarchical agglomerative clustering methods lexical affinities input 
approaches rely semantic networks capture relationship stored software components 
example esprit project ithaca addresses software reuse conceptual modeling language telos describe stored components semantic similarity 
approach combines faceted classification software components semantic networks reported 
semantic similarity components captured weighted connections semantic network 
approach software classification artificial neural network model reported 
particular feed forward recurrent artificial neural networks adhering supervised learning paradigm assign comments identifiers extracted source code concepts 
artificial neural network performs analysis informal information contained program 
organized follows 
section describe architecture learning algorithm artificial neural network experiments 
section provides results structuring software library 
additionally describe task locating reusable software components give performance comparison artificial neural network traditional models 
section contains concluding remarks 

self organizing feature maps 
architecture self organizing feature maps architecture self organizing feature map consists layer containing input units 
units receive input data represented dimensional vectors outside neural network 
input data application description various software components 
defer comments component representation section 
task input units propagate input vectors grid socalled output units 
output units arranged topological order 
dimensional array output units assigned dimensional weight vector 
initially components weight vectors assigned small random values range 
learning process input vectors repeatedly output units 
produces output value proportional similarity current input vector unit weight vector 
similarity commonly measured terms euclidean distance vectors 
output value referred unit activation unit response input 
ieee 
learning self organizing feature maps learning algorithm self organizing feature maps may described steps 
steps referred learning iteration 
firstly input time randomly selected set possible input data 
input terms corresponding input vector mapped output units selforganizing feature map 
secondly activation output unit determined unit strongest response selected 
unit referred winning unit best matching unit winner short 
thirdly weight vector assigned winning unit weight vectors neighboring units adapted 
adaptation means components weight vectors changed 
weight vectors adapted way get nearer current input vector 
words weight vectors changed order assure corresponding units exhibit higher activation respect input 
steps performed repeatedly self organizing feature map converges stable state 
stable state arrangement weight vectors changes components observed presentation input vector 
conclude numerous repetitions steps learning process weight vectors get better approximations input vectors 
furthermore due adaptation set neighboring units units respond similarly input vectors 
precisely crucial steps learning process may described equations 
vectors typed italics functions denoted greek characters 
jo equation contains computation activation output units output space computation performed inner product transposed input vector weight vector output unit consideration 
max equation selection winning unit described 
winning unit unit highest activation output units ij equation describes adaptation weight vectors pragmatically speaking adaptation moves weight vector input vector learning iteration adaptation depends difference current input vector weight vector 
amount adaptation graded scalar functions ij function gain function produces socalled learning rate range output 
obviously high learning rate leads large changes weight vectors input vector 
second function ij neighborhood function determines amount adaptation output unit depending distance winning unit grid output units 
output function winning unit decreases gradually proportional distance unit consideration winning unit 
output range 
order guarantee convergence self organizing feature map outcome gain function decrease gradually time 
additionally neighborhood function sure number units subject weight changes gets smaller increasing time 
learning process weight vector winning unit adapted weight vectors remain unchanged 
obvious restrictions learning process converge stable state self organizing feature map 
takes large number learning iterations stable state reached 
contains schematic representation learning process 
input vector mapped grid output units winning unit selected 
winning unit depicted black node 
weight vector winner 
moved current input vector 
learning iteration unit produce higher response input vector due fact unit weight vector nearer input vector apart winning unit adaptation neighboring units 
units subject adaptation depicted shaded nodes 
shading nodes degree adaptation 
generally units geographically near winner adapted strongly drawn darker shade 

self organizing feature map input space output space ieee 
lateral inhibition output units parameters crucial determine efficiency self organizing feature map number learning iterations needed process converges stable state approximation input vectors 
obviously learning function provide fast convergence time high approximation input data 
approximation input data usually measured terms euclidean distance input vector weight vector corresponding best matching unit 
order provide improved learning function changed original neighborhood function proposed ritter kohonen capture lateral inhibition output units 
original neighborhood function moves weight vectors neighboring units current input 
words weight vector unit adapted way distance current input vector decreases weight vector changed 
changed neighborhood function provide biologically plausible learning process lateral inhibition output units 
roughly speaking lateral inhibition process activation neuron blocks activation neurons 
words activation neuron reduces inclination activating neurons 
phenomenon observed example visual system brain 
terms self organizing feature map described activation unit corresponds similarity unit weight vector current input vector 
term blocking activation units denote enlargement distance unit weight vector current input vector 
activation unit smaller respect actual input vector 
assumptions applying lateral inhibition output units 
firstly convergence learning process faster establishing clusters facilitated due fact units farther away winning unit dissimilar 
secondly approximation input data better oscillations mapping reduced 
oscillations denote observation learning process input vectors mapped different units 
observed hundreds learning iterations initial phase learning process inputs mapped randomly grid output units 
speaking input data jump grid output units 
precisely perform lateral inhibition equation 
ij sin ij neighborhood function determining strength adaptation weight vectors distance winning unit unit 
parameter influence width function terms lateral excitation adaptation input lateral inhibition adaptation away current input 
roughly speaking units near distance winning unit pulled current input units far distance pushed away 
time dependent function similarly function equation 
function inhibition rate determines strength adaptation 
analogy equation output function limited range additionally output decreases gradually time 
learning process winning unit adapted agreement restriction neighborhood functions described 
easy see new neighborhood function computes output values range 
gives schematic representation lateral inhibition output units 
winning unit depicted black node 
arrows indicate shaded units inside bold cycle moved current input hatched nodes outside bold cycle moved away current input 
shading nodes corresponds strength adaptation 

experimental results section describes application selforganizing feature maps structure software library 
firstly give brief description test environment 
secondly provide graphical 
lateral inhibition output units ieee representation structured library 
give performance evaluation concerning query processing 

environment remainder set operating system commands examples reusable software components 
particular library contains ms dos commands 
description commands extracted words operating system manual describe behavior commands 
chose binary single term indexing basic means extract needed information 
extracted words form list possible features software components 
example consider containing observed features operating system commands copy del 
final representation commands follows vector space model input data queries represented vectors dimensional hyperspace 
component vectors corresponds possible feature valued 
entry zero denotes fact corresponding feature description software component 
contrary entry means corresponding feature describe software component 
case test environment identified features command represented dimensional feature vector 
feature vectors input selforganizing feature map 
detailed description test environment published 
results software libraries reported 
copy file copy append 
indexing operating system commands del file delete file directory copy 
semantically structured software libraries feature vectors representing operating system commands input vectors learning process self organizing feature map 
particular rectangular self organizing feature maps output units 
state learning process observed learning iterations represented graphically map operating system commands 
graphical representation learning process may interpreted way 
grid output units depicted rectangular plane contains entries output units dimensions 
entry constitutes name operating system command dot 
appearance command means corresponding unit exhibits highest activation respect input representing command winning unit 
words weight vector assigned output unit smallest euclidean distance corresponding input vector 
due limited space figures name command printed case command assigned unit 
multiple assignments commonly occur learning process weight vectors tuned 
appearance dot figures denotes fact corresponding unit winning unit command unit exhibit highest response input vectors 
figures show state learning self organizing feature map observed different learning iterations 
sake completeness state learning rate equation set initially parameter equation value 
initial mapping operating system commands grid output units 
stage learning performed 
location commands purely random weight vectors initially filled random values 
contains state learning process 
del 



initial mapping map ieee iterations 
iteration operating system commands separated 
furthermore formation clusters visible 
example lower left part map find commands type dir mem time date display information screen 
contains mapping commands shortly stable state reached 
observed learning iterations 
stage commands separated 
clusters stage quite stable means little change noticeable location winning units corresponding commands 
please note selforganizing feature map recognizes example complementary commands mirror recover backup restore 
contains final location operating system commands learning iterations self organizing feature map reached stable state 
please note define stable state learning iteration changes location winning units perceptible 
approximation input vectors improves iteration 
due fact learning rate equation decreased zero learning iteration 
parentheses state original model lateral inhibition output units needed learning iterations converge stable state initial parameters learning process 
easy observe semantically similar operating system commands assigned topologically near units 
find cluster commands aiming displaying information 
final mapping cluster moved little center plane 
just point interesting area lower middle part find commands aiming deletion information 

rmdir del restore 
mirror 
tree find 

format 
backup copy dir ren 

time 
type mem date assign 
map learning iterations assign mkdir 
edit format join backup 

restore 
time ren 
mem date 
fc replace 

copy attrib 
type 
cls 
chdir tree del rmdir find 
map learning iterations assign mkdir 
edit format join backup 

restore mirror recover time ren 
path mem date 
comp replace 
fc append copy attrib cls type tree del 
dir chdir rmdir find 
stable state map learning iterations ieee del rmdir cls 
neighboring unit del assigned command essentially complementary command 
furthermore assigned close plausible commands reset information 
series tests revealed lateral inhibition output units self organizing feature map converges faster better approximation input vectors 
results tests summarized table 
table contains average convergence time terms learning iterations changes location winning units observed 
furthermore average mismatch respect components input vectors weight vectors corresponding winning units provided 
figures learning functions lateral inhibition output units li nli respectively 
legend learning iterations average mismatch li lateral inhibition nli lateral inhibition table 
results different learning functions 
retrieval software components stated reason applying artificial neural network model provide assistance user locating reusable software components meet specified functionality 
commonly required functionality described query 
query turn easily transformed feature vector means setting features query features zero 
query vector treated artificial software component best matching stored components searched 
precisely distinguish possibilities query processing 
firstly determine best matching unit respect query vector 
location best matching unit neighborhood user 
cases relevant components near neighborhood winning unit 
secondly stored components user ranked list 
ranking performed decreasing similarity query vector weight vector units nli nli li li assigned software components 
relevant components appear top ranked list 
strong evidence assumption may example 
order determine efficiency respect retrieval relevant components consider queries shown table 
example query table describes need operating system command deletes contents files directories disks 
apparently universal command available ms dos 
commands closest requested functionality del rmdir 
del rmdir appear relevant components table 
remaining queries may similarly interpreted 
queries give commands expected relevant 
table 
test collection queries queries turn mapped grid output units winning unit queries determined 
depicts location winning units 
time final map applied different parameters learning process 
precisely utilized output space different initial value gain function different random sequence input presentation 
obviously final location commands clusters 
map assignment command output unit observed 
particular multiple assignments occurred 
replace append fc comp del mapped output unit respectively 
multiple assignments appear quite natural look functionality commands 
replace substitute files directory append provides utilization files current directory 
fc comp essentially functionality compare features query relevant components file directory disk delete contents del rmdir disk backup copy restore backup restore disk delete format format file directory disk compare comp fc directory display contents dir ieee files 
del complementary operations delete recover previously deleted files 
closer look location winning units corresponding queries find fourth query assigned relevant command 
winning unit neighboring relevant components fc comp 
please note comp assigned unit fc 
interested ranked list software components decreasing similarity query weight vectors units assigned components 
table provides ranked lists relevant components typed bold face letters 
particular table contains results selforganizing feature maps different learning parameters maps learning iterations map learning iterations 
results queries compared hierarchical cluster analysis applying ward method euclidean distance similarity measure 
similar results obtained complete linkage clustering 
seen relevant components appear top ranked commands self organizing feature map 
contrary cluster analysis completely fails uncovering similarity query relevant components del rmdir 

reported novel approach structure libraries reusable software components 
goal organize software library semantic similarity stored software components 
words functionally related components ought identified easily 
applied artificial neural network adhering unsupervised learning paradigm achieve goal 
input learning process artificial neural network software description directly derived manual 
pointed benefits user library 
firstly semantic relationship various software components explicit terms geographical neighborhood components 
secondly capability artificial neural network retrieving relevant components proved paramount conventional techniques cluster analysis 
furthermore series tests learning functions lateral inhibition output units summarized 
tests indicated lateral inhibition leads faster convergence artificial neural network better approximation input data 
authors indebted patient assistance shaping english 
due jan provided assistance time transferred software originally written pc sun workstation 
biggerstaff perlis eds 
software reusability 
volume concepts models 
volume ii applications experience 
reading ma addisonwesley 

jarke mylopoulos vassiliou 
software information base server reuse 
technical report 
institute computer science 
heraklion 
crete 
february 
kohonen 
self organized formation topologically correct feature maps 
biological cybernetics 

kohonen 
self organization associative memory rd edition 
berlin springer 

kohonen 
self organizing map 
proceedings ieee 

krueger 
software reuse 
acm computing surveys 

maarek smadja 
full text indexing lexical relations application software libraries 
edit format backup assign mirror restore 
recover tree chdir 
copy 
cls rmdir join mkdir 
del type path date time fc replace dir mem attrib ren find winning unit winning unit winning unit winning unit winning unit 
map learning iterations legend ieee table 
ranked results cluster analysis rmdir mkdir append join del del rmdir mkdir rmdir del cls mkdir format backup restore backup restore backup restore backup copy restore mirror recover format assign format assign format mirror format assign cls comp fc del replace dir path dir comp fc tree replace fc comp dir path comp fc attrib ren find type dir fc attrib comp type dir fc attrib comp dir replace path chdir type type dir chdir tree replace proceedings th int acm sigir conference research development information retrieval 

maarek berry kaiser 
information retrieval approach automatically constructing software libraries 
ieee transactions software engineering se 

merkl tjoa kappel 
structuring library reusable software components artificial neural network 
proceedings nd int conference achieving quality software 
venice 
italy 

merkl 
structuring software reuse case self organizing maps 
proceedings int joint conference neural networks ijcnn 
nagoya 
japan 

merkl tjoa kappel 
application selforganizing feature maps lateral inhibition structure library reusable software components 
proceedings ieee int conference neural networks icnn 
orlando fl 

merlo de mori 
source code informal information analysis connectionist models 
proceedings th int joint conference artificial intelligence ijcai 

france 

de mey nierstrasz 
ithaca application development environment 
visual objects tsichritzis ed 
centre universitaire informatique 
university geneva 


reusability 
modern software engineering foundations current perspectives ng yeh eds 
new york van nostrand reinhold 

mylopoulos borgida jarke koubarakis 
telos representing knowledge information systems 
acm transactions information systems 

hendler prieto diaz braun 
computing similarity reuse library 
acm transactions software engineering methodology 

prieto diaz freeman 
classifying software reusability 
ieee software 

ritter kohonen 
self organizing semantic maps 
biological cybernetics 

salton mcgill 
modern information retrieval 
new york mcgraw hill 


similarity analogical software reuse conceptual modeling approach 
proceedings th int conference caise 
berlin springer lncs 

turtle croft 
comparison text retrieval models 
computer journal 

