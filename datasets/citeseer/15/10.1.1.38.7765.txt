mit media laboratory technical report appears mit thesis degree master engineering electrical engineering computer science supervised picard image database browser learns user interaction thomas minka vision modeling group mit media laboratory ames street cambridge ma media mit edu digital libraries images video rapidly growing size availability 
avoid expense limitations text considerable interest navigation perceptual automatically extractable attributes 
unfortunately relevance attribute query obvious 
queries go explicit color shape positional cues incorporate multiple features complex ways 
dissertation uses machine learning automatically select combine features satisfy query positive negative examples user 
learning algorithm just learn course session learns continuously sessions 
learner improves learning ability dynamically modifying inductive bias experience multiple sessions 
experiments demonstrate ability assist image classification segmentation annotation labeling image regions 
common theme applied computer vision database retrieval machine learning building flexibility allow adaptation changing goals 
supported part bt hp ibm 
contents visual indexing hard browsing similarity measures browsing relevance feedback foureyes extension photobook applications example user interfaces attribute selection pattern recognition user adaptive image segmentation bias selection machine learning malleable solutions ill posed problems retrieval annotation multiple similarity measures dual view retrieval document retrieval image retrieval vector quantization foureyes combining multiple similarity measures query formulation problem dual view applied annotation constructing groupings images learning image annotations machine learning formalism classical algorithms set covering decision list decision tree experiments brodatz collection vistex collection bt collection bias selection historical trends bias selection continuous learning vocabulary weighting vocabulary modification hierarchy improvement algorithm experiments related summary segmentation ground truth chapter digital libraries images video rapidly growing size availability 
visualization tools advancing database systems human generated text annotations worse flip books basic navigation 
annotations costly produce ineffective kinds search 
ineffectiveness due ambiguous wording busy scene data picture worth words lack vocabulary word visual texture fast moving cars night 
inadequacies searching digital image data emerging major cost factor multimedia applications 
section examines looks alternatives pursued researchers practitioners discussion motivate design philosophy foureyes extensible power assisted self improving database browser annotator 
visual indexing hard data visual important able navigate directly perceptual features data 
example finding particular texture 
useful able automatically extract annotations data 
tasks plagued difficulties 
research capability mimic human perception general contexts 
computational models isolate perceptual component texture succeeded benchmarks brodatz collection prove naturally occurring imagery textures may occluded perspective warped surface different scales different colors contrast different lighting 
consequently perceptual models highly data dependent see example 
unfortunately nature specialization rarely clear definition model 
see attempt 
clear separation perception semantics image similarity 
right notion similarity changes depending context particular image particular database particular user particular task see example 
top subjectivity common database requests require complex interactions multiple image attributes 
example stock photography agency may get request dry open shot weird angle threatening forest repeating pattern group people reflecting rippled water 
unsolved problem derive annotations automatically offer database navigation kinds abstractions 
fielding requests require close coupling perceptual modeling semantic modeling domain knowledge gleaned sources diverse computed attributes text annotations usage patterns subjective opinions 
mrsar ev data dependent performance texture models 
patterns right retrieved similarity pattern left particular model space ev mrsar 
multi resolution simultaneous auto regressive mrsar model attempts model fixed size neighborhoods misses high level structure shift invariant eigenvector ev model 
images mrsar substantially better ev 
brick straw matting oriental brick image similarity perceptual semantic 
influenced culture visual context user preference 
computational models notice similar horizontal vertical structure 
model may decide brick images similar model may decide top images similar 
people arrive different decisions depending goals 
browsing similarity measures common approach undertaken today attach real valued attributes visual data attribute parameter perceptual semantic model 
distance function euclidean mahalanobis attributes define measure image similarity 
euclidean case image patch sequence maps point possibly high dimensional space nearby points similar way 
user chooses similarity measure navigates space dimensional display represents distance prototype dimension common 
prototype datum may selected database provided user sketchpad 
examples approach qbic swim core photobook 
unfortunately approach scale arbitrary content relies crucially similarity measure user ability choose 
designing measures extremely hard difficulties identified apply 
furthermore user asked select data dependent model context dependent phenomenon knowing extent dependencies 
knowing combine models achieve complex task completely question 
short approach navigation expects users project intent information need limited set opaque algorithms 
browsing relevance feedback foureyes attempts automate transformation intent algorithm rich example interaction user 
recognizing user desires modeled chooses resolve ambiguity late possible 
representing similarity groupings high dimensional spaces foureyes provides lingua franca diverse information sources allowing readily combine complex controllable ways chapter 
selection combination process furthermore controlled learning positive negative examples user 
example objects cars telephones kind images looking aren kind interaction known relevance feedback common document retrieval 
foureyes puts emphasis learning select combine similarity measures designing understanding 
making user interaction easier approach eases development browsers newly discovered measures added invisibly user 
potential completely take burden modeling complex phenomena powerful scenes powerful learning mechanism 
fortunately mechanisms generalpurpose constructed known parts 
algorithms machine learning literature adapted sets foureyes grouping representation demonstrated chapter 
foureyes scenario number advantages conventional learning scenarios 
examples tend directed useful learner typical pattern recognition problem examples chosen randomly 
second data may utilized times similar purposes allowing learner usefully integrate training multiple users uses 
foureyes self improve 
idea continuous learning demonstrated chapter significantly improve learning performance foureyes usefulness retrieval annotation 
foureyes extension photobook foureyes interface intended allow selection image regions requiring user carefully outline region interest 
paradigm similar perceptually organized editing program 
paradigm single object hierarchy conventional paint programs traded multiple possibly conflicting organizations 
amount structure imposed system mediated example interaction user 
image organization process discovery system user 
example search photobook 
user queried images similar upper left space 
ordering dimensional displayed raster scans 
image image database image groupings image groupings basic task image database retrieval annotation tools addressed foureyes recovering useful image image groupings 
note generally captured single model single partition hierarchy similarity measure required induce groupings may quite complex 
user indicate region interest line drawing making gesture similar shape 
foureyes user indicates region interest arbitrary color image tapping points mouse clicking sweeping path region 
touched pixels positive examples system immediately attempts extrapolate precomputed image image groupings provided image similarity measures computation described section 
negative examples pixels part region entered fashion different mouse button 
components foureyes shown 
foureyes forming compound grouping known groupings union set cover arbitrary set operations decision list decision tree 
third stage closest user described chapter 
user satisfied system selection foureyes updates bias weights groupings groupings described chapter 
backwards learning arcs 
cause groupings form selection favored time selection cases single tap needed region part region operated 
foureyes automatically chooses combines constructs appropriate similarity measures segmentation 
foureyes allows attachment label selected region 
attachment part example interaction annotation image regions database 
segmentation precomputed groupings image regions time images selected combined extrapolate new annotations 
attachment label adds positive example label negative example labels 
exclusivity assumption correct greatly reduces total number user examples needed get satisfactory labeling database 
attached labels generate context dependent semantic keys querying retrieving database contents 
conceivable operations selection region labeling include paint tools modifying color moving export database tools retrieving similar regions special case labeling pasting image 
task foureyes determine single correct similarity measure correct grouping database regions correct segmentation image 
chapter contribute rich repertoire reasonable groupings 
chapter select combine groupings match example set groupings multiple measures necessary 
chapter learn weighting groupings alter groupings useful ones recovered examples 
foureyes computer assisted annotation tool 
user mouse clicked patches sky right images assigned label sky 
image groupings allowed foureyes grow labeled patches larger sky regions indicated image groupings allowed foureyes place tentative labels left images 
menu buttons allow user control sets groupings available learner 
model features hierarchical clustering sc dl dt features hierarchical clustering manual groupings user data groupings compound weighted groupings groupings examples pos neg generation weighting collection slow stable fast plastic grouping grouping grouping model learning learning weight size multi layer learning structure foureyes 
arrow bottom describes rate stages learn 
stages detailed chapters respectively 
appropriate performance metric number examples required user satisfied response 
assumes course possible generalizations user examples equally valid assistance provided 
challenge foureyes determine likelihood function submit responses accordance 
additional constraint occur interactive time 
saving wall clock time database access objective system processes examples second requires examples order magnitude better system processes examples minute requires examples 
foureyes stage method meeting goals differs conventional attribute extraction followed object classification crucial ways 
feedback arc classifier attributes performed computer designer 
avoids usual human cycle trying lots classification rules lots attributes trying find combination best problem hand 
second stage develops different times different rates stages closest user changing fastest 
allows computations distributed time space facilitating interactive incorporation complex models 
third training accumulated sessions user system improves time solve similar problems better learn faster time 
applications example user interfaces foureyes viewed user interface agent 
agents automate repetitive tasks fill niche missed macros scripting learning examples 
foureyes repetitive task grouping pixels objects objects classes 
conventional programs support considerable user effort 
grouping done scripts complex image processing tools world know perceive 
macros grouping commands provided paint program hand brittle capture level abstraction point view 
text manipulation indexing solved generally sought images usable text 
example text editor user switch levels abstraction character word sentence paragraph keystroke 
search word database including misspellings synonyms 
numerous obstacles achieving kind efficiency multimedia 
far levels abstraction data perceived 
data decision classification attributes extraction rule utility system designer user attribute pattern recognition approach foureyes extends utility feedback arc notion misspelling synonym harder quantify 
see example 
second automatic modeling human perception unsolved requiring degree user guidance 
raise standards text manipulation example domain meaning find difficulties 
foureyes presents approach interaction multiple levels abstraction multiple media purely example metaphor 
positive negative examples automatically choose combinations known groupings induced similarity measures previous users 
grouping representation lingua franca groupings come diverse information sources interaction section 
example learning supported continuous self improvement allows rapid natural specification level abstraction 
attribute selection pattern recognition solutions pattern recognition problems generally described stage process see section 
raw input data projected suitable attribute space decision rules applied attribute values 
example rule red apple color representation input 
realistic application distinguishing various letters alphabet locations edges paradigm 
model useful suitable attributes developed methods finding optimal decision rules 
determination attributes remains 
posed problems possible common designer experiment different set attributes pick best test set fix stage 
system put practice applied specific problems decision stage adapted automatically 
unfortunately conditions approach insufficient 
available test data may insufficient determine correct attributes 
second designer may find single set attributes gives acceptable performance problems interest 
third problem may ill posed set acceptable solutions dynamic dependent context little known context system placed 
conditions important system adapt attributes addition decision rule 
decision rule readily adapted problem problem attributes adapt slowly 
reason attributes constitute important bias chapter system assumes world sees problem 
chosen properly bias allows learner quickly accurately find decision rule 
example usually date see object judging apple 
may change fruit month club 
witnessing number tasks drawing analogies system reason alter bias 
attribute extraction stage change response evolving needs decision stage 
accomplished adding missing feedback signal decision stage attribute extraction stage 
foureyes demonstrates way add feedback signal hierarchy improvement algorithm section 
implementation continuous learning tries push complexity decision rule back attributes 
user adaptive image segmentation foureyes part step progression image segmentation tools 
progression moving optimization blackboards user guidance marked increasing utilization domain knowledge context retaining ambiguity image 
segmentation approach existing research region competition attempts optimize general purpose fitness function region interiors region boundaries 
algorithm strives simplest segmentation smoothest interiors smoothest boundaries 
intractable optimization problem solved approximate method region growing 
algorithm scatters seed regions randomly image iteratively grows way incrementally produces greatest increase fitness function 
optimization approach segmentation powerful various kinds image attributes inserted optimization function 
principle segmentation realized proper choice optimization function aside limitations approximation method region growing 
general purpose image segmentation remains unsolved 

optimization model provides capability encoding user desires straightforward 
problem database browsers 
segmentation ill posed fixed algorithm satisfy 
true solution provide infinite set algorithms mapping tasks algorithms 
quality solution dependent quality mapping 
unfortunately optimization algorithms region competition bury parameters split merge smoothing thresholds conspire alter resulting segmentation 
designer algorithm hard pressed transform segmentation task suitable set parameters 
example applies sophisticated optimization algorithm simply find right parameters phoenix segmentation algorithm images taken different conditions 
phoenix uses histogram splitting top segmentation split chosen optimize fitness function 
interestingly important parameters fitness function size histogram smoothing window min max threshold histogram peak determination 
quite far removed goals segmentation application 
consequently application goals change hard know adapt segmentation parameters 
concise statement complaint segmentation fitness functions weak connection domain knowledge 
domain knowledge information hand express kinds segmentations want interested segmenting people people usually limbs head 
problem optimization algorithms strive destroy ambiguity inherent image 
automated tasks robot tanks driving mph disambiguation necessary decision making 
power information tools user ability view data different ways 
important database application machine decompose image multiple ways ordered probable utility user 
approach addresses complaints context vision practiced condor 
condor works updating blackboard segmentation classification hypotheses explicit rules creating removing hypotheses 
rules specify variety specialized algorithms agents consulted produce hypothesis 
condor robust apply agent proper context contingent 
note agents may arbitrarily complex region competition 
rules specify conditions exclude hypothesis consideration 
provides direct representation domain knowledge sky ground 
specialized agents encode domain knowledge 
kind control available optimization techniques 
termination blackboard contains segmentations suggested agents consistent domain knowledge 
provides desired ambiguity 
program employing similar approach geared interactive applications perceptually organized graphical editor 
agents propose segmentations rules eliminate hypotheses hypotheses available user 
navigating menus forced cope imposed structure user indicates desired segmentation just giving gestural examples 
image manipulated objects pixels full ambiguity 
domain knowledge provided agents 
foureyes continues trend direction adaptive segmentation tool strives improve segmentation performance 
foureyes employs agents build hypotheses mediated user differs agents learn experience 
foureyes synthesize new segmentations user request combining hypotheses suggested agents manual outlining 
synthetic hypotheses part tool suggested agent 
way user simply observe blackboard condor may contribute 
bias selection machine learning learning induction black magic 
learners having bias supplies information missing training data chapter 
success learner hinges bias 
unfortunately learning algorithms obvious bias clear bias appropriate 
example parameters neural network genetic learning constitute bias weak connection domain knowledge seldom clear change match particular problem domain 
learners similarity measures indexing image segmentation algorithms solutions ill posed problems 
robust learner bias flexible possible changed dynamically objective determined way foureyes indexes multiple similarity measures learning user 
idea continuous learning component foureyes described chapter allows integrate training multiple uses determine bias 
malleable solutions ill posed problems common theme thesis database retrieval computer vision pattern recognition machine learning solutions ill posed problems malleable easy adapt changing requirements 
solving posed problem adding 
requirement parallels software engineering written programs easy adapt 
cases correct solution fixed program set powerful tools language combining 
order adaptation easy language straightforward predictable sufficient 
easier adaptation useable solution 
example learning algorithm foureyes designed parameters groupings direct correspondence user desires 
easy automatically change parameters track user section 
chapter retrieval annotation multiple similarity measures retrieval annotation complicated abundance attributes fulfill query irrelevance available attributes 
attribute combination attribute selection problems respectively 
chapter describes dual view retrieval examples document retrieval image retrieval helpful attacking problems 
dual view comes fact items retrieve represented complementary ways points arranged metric space members fuzzy sets 
relevance feedback crucial attacking problems 
foureyes incorporates relevance feedback purpose annotation labeling image regions 
presence relevance feedback dual view instrumental allowing attribute selection combination handled rule induction methods machine learning 
dual view retrieval document retrieval generalized retrieval problem discussed user wants find document containing certain terms 
term single word rocket multiple words rocket fuel national aeronautics space administration 
terms indexing decided database populated 
fulfill request retrieval system gives document relevance score presents user order relevance 
approach achieving table maps documents term vectors 
vector component intended encode strength connection document term relevance document user searching term document term normalized frequency occurrence term document 
vectors interpreted rows document term connection matrix see section 
system assigns document relevance score similarity terms query connection strength terms document 
query represented term vector component quantifying user desire term simple dot product document corresponding matrix vector multiply database accomplish task 
alternatively query specified boolean formula terms case complicated similarity relation 
case documents represented points metric space 
example may cluster documents accelerate search 
approach table maps terms documents 
corresponds storing document term connection matrix columns rows 
column interpreted fuzzy set documents connection strengths indicate degree sets attributes items points dual view database items points metric space members fuzzy sets 
document term rocket fuel nasa query scores retrieval document term matrix term documents rocket fuel nasa query rocket fuel output half membership retrieval document sets membership 
output system relevance scores interpreted fuzzy set documents 
boolean queries correspond fuzzy set operations columns require scanning document entire term vector approach 
clustering documents representation allows cluster terms automatic thesaurus generation 
membership values term column define similarity measure documents 
example rocket document similar 
placing particular terms query user selecting measures combined get relevance scores 
sense document retrieval interaction identical photobook interaction section 
image retrieval basic problem computer vision extracting useful information visual data 
document retrieval desired attributes seldom raw components data functions local regions entire image 
unfortunately kind information robustly extracted tiny subset information 
compute attributes correlate want restricted domain interest 
computer vision approach retrieval user wants find image certain attributes 
attributes computed pixel values may correspond local color texture matched filter response shape arrangement regions defined attributes iterations analysis rare 
fulfill request system gives image relevance score presents user order relevance 
approach photobook 
difference problem document retrieval problem attributes 
document attributes scalar image attributes typically vectors 
example color may described histogram pixel values 
analogous image attribute connection matrix vectors cells 
query represented row vectors scanning vectors compared scalars 
comparison done vector product euclidean mahalanobis bayes distance unfortunately requires touching entire database document retrieval example 
efficient column approach modification 
fuzzy set interpretation attributes requires scalar membership values vectors 
transformation achieved vector quantization retrieval attributes quantization bins original image attributes 
grouping new scalar attributes viewed fuzzy set 
word group distinguish group theory 
vector quantization unique optimization metric different quantizations different groupings possible single image attribute 
quantizations best unknown retrieval time heuristic preferences 
solution compute quantizations attribute store resulting groupings 
solution periodically re quantize basis usage patterns 
vector quantization foureyes foureyes allows row column approach hierarchical clustering quantize visual attributes 
example shows textures clustered foureyes basis mrsar texture model 
document term matrix output hierarchical clustering interpreted ways 
new metric images distance images corresponds height lowest common ancestor section 

collection sets node tree set images node 
hierarchical clustering mrsar photobook raw similarity measures uses representation 
second representation benefit speeding database access raising recall rates automatically introducing visual vocabulary providing basis fusion diverse similarity measures 
hierarchical clustering obtained foureyes shared neighbor algorithm 
define nearest neighbor nearest data points vector space 
define partition induced equivalence relation nearest neighbors nearest neighbors common 
hierarchical clustering obtained sequence 
defines hierarchy points distinct points merge sufficiently large 
algorithm single link method clusters merge hierarchy soon members get close 
opposed complete link methods merge clusters members close 
complete link clustering similar minimizing mean squared error popular means algorithm 
expected give best preservation distances original vector space 
reason believe desired perceptual groupings occur equal sized spheres attribute space 
single link clustering preferred application capture broader range structure 
example shared neighbor algorithm separate concentric rings parallel surfaces spheres differing sizes 
single link clustering tends group areas similar density opposed complete link methods seemingly arbitrary cuts regions constant density 
advantages single link clustering appropriate perceptual problems demonstrated literature see section 
heuristic doubt add additional quantizations mentioned earlier 
metric interpretation resulting hierarchy yields distance function similar provided original vector space 
unnecessary distinctions images removed 
consequence retrieval hierarchy faster row scanning database 
furthermore single link algorithm extract nonlinear structure linear vector space improving performance cases 
drawback single link method flexibility defining clusters easily distracted noise 
see phenomenon different context section result weak inductive bias 
set interpretation hierarchy useful 
view grouping hierarchy node term basic set operations answer queries concerning terms column approach document retrieval 
groupings form foureyes vocabulary images 
similar process automatic thesaurus construction document databases 
unclear new visual vocabulary manually formulating queries 
automatic approach formulation needed 
return issue 
top performer mrsar places bricks water feature space 
retrieval results photobook left image query images ordered mrsar similarity measure 
vector distance get mixing textures 
clustering texture groups separate 
system allowed arrange images group 
document measure measure fusion relevance rank relevance rank min max avg data fusion ranks combining multiple similarity measures unreasonable expect photobook database browsing involve single attribute time 
visual domain low level attributes picture conspire generate attributes call grass buildings threatening scene 
order navigate user desired level abstraction attributes need considered simultaneously 
furthermore existing visual attribute mimics human perception general imagery 
multiple similarity measures individually specialize image domain cooperate simply provide color texture navigation 
short human desires rarely map single extractable visual attribute 
basic issue combining multiple measures known data fusion comparison 
measures linear tradeoff simply combined composite vector 
fact constitutes definition feature vector collection dimensions known exchange rates 
unfortunately color texture texture attributes generally exhibit linear exchange 
mere concatenation just easily degrade performance enhance 
important step combining measures subject nonlinear transformation commensurate 
transformation terms document retrieval rank document ordered relevance score 
example documents assigned relevance scores different measures 
combine scores differ range mean variance combine ranks minimum rank average rank get new measures documents may ordered 
document retrieval shown give substantially better results measure 
similar adaptive method achieved synergy texture models 
ranks combined unequal weight determined measure periodicity texture 
weights determined priori careful selection models combined 
hierarchical clustering allows foureyes take different approach 
quantization transformation measures commensurate 
metric interpretation hierarchy ranks gives neutral notion distance 
benefit emergent clusters shattered hierarchy metric ranks discriminate points cluster 
set interpretation hierarchy allows hide fact doing fusion 
consider sets hierarchies part big amorphous collection sets form normal queries expanded vocabulary 
approach foureyes 
places burden query formulation discussed 
benefit hierarchies fusion include measures defined vector space 
example include clusterings directly input users derived usage patterns chapter 
important note attributes people want navigation computed image data 
involve unsolved problems modeling perception subjective solely current user collaborative judgements users 
technique collecting non computable attributes allow users database attach annotations 
general purpose retrieval systems attempt fusion prepared large number irrelevant attributes may necessarily represented outset vector space 
query formulation problem remaining problem solutions require users translate information need query 
specifically means know relevance terms visual attributes need 
example user wants know nasa research just want find documents containing exact word rocket probably propulsion spacecraft different weights 
methods explored document retrieval alleviate burden 
thesaurus add new terms query query expansion 
second terms rarely occur documents automatically increased importance 
third user indicate documents relevant relevance feedback section allowing term weights automatically modified 
methods rare visual retrieval apparent reason 
belief remains computed visual attributes mrsar ev sufficiently intuitive 
problem retrieval solutions describe documents images literal terms pixel values contain 
document term matrix generalized document attribute matrix attributes nasa joe liked written lots pictures 
applies images users may want dry open shots powerful lines 
arbitrary attributes significantly improve perceived performance queries users may just irrelevant 
fact addition significantly worsens query formulation meaning attributes obscure subjective virtually infinite number attributes possible 
relevance feedback potential alleviate problem attributes weights deduced examples 
initial query formulation user skipped altogether favor examples give documents 
advantage user need give formula want need know terms extracted documents attributes available 
system select irrelevant possibly subjective attributes best ones assigning relevance scores 
realizes performance advantage specialized attributes burdening user 
documents attributes attached attribute selection kind information retrieval problem 
natural computer assist user task 
dual view applied annotation semi automatic annotation image regions patches closely related retrieving images 
annotation merges seamlessly relevance feedback tool constantly suggests user corrects 
interaction pure example flavor recommended 
annotation chosen vehicle research results apply retrieval 
vector space set labeled patches heuristics applied propagate label new patches 
example label spread patches inside convex hull defined labeled patches 
alternatively label spread patches closer labeled patch labeled patch closer farthest labeled patch 
various classification algorithms apply 
foureyes metric interpretation groupings hierarchy nodes 
set interpretation groupings different set heuristics emerge 
set patches receive label determined set operations groupings 
possible rule take union sets containing labeled patch 
takes minimal union needed include labeled patches 
methods directly analogous vector space methods difference conceptual 
foureyes focuses approach 
employs methods rule induction machine learning sets 
note automatically allows annotation multiple similarity measures hierarchy combination method 
transformation annotation problem rule induction problem detailed chapter 
constructing groupings images groupings building blocks labelings constructed 
grouping set image regions patches associated way 
webster definition grouping set objects combined group 
elements grouping may necessarily come image 
representation useful admits different kinds associations adding complexity 
example set may represent regions containing blue pixels may represent regions containing may represent regions browsed week 
allows specific associations patches weighted independently set may weight 
important example may best grouped shape sky may best grouped brightness location image 
notion grouping including generation methods described apply kinds data just images 
foureyes computes image groupings similarity measure color texture steps illustrated 
dense feature image computed source image 
point feature image feature vector color histogram computed neighborhood corresponding point source 
normally source image color images sequence source image optical flow data 
feature image ideally resolution source may coarser depending computational constraints 
second coarse feature image computed computing neighborhood average covariance 
step segmentation performs local smoothing obtains feature covariances mahalanobis similarity step 
third coarse feature image hierarchically clustered shared neighbor algorithm produce image groupings 
typical image size theta coarse feature image size theta 
size reduction significantly reduces number possible groupings leaves choose subsets elements patches grouping need connected image 
result stage hierarchical set image regions image measure 
may directly segmentation step computation image groupings 
image groupings computed hierarchical clustering attribute measured image groupings 
image groupings need correspond attribute image grouping may come optical flow manual outlining 
image groupings simply provide information image regions usefully taken 
example image grouping utilizes face detection produce segments containing faces image grouping face classifier 
advantage incorporating image relationships image annotation described 
color annotation image regions demonstrated clear qual image feature image coarse feature image feature local estimation mean std local hierarchical clustering image image feature space groupings image image grouping image grouping hierarchical clustering groupings new groupings computing image image groupings 
image grouping contains contains house door house window door 
projected feature space considered individually look different 
resulting clustering says looks ity improvement scene adaptive class thresholds preserving continuity image class likelihood histogram fixed universally optimized thresholds 
foureyes approximates behavior forming image groupings image groupings 
shared neighbor clustering algorithm foureyes behaves similarly histogram splitting image groupings generated methods similarly preserve class likelihood continuity 
image image groupings computed line user begins interaction system 
slow learning stage separate fast learning stage user important practical implementation real image database retrieval system 
example clustering happens line perform extensive cross validation noise sensitivity stability checks possibly utilizing different algorithms 
level evaluation currently infeasible line line allows state art results pattern recognition incorporated improving system performance 
feature extraction routines run line likewise larger neighborhoods accurate estimators diversity 
new feature extraction clustering methods developed independently components 
engineering concerns important construct real systems 
disadvantage precomputing groupings recomputed novel image added data set 
foureyes done full reclustering similarity measures faster gradual incremental algorithm 
queries occur orders magnitude additions database interactive speedup offset recomputation cost 
stages system see groupings feature values necessary continuous similarity spaces 
example advantageous incorporating subjective associations content 
humans easier specify groupings image regions attach meaningful consistent attributes 
chapter learning image annotations chapter showed different similarity measures placed common vocabulary groupings 
chapter describes user input form positive negative examples transformed query labeling rule terms groupings 
viewing problem concept induction ai sense apply standard machine learning algorithms terms set attributes 
adapted algorithms set covering decision list decision tree described 
chapter drives home point learning performance hinges bias learner 
robust solution choice flexible possible adapt runtime 
extension explored chapter 
training set test set attribute category 
pattern classification task take moment consider classification problem taken 
determine categories test patterns 
know right answer 
common assumption simplest rule best case attributes making test patterns category rules consistent training set assume training patterns noisy attribute values randomly flipped rules possible 
case explanation choosing xor bias simpler rules 
section formalizes learning process 
machine learning formalism instances members universe interest 
instance defined attributes describe 
set instances class positive examples instances class negative examples task determine target concept full set instances class 
concept just set instances may may target concept 
problem ill posed unique answer 
set instances consistent contains positive examples negative examples candidate target concept 
set instances infinite nearly infinite set candidate concepts amount examples practical amount examples pin target universe concept examples examples candidate target concept positive negative attribute attribute illustration machine learning terms 
concept basis consistency 
possibility noise examples attributes may prevent concepts excluded 
fortunately learning problems interest case candidate concepts equally 
demonstrated fact humans regularly solve ostensibly ill posed learning problems 
problems ill posed taken context chapter 
unfortunately exactly happens learning problems computer 
issue ill posedness lack context 
context provider inductive bias reason choose candidate concept :10.1.1.19.5466
classical algorithm concept learning choose candidate concept favored bias 
methodology analogous bayesian inference likelihood function serves isolate candidates prior provides bias :10.1.1.27.9072
important axes biases vary strength precision accuracy correctness 
strong biases focus learner relatively small number concepts 
assigning equal probability candidate concepts weakest possible bias 
weakness bias analogous entropy prior probability distribution 
intuitively speaking express amount priori uncertainty target concept 
strong biases accurate target concept focus strong accurate biases best biases learner 
assigning nonzero probability concept strongest possible bias slim chance accurate 
bias assigns nonzero probability concepts accurate 
definition accuracy weak 
learning algorithms variety means specify bias 
include restricting space concepts employing particular search algorithm finding favored candidate 
declarative procedural biases respectively 
common bias allow concepts expressible concept grammar disjunctions attributes set instances attribute attribute 
hazardous exclude xor equivalent grammar get right answer problem introduced chapter 
approach reduce number attributes reducing number instances sets instances concepts 
restriction concept vocabulary 
example problem attributes irrelevant general equally hazardous know priori attributes remove 
objective designing learning algorithms provide useful bias giving generality 
classical algorithms context annotation assume absence noise finite number distinguishable candidate concepts 
situation learner isolate target concept examples regardless bias 
called nonparametric unbiased robust methods exploit situation requiring little knowledge problem training examples 
applications including examples costly 
proper bias selection needed performance measured terms accuracy concepts number examples required achieve concept learning efficiency 
algorithms concept grammar bias occam razor 
attempt choose candidate concept shortest representation particular grammar 
choice grammar guided belief teacher grammar mind grammar serves convenient means expressing preference ordering large set concepts 
methodology analogous expressing bayesian inference terms minimum description length inventing suitable notion description :10.1.1.27.9072
concept vocabulary available algorithms groupings derived hierarchical clustering chapter 
suitable vocabulary described grammars sufficient encode concepts 
number candidate concepts set examples grammars 
traditional analysis methods unable distinguish grammars vc dimension infinite accurate 
shown significant performance differences 
explained 
observation grammars differ ways express particular concept concept defined set instances set may constructed expressed way 
capture difference rule particular expression concept predicate order logic 
may different rules denote concept 
confusion literature difference rule concept see importance distinction 
terminology say algorithm differs rules particular concept 
direct effect precision shall see rules denoting concept may different sizes 
effect accuracy problematic 
target concept candidate authors say biases accurate informative 
better definition include preference assigned target concept average size rules represent 
left research exercise 
set covering set covering algorithm represents concepts set union 
assortment sets choose algorithm job find smallest union sets covers includes positive examples negative examples 
problem np complete variety greedy methods get close optimum covering 
example algorithm sc 
choose set covers positive examples negative examples 
resolve ties coin toss 

remove positive examples covered consideration 

positive examples remain go back step 
greedy algorithm iteration take action gets closest goal 
resulting concept union chosen covers 
algorithm succeed long way progress iteration set covers positive example negative examples 
guarantee sure contains possible sets containing instance singleton sets 



concept concept concept concept decision list test instance contained concept 
instance direction determined decision 
decision checks membership particular set contains positive examples negative examples point list 
consequently decision classification set instances happen possible ways see table 
guaranteed algorithm produces candidate concept target concept 
candidates chooses smallest terms number sets merged 
inductive bias 
bias appropriate usual depends learning task 
fact algorithm approximation adds bias case know approximation error great 
decision list requirement sc step cover negative examples somewhat harsh asymmetric 
decision list algorithm chooses sets include positive negative examples include negative positive examples 
complements sets may explicitly contained resulting rule form list decisions outcome decision determines classification set instances see 
decision queries membership particular set outcome classifies instance passes instance decision 
decision list completely described list pairs element pair indicates set membership test second element indicates instances inside vs outside classified classified concept 
possibilities corresponding rows table instance 
class 
outcome doesn match column instance passed decision list 
cases cover complementation decision list constructed incrementally greedy algorithm dl 
set examples positive negative say concept concept respectively 

choose assigns correct classification examples add decision list 
resolve ties coin toss 

remove examples classified consideration go back step 

concept concept 
concept concept 
decision tree test instance contained concept 
decision list instance direction determined decision 
decision checks membership particular set may set divides instances parts 
require singleton sets included guarantee solution 
sc special case dl 
dl produce rule sc 
know sc output target concept extra rules redundant 
learners bias shorter rules redundancy causes dl weaker bias fixed rule size candidate concepts expressible decision list set cover 
difference demonstrated experiments section 
decision tree decision list algorithm requires examples removed consideration step loop 
means decision sufficient determine classification set instances 
relaxation constraints allows learner choose decision simply best division examples parts may require division 
yields greedy algorithm builds decision trees see 
methods re going stopping pruning criterion halt tree growth interactive setting learner expected classify training points correctly 
difference dt uses set membership decisions raw attribute values 
criterion selecting decision softer version decision list 
set divides examples members gamma 
intuitively want decision minimizes impurity ideally positive examples negative examples vice versa 
consider examples data points value zero positive negative respectively 
useful definitions impurity sample variance sample entropy independence assumption splitting rules suggested ffl fraction positive examples ffl po fraction positive examples ffl gamma ffl sample variance ji jo ffl gamma log gamma ffl sample entropy ji po jo ffl impurity sample variance entropy example value negative impurity set contained 
value decision average value example 
example covered contains positive negative examples negative sample variance gamma gamma contribution value dt algorithm 
set examples positive negative return leaf concept concept respectively 

choose set largest value defined 
resolve ties coin toss 
create tree node 
recurse separately examples sets yield subtrees difference experiments variance entropy simpler variance rule chosen 
dl special case dt impurity zero meaning pure meaning mixture positive negative 
dt general learning schemes 
performance actual problems depends closely bias weaker dl sc suits domain 
experiments algorithms tested independently simulated annotation environment different data sets 
case learner sc dl dt algorithm number hierarchies groupings see chapter draw sets 
hierarchies correspond algorithms 
learner began instances images patches unclassified 
learner trained repeatedly querying classes instances tallying errors choosing erroneously labeled instance random informing learner proper class instance 
similar way training occur user quite different conventional pattern recognition classification done comparing heavily prototypes feature distributions line feedback 
learner gets examples relevant getting arbitrary line selection examples 
step instance unclassified learner scored error instance misclassified learner scored errors blind guessing disadvantageous 
formula error count false negatives false positives 
progression error counts forms learning curve algorithms judged 
learner remembered examples received reconstructed candidate concept scratch new example 
learner assumed classes disjoint positive example class negative example classes 
facts imply learner converge zero error steps number instances 
minimum number examples required equal number classes learner see member class order speculate 
experiments disjoint class assumption valid significantly reduced number examples required learn classifications 
modifications allow non disjoint classes 
assumption abandoned outset 
unnecessarily large number examples required classes really disjoint 

assumption abandoned contradiction detected 

classes meaningful names leaves foliage domain knowledge provide clues overlap 
errors number training examples random ev mrsar brodatz learning performance sc different sets groupings 
faster curve drops better performance 
randomness teacher selection examples learner selection concept described shape final point curves varies little multiple runs 
stability largely due line example selection 
results run shown average suppress variation error run 
brodatz collection data set consisted images digitized monochrome brodatz texture album 
textures album equally divided theta non overlapping images 
desired classification corresponded disjoint groups containing original texture classes 
members class chosen random consequently learning difficulty similar learning textures individually 
fact images belong class means large number candidate concepts example sets making learner bias important 
experiments sc algorithm single hierarchy time shown 
experiment provides baseline learner available hierarchies 
total groupings excluding singleton groupings 
knowledge sc required examples reach zero error call learning time 
second experiment hierarchy generated clustering images ev features hierarchy contained groupings 
knowledge reduced learning time examples 
third experiment hierarchy generated clustering images mrsar hierarchy contained groupings 
mrsar demonstrated excellent matching performance database earlier experiments expect learning proceed faster 
case learning time examples 
fourth experiment hierarchy brodatz explicitly contained textures groupings groupings total 
learner quickly exploited knowledge obtaining time examples 
get zero error minimum number examples learner available correct groupings bias favors groupings 
case explored chapter 
sc efficient learning algorithm 
groupings hierarchies errors number training examples random ev mrsar brodatz learning performance dl different sets groupings 
simultaneously learner processed examples cpu second benchmarked groupings patches process examples cpu second hp workstation 
time complexity constructing candidate concept example set constant times product number examples number trees height tree directly dependent total number groupings total number instances 
time complexity retrieving instances candidate concept linear size concept 
tree heights experiments 
experiments repeated dl dt algorithms figures 
note significant variation performance examples noise curves 
learner resolves ties coin toss ties dl dt sc weaker bias 
interactive application noise quite disturbing requiring special modifications 
example learner repeatedly queried candidate concept leaving example set different candidates averaged 
alternatively learner modified directly output estimate probability instance membership target concept 
may result rule longer assumed concept grammar 
dt algorithm large proportion false positives making performance sensitive false positive penalty factor case 
dl dt algorithms took orders magnitude longer sc run 
difference continues experiments thesis 
deficiency alleviated somewhat incremental lazy concept modification rebuilding scratch new example sc benefit enhancements 
suitability algorithms interactive annotation system depends granularity theta tessellation images speed difference may noticeable pixel level sc choice 
complexity added need handle set complementation produces new sets escape hierarchical pattern 
time complexity algorithms multiplies sc number decisions list tree worst twice number examples 
reason decisions reevaluated position list tree 
number examples reach clear order magnitude difference arise 
hierarchies exhibit synergy task combining yields performance slightly worse best contributor 
performance worse hierarchies add grouping noise confuse algorithm spurious candidate rules 
plots effect errors number training examples random ev mrsar brodatz learning performance dt different sets groupings 
random ev mrsar brodatz learning time varying amounts spurious groupings 
learning time number examples zero error wall clock time differs dramatically algorithms 
set bars sc dl dt respectively 
note sc worst grouping noise gradually best 
fastest wall clock time 
histogram sc dl dt learning time combination equally similarity measures 
notice dl sc catch dt 
learning time algorithms inferior hierarchies added 
run information run noise 
example brodatz column shows time brodatz noise mrsar brodatz ev mrsar brodatz random ev mrsar brodatz noise 
sc bars overlap 
see dl dt sensitive grouping noise sc 
expected weaker focused bias 
dt exploits single hierarchy best adding groupings allows sc overtake 
relative application performance learners depends amount grouping noise number irrelevant similarity measures image video libraries expected high chapter 
dominance similarity measures mrsar obvious previous experiments need general 
set experiments attempted achieve synergy roughly equally measures euclidean gray level histogram distance tree structured wavelet transform 
histogram distance squared error pixel value bins accumulated entire image 
learning time algorithms hierarchies shown 
hierarchy order best worst dt dl sc 
see case constraining bias advantage 
hierarchies algorithms roughly equivalent dl sc position 
explanation dt superior expressive power ambiguity introduced second hierarchy 
vistex collection data set challenging 
textures color vistex collection www white media mit edu vismod imagery equally divided theta non overlapping images 
desired classification corresponded mutually exclusive categories bark clouds water attached creators distribution 
previous test classes ranged size clouds fabric 
database considerably variety image homogeneity brodatz classes semantic 
example images classified building dark building top green grass bottom close different building oblique angle 
learning times algorithms 
color histogram metric histogram mrsar plus vistex sc dl dt learning time summary vistex collection 
gray level histogram metric color channels theta bins 
color space ohta principal components rgb cube 
vistex hierarchy contains image groupings defined cropping process 
dt algorithm best exhibits diminishing returns hierarchies sc 
best time indicating little perceptual consistency observed categorization learner bias properly focused 
chapter show deal situation changing parameters bias improving groupings available learner 
bt collection performance algorithms measured labeling color natural scenes bt images www white media mit edu vismod imagery bt photos 
human subjects asked freehand outline regions natural scenes assign labels building car grass leaves person sky water 
asked precise boundaries decisions strictly perceptual basis aided learning 
majority vote subjects derive single approved ground truth segmentation labeling images 
image groupings computed theta tessellation ground truth segmentations quantized resolution 
yielded labeled patches theta pixels 
resulting ground truth shown appendix learning benchmark simulate labeling process computer assistant user tapped patches class carefully outlining occurrences 
results image groupings shown 
learners available groupings mrsar ohta histogram measures computed theta tessellation images see section 
tessellation coarser ground truth usual singleton groupings finest scale available learner 
adding groupings computed theta tessellation improve learning time 
surprisingly image groupings computed measures improve learning 
indicates image perceptual variations semantic classes escaped measures 
hierarchy improvement section required deal task 
sc dl dt size bias size bias learning time summary bt collection 
shown times size bias explained section note superiority sc improved bias 
time grammars size bias collection slightly easier vistex 
experiments chapter illuminated effect bias learning process 
sc algorithm having strongest bias stable new examples arrived sensitive spurious groupings 
dt algorithm having weak bias best available groupings highly sensitive spurious groupings 
dl algorithm fell extremes 
robustness speed sc emerges preferred learning algorithm foureyes 
best cases consider concept grammar 
learner strong accurate bias 
bias strong inaccurate weak bias may better 
bias strength derived algorithm bias accuracy 
choosing bias strength learner somewhat gamble 
observed respect decision tree algorithm cls faces potential user cls program paradox 
depending type problem particular program 
know type problem applies program solve problem 

fortunately faced similar problem 
task dependent biases learning resemble attributes image retrieval 
attributes improve robustness flexible architecture require commitment ad hoc choice 
computer choice information available 
topic chapter 
chapter bias selection machine learning examples bias play crucial roles demonstrated previous chapter 
want design system learns choose bias carefully 
tempting think choice avoided assigning equal weight answers position ignorance strong statement world 
matter going bias sense take bull horns sure bias 
argument similar bayesian priors regularization 
interestingly bias selection common designing learner typically done manually designer arbitrarily trial error 
manual intervention bias fixed making learner inflexible new contexts 
bias includes ffl instance attributes color texture shape ffl learning algorithm set covering decision tree multi layer perceptron ffl parameters algorithm pruning thresholds initial weights weight decay factors 
chapter investigates automation bias selection process 
relieve designer burden resulting learner accurate systematic feasible learner change bias dynamically problem problem 
learner improve learning ability 
historical trends bias selection task building efficient learner generally known speedup learning learner attempts improve learning performance 
task learning learn cropped repeatedly machine learning artificial intelligence literature culminating topic multistrategy learning learner multiple biases choose 
fundamental adversary fought generality efficiency tradeoff learner weakens inductive bias allows greater variety concepts learned takes examples average learn particular concept 
example markov modeling number examples required training increases dramatically discrete mixture output densities gets worse stochastic grammars 
general learners solve problems low efficiency worthless 
extreme learners perform tend ones strongest accurate inductive bias 
accuracy determined empirically important able change track problem domain 
provocative early ai experiments biases modify bacon am 
bacon task discover simple mathematical equations explain data set table planetary revolution times versus orbital distance 
program able discover kepler law exploring refining hypotheses arithmetic combinations attributes 
power came constrained search concept space trimmed choices available learner reduced apparent complexity problem described 
am heuristics mathematical discovery time optimize notion interestingness 
starting set theory general heuristics program synthesize concepts ranging natural numbers fundamental theorem arithmetic 
bacon build new concepts heuristic refinements old ones rapidly reach respectable level sophistication getting lost combinatorial explosion 
notion heuristics prune search concept space gave rise term inductive bias basis choosing generalization strict consistency observed training examples :10.1.1.19.5466
bias crucial problem possible answers may valid problems ill defined representation extrapolation segmentation clustering 
recognizing subsequent learners added control knobs bias 
example clustering program provided user detailed control cluster evaluation function bias terms simplicity commonality fit discrimination 
underlying attributes controls clustering process intuitive 
furthermore control knobs insufficient representing possible clusterings learner produce 
note similarity situation database query situation described chapter 
noticed appropriate leave clustering process ambiguous user order allow automatic construction clustering rules sophisticated measured attributes useful 
see attempts reduce burden setting control knobs 
approach transfer training old new problems 
example backpropagation training artificial neural networks biased initial connection weights 
property exploited incorporating networks parts new larger network retrained new problem required fewer examples accurate 
unfortunately technique require careful selection networks form deliberate noise suppression 
dt suffered problem section 
automatic approach learner modify bias runtime changing set available concepts 
utgoff showed learner automatically enlarge concept language current unable express target concepts inconsistent examples 
limited weakening bias eventually degrade performance corresponding strengthening operation 
multistrategy task adaptive learners choose fixed set biases problem dependent manner 
example chain learners explanation strongly biased theory driven biased similarity weakly biased applied turn produced consistent concept 
problems benefited domain knowledge explanation learner solved component problems solved raw similarity considerations terms attribute clusters handled component 
continuous learning continuous learning attempts speedup learning continuously problems just 
opposed multistrategy learning selects fixed biases continuous learning adds ability construct new biases fly 
continuous learner collects knowledge problems problem domains experienced 
acquires experience develops new bias 
continuous learning generalizes multistrategy learning transfer training new problems learning analogy previous problem solutions concept language modification 
individual tasks assigned learner may supervised continuous learning unsupervised process 
learner explicitly told aspects problem carry 
continuous learner self organize 
gives hint construct call self organization principle solving problem change allow system perform better problem 
example neural algorithms follow self organization principle continuous learners problem single input point kohonen map maintains set cluster centers 
point approximate map outputs nearest cluster center 
moves center possibly slightly closer input point 
idea approximation better point 
result simple algorithm cluster centers arrange match distribution input mean squared error approximation minimized 
backpropagation modifies connection weights minimize difference output desired output 
network weights arrange approximate desired input output mapping 
perceptron modifies normal linear discriminant point closer positive example farther negative example 
effect find boundary classes 
sparse distributed memory maintains randomly distributed set fixed location memory cells 
write value arbitrary location average value contents nearest cells 
read value arbitrary location take average nearest cells 
self organization principle searchable data structures dynamically optimize distribution queries 
example move front rule sequential lists move root rule binary trees 
course query distribution fixed known static huffman tree optimal usual tradeoff line line solutions 
unfortunately continuous learner determining bias 
derive self organization principle continuous learning modify parameters learner bias place favor target concept just learned 
algorithms described chapter bias determined concept grammar set cover decision list decision tree vocabulary way continuous learner learn algorithm multistrategy combination output different grammars increasing weight grammar gave correct answer time 
may produce slow system 
second way modify vocabulary include groupings simplify expression target concept 
explored section 
third way extend bias algorithm just concept language include parameters modify 
option explored 
vocabulary weighting straightforward bias extension described algorithms give grouping weight vary depending kind problem solved 
bias concept function description length plus total weight groupings 
allows learner encode plausibility grouping 
experiment learn best weight function groupings kind sparse distributed memory yielded significant decrease learning time sc 
lookup table store errors number training examples random random bias ev ev bias mrsar mrsar bias brodatz brodatz bias sc grouping size bias brodatz collection 
includes 
size information produces faster drop 
weights generalization achieved computing grouping weight characteristic grouping just name 
informative characteristic grouping size inspection weight lookup tables revealed certain conditions simple relation size grouping weight 
relation size weight computed sc straightforward manner 
groupings contain positive example negative examples contribute set cover grouping weight proportional probability occurrence randomly chosen set examples 
assume benchmark program section instances chosen uniform probability 
convenient visualization throwing blindfolded dart lands target positive example negative 
positive negative examples randomly produced number proportions depending size target concept 
example target covers third space proportions 
consider grouping covers fraction instance space covers instances covers instances 
probability grouping cover negative examples gamma similarly probability cover positive examples gamma gamma probability grouping satisfies set cover requirement gamma gamma gamma algorithms works iteratively evaluating sets cover decision 
incorporate size bias simply multiply grouping weight previous optimization criterion number covered examples sample variance 
corresponds second stage independent learning algorithm third stage 
parameters arbitrarily fixed reducing formula gamma 
brodatz test correct tests approximation class sizes vary 
formula offset slightly prevent reaching zero 
data sets chapter attempted grouping size bias 
sc curves brodatz test shown dotted curves size bias solid 
size bias reduced learning time examples corresponding example texture group 
extra examples apparently just served isolate right sized candidate concept 
note error variance steps decreased fewer ties covers 
similar speedups occur hierarchies histogram sc sc bias dl dl bias theta theta theta theta dt dt bias learning time combination equally similarity measures size bias 
includes 
data sets figures 
size bias effective dl dt 
surprising sc value grouping depends previous decisions may change radically development decision structure 
particular relative proportions positive negative examples may differ different paths decision node 
size bias improves performance problems 
unfortunately appears increase sensitivity grouping noise figures 
experiments demonstrate effectiveness size bias certain proportions sure learner estimate standard self organization algorithm simple averaging generalize problems 
dl dt algorithms benefit principled way propagate class proportions decisions 
vocabulary modification speedup learning understood terms data compression 
successful learner represent form target concepts expected 
algorithms seen far set cover decision list decision tree require examples learn complex concepts try produce simplest concept consistent examples 
reduce average number examples required learner design concept language frequent targets shorter expression 
consider possibly infinite message consisting target concepts expected learner occurring proportion probability 
concept language achieve compression message 
compression may required learning 
suggests algorithm language construction analysis stream target concepts requested learner lines lzw line compression algorithm 
note follow self organization principle 
section presents algorithm works incremental modification grouping hierarchies 
algorithm executed learner target concept identified 
hierarchy improvement algorithm closely follows self organization paradigm modification chosen reduce number examples learner require target 
addresses concept vocabulary grammar algorithm equally random ev mrsar brodatz learning time varying amounts spurious groupings size bias 
set bars sc dl dt respectively 
little noise learning time decreased compared 
dl dt sensitive noise 
histogram mrsar plus vistex sc sc bias dl dl bias theta theta theta theta theta dt dt bias grouping size bias vistex collection 
includes 
note eventual superiority sc size bias 
compatible sc dl dt corresponds modifying stage independent happens second third stages 
hierarchy improvement algorithm hierarchy improvement algorithm executed learner succeeded learning target concept leaves labeled positive included target negative included target 
interior node positive parents positive nodes 
algorithm iterates positive non root nodes parent hierarchy starting leaves tries apply modification rules promotion positive siblings sibling union construct new positive node contain positive siblings 
running time algorithm linear number nodes tree 
nodes single child omitted tree number nodes linear number leaves making total time linear number leaves 
thesis number leaves number instances database entries 
shows effect algorithm entire hierarchy 
nodes single child redundant omitted 
execution algorithm hierarchy unify positive nodes 
algorithm eventually 
algorithm satisfies self organization principle reduces number examples learner require target 
proof 
algorithm increases size representation target concept sc dl dt grammars 
follows fact increases impurity node alters internal structure positive nodes 

algorithm eventually contain target concept grouping executed times 
follows fact positive nodes eventually join positive node 
positive nodes siblings merged union rule 
positive nodes siblings promoted siblings happen reach root 
claim shows learning time nonincreasing second shows converges optimum 
learning time may remain hierarchy improvement indefinitely 
algorithm judged satisfies reasonable requirements successful self organization algorithm generalization modification problem improves performance subsequent problems 
retention modification problem previous modifications 
hierarchy improvement 
groupings contain mixing positive negative nodes 
neural networks generalization ensured making changes average retention ensured annealing constant 
hierarchy improvement algorithm achieves generalization allows new groupings combined concept grammar 
subsequent problems utilize new groupings target concepts significantly different see transfer experiments 
example targets agree instances grouped may say positive negative strive benefit unification targets disagree unification hierarchy oscillate 
points agreement points disagreement flexible see 
hierarchy improvement algorithm achieves retention curious automatic annealing process 
shorter hierarchies amenable change number siblings effect union rule high 
application union rule hierarchy 
short trees rapidly tall ones number siblings low union rule effect 
demonstrated experiments 
experiments effect hierarchy improvement learning single target shown 
ideal case continuous learner 
task class problem section algorithm sc starting mrsar hierarchy 
graph shows learning curve successive hierarchy improvements cut learning time half 
improvements learning time fell examples 
shows evolution learning time algorithms 
learning time reduction passes depends structure tree described 
shows experiments starting random mrsar hierarchy 
improvement proceeds slowly 
tree improvement grouping size bias completely compatible learning times reduced 
transfer training tested 
classification problems constructed common viz 
brodatz image texture groups 
problems randomly merged groups form equal sized classes 
learner asked learn problems sequence 
hierarchy improvement algorithm starting mrsar hierarchy applied solving problem 
shown sc able achieve significant performance gains problems having seen 
graph includes ideal learning time resulting simply running learner improvements flat tree resulting target concepts fa cg fd fg fa fc fg 
fa bg fd fg formed persistent groups loose 
asymmetry due execution order sensitivity self organization algorithms 
errors number training examples improvement improvement second improvement third improvement fourth improvement sc hierarchy improvement mrsar iterated problem 
sc dl dt improvement improvement second improvement third improvement theta theta theta theta fourth improvement learning time hierarchy improvement mrsar iterated problem 
sc dl dt improvement improvement second improvement third improvement theta theta theta theta fourth improvement fifth improvement learning time hierarchy improvement random iterated problem 
classification task training ideal pass second pass theta theta theta theta theta theta theta theta theta theta theta third pass learning time hierarchy improvement mrsar running similar problems 
training ideal curves results running learner twice problem 
curves result sequential training tilt slightly right 
problem twice previous experiment 
performance problem pass indicates training half training number 
explanation hierarchy develops groups textures learner sees groups isolation mixed part texture class 
demonstrates group extraction happens 
second third run problems hierarchy develops groups effectively brodatz hierarchy section 
best time achieved subsequent passes approximately time brodatz hierarchy cf 

retention required ideal times achieved hierarchy opposed experiment flat hierarchy 
reports experiment starting flat hierarchy singleton groupings 
improvement hierarchy yields generalization problems require examples 
tree malleable training problem gives learning times examples bottom graph 
sequential problem solving improvement eventually yields results comparable starting mrsar hierarchy 
second pass see effect annealing 
hierarchy adapted problems better rest performance greatly improved 
demonstrates ability retention generalization 
check experiment repeated random common class problems 
shows sc mrsar reaps little transfer benefit especially compared ideal performance 
passes retention 
shows sc starting flat hierarchy 
annealing retention dramatic seen skew second pass 
problems initial problem ordering doesn matter 
hierarchy adapted problem requiring examples 
retention allowed performance problem improve slightly seen problems 
specifically takes examples passes compared passes mrsar 
classification task training ideal pass second pass theta theta theta theta theta theta theta theta theta theta theta learning time hierarchy improvement flat running similar problems 
cf 

classification task training ideal pass fourth pass theta theta theta theta theta theta theta theta theta theta theta learning time hierarchy improvement mrsar running random problems 
contrast little transfer training 
classification task training ideal pass second pass theta theta theta theta theta theta theta theta theta theta theta learning time hierarchy improvement flat running random problems 
cf 

ideal times bottom graph 
related improving vocabulary available learner investigated 
inspired observation large decision trees produced concepts having separate peaks attribute space attributes defining peak 
performance decision tree learner improved applying various transformation operators attributes attempt reduce notion complexity called dispersion 
hierarchy improvement accomplishes goals scheme different notion complexity number positive nodes non positive siblings impurity measures section 
general scheme carries learning problems linear continuous attributes suitable transformation operators definition dispersion 
research needs done finding validating instantiations scheme 
attribute construction heuristic decision trees proposed 
inspired observation decision trees explicitly represent branching conjunctions decisions 
concept requires conjunction decisions identical subtrees created call replication problem 
heuristic construct new candidate decisions conjunctions consecutive decisions appear decision tree 
consecutive decisions appear decision decision tree 
call tree compaction similar compression heuristics self adjusting data structures 
unfortunately give proof convergence demonstrate transfer training new problems 
constructed attributes may form hierarchical structure making algorithm difficult implement efficiently 
continuous learning shares qualities reinforcement learning involves simultaneous modeling world acting model 
reinforcement learning learning learns observation action rules direct interaction world continuous learning indirect provides bias general purpose learner interaction 
continuous learning applied learning problem including reinforcement learning 
example reinforcement learners look table store values watkins suggests generalizer neural network 
generalizers benefit continuous learning adapt bias 
neural networks genetic algorithms number parameters bias notoriously difficult adjust brute force search 
learning problem required increasing intelligence organizer planner navigation sensors sensor coordinator control sensor guidance servo control robot increasing precision multi scale structure intelligent control taken editor 
determine best learning parameters may exceed difficulty original learning task 
sense resemble optimization schemes image segmentation section 
learners sc dl dt suitable continuous learning malleable knowledge 
intelligent control discipline continuous learning great potential 
solutions complex control problems generally multi scale structure shown 
slow imprecise layers feed information dumb fast precise layers 
layers learn higher layers effectively provide bias lower layers report performance higher layers adapt 
multi scale learning architecture foureyes numerous parallels ffl data reductions induction deduction calculation access 
reduction produces simpler computational task 
give example inducing quadratic equation examples vs deducing formula roots vs calculating roots formula vs memorizing roots 
ffl program compilation refining task essentials performance 
ffl conscious unconscious thought acting vs 
ffl reasoning perception 
eyes may learn right focus light setting minds learn occlusion grouping 
ffl evolution 
creature perceives reasons participating common learning process 
advantage multi scale learning architecture avoids stability plasticity tradeoff faced single layer 
layer learns fast forgets fast take account seen 
adding slower long term memories gets best worlds system balance computational effort fast decisions informed experience 
speed coupled scale call method multi speed learning 
multi speed learning viewed line version structural risk minimization 
structural risk minimization arises observation learner bias suitable certain numbers examples 
example number parameters function approximator determines examples needs understand 
structural risk minimization chooses number parameters number examples 
example different function approximator examples received examples received degrees freedom build accurate model overfitting 
alternative description infinite supply parameters associated learning rate specifies examples received learner change parameter 
just way describing multi speed multiscale learning deal appropriately small large numbers examples 
chapter methods bias selection continuous learning tuned sc vocabulary weighting set combining algorithm vocabulary modification 
significantly improve learning performance exploiting properties problem domain average class size common sub problems 
general continuous learning approach extend properties 
inspection approaches speedup learning suggest general approach call meta learning 
consider target concept single example goal metalearning recover model examples meta concept 
example attributes average class size section instances get grouped section extracted target values estimated targets 
chapter followed self organization approach meta learning problem approached learning problem sc dt neural network 
meta learning done continuously leading speedup meta learning meta meta learning 
remember just conceptual distinction necessarily implementational 
parameters learner chosen meta concept give strongest accurate bias 
meta learning requires accurate way transform known attributes domain bias learning 
learning algorithms differ straightforward 
call malleability 
algorithms quite malleable expect neural genetic algorithm 
discovery just new learning algorithms malleable learning algorithms important research topic 
extreme form malleability learner capable reflecting learner complete access control learning mechanisms rewriting program 
sense reflective learner self aware conscious situation conscious situation 
correspond understanding attributes target concepts inductive bias exploits respectively 
reflective learner information choose better bias modifying design 
subsumes modifying parameters hierarchies 
system built profound influence design machine learning algorithms 
chapter summary foureyes extensible self improving learning system assisting users digital library segmentation retrieval annotation 
digital library access requires contextdependent noisy features relevance obvious 
foureyes addresses problem multiple fronts 
tentative organizations data form groupings 
grouping representation provides lingua franca different measures similarity 
groupings manually provided induced color texture models derived optical flow information audio foureyes uses image groupings image groupings composed chapter 

user longer choose features set feature control knobs 
user provides positive negative examples allow foureyes choose groupings similarity measures automatically 
interaction conversation parties give prompt relevant feedback order resolve ambiguities 
implementation feedback foureyes cast concept learning problem approached adaptation machine learning methods 
learning algorithms available foureyes set cover decision list decision tree adapted grouping representation analyzed chapter 
groupings choose number examples required isolate groupings get large 
foureyes circumvents continuous learning shown significantly improve learning performance learning algorithms chapter 
compatible approaches supported placing weights groupings size changing available groupings reduce complexity learned rules 
way foureyes learns multiple scales small scale request longer scale requests 
dissertation introduced malleability general criterion evaluating concept learners 
practical mitchell proposition inductive bias criterion comparing evaluating concept learners quality inductive bias varies problem domain :10.1.1.19.5466
malleability ability learner adapt bias problem domain 
malleable learner allows direct translation domain characteristics learning parameters 
translation useful done automatically inspection sample problems domain 
inference learning parameters concept learning problem meta learning section handled way 
combination malleable learner meta learner gives continuous learner chapter capable modifying concept language done choosing strategies transferring training old problems new done making analogies 
may ask highly interactive systems spirit foureyes practical autonomous systems robot cars satellites obvious source feedback 
furthermore may cases operator incapable giving advice 
source feedback system design satellite years crashes moon 
point remain better design incorporate feedback directly re engineered 
designing solution difficult ill posed problem kind reinforcement learning undertaken humans bunch choices feature space clustering algorithm learning architecture get reward system works practice iterate new experience 
costly error prone procedure ripe streamlining automation 
appendix segmentation ground truth twelve natural scenes ground truth labelings 
regions labeled building colored black car yellow grass green leaves cyan person red sky blue water purple 
unlabeled regions white 
remaining thirteen natural scenes ground truth labelings 
bibliography allen munro 
self organizing search trees 
journal acm 
bir bhanu lee 
genetic learning adaptive image segmentation 
kluwer academic publishers 
bookstein 
generalized retrieval problem 
crouch editor proc 
th int conf info 
stor 
retr volume pages 
acm sigir summer 
breiman olshen stone 
classification regression trees 
wadsworth brooks cole advanced books software 
brodatz 
textures photographic album artists designers 
dover new york 
carpenter grossberg 
art self organization stable category recognition codes analog input patterns 
applied optics 
chang kuo 
texture analysis classification tree structured wavelet transform 
technical report usc university southern california los angeles ca february 
allen cypher editor 
watch programming demonstration 
mit press cambridge ma 
jon doyle 
model deliberation action introspection 
phd thesis massachusetts institute technology 
mit artificial intelligence laboratory memo aim tr 
duda hart 
pattern classification scene analysis 
wiley interscience 
michael garey david johnson 
computers intractability guide theory np completeness 
freeman 
hall 
mathematical techniques multi sensor data fusion 
artech house 
harris editor 
advances intelligent control 
taylor francis 
haussler 
quantifying inductive bias ai learning algorithms valiant learning framework 
artificial intelligence 
earl hunt janet marin philip stone 
experiments induction 
academic press 
jain dubes 
algorithms clustering data 
prentice hall englewood cliffs nj 
jarvis patrick 
clustering similarity measure shared near neighbors 
ieee comp pages nov 
sparck jones 
statistical interpretation term specificity application retrieval 
journal documentation 
kanerva 
sparse distributed memory 
mit press cambridge ma 
kohonen 
self organization associative memory 
springer berlin heidelberg 
rd ed 

langley 
rediscovering physics bacon 
proc 
fifth international joint conference artificial intelligence pages 
lenat 
am discovery mathematics heuristic search 
davis lenat editors knowledge systems artificial intelligence pages 
mcgraw hill 
lenat hayes roth 
cognitive economy artificial intelligence systems 
ijcai 
liu picard 
periodicity directionality randomness wold features image modeling retrieval 
ieee patt 
analy 
mach 
intell appear 
mit media laboratory perceptual computing tr 
mackay :10.1.1.27.9072
bayesian interpolation 
neural computation 
maclin shavlik 
refining knowledge neural networks improving chou algorithm protein folding 
machine learning 
mao jain 
texture classification segmentation multiresolution simultaneous autoregressive models 
patt 
rec 
michalski 
theory methodology inductive learning 
artificial intelligence 
michalski 
inferential theory learning conceptual basis multistrategy learning 
machine learning 
michalski editor 
multistrategy learning 
kluwer academic publishers 
michalski stepp 
automated construction classifications conceptual clustering versus numerical taxonomy 
ieee patt 
analy 
mach 
intell pami july 
minka picard 
interactive learning society models 
submitted publication 
appears mit media lab perceptual computing tr 
mitchell :10.1.1.19.5466
need biases learning generalizations 
technical report cbm tr rutgers university may 
niblack barber equitz flickner petkovic yanker faloutsos 
qbic project querying images content color texture shape 
niblack editor storage retrieval image video databases volume feb 
ibm techreport rj feb 
ohta kanade sakai 
color information region segmentation 
comp 
graph 
img 
proc 
pagallo haussler 
boolean feature discovery empirical learning 
machine learning 
pavel gluck 
constraints adaptive networks modeling human generalization 
touretzky editor advances neural information processing systems volume pages 
pazzani 
learning causal patterns making transition data driven theory driven learning 
machine learning 
pentland picard sclaroff 
photobook tools content manipulation image databases 
spie storage retrieval image video databases ii pages san jose ca feb 
picard 
visual thesaurus 
springer verlag workshops computing 
appear appears mit media lab perceptual computing tr 
picard 
computer learning subjectivity 
acm computing surveys 
appear appears mit media lab perceptual computing tr 
picard 
finding similar patterns large image databases 
proc 
ieee conf 
acoustics speech signal proc pages minneapolis mn 
picard liu 
real time recognition entire brodatz texture database 
proc 
ieee conf 
computer vision pattern recognition pages new york june 
pratt 
experiments transfer knowledge neural networks 
hanson rivest editors computational learning theory natural learning systems volume pages 
mit press cambridge ma 
quinlan 
induction decision trees 
machine learning 
rendell 
learning hard concepts constructive induction framework rationale 
hanson rivest editors computational learning theory natural learning systems volume pages 
mit press cambridge ma 
rivest 
learning decision lists 
machine learning 
rivest 
self organizing sequential search heuristics 
comm 
acm 
romer 
kodak picture exchange april 
seminar mit media lab 
rumelhart hinton williams 
learning representations backpropagating errors 
nature october 
tekalp 
annotation natural scenes adaptive color segmentation 
spie electronic imaging feb 
san jose ca 
salton 
automatic information organization retrieval 
mcgraw hill new york 
saund moran 
perceptual organization interactive sketch editing application 
proc 
fifth international conference computer vision pages cambridge ma june 
strat fischler 
context vision recognizing objects information imagery 
ieee patt 
analy 
mach 
intell oct 
richard sutton editor 
reinforcement learning 
kluwer academic publishers 
reprinted machine learning vol 
nos 


decision estimation classification 
john wiley sons new york 
turk pentland 
face recognition eigenfaces 
proc 
ieee conf 
computer vision pattern recognition maui hi june 
utgoff 
machine learning inductive bias 
kluwer academic publishers boston ma 
vapnik 
principles risk minimization learning theory 
moody hanson lippmann editors advances neural information processing systems volume pages 
watkins dayan 
technical note learning 
machine learning 
wu narasimhalu lam gao 
core content retrieval engine multimedia information systems 
multimedia systems feb 

zhang smoliar 
developing power tools video indexing retrieval 
niblack jain editors proceedings spie storage retrieval image video databases ii pages san jose ca feb 
spie 
vol 

zhu lee yuille 
region competition unifying snakes region growing energy bayes mdl multi band image segmentation 
int 
conf 
computer vision pages boston ma 

