technical report department statistics university toronto split merge markov chain monte carlo procedure dirichlet process mixture model sonia jain radford neal july 
propose split merge markov chain algorithm address problem inefficient sampling conjugate dirichlet process mixture models 
traditional markov chain monte carlo methods bayesian mixture models gibbs sampling trapped isolated modes corresponding inappropriate clustering data points 
article describes metropolis hastings procedure escape local modes splitting merging mixture components 
metropolis hastings algorithm employs new technique appropriate proposal splitting merging components obtained restricted gibbs sampling scan 
demonstrate empirically method outperforms gibbs sampler situations components similar structure 
key words dirichlet process mixture model markov chain monte carlo metropolis hastings algorithm gibbs sampler split merge updates mixture models applied density estimation latent class analysis classification problems discussed example everitt hand mclachlan basford titterington 
bayesian approach mixture models generated interest due advances statistical computation particular markov chain monte carlo see tierney gilks 
article consider bayesian mixture models dirichlet process prior mixing distribution handle countably infinite number mixture components 
earlier computational techniques dirichlet process mixture models include escobar escobar west maceachern bush maceachern neal richardson green neal 
gibbs sampling common markov chain monte carlo scheme sampling posterior distribution dirichlet process mixture model 
conjugate priors gibbs sampling procedure easily constructed 
fully exploit conjugacy model analytically inte department statistics university toronto toronto ontario canada 
email sonia toronto edu internet www toronto edu sonia department statistics department computer science university toronto toronto ontario canada 
email radford toronto edu internet www cs toronto edu radford grating away mixing proportions components parameters component 
result gibbs sampling procedure updates latent indicator variables associating mixture components data observations 
particular gibbs sampling method discussed neal models categorical data maceachern normal mixture models 
gibbs sampling approach straightforward easily implemented slow converge mix poorly 
mixture components similar parameters gibbs sampling method may trapped local mode corresponds incorrect clustering data points 
celeux attribute mixing problem incremental nature gibbs sampler unable simultaneously move group observations new mixture component 
incremental updates move single observation new mixture component intermediate state low probability 
sampling scheme allows group observations updated simultaneously may remedy problem neighbouring observations support formation new component appropriate 
split merge birth death updates previously proposed phillips smith green richardson 
phillips smith consider birth death mixture components jump diffusion sampling algorithm 
mixture model context wish estimate number components approach model comparison problem models varying dimensions 
algorithm generates markov chain local jump step discrete transitions mixture models differing component diffusion step model parameters updated jumps 
dimensional example parameters possible new component drawn prior 
proposing create delete component weights adjacent components modified appropriately 
method generalize easily higher dimensional problems 
green richardson introduce complex split merge scheme reversible jump framework green 
changes parameter space proposed increasing decreasing number mixture components single iteration 
split merge proposals conserving specific moment conditions evaluated metropolis acceptance probability 
method requires inclusion tuning parameters asymmetrical splits may proposed adjust unequal sample sizes merging 
depending type statistical mixture model numerous ways propose appropriate split proposal 
green richardson provide general guidelines constructing split proposals satisfy various moment conditions 
clear adequate split proposals simple construct compute high dimensional multivariate mixture problems technique 
article introduces new metropolis hastings method avoids problems associated gibbs sampling procedure suitable high dimensional data 
typically updates involve simple parametric distributions proposal distribution 
split mixture components method employs complex proposal distribution obtained restricted gibbs sampling scan latent class variables 
method able quickly traverse state space frequently visit high probability modes splits merges group observations update bypassing incremental updates gibbs sampler 
furthermore proposal distribution complex need specially tailored model scheme applied model conjugate prior 
section notation terminology dirichlet process mixture model gibbs sampling algorithm suitable conjugate priors introduced 
section presents split merge metropolis hastings procedure variants 
section empirically compare method gibbs sampler demonstrate improved performance categorical data problem 
conclude section discussing possible extensions split merge algorithm general applicability new metropolis hastings technique 
gibbs sampling procedure dirichlet process mixture model section dirichlet process mixture model early see ferguson antoniak describe gibbs sampling algorithm sample posterior model 
procedure completely utilizes conjugacy model integrate away model parameters mixing proportions eliminating algorithm 
section version gibbs sampler compared new split merge metropolis hastings algorithm 
dirichlet process mixture model consider hierarchical mixture model observations modelled mixture distributions having form 
restriction dimensionality data may categorical quantitative 
observation model parameters considered independent draws mixing distribution requiring take parametric form dirichlet process prior distribution space distribution functions placed yields mixture model form dp ff ff dirichlet process parameters 
defines baseline distribution dirichlet process prior ff total mass parameter takes values greater zero 
equation represents basic dirichlet process mixture model 
stages may added hierarchy placing priors ff parameters example see maceachern 
usual conditional independence assumptions hierarchical model apply dependencies explicitly shown 
model may regarded countably infinite mixture model ferguson view adopted remainder article 
integrated prior distribution equation see follow generalized polya urn scheme details refer blackwell macqueen ferguson 
prior distribution may represented conditional distributions gamma gamma ff gamma ffi ff gamma ff ffi distribution point mass model equation simplified integrating away random distribution represent fact results identical setting oe represents latent class associated observation polya urn scheme sampling equivalent scheme sampling latent variables associated oe gamma gamma ff gamma ff gamma ff number equal labelling indicator irrelevant probabilities counts equal matter 
oe drawn independently initial distribution probabilities shown define dirichlet process model equivalent mixture model equation 
notation employed subsequent sections 
gibbs sampling conjugate dirichlet process mixture model model conjugate prior straightforward sample posterior distribution gibbs sampling 
gibbs sampling approaches proposed dirichlet process mixture model literature consider procedure conjugacy fully exploited introduced neal maceachern 
procedure integrates away model parameters oe eliminating oe simplifies algorithm considerably state markov chain gibbs sampler consists indicators markov chain initialized setting initial state example set component different components 
updated gibbs sampling repeatedly drawing new value conditional distribution proportional product conditional prior likelihood 
observations exchangeable conditional prior derived equation considering observation observations 
yields conditional probabilities gammai gammai gamma ff oe dh gammai oe gammai ff gamma ff oe dg oe gammai represents gammai number equal number observations gammai posterior distribution oe prior data observations oe likelihood appropriate normalizing constant probabilities sum 
conjugate prior integrals oe dg oe oe dh gammai oe may analytically computed 
maceachern demonstrated application gibbs sampling algorithm conjugate normal mixture model fixed variance 
case normal distribution oe normal distribution conjugate similarly neal illustrated algorithm applied categorical data 
example data observations dichotomous product independent bernoulli distributions product independent beta prior distributions 
section consider similar categorical data example establish algorithm improvement gibbs sampling 
split merge metropolis hastings algorithm conjugate dirichlet process mixture model mixture components similar parameters gibbs sampling inefficient 
markov chain trapped local mode distinct mixture components merged assigned parameters compromise separate components 
gibbs sampler incrementally updates observation markov chain pass low probability intermediate state order split component 
leads slow convergence true posterior distribution 
introduce nonincremental markov chain sampling method metropolis hastings algorithm avoids problem 
algorithm able split merge groups data points avoiding need pass low probability intermediate state order major changes 
reviewing metropolis hastings updates 
discuss possible proposal distributions metropolis hastings updates dirichlet process mixture model simple random split complex restricted gibbs sampling scan 
algorithms ergodic performance may improved combining split merge updates regular gibbs sampling scan 
metropolis hastings updates algorithm propose split merge moves update groups observations form metropolis hastings algorithm metropolis hastings 
metropolis hastings algorithm samples distribution density drawing candidate state proposal density jx 
proposed state evaluated metropolis hastings acceptance probability calculated follows min xjx jx state set candidate state probability 
new state remains current state metropolis hastings updates leave posterior distribution invariant produce valid markov chain monte carlo sampling scheme provided chain ergodic 
discussed tierney constructing markov chains acceptable select transition probability random set appropriate transition probabilities 
particular may randomly choose valid metropolis hastings algorithms randomly selecting proposal distribution jx 
note calculating metropolis hastings acceptance probability ratio xjx jx may calculated particular proposal distribution chosen computing ratio summing possible proposal distributions 
lead valid metropolis hastings updates calculation may computationally infeasible 
random split merge proposals introduce split merge algorithm proposal distribution simple random split subset observations associated mixture component separate components properties observed data 
simplest version split merge algorithm expect illustrates basic construction 
elaborate version procedure restricted gibbs sampling proposal produces sensible splits discussed section 
split merge approach applied conjugate dirichlet process mixture model random distribution model parameters oe integrated away 
state markov chain consists mixture component indicators markov chain initialized assigning observation mixture component 
typical initial states placing data component assigning observation different component 
outline steps simple random split merge procedure 
algorithm simple random split merge procedure 
select distinct observations random uniformly 
random selection items decide particular metropolis hastings proposal distribution considered 

denote set observations ng 
items belong mixture component propose new assignment data items mixture components denoted split component split separate components split split define element proposal vector split follows ffl split new component split fc ffl split ffl observation split randomly set independently equal probability component split split ffl observations fi jg split evaluate proposal metropolis hastings acceptance probability split discussed 
proposal accepted split state markov chain 
proposal rejected original vector remains state 

belong different mixture components propose new assignment data items mixture components denoted merge components combined single component 
element proposal vector merge assigned follows ffl merge ffl merge ffl observation merge ffl observations fi jg merge evaluate proposal metropolis hastings acceptance probability merge 
proposal accepted merge state 
merge proposal rejected original configuration remains state 

repeat steps iterations 
note numerical values irrelevant matter steps item remains fixed original mixture component 
labels significant distinguish items grouped mixture component 
note vectors split merge designate mixture component assigned observation data just observations involved split merge steps 
items associated set remain unchanged unaffected metropolis hastings update 
metropolis hastings acceptance probability simple random split algorithm updating vector associating observations mixture components acceptance probability equation takes form min jc jy cjy vector split merge depending type proposal 
posterior distribution equation expanded product factors prior likelihood cjy vector observations 
note factors involving may ignored 
prior distribution entire vector product distinct fc factors equation 
product yields prior distribution ff fc ng gamma 
ff gamma number distinct mixture components contained vector count items belonging mixture component notice ratio prior distributions equation simplifies considerably denominator equation cancel factors equation associated components directly involved metropolis hastings update 
split proposal prior distribution ratio reduces split ff split gamma 
split gamma 
gamma 
represents original state belong mixture component 
split split represent number observations belong split mixture components 
factor ff numerator arises greater split similarly merge proposal prior ratio simplifies merge ff merge gamma 
gamma 
gamma 
represents original state items belong separate components 
likelihood vector component indicators product observations cjy oe dh oe posterior distribution oe prior observations assume integral oe dh oe analytically tractable case conjugate prior 
note item observed particular component prior distribution data mixture component precedes item alternatively cjy may expressed double product components items ng associated component cjy oe dh oe number distinct components 
factors involving items associated components directly involved split proposal cancel ratio likelihoods equation reduces split jy cjy split split oe dh split oe split split oe dh split oe oe dh oe similarly merge proposal ratio likelihoods merge jy cjy merge merge oe dh merge oe oe dh oe oe dh oe random split proposal distribution simplest version algorithm 
selection observations decides metropolis hastings proposal 
result calculating acceptance probability fixed 
probability proposing particular split items set merged state split jc split split gamma notice split jc equivalent merge 
probability proposing merge move items split state merge jc way assign items component 
note merge jc equivalent split 
follows equations appropriate ratio transition probabilities split proposal split split jc jn split split gamma similarly appropriate ratio transition probabilities merge proposal merge merge jc nc nc gamma resulting acceptance probability split proposal product equations 
likewise acceptance probability merge proposal product equations 
employing hastings version metropolis algorithm calculating acceptance probability correct fact probability proposing particular split smaller probability proposing merge resulting components 
basic form procedure illustrates may update groups observations 
proposed split appropriate data accepted neighbouring observations lend support creation new component bypassing problem trapped local mode 
unfortunately stated earlier expect simple random split version algorithm perform 
components split observed data split proposals appropriate accepted 
restricted gibbs sampling split merge proposals describe proposal distribution properties observed data decide split mixture components restricted gibbs sampling scan 
yields method reasonable splits components frequently proposed 
way validate main algorithm introduce gibbs sampling split merge proposal state prior gibbs sampling scan fixed 
pre gibbs state referred launch state 
generalized version launch state randomly selected particular select launch state conducting intermediate restricted gibbs sampling scans 
restricted gibbs sampling proposals fixed launch state replace simple random split step algorithm section restricted gibbs sampling scan component indicators starting predetermined fixed launch state 
fixed state version algorithm expected particular method prove validity subsequent random launch state algorithm 
restricted gibbs sampling proposal distribution elaborate typical metropolis hastings proposals 
fixing state prior scan proposal probabilities may explicitly computed yields valid metropolis hastings update 
fixed launch state gibbs sampling scan defines particular metropolis hastings algorithm 
fixed launch states produce valid proposal distributions satisfy usual requirements independence proposals past states 
split proposal observations assigned different components observations belong merged component assigned split components predetermined manner 
launch state launch determined restricted gibbs sampling scan conducted decide items allocated split components 
gibbs sampling scan restricted performed subset data set assign items mixture components 
update restricted gibbs sampling new value drawn restricted conditional distribution follows gammak gammak oe dh gammak oe gammak oe dh gammak oe gammak oe dh gammak oe simplify notation refer component indicators launch state equation 
gibbs sampling scan values values terms continually modified incrementally updated computation leading split gammak represents fi jg gammak number fi jg equal oe likelihood gammak posterior distribution oe prior data observations fi jg conjugate prior integrals may analytically computed 
general transition probability full sequential gibbs sampling scan product conditional probabilities individual update 
restricted gibbs sampling transition probability state launch split product probabilities assigning observation particular split mixture component gibbs sampling fixed launch state equation 
algorithm product metropolis hastings proposal probability split jc 
merge proposal simple random split merge procedure way merge items components component merge jc 
obtain corresponding probability merge need calculate probability generating original split state fixed launch state gibbs sampling scan 
done way split proposal actual sampling performed split state known 
simple random split case obtain metropolis hastings acceptance probability appropriate split merge proposal distribution ratio restricted gibbs sampling substituted equation 
prior likelihood ratios equation remain shown section 
scan gibbs sampling conducted expect allocation items components reached equilibrium 
metropolis hastings proposal distribution take form produce valid algorithm lack convergence invalidate algorithm 
quite proposed splits single iteration gibbs sampling may sensible 
improve proposals splits proposed appropriate data 
restricted gibbs sampling proposals random launch state fixed launch state algorithm previous section produces valid update 
discussed section may select markov chain transition random set valid transitions tierney 
launch state restricted gibbs sampling scan may chosen random set fixed states 
single scan gibbs sampling done launch state chosen uniformly random may lead unreasonable assignment observations mixture components 
achieve reasonable splits intermediate restricted gibbs sampling scans performed final scan 
calculating split proposal probability result intermediate gibbs sampling scan considered random launch state restricted gibbs sampling transition probability explicitly calculated 
prefer incorporate intermediate gibbs sampling scans proposal distribution summing probabilities intermediate states computationally infeasible 
equilibrium probably reached restricted gibbs sampling scans clustering observations mixture components probably better reflection actual attributes data compared just single scan gibbs sampling 
version algorithm shows improvement traditional gibbs sampling method previous versions algorithm simple random split gibbs sampling proposals single fixed state 
increasing number intermediate gibbs sampling scans leads improvement convergence measured iterations cost computational time iteration 
section effect varying tuning parameter examined 
split proposal probabilities calculated way fixed launch state gibbs sampling proposals equation 
merge proposal obtain merge intermediate gibbs sampling operations performed proposing split conducted arrive launch state actual split performed 
gibbs sampling transition probability calculated launch state intermediate gibbs sampling state original split state 
operations necessary order produce correct proposal ratios 
procedure restricted gibbs sampling split merge procedure random launch state summarized 
algorithm restricted gibbs sampling split merge procedure 
select distinct observations random uniformly 

denote set observations ng 
define launch state launch compute gibbs sampling probabilities 
launch set new component launch fc launch launch launch set launch distinct components launch launch follows ffl select initial state randomly setting independently equal probability launch launch launch ffl modify launch performing intermediate restricted gibbs sampling scans 

items mixture component propose new assignment data items mixture components denoted split component split separate components split split define element proposal vector split follows ffl split launch note launch fc ffl split launch ffl observation split set component split split conducting final gibbs sampling scan launch state launch ffl observations fi jg split calculate proposal probability split jc computing gibbs sampling transition probability launch state launch final proposed state split gibbs sampling transition probability product probabilities setting split final value final gibbs sampling scan 
evaluate proposal metropolis hastings acceptance probability split 
proposal accepted split state markov chain 
proposal rejected original vector remains state 

different mixture components propose new assignment data items mixture components denoted merge components combined single component 
assign element proposal vector merge follows ffl merge ffl merge ffl observation merge ffl observations fi jg merge calculate proposal probability merge computing gibbs sampling transition probability launch state launch original split configuration gibbs sampling transition probability product probabilities setting original split state original value hypothetical gibbs sampling scan launch state 
evaluate proposal metropolis hastings acceptance probability merge 
proposal accepted merge state 
merge proposal rejected original configuration remains state 

repeat steps iterations 
computational issues calculating prior ratios equations counts observations associated mixture component required 
similar counts needed performing gibbs sampling seen equations 
improve efficiency useful maintain counts incrementally array decrementing incrementing appropriate counts observations moved components 
depending statistical model incremental update relevant sufficient statistics may advantageous likelihood calculations 
way reducing computational cost minimize time spent searching arrays mixture components 
numerical value component indicator relevant distinguishes components creating new components previous labels currently utilized may recycled 
reduce time required search count arrays 
final consideration calculating ratios metropolis hastings acceptance probability equation avoid numerical overflow problems priors likelihoods proposal probabilities calculated terms logarithms 
cycling metropolis hastings gibbs sampling updates split merge metropolis hastings algorithm produces markov chain leaves posterior distribution invariant 
markov chain irreducible statistical model data non zero prior probability non zero probability chain started initial state assign observation separate mixture component result series split moves 
extremely degenerate models markov chain aperiodic non zero probability chain remain current state split merge proposals non zero probability rejected 
split merge algorithm ergodic 
algorithm ergodic produces nonincremental splits merges components improvements convergence may obtained combining algorithm traditional gibbs sampling 
algorithm addresses problem making major changes allocation items moving observations cluster single iteration 
may take longer move single observation components 
situation fine tuning approach required regular gibbs sampling scan provide 
consequently propose combining algorithms alternately performing metropolis hastings update full scan gibbs sampling 
doing exploit nonincremental major changes step incremental minor refinement gibbs sampling step 
tierney section notes markov chain transition kernels applied cycles kernels ergodic cycle kernel guaranteed ergodic 
case metropolis hastings gibbs sampling steps non zero probability leaving state unchanged applying transition turn produce ergodic markov chain 
tune algorithm modifying number metropolis hastings updates number final gibbs sampling scans full iteration 
expected increasing values tuning parameters convergence measured full iterations improved cost computation time iteration 
section illustrate effects modifications provide guidelines setting tuning parameters 
example bernoulli data conjugate beta prior section empirically compare split merge procedure variants gibbs sampling 
consider categorical mixture model data independent identically distributed observation bernoulli attributes im 
class observation belongs item attributes independent 
type model common latent class analysis see example everitt mixture components considered latent classes represent heterogeneous mechanisms underly produce observed data 
neal considered similar model examining performance gibbs sampling procedure discussed section 
simplicity exposition consider dichotomous attributes model algorithms easily generalize categorical attributes values 
statistical model bayesian framework bernoulli data modelled dirichlet process mixture model 
observations im multivariate bernoulli ih bernoulli ih likelihood follows ih ih gamma ih gammay ih parameters component give probabilities attribute having value 
probability beta distribution prior parameters fi fi 
prior vector independent 
note subscripts denote different attributes different observations 
density gamma fi fi gamma fi gamma fi fi gamma gamma fi gamma fi fi greater zero 
conjugate prior model parameters may integrated away 
update gibbs sampling new value drawn conditional distribution equation model beta prior bernoulli likelihood substituted gammai gammai gamma ff ffi ffi kh ih fi ih gammai fi fi gammai ff gamma ff fi ih fi fi delta function ffi equal zero 
term ffi ffi kh ih counts number observations associated component match respect attribute second formula gives probability setting new mixture component currently assigned observation 
equations factor normalizes distribution sum 
metropolis hastings acceptance probability equation prior calculated shown equation 
appropriate ratio transition probabilities restricted gibbs sampling obtained formula equation 
likelihood equation bernoulli beta model follows cjy ffi ffi ih kh fi kh fi fi interchange products equation simplifies computed follows cjy gamma ffi ffi kh fi gamma fi gamma ffi ffi kh fi gamma fi gamma fi fi gamma fi fi mentioned section useful incrementally update statistics model 
efficiency improved maintaining count items associated mixture component having particular values attribute 
array counts gibbs sampling equation likelihood calculation equation 
simulated data sets dirichlet process mixture models consider number mixture components countably infinite model applied finite mixtures 
prior chosen ensures infinite number components significant probability overfitting occur 
model assign small probability observations infinite number additional components cause serious problems 
simplicity test algorithms simulated data finite mixture 
primary goal partition observations appropriate latent classes dirichlet process mixture model 
computationally classification problem difficult dimensionality increases sets attributes distinguish various components similar structure 
illustrate difficulty considering simulated data sets number attributes increased different components appear alike dimensionality increases 
data composed equally probable mixture components component produces distribution dichotomous attributes 
maintain uniformity examples observations produced example observations generated mixture components 
data simulated examples randomly generated mixture distributions shown tables 
mixture components distinguished attributes consistency kept constant examples 
examples differ number additional attributes 
dimensionality increased simply replicating distribution attribute components similar difficult distinguish 
note intentional asymmetry construction mixture components components similar components 
intended test split merge algorithms handle way splits 
demonstrations algorithms dirichlet process parameter ff set 
small value ff implies number mixture components data set small 
fi fi parameters beta prior distribution set 
priors may realistic consistency values fixed simulations 
actual problems ff fi set prior knowledge higher level priors 
table true mixture distribution example 
ih jc table true mixture distribution example 
ih jc table true mixture distribution example 
ih jc performance algorithms example gibbs sampling algorithm compared versions split merge algorithm simple random split split merge split merge split merge split merge 
number parentheses number intermediate gibbs sampling scans reach launch state second number metropolis hastings updates single iteration third number complete gibbs sampling scans final update 
algorithm observations assigned mixture component initial state algorithm run iterations 
simulations performed matlab version sgi system mhz mips processor 
performance algorithm evaluated examining trace plots figures computation time iteration table 
trace plot values plotted fraction observations associated common common common common common mixture components 
components appear equally samples true situation captured exactly traces occur values 
table time iteration seconds algorithms tested 
algorithm example example example gibbs sampling simple random split split merge split merge split merge split merge example example simplest 
relatively low dimensional attributes separated mixture components 
trace plots appears algorithms simple random split appropriately separated data iteration gibbs sampling iteration simple random split iteration split merge iteration split merge iteration split merge iteration split merge trace plots algorithms example 
iteration gibbs sampling iteration simple random split iteration split merge iteration split merge iteration split merge iteration split merge trace plots algorithms example 
iteration gibbs sampling iteration simple random split iteration split merge iteration split merge iteration split merge iteration split merge trace plots algorithms example 
mixture components 
discussed earlier simple random split expected converge rapidly simple situations split proposals usually nonsensical 
inspection gibbs sampling split merge short burn times mix equally 
examine performances algorithms autocorrelation times computed trace plots corresponding fraction items associated common mixture component indicator variable called codes observations assigned mixture component 
items generated components respectively 
due random noise item differs distinguishing attributes true distribution table come component actual component 
consequently mean indicator function approximately items grouped half time 
autocorrelation time defined plus twice sum autocorrelations quantity lags infinity 
factor sample size effectively reduced estimating expectation quantity compared estimate independent draws posterior distribution ripley section 
autocorrelation time estimated plus sum estimated autocorrelations lag autocorrelations approximately zero 
results table 
table autocorrelation times algorithms converged example 
algorithm trace indicator gibbs sampling split merge split merge gibbs sampling split merge split merge approximately equal autocorrelation times 
note slightly higher trace autocorrelation time split merge statistically significant due chance distribution occasional sharp peaks visible trace plot 
simple problem gibbs sampling successful correctly splitting items components split merge algorithms necessary 
comparing split merge see addition intermediate gibbs sampling scans improve autocorrelation times worth extra computation time 
split merge include final complete gibbs sampling scan separate data components 
trace plots clear mixing slower intermediate gibbs sampling scans included 
example example higher dimensional problem fifteen attributes posterior distribution priors assigned gives substantial probability configurations main components 
trace plots see gibbs sampling extremely slow separate data components remains stuck particular configuration long time 
gibbs sampling initialized observations different mixture component plot shown observations quickly split components stays configuration long time failing explore true posterior distribution 
hand mixes rapidly component configurations 
drawback split merge takes double time iteration compared gibbs sampling 
gibbs sampling fails mix adequately posterior distribution split merge reaches equilibrium immediately extra time iteration clearly spent 
split merge reasonably moving components minor changes moving observations components problem 
split merge algorithms intermediate gibbs sampling scans slow mixing components split merge mix better gibbs sampling 
simple random split unable separate data adequately performs worst 
example example highest dimensional example eighteen attributes considered 
posterior distribution prior distribution mixture mainly component configurations 
trace plots show gibbs sampling remains incorrect split typical true posterior distribution entire iteration run 
mixture components quite similar incremental creation new component gibbs sampling quite rare 
item initially assigned different mixture component plot shown gibbs sampling splits data components immediately takes roughly iterations move component configuration showing mixes poorly component configurations 
split merge separates observations proper configuration immediately mixes components 
split merge mixes components minor adjustments slow 
split merge algorithms intermediate gibbs sampling scans find component configurations stuck component split long time 
result non optimal metropolis hastings split proposals 
simple random split performs extremely poorly 
summary results gibbs sampling split merge methods reasonably low dimensional cases 
classification task increasingly difficult gibbs sampling mixes exceedingly poorly 
cycled split merge version includes intermediate gibbs sampling scans full gibbs sampling scan successful split merge variation 
split proposals appropriate yielding better mixing different major configurations final gibbs sampling scan handles necessary minor adjustments 
split merge algorithms include intermediate gibbs sampling scans successful handling way splits done way splits 
computation time iteration greater gibbs sampling situations gibbs sampling unable arrive correct stationary distribution reasonable length time burden clearly acceptable 
tuning parameters section examine role tuning parameters play split merge algorithm 
adjustable parameters number intermediate gibbs sampling scans number metropolis hastings updates conducted single iteration number complete gibbs sampling scans conducted metropolis hastings updates 
reconsider data example examine effect varying tuning parameter holding parameters constant 
table displays computational time iteration autocorrelation times trace indicator various settings algorithm 
trace plots shown figures 
table effects tuning parameters 
time iteration autocorrelation autocorrelation algorithm seconds time trace time indicator split merge split merge split merge split merge split merge split merge split merge split merge split merge split merge split merge split merge split merge split merge split merge split merge iteration split merge iteration split merge iteration split merge iteration split merge iteration split merge iteration split merge trace plots examining effect varying number intermediate gibbs sampling scans 
iteration split merge iteration split merge iteration split merge iteration split merge trace plots examining effect varying number metropolis hastings updates single iteration 
iteration split merge iteration split merge iteration split merge iteration split merge trace plots examining effect varying number final complete gibbs sampling scans 
varying number intermediate gibbs sampling scans better splits expected intermediate gibbs sampling scans performed proposed splits closer restricted equilibrium distribution 
observe improved mixing number intermediate gibbs sampling scans arrive launch state increased 
autocorrelation times trace lower compare vs intermediate scans increased cost computational time iteration vs seconds 
intermediate scans improvement fairly minimal 
table acceptance rates metropolis hastings updates 
minor improvement acceptance rate scans 
simulations repeated different pseudo random seeds occasion intermediate gibbs sampling scans appear intermediate scans terms rejection rate autocorrelation times 
improvements level intermediate gibbs sampling scans additional scans worth increased computational time 
table acceptance rate different numbers intermediate gibbs sampling scans 
algorithm acceptance rate percent split merge split merge split merge split merge split merge split merge varying number metropolis hastings updates iteration autocorrelation times decrease mixing components improves number metropolis hastings updates increased 
improvements taper metropolis hastings updates computational cost continues increase 
updates iteration best number particular example 
suggests full gibbs sampling scan imperative metropolis hastings update may waste computational time 
varying number final complete gibbs sampling scans observe metropolis hastings updates need supplemented complete gibbs sampling scans order minor clustering changes 
evident autocorrelation times drop full scan gibbs sampling included 
splitting merging particular observations easily done small scale incremental update 
autocorrelation time trace decreases number final scans increased 
trace plots appears differences mixing minimal 
time iteration grows number final gibbs sampling scans increased gibbs sampling scan iteration appear improvements autocorrelation times offset 
guidelines selecting tuning parameters critical tuning parameter number intermediate gibbs sampling scans controls quality metropolis hastings split proposals 
problem similar problems examined small number scans say best compromise computation time autocorrelation time 
data sets number observations mixture component large intermediate gibbs sampling scans may required 
metropolis hastings updates iteration appear provide balance computation time autocorrelation time 
final gibbs sampling scans take long time compute better perform multiple metropolis hastings updates relatively faster 
conducting final gibbs sampling scan metropolis hastings updates shown necessity 
final gibbs sampling scans computationally expensive performing scan undesirable 
discussion split merge metropolis hastings procedure shown improvement traditional gibbs sampling high dimensional problems mixture components similar 
nonincremental clustering changes method avoid problem trapped local modes posterior distribution fully explored 
implementing method relatively simple difficult higher dimensions 
straightforward apply method conjugate model including normal mixture models real valued data conjugate normal inverse gamma priors mean variance 
quality proposals controlled varying number intermediate gibbs sampling scans 
inefficiency procedure currently investigating random selection treatment observations define split merge operation 
initially component split proposed probability correct split configuration proposed low split components equal size 
set different mixture components component labels change 
types problems may arise restriction 
belong mixture component items unnecessarily separated 
problem occur separated labelling problem possible 
initial random split items merged component assign labels biased split opposite fixed labels intermediate gibbs sampling scans may overcome initial bias expected component labels involved intermediate scans 
wrong mixture components 
fixed allowing labels adapt intermediate gibbs sampling need accounted metropolis hastings acceptance probability 
algorithm easily modified replacing intermediate restricted gibbs sampling scans markov chain updates type 
replacing final gibbs sampling scan launch state update possible transition probability update calculated 
data high dimensional observations possible algorithm may rarely accept splits merges proposed appropriate 
potential problem easily seen merge proposals high probability accepted current split configuration high probability produced launch state single gibbs sampling scan 
difficult problems distance traversed gibbs sampling scan may small compared extent posterior variation 
determining split merge proposal accepted analogous problem bayesian model choice intermediate models useful see gelman meng 
analogous technique may useful low acceptance rate split merge proposals proves problem practice 
main priority research extending algorithm handle non conjugate models model parameters analytically integrated away 
believe possible resulting algorithm remains seen 
technique producing metropolis hastings proposals restricted gibbs sampling scans may applicable contexts 
simple gibbs sampling fails dependencies variables prevent changing fixed 
overcome performing gibbs sampling blocks variables provided conditional distribution variables block sampled 
sampling variables block infeasible propose change variables block simultaneously metropolis hastings update finding suitable multi dimensional proposal distribution difficult 
alternative worth exploring initially propose change variables block find appropriate proposed values variables block restricted gibbs sampling updates randomly chosen initial state 
possible compute suitable acceptance probability valid markov chain update algorithms article 
author acknowledges support natural sciences engineering research council canada postgraduate scholarship 
second author research supported natural sciences engineering research council canada institute robotics intelligent systems 
antoniak 
mixtures dirichlet processes applications bayesian nonparametric problems annals statistics vol 
pp 

blackwell macqueen 
ferguson distributions urn schemes annals statistics vol 
pp 

bush maceachern 
semiparametric bayesian model randomised block designs biometrika vol 
pp 

celeux robert 
computational inferential difficulties mixture posterior distributions journal american statistical association appear 
escobar 
estimating normal means dirichlet process prior journal american statistical association vol 
pp 

escobar west 
bayesian density estimation inference mixtures journal american statistical association vol 
pp 
everitt 
latent variable models london chapman hall 
everitt hand 
finite mixture distributions london chapman hall 
ferguson 
bayesian analysis nonparametric problems annals statistics vol 
pp 

ferguson 
bayesian density estimation mixtures normal distributions rizvi editors advances statistics pp 
new york academic press 
gelman meng 
simulating normalizing constants importance sampling bridge sampling path sampling statistical science vol 
pp 

gilks richardson spiegelhalter 
editors markov chain monte carlo practice london chapman hall 
green 
reversible jump markov chain monte carlo computation bayesian model determination biometrika vol 
pp 

green richardson 
modelling heterogeneity dirichlet process draft manuscript 
hastings 
monte carlo sampling methods markov chains applications biometrika vol 
pp 

maceachern 
estimating normal means conjugate style dirichlet process prior communications statistics simulation computation vol 
pp 

maceachern 
computational methods mixture dirichlet process models dey editors practical nonparametric semiparametric bayesian statistics pp 
new york springer verlag 
mclachlan basford 
mixture models inference applications clustering new york marcel dekker 
metropolis rosenbluth rosenbluth teller teller 
equation state calculations fast computing machines journal chemical physics vol 
pp 

neal 
bayesian mixture modeling smith erickson editors maximum entropy bayesian methods proceedings th international workshop maximum entropy bayesian methods statistical analysis seattle pp 
dordrecht kluwer academic publishers 
neal 
markov chain sampling methods dirichlet process mixture models journal computational graphical statistics vol 
pp 

phillips smith 
bayesian model comparisons jump diffusions gilks richardson spiegelhalter 
editors markov chain monte carlo practice pp 
london chapman hall 
richardson green 
bayesian analysis mixtures unknown number components discussion journal royal statistical society series vol 
pp 

ripley 
stochastic simulation new york wiley 
tierney 
markov chains exploring posterior distributions discussion annals statistics vol 
pp 

titterington smith makov 
statistical analysis finite mixture distributions chichester new york wiley 

