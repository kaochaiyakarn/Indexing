poisson mixtures kenneth church william gale bell laboratories murray hill nj usa research att com shannon showed wide range practical problems reduced problem estimating probability distributions words ngrams text 
standard practice text compression speech recognition information retrieval applications shannon theory introduce bag words assumption 
obviously word rates vary genre genre author author topic topic document document section section paragraph paragraph 
proposed poisson mixture captures heterogeneous structure allowing poisson parameter vary documents subject density function intended capture dependencies hidden variables genre author topic 
negative binomial known special case distribution 
poisson mixtures fit data better standard producing accurate estimates variance documents entropy inverse document frequency idf adaptation pr 

problem word rates highly variable applications statistical natural language processing called bag words assumption 
course known word rates depend factors genre author topic table example shows said frequent types texts frequent 
table frequency said depends source source freq words department energy abstracts encyclopedia federalist papers hansard harper row books brown corpus wall street journal associated press associated press associated press associated press associated press associated press word brown corpus francis kucera constructed help researchers better understand word rates vary document document genre genre 
corpus consists excerpts approximately words selected wide variety genres press documents religion hobbies popular lore belle government house organs learned fiction humor 
figures demonstrate structure dramatic impact frequency said 
shows frequency said documents 
similar brown corpus replaced corpus documents randomly generated binomial distribution said brown corpus document number press religion hobbies lore belle gov learned fiction humor said frequent genres frequent 
binomial prediction said document number press religion hobbies lore belle gov learned fiction humor word rates relatively constant binomial particular preference press fiction humor 
pr 
binomial parameters binomial document length word rate chosen match brown corpus words document 
usually interpreted probability success 
case probability word document said 
probability failure word said 
number trials size document 
frequency varies considerably document 
observation precise terms variance variance considerably larger variance 
experience observed variance frequency word ngram documents larger mean larger expected binomial poisson 
errors observed variance poisson prediction tend particularly noticeable content words large diverse collections 
suspect binomial poisson systematically underestimate variance assume dependencies hidden variables genre author topic factors conspire inflate variance 
oo oo oo oo poisson doesn fit freq poisson line fit observed data circles 
data 
line shows poisson pr 
instances said document 
number documents frequency predicted pr number documents collection 
oo oo oo oo poisson mixtures fit better frequency poisson nb poisson mixtures fit data better simple 
negative binomial thin line mixture medium thickness line fall closer observed data points circles simple poisson thick line 
data 
negative binomial pr nb 
pr nb 

figures show poisson fit data alternatives negative binomial mosteller wallace section johnson kotz chapter katz mixture katz personal communication better 
negative binomial poisson word rate parameter allowed vary documents subject density function models dependence possible combinations hidden variables genre author topic johnson kotz pp 
survey number applications negative binomial variety fields medicine psychology economics marketing conclude negative binomial frequently substitute poisson doubtful strict requirements poisson cumulative distribution said poisson nb cumulative probability distribution shows negative binomial thin line mixture medium thickness line fit data circles remarkably especially compared simple poisson thick line 
appear negative binomial data high frequencies overshoot point small amount 
particularly independence satisfied 
mixtures fit data better standard poisson entire frequency range readers may concerned tail mixtures 
mixtures appear data right side graph fact slightly high shown 
case mixtures fit data better poisson 
table content df freq df word kennedy east letter production son statement increased results thinking start addition showed decided binomial poisson 
words deviate binomial said particularly unusual 
table shows words appeared times brown corpus 
kennedy president united states brown corpus collected bunches just documents 
loaded words farther list tend bunched bunched expected binomial poisson model 
binomial df documents words document 
poisson df 
kennedy production statement thinking showed poisson size tail pr varies inversely df 
kennedy large tail small df conversely showed small tail large df 
solid lines show words fit data table mixture model 
words smaller dfs larger tails poisson dotted line 
proposed mixture models differences df produce improved estimates pr 
shows mixture fits words selected table 
tail pr kt varies inversely df 
kennedy large tail small df conversely showed small tail large df 
cases tail larger poisson dotted line 
experience words ngrams larger tail smaller df expected poisson model 
figures examine kennedy showed detail 
shows kennedy mentioned document genre relatively mentioned document genre 
result mentioned fewer documents genres words frequency 
effect pronounced showed word displays strong deviations binomial poisson behavior especially learned genre 
kennedy brown corpus document number press religion hobbies lore belle gov learned fiction humor content words kennedy tend contagious 
appear documents genres appear abundance 
showed brown corpus document number press religion hobbies lore belle gov learned fiction humor showed word relatively little content diffuse binomial spikes learned genre 
table similar table 
table contrasts words appear document df words appear different documents df freq 
class bursty binomial predict 
note tend semantically loaded words proper nouns acronyms technical terms 
second class diffuse expected binomial 
tend relatively devoid strong dependencies hidden variables content document topic genre table bursty words tend content freq df bursty df freq diffuse blackman drug hardy collage naturally norman cease claiming clue confident indispensable landed originated restricted sweep termed handley nicolas willis clover leveling back right absurd appearing collect deserves devised discussing faster inherited legitimate lined link men persuade praise refuse severely shops sole spreading unnecessary bod krim accelerometer go artificial capture consistently designated expecting formally grasp lit obscure pushing respective spontaneous surprisingly vitality andrei keys langford madden saxon avoided birthday emphasized escaped gather instantly packed proceed repeatedly submit surrounded alike amazing bold happily notable overwhelming remainder rid rush savage far mentioned ways think variability word rates documents 

variance statistics 
document frequency information retrieval 
content linguistics variance natural way thinking variability especially statistics literature 
document frequency df closely related inverse document frequency idf quantity information retrieval sparck jones 
idf usually thought indicator variability may certain robustness advantages variance sample variance notoriously sensitive outliers 
measures variability introduced shortly entropy burstiness adaptation 

poisson pr probability document exactly instances term 
pr probability poisson model pr 

poisson parameter estimation number theoretical models poisson various mixtures 
presenting models mention ways estimate parameters model 
simplest method called method moments equates theoretical values moments sample estimates solves system equations unknowns obtain estimates parameters 
case poisson parameter consequently equation 
method moments simply equates theoretical value mean sample mean mean term frequencies 
element denotes frequency term th document 
assume documents length 
variability usually thought estimate mean word rate interpreted estimate variance word rate documents 
poisson mean variance equal mentioned problems poisson sample variance usually larger mean considerable margin 
models soon allow flexibility variance 
variance just ways measure variability word rates documents 
measures considered variance entropy burstiness inverse document frequency idf adaptation 
poisson summary statistics wish consider depend single parameter simple closed form mean variance idf log pr log inverse document frequency idf pr log pr entropy pr burstiness pr pr pr qe adaptation notation pr short hand pr 
mentioned variance natural way thinking variability 
idf borrowed information retrieval sparck jones 
entropy information theory bell cover 
accurate predictions entropy important applications shannon theory 
burstiness katz innovation katz personal communication 
mean ignores documents instances word 
burstiness convenient quantity working mixture 
pr useful modeling adaptation 
adaptive language models popular speech recognition literature lau 
standard independence assumptions extremely lightning strike twice half dozen times document 
text contagious disease lightning 
see instance contagious disease tuberculosis city surprised find quite 
similarly instances said observed document probably 

empirical estimates variability mean measures variability estimated empirically vector term 
pr fraction documents term frequency mean variance idf log pr inverse document frequency idf pr log pr entropy pr burstiness pr pr pr adaptation table shows typical values measures 
words selected brown corpus extracting upper case words narrow frequency range 
list sorted variance 
poisson predict values 
rows labeled poisson computed poisson 
qs words fall range poisson appropriate model words observed variance range observed idf 
data show poisson systematically overestimates entropy underestimates values 
errors particularly large content words relatively important dependencies hidden variables genre 
church example large variance genre religion 
similarly government large variance genre government house organs 
island somewhat surprising turns brown corpus collected rhode island consequently island highly associated genre government house organs 
word rates vary genre genre topic topic author author document document section section paragraph paragraph 
factors tend decrease entropy increase test variables 
table variability highly associated content mean var idf entropy burstiness adaptation government island church federal christian kennedy soviet east william north french george city poisson poisson columns table highly correlated shown table 
poisson values predictable systematic structure group words share experience large correlations table expected group words ngrams similar means 
table pairwise correlations values table variance idf entropy burstiness adaptation variance idf entropy burstiness adaptation table shows idf variance years associated press ap set words 
table shows estimates year predictors estimates year 
idf tends larger correlations year variance indicating idf somewhat robust variance 
estimates tend degrade time indicated larger correlations adjacent years non adjacent years 
want predict idf year ap better data year data decade ago 
time structure varies word course 
idf george relatively low george bush running president 
similarly idf east relatively low berlin wall falling 

tools accessing ap corpus difficult 
table estimates different years associated press ap newswire idf variance government island church christian kennedy french east federal north william city soviet george table correlations columns table idf variance table shows residual idf difference observed idf value predicted poisson model 
table shows residual variance 
residual idf idf idf idf log residual residuals table large systematic 
systematic distinguish better keywords keywords 
customary information retrieval weight words idf conjecture residual idf better 
consider french 
roughly idf quite different residual 
intuitively french better keyword 
document mentions word practically 
words table sorted idf words table sorted residual idf values 
believe words top table tend better keywords words top table 
residual idf better job weighting words years 
consider words george east 
cases idf relatively large residual idf relatively small vice versa 
residual idf values sense 
weight george example ought relatively large election years weight east ought relatively large east germany merged west 
magnitude deviations poisson depends factors linguistic 
deviations noticeable larger diverse collections especially words associated particular type diversity salient collection 
effects larger content words presumably variations content tend dominate factors stylistic variation 
general semantic content leads variance idf entropy table residuals poisson residual idf residual variance soviet kennedy north french church east christian city island federal george william government burstiness adaptation 

poisson model poisson model simple example poisson mixture 
pr poisson model information retrieval literature bookstein swanson harter account fact relevant documents tend different frequencies irrelevant documents 
harter showed method moments fit parameters poisson model moments 
th moment zero 
estimated empirically pr 
roots quadratic equation aq bq tables show outperform single poisson 
obs values table borrowed table 
est values computed poisson model 
poisson model smaller errors standard poisson model 
err ors computed difference estimated values observed values 
rms root mean square errors table computed err word est obs rms error rms errors show poisson model outperforms poisson respects idf entropy adaptation improvement entropy particularly impressive 
table estimation errors poisson idf entropy adaptation est obs err est obs err est obs err government island church federal christian kennedy soviet east william north french george city table rms root mean square errors words table poisson poisson poisson idf entropy adaptation unfortunately probably illustrated 
bookstein swanson came suggested poisson model noted require parameters poisson model 
suggested negative binomial viewed continuous mixture infinitely multiple poisson distribution may course generalized continuous association strength 
negative binomial distribution examined mosteller wallace form 
bookstein swanson 
poisson mixtures poisson mixtures thought generalization poisson model mixing parameter replaced arbitrary density function density function intended capture dependencies hidden variables genre topic author general form poisson mixture pr dq 
poisson 

arbitrary density function 
density function integrate 
dq 
frequency nb poisson poisson model thick line exhibits serious dropout qs 
case qs mixing parameter 
negative binomial thin line mixture medium thickness line data circles 
special cases discussed poisson pr nb negative binomial pr nb mixture pr dirac delta function density function 

nb known gamma distribution 

negative binomial negative binomial named analogy binomial 
pr pr nb formulated th term binomial expansion binomial pr 
negative binomial pr nb 
assure probabilities sum required binomial case negative binomial case 
positive non zero 
consequently binomial case negative binomial case 
binomial model interpreted probability success word said probability failure word said number trials size document 
interpretations apply case negative binomial greater need integer 
variability mean word rate measures variability expressed terms parameters negative binomial negative binomial nb np mean nb variance idf nb log pr nb log inverse document frequency idf nb pr nb log pr nb entropy nb np burstiness pr nb pr nb pr nb adaptation note binomial negative binomial symbolic expression mean np variance 
binomial greater negative binomial 
consequently variance mean binomial greater mean negative binomial 
mentioned previously sample variance larger sample mean consequently negative binomial appropriate binomial 
parameter estimation johnson kotz chapter series methods estimating parameters negative binomial distribution 
possible estimate pair parameters discussed mean variance idf burstiness adaptation entropy 
johnson kotz method uses mean variance method uses mean idf 
method simply method moments equates sample mean variance theoretical values solves equations solved negative binomial inappropriate case 
fall back poisson sample mean exceeds sample variance 
usually happens insufficient data just observation word just document 
tables show negative binomial outperforms poisson poisson table estimation errors negative binomial method idf entropy adaptation est obs err est obs err est obs err government island church federal christian kennedy soviet east william north french george city table rms root mean square errors poisson poisson poisson negative binomial negative binomial method method idf entropy adaptation words table 
method moments simplest method implement experience method produces robust estimates sample variance notoriously sensitive outliers 
may explain modest improvement table rms error reduced entropy adaptation 
method simply equates sample mean idf theoretical values solves log pr log equations follows satisfy constraint log unfortunately closed form solution fragment code katz personal communication uses iterative approximation 
input parameters correspond pr respectively 
define precision double method double double log double double double log precision return 
mixture recall mixture pr mixture 
mixture close negative binomial pr pr nb closer data 
convenient mixture easier 
variability mean word rate measures variability expressed terms parameters mixture mean variance idf log pr log ab inverse document frequency idf log pr log pr entropy burstiness pr adaptation fact adaptation property stronger mixture 
models considered mixture predicts adaptation constant pr 
constant adaptation mixture simple closed form expression tail probability pr 
tail probability parameter estimation estimated method moments 
equate sample mean variance theoretical values solve negative binomial method moments produces range parameter estimates distribution inappropriate 
poisson happens 
easier estimate sample mean sample estimate burstiness burstiness particularly convenient form 
simply assume sample mean sample estimate burstiness theoretical values 
equate solve method burstiness respect mixture convenient negative binomial 
parameters mixture closely related observable quantities contrast relationship parameters negative binomial obvious 
equations relating closed form solution solved iterative approximation 
negative binomial method mixture method outperform poisson method moments poisson method moments seen comparing rms errors table rms errors table 
test idf tables replaced test variance variance fit parameters tables idf fit parameters table 
table suggests negative binomial slightly superior mixture experience methods extremely similar 
methods closer data 
errors methods highly correlated 
pair columns table correlation second pair correlation pair correlation 
mixtures fit data better alternatives considered large correlations indicate plenty room improvement 

variable length documents far ignoring fact documents longer 
document lengths modeled adding second parameter general form poisson mixture 
length document 
general form mixture pr wq dq 
special cases poisson pr wq wq nb negative binomial pr nb wp table errors mixture negative binomial variance entropy adaptation mix nb mix nb mix nb government island church federal christian kennedy soviet east william north french george city rms error mixture pr wb wb wb formulas stated previously mean measures variability generalized account variable length documents introducing constraints wq wq poisson wp negative binomial wb mixture 
possible applications shannon standard practice assume constant word rates important practical applications compression speech recognition information retrieval 
model variable word rates poisson mixtures important ramifications applications 
probability string estimated breaking string pieces letters words ngrams 
probability entire string computed multiplying probabilities pieces practice justified introducing multinomial assumption assumes constant 
course seen word rates constant 
second instance word ought surprising 
ought quantity discount 
adaptive language models quite popular speech recognition 
lau example introduce cache remember words 
estimates long term evidence combined set estimates short term evidence cache 
principle necessary introduce cache order capture variable word rates 
required probabilities completely determined pr 
imagine different distribution pr word initially probability word seen document length pr suppose word probability instance jumps probabilities words adjusted downward probabilities sum 
way ought possible estimate probability string introducing constant word rate assumption ad hoc device cache 
course remains done show possibility practical realistic application 
poisson mixtures somewhat simpler apply necessary know detailed order words document 
information retrieval author identification cases 
information retrieval ir author identification word sense disambiguation think probabilistic retrieval model van rijsbergen chapter salton section application understood bayesian discrimination methods studied extensively mosteller wallace section investigation authorship federalist papers 
discrimination process consists phases training phase followed testing phase 
training phase classes documents asked construct discriminator distinguish classes documents 
discriminators applied new documents testing phase 
author identification task example training set consists documents written authors hamilton madison 
resulting discriminator tested documents authorship disputed 
information retrieval application training set consists query set relevant documents set zero irrelevant documents 
resulting discriminator applied documents library order separate relevant ones relevant ones 
wealth information collection documents basis discrimination 
common practice treat documents homogeneous bag words ignore heterogeneous structure including linguistic factors dependencies word order correlations pairs words 
words assuming sources word probabilities rel rel ir application author author author identification application 
training phase attempt estimate pr probability observing instances document generated source words vocabulary sources testing phase score documents follows select high scoring documents relatively generated source interest 
score doc doc pr rel pr rel information ir score doc doc pr author pr author author identification mosteller wallace framework address variety problems natural language processing 
gale 
looked word sense disambiguation problem showed ambiguous words sentence disambiguated treating word context surrounding word document score doc context pr sense pr sense word sense disambiguation scoring procedure asks context question contexts sense contexts sense 
large amounts testing training material obtained making parallel texts texts canadian parliamentary debates available multiple languages 
assumed sentence translated french judicial sense sentence translated phrase syntactic sense 
subsequent yarowsky shown free system dependence parallel text source testing training material 
example poisson mixtures discrimination task mosteller wallace studied federalist papers small collection articles words written alexander hamilton john jay james madison single question citizens state new york proposed constitution 
despite limited number authors subject matter articles cover nearly phase proposed constitution 
federalist papers far homogeneous demonstrated 
variance word courts federalist papers times larger poisson predict 
suppose illustrative purposes wanted discriminate documents documents 
call classes seen table word courts useful discriminator 
intuitively instances courts document strong evidence class instances weak evidence class zero instances courts evidence way classes documents instances courts 
courts evidence way courts weak evidence courts strong evidence argument rigorous terms log likelihood function indicates adjust belief category instances word single document 
poisson model mosteller wallace show log pr pr poisson parameters categories 
similar document scoring procedure discussed 
similar bookstein pp 
referred poisson model somewhat different authors mean refer poisson model 
mosteller wallace concerned strict requirements poisson repeated study negative 
reasoning nb defined analogously replacing poisson numerator denominator pr pr nb pr 
poisson model likelihood ratio departs treatments information retrieval literature 
functions shown parameter estimates 
poisson 
mosteller wallace study content words courts placed list variations content interfere author dependent factors interest 
information retrieval programs just reverse function words put list stylistic factors interfere content dependent factors interest 

mosteller wallace refinement mentioned previous section allows documents different lengths dropped variable order simplify discussion complicated poisson mixtures 

harter interested question estimating parameters poisson model unlabeled training material 
author identification case training material labeled told training documents written madison ones written hamilton 
mosteller wallace estimate qs simply counting word rates classes 
information retrieval case may told documents relevant 
harter fit qs assuming observed word rates generated mixture relevant irrelevant documents ap departed bookstein swanson bookstein abandoned log likelihood framework 
robertson walker discuss poisson model model notion quite different versions model 

poisson 
negative binomial 
mixture courts federalist papers document number mh mmm jh federalist papers smaller diverse brown corpus federalist papers far homogeneous 
word courts example striking dependence document number large bursts documents 
strong dependence authorship courts associated hamilton madison jay 
table data class doc freq class doc freq courts vs frequency nb poisson poisson log likelihood functions nb 
shows straight line ends suspect 
high frequencies high 
favors class incredible odds 
especially hard believe data points 
scale predicts instances courts weak evidence odds class courts freq dip middle dropout problem illustrated 
poisson model exhibits dropout qs indicated vertical lines case 
third help fill dropout continuous mixture infinitely qs better 
runs counter intuition stated 
zero instances evidence way classes number documents zero instances courts 
conclude poisson model describe straight line rigid account description 
mixtures parameters poisson consequently ls need straight 
bent ls credible ends 
unfortunately poisson introduces spurious dip dropout qs shown 
poisson produce strange undesirable behavior multiplied divided calculation log likelihood functions 
negative binomial mixture fit intuition better poisson poisson 
ends credible embarrassing dip middle size improvement substantial orders magnitude range interest 

poisson mixtures fit data better standard poisson 
standard poisson text modeled homogeneous bag words constant documents proposed mixtures heterogeneity text modeled allowing vary documents subject density function designed capture dependencies hidden variables genre topic author factors effect variance entropy burstiness inverse document frequency idf adaptation pr systematic ways 
hidden variables tend conspire decrease entropy increase measures variability poisson 
deviations poisson noticeable larger diverse collections especially words associated particular type diversity salient collection 

may problem versions poisson model information retrieval generally multiply divide poisson model way 
deviations poisson salient interesting words government courts boring functions words 
applications information retrieval word sense disambiguation attempt discriminate documents basis certain hidden variables topic author genre style keyword ngram deviates poisson stronger dependence hidden variables useful keyword ngram discriminating documents basis hidden dependences 
similar arguments apply host important applications text compression language modeling speech recognition desirable word ngram probabilities adapt appropriately frequency changes due various hidden dependencies 
collections looked effects larger content words government smaller function words 
size deviations poisson dramatic 
previous section standard poisson model predicted incredible odds case truth probably closer negative binomial prediction 
acknowledgments benefited considerably extensive discussions slava katz 
bell cleary witten 
text compression prentice hall new jersey 
bookstein 
explanation generalization vector models information conference research development information retrieval sigir pp 

bookstein swanson 
probabilistic models automatic indexing journal american society information science pp 

cover thomas 
elements information theory john wiley sons new york 
francis kucera 
frequency analysis english usage houghton mifflin boston 
gale church yarowsky 
method disambiguating word senses large corpus computers humanities pp 

harter 
probabilistic approach automatic keyword indexing part distribution specialty words technical literature journal american society information science 
johnson kotz 
discrete distributions houghton mifflin boston 
katz 
personal communication 
lau rosenfeld roukos 
adaptive language modeling maximum entropy principle arpa sponsored workshop human language technology morgan kaufmann publishers san francisco ca isbn pp 

mosteller david wallace inference disputed authorship federalist addison wesley reading massachusetts 
robertson walker 
simple effective approximations poisson model probabilistic weighted retrieval sigir 
salton 
automatic text processing addison wesley 
shannon 
mathematical theory communication bell system technical journal 
sparck jones 
statistical interpretation term specificity application retrieval journal documentation pp 

van rijsbergen 
information retrieval second edition butterworths london 
yarowsky 
word sense disambiguation statistical models roget categories trained large corpora coling 
