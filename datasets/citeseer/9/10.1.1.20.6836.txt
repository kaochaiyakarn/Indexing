identifying dynamic replication strategies high performance data grid ranganathan ian foster department computer science university chicago th street chicago il cs uchicago edu foster mcs anl gov 
dynamic replication reduce bandwidth consumption access latency high performance data grids users require remote access large files 
different replication strategies defined depending replicas created destroyed 
describe simulation framework developed enable comparative studies alternative dynamic replication strategies 
preliminary results obtained simulator evaluate performance different replication strategies different kinds access patterns 
data scenario read consistency issues involved 
simulation results show significant savings latency bandwidth obtained access patterns contain small degree geographical locality 
data grid connects collection geographically distributed computer storage resources may located different parts country different countries enables users share data resources :10.1.1.32.6963
research projects griphyn ppdg eu datagrid aim build scientific data grids enable scientists sitting various universities research labs collaborate share data sets computational power 
physics experiments cms atlas ligo sdss churn large amounts scientific data cases scale petabytes year 
data needs thousands scientists world 
sheer volume data computation involved poses new problems deal data access processing distribution 
aspects grid sharing data sharing resources 
scientist located small university may need run time consuming processing job huge data set 
may choose get data exists local computing resource run job 
alternatively may better transfer job data exists job specification data may sent third location perform computation return results scientist 
focus data distribution aspect grid 
data grid envisioned griphyn project hierarchical nature organized tiers 
source data produced denoted tier cern 
tier national centers tier regional centers rc tier workgroups tier consists thousands desktops 
replication user generates request file large amounts bandwidth consumed transfer file server client 
furthermore latency involved significant considering size files involved 
study investigates usefulness creating replicas distribute data sets various scientists grid 
main aims replication reduce access latency bandwidth consumption 
replication help load balancing improve reliability creating multiple copies data 
static replication achieve mentioned gains drawback adapt changes user behavior 
scenario data amounts petabytes user community order thousands world static replication sound feasible 
system needs dynamic replication strategies replica creation deletion management done automatically strategies ability adapt changes user behavior 
study examines different dynamic replication strategies grid 
related questions resource discovery request goes nearest replica furthermore distribute requests replicas archive best results 
study shall concentrate replica placement issue 
fundamental questions replica placement strategy answer replicas created 
files replicated 
replicas placed 
answers questions lead different replication strategies 
simulation evaluate performance different strategy 
datasets scientific data grid scenario read consider overhead updates 
describes grid simulator framework reports preliminary results obtained simulator 
rest organized follows 
section describes specific grid scenario simulations 
section discusses simulator built conduct experiments 
section describes replication caching strategies evaluate 
section presents results experiments interpret results experiments section 
directions section 
grid scenario particular data grid setup studying described 
tiers grid data produced top tier root 
tier consists regional centers country scenario considered experiments reported 
tier composed groups universities research labs 
final tier individual workstations consists sources requests arrive 
total nodes grid generating requests 
storage capacity tier table 
experiments assume network links mbytes sec bandwidth 
reality network bandwidths vary widely tiers 
total data generated source assumed petabytes 
data stored files uniform size gigabytes 
table 
system parameters 
network performance node capacity node tier hierarchy described 
tier network bandwidth tier mb storage capacity tb requests files generated leaf nodes 
request patterns files exhibit various locality properties including temporal locality accessed files accessed 
geographical locality client locality files accessed client accessed nearby clients 
spatial locality file locality files near accessed file accessed 
definition spatial locality specify near means 
definition involves study nature data files relate files 
deals general data grid defer study relationships data model specific grid 
know extent file access patterns exhibit locality properties described locality 
educated guesses point 
worst case scenario access patterns exhibit locality generating random access patterns simulate situation 
methodology study identify suitable replication strategy high performance data grid decided simulator 
tools currently available exactly fitted needs built simulator model data grid data transfers 
simulator uses parsec discrete event simulation tool model events file requests data transfers 
simulator simulator consists parts 
basic core simulates various nodes different tiers data grid links file transfers tier 
various replication strategies built top core compose layer 
final component driver entity program triggers file requests 
driver entity reads input file specifying access patterns simulated 
simulation works topology specification starting simulation involves specifying topology grid including number nodes tier connected bandwidth link location files various nodes 
starting simulation access patterns read file line representing access specifying time node needs particular file 
driver reads data triggers corresponding node 
node receives file needed trigger needs locate request nearest replica file 
locating nearest replica various proposed methods locating nearest replica involve complex algorithms identify closest copy 
location best replica related different topic trying answer 
concentrates designing isolating best replica placement strategy grid 
show effectiveness dynamic replication strategy node needs able identify nearest replica 
solve problem number hops heuristic 
nearest replica number steps away node 
case tie replicas selected randomly 
file transfer server gets request file sends client 
tree structure grid means shortest path messages files travel get destination 
file transferred link link busy transport file duration transfer 
delay incurred transferring file depends size file bandwidth link number pending requests 
node busy duration transfers file network incoming data wait current transaction finish 
record keeping node keeps record time took file requested transported 
time record forms basis compare various replication strategies 
series file request run different strategies lower average response time considered better 
various replication strategies described section 
scaling amount data system order petabytes 
enable simulation large data values scale 
number files system reduced factor 
accordingly storage capacity tier reduced 
table illustrates fact 
table 
grid parameters scaling actual size scaling number files storage tier terabytes gigabytes tier terabytes gigabytes tier terabytes gigabytes link bandwidths file sizes remained 
reason scaling factor simulation system feasible single machine 
meta data files memory intensive 
need scale number files storage capacity tier needs scaled accordingly 
performance replication strategies directly dependent percentage files stored node 
scaling number files system capacity node achieves 
scale file sizes increasing effect percentage files stored node 
file sizes scaled bandwidths remain unscaled transport latency modeled correctly 
individual workstations assumed able cache file size gigabytes 
access patterns physics data grid functional actual file access patterns available artificial traces 
derive traces 
simulation run random access patterns 
worst case scenario realistic access patterns contained varying amounts temporal geographical locality generated different kinds traces described random random access patterns 
locality patterns 
data contained small degree temporal locality data containing small degree geographical temporal locality index measure amount locality patterns denoted 
means requests completely random locality 
spectrum means requests file 
generate data small degree temporal geographical locality geometric distribution file popularity 
performance evaluation compare different replication strategies measuring average response time total bandwidth consumed 
response time time elapses node sends request file receives complete file 
local copy file exists response time assumed zero 
average response times length simulation calculated 
bandwidth consumption includes bandwidth consumed data transfers occurred node requests file server creates replica node 
replication caching strategies implemented evaluated different strategies 
results demonstrate simulator capable doing help understand dynamics grid system 
distinguish caching replication 
replication assumed server side phenomenon 
server decides create copy files 
may randomly recording client behavior means 
decision copy replica send node taken solely server 
caching defined client side phenomenon 
client requests file stores copy file locally 
nearby node request cached copy 
different strategies discussed 
strategy replication caching base case compare various strategies replication takes place 
entire data set available root hierarchy simulation starts 
run set access patterns calculate average response time bandwidth consumed replication involved strategy best client node maintains detailed history file contains indicating number requests file nodes request came 
replication strategy works follows time interval node checks see number requests file exceeded threshold 
best client file identified 
best client generated requests file 
node creates replica file best client 
files exceed threshold number requests replicated 
replica created request details file server node cleared 
recording process begins file replacement algorithm node discussed replication strategies facilitate common ground comparing 
strategy cascading replication best analogy strategy fountain 
water originates top 
fills top ledge overflows level 
level overflows water reaches lowest part 
data strategy flows similar way 
threshold file exceeded root replica created level path best client 
new site replica ancestor best client 
threshold file exceeded level replicated lower tier 
popular file may ultimately replicated client 
advantage strategy storage space tiers 
advantage access patterns exhibit high degree temporal locality geographical locality exploited strategy 
replicating source requests higher level data brought closer nodes sub tree 
exceeds threshold fig cascading replication 
root number requests exceed threshold copy sent layer 
eventually threshold exceeded layer copy sent client strategy plain caching client requests file stores copy locally 
files large gigabytes client space store file time files get replaced quickly 
strategy caching plus cascading replication combines strategy 
client caches files locally 
server periodically identifies popular files propagates hierarchy 
note clients located leaves tree node hierarchy server 
specifically client act server siblings 
siblings nodes parent 
strategy fast spread method replica file stored node path client 
client requests file copy stored root exceeds threshold tier way 
leads faster spread data 
generic file replacement strategy cases discussed 
file replacement strategy storage spaces levels eventually fill 
efficient file replacement strategy needed popular files retained displaced new files arrive 
initially decided popular file list 
delete relatively new file just come requested popular 
needs measure time age file cache 
replacement strategy employed takes care aspects combination popular age file 
file equally unpopular oldest file deleted clear popularity logs time interval order capture dynamics access patterns 
time users may shift group files group expect effectiveness strategy depend time interval tuned access behavior 
parameter tuned scenario threshold 
number requests exceeds threshold file replicated 
imagine refining algorithm time check interval threshold automatically change user behavior 
left 
experimental results results strategies replication plain caching best client caching cascading fast spread 
discuss pure cascading results strategy cascading caching 
experiments run access patterns defined earlier random simulation run requests 
random patterns worst case scenario sensible assume patterns exhibit amount geographical temporal locality scientists tend groups projects 
said proceed discuss results obtained experiments 
random data strategies best client cascading show significant improvement access latency compared case replication 
best client random access patterns average response time times replication caching policy 
terms bandwidth savings best client utilizes amount bandwidth base case replication 
case access patterns patterns small amount temporal locality strategies best client yield positive savings access latency bandwidth consumption 
case patterns temporal geographical locality best client show savings 
case bandwidth savings best client marginal savings compared base case replication caching latency savings significant compared base case 
best client consistently performs worse plain caching 
discuss results obtained strategies best client candidate replication strategy grid 
graphs contain results cascading caching fast spread plain caching 
strategies compared plain caching standard comparison 
graphs illustrate savings achieved fast spread cascading achieved caching files 
fig indicates cascading access patterns contain locality 
random data response time far better plain caching cascading 
fast spread works better plain caching random data 
reduction response times case fast spread 
random access patterns 

fast spread 

fast spread random access patterns fig 
percentage savings response time left bandwidth consumption right compared plain caching kinds access patterns case patterns advantage fast spread caching decreases cascading works caching 
data contains locality case cascading significant improvement performance average response time plain caching 
fast spread improvement caching patterns contain geographical locality 
results interpreted section 
discuss amount bandwidth savings different cases 
shown fig cascading differ significantly caching terms bandwidth consumption 
difference strategies falls range plus minus access patterns 
fast spread hand leads large savings bandwidth usage access patterns contain locality 
discussions methods replication consider best client performs worst 
cases overheads creates advantages strategy performs worse base case replication 
considering remaining candidates plain caching cascading caching fast spread sure best strategy scenarios 
fast spread consistently performs better caching terms response time bandwidth savings 
spite overhead fast spread terms excessive creation replicas advantages caching plainly evident 
bandwidth savings fast spread caching refer fig 
disadvantage high storage requirements 
entire storage space tier fully utilized fast spread 
cascading hand utilizes storage space tier involves judicious creation replicas 
bandwidth requirements cascading greater fast spread 
replica created sent separately new location opposed fast spread copy created process transferring requested file 
cascading access patterns totally random 
fact caching random user patterns 
attributed fact overhead creating extra copies files offset advantage moving closer users 
copied files asked justify increased data movement 
patterns contain small amount locality performance cascading improves significantly 
performs better fast spread patterns average response time better fast spread 
significant bandwidth savings cascading caching assume small amount geographical locality 
results lead conclude grid users exhibit total randomness accessing data strategy best fast spread 
sufficient amount geographical locality access patterns cascading replication policy better 
amount bandwidth utilization caching cascading lowers response times significantly judiciously storage space 
results indicate depending important grid scenario lower response times lesser bandwidth consumption tradeoff cascading fast spread 
chief aim elicit faster responses system cascading better 
hand conserving bandwidth top priority fast spread better grid replication strategy 
evaluated dynamic replication strategies managing large data sets high performance data grid 
replication enables faster access files decreases bandwidth consumption distributes server load 
contrast static replication dynamic replication automatically creates deletes replicas changing access patterns ensures benefits replication continue user behavior changes 
discussed components simulator built explained simulator study performance different replication strategies grid environment generated different kinds access patterns random temporal geographical showed bandwidth savings latency differ access patterns 
strategies performed best tests cascading fast spread 
fast spread worked random request patterns cascading worked better small amount locality 
analyzed thought best strategies pros cons method 
want simulator test performance advanced replication strategies 
plans extend simulator plug different algorithms selecting best replica 
far replication strategies discussed exploit temporal geographical locality request patterns 
put considering spatial locality requests 
better understand relationship various files scientific data set amount pre fetching possible 
area research study movement code data 
assumed clients ask files locally run data code analyze data 
considered moving data code 
option move code data resides communicate result computation back client 
feasible option considering data grid scenario data may tens thousands times larger code result 
data grid enables thousands scientists sitting various universities research centers collaborate share data resources 
sheer volume data computation calls sophisticated data management resource allocation 
step better understanding dynamics system issues involved increasing efficiency grid intelligent replica creation movement 
research supported national science foundation griphyn project contract itr 

acharya zdonik efficient scheme dynamic data replication technical report cs brown university 
quantifying impact caching replication web 
university kaiserslautern february 
bestavros cunha server initiated document dissemination www 
ieee data engineering bulletin vol 

bestavros demand document dissemination reduce traffic balance load distributed information systems 
ieee symposium parallel distributed processing san antonio tx 
calvert zegura self organizing wide area network caches 
georgia institute technology git cc 
chervenak foster kesselman salisbury tuecke data grid architecture distributed management analysis large scientific data sets 
network computer applications 
chuang distributed network storage quality service guarantees 
proc 
inet 
fan cao almeida broder summary cache scalable wide area web cache sharing protocol 
acm sigcomm 
foster kesselman 
eds grid blueprint new computing infrastructure 
morgan kaufmann 
foster kesselman tuecke anatomy grid enabling scalable virtual organizations 
intl 
supercomputer applications appear 

griphyn grid physics network project www griphyn org 
gwertzman seltzer case geographical push caching 
th annual workshop hot operating systems 
benchmark virtual data grid schedulers 
home cern ch tmp ps 
michel nguyen rosenstein zhang floyd jacobson adaptive web caching new global caching architecture 
proceedings rd international www caching workshop 
parsec home page pcl cs ucla edu projects parsec 
rabinovich aggarwal radar scalable architecture global web hosting service 
th int 
world wide web conf may 
samar stockinger grid data management pilot gdmp tool wide area replication 
iasted international conference applied informatics innsbruck austria 
wolfson jajodia huang adaptive data replication algorithm 
acm transactions database systems 
