compositional syntax cultural transmission henry brighton growing body demonstrates syntactic structure evolve populations genetically identical agents 
traditional explanations emergence syntactic structure employ argument genetic evolution syntactic structure speci ed innate language acquisition device lad 
knowledge language complex data available language learner sparse 
situation termed poverty stimulus accounted placing speci cation language lad 
assumption characteristic structure language coded genetically 
ect language evolution cultural substrate absence genetic change addressed explanation 
show poverty stimulus introduces pressure compositional language structure consider language evolution resulting iterated observational learning 
mathematical model map space parameters result compositional syntax 
hypothesis compositional syntax explained understanding lad compositionality emergent property dynamics resulting sparse language exposure 
keywords language evolution syntax learning compression 
children master complex features language basis surprisingly little evidence 
phenomenon termed poverty stimulus taken evidence innate linguistic knowledge chomsky 
lec dept theoretical applied linguistics university edinburgh uk rich body linguistic knowledge induced impoverished body linguistic evidence 
traditional explanation appeals innate language acquisition device lad chomsky 
lad language speci module encapsulating knowledge language induced primary linguistic data 
explanation attributes bulk speci cation language human biological endowment linguistic stimulus 
short considering origins language humans possibly developed language directly result biological evolution lad pinker bloom nowak 
syntactic structure language explained terms lad fundamentals story language emerged language structure place 
important linguistic dynamic missing explanation language evolve cultural substrate genetically homogeneous language users 
investigating degree dynamic result language evolution agent simulations questioned primacy accounts linguistic structure 
example kirby batali demonstrate recursive compositional syntax emerge absence genetic change 
experiments raise important questions 
explain language characteristic structure analyzing lad 
structure language determined dynamics resulting evolution cultural substrate 
situation characterized poverty stimulus poses design problem 
solution innate structure specifying lad 
mathematical model propose alternative solution 
poverty stimulus introduces strong pressure compositional structure take account cultural evolution hypothesis characteristic structure language compositionality distinctive feature emerges result pressures transmission 
important stress degree innate lad required language syntactic structure language need explicitly speci ed lad 
previous characterized establishing compositional recursive syntax language emerge relative population genetically homogeneous language users kirby batali 
build rst mapping space model parameters lead linguistic structure second introducing model accounts statistical ects language acquisition process 
aiming demonstrate linguistic structure emerge absence genetic change methodology focuses establishing range conditions emergence 
results show characterizing poverty stimulus constraint transmission overcome lad best conceived determinant evolution compositional syntax 
argue compositional structure best explained terms dynamics language evolution internals lad 
model language evolution iterated learning model agent model cultural evolution language change results repeated observational learning kirby brighton kirby 
section details iterated learning model 
model agent ability generalize observed examples language 
generalization bias agent determines language changes generation generation 
section introduce generalization procedure minimum description length principle 
abstracted form generalization procedure developed section form part mathematical model 
model covered section map parameter space 
results discussed section 
iterated learning model iterated learning model ilm provides framework modeling cultural evolution language kirby brighton kirby 
language transmitted generation agent forming utterances agent observing utterances 
process repeated generation generation linguistic terms characterizes translation language performance language language competence language knowledge observational learning agents induce knowledge language observation 
ilm captures important characteristic language 
language learned language users input learning process output learning 
important phenomenon 
space possible languages restricted contain languages produced agent 
ects bias learning production ampli ed generation 
ilm agent model agent represents language user 
basic population model generation comprises agent ilm proceeds follows 
rst generation agent forms utterances 
second generation agent observes utterances forms hypothesis account 
generation proceeds producing utterances agent observes 
process repeated thousands generations 
example language di er system reach steady state 
kind languages emerge characterize steady states 
ilm allows answer questions 
exploring properties agent certain pressures information transmission try attack issue certain language structures evolve 
key idea language evolves agent model initially identical 
agents arrive di erent hypotheses exposed di erent bodies linguistic evidence 
examining ilm detail worth making explicit simplifying assumptions construction model 
example generation contains agent observational learning unidirectional adults learn infants 
important simpli cation allowing agents ability mind read 
agent observes signal intended meaning signal 
simpli cation avoids problem modeling ascription meaning signal 
agent associate signal meaning regard separate non trivial problem steels smith 
short assume meaning transmission occurs noiseless channel 
modeling perspective simpli cations crucial 
ultimately seek minimal set assumptions hypotheses linguistic structure explained 
example population ects may important rst see explained 
important note elements language structure evolve iterated learning biological evolution form backbone story language evolution see example dunbar 
structure agents learn conceptualize relevant situations taken biologically determined 
iterated learning detail agent senses external environment contains objects 
object represents relevant situation 
attempt specify details situations 
important point objects conceptualized terms meaning space internal agent 
agent object corresponds point meaning space 
illustrates relationship external environment internal meaning space 
formally meaning vector drawn space de ned parameters number features meaning space agent environment relationship environment agent 
environment contains set objects 
represent relevant situations 
objects perceived agent terms semantic space 
example dimensional meaning space represent objects internally 
cardinality set feature values drawn 
simplifying assumption feature number possible values assumption simpli es mathematics model 
short meanings points dimensional space dimension having discrete values 
environment remains constant time assume di erent agents sense object way correspondence object meaning identical di erent agents 
mapping objects meanings random 
point view agent objects indistinguishable meaning label object 
language mapping meanings signals agents basis linguistic competence utter signals stand meanings 
language de nes relationship meanings signals 
formally language mapping meanings signals 
de ne follows 
signals de ned strings symbols drawn alphabet 
maximum length signal max meaning signal meaning signal pair denoted hm si drawn meaning space fg signal drawn signal space fw max language de ned set meaning signal hm si pairs 
space languages classes language structure considered holistic languages compositional languages 
central distinction relationship meaning structure signal structure 
holistic language exhibits structural relationship meanings signals signal stands meaning parts signal referring parts meaning 
compositional language relationship exist meaning signal meaning signal function meaning parts assembled montague 
holistic languages compositional languages constructed follows construct holistic language holistic set meanings random signal assigned meaning 
signal random length max composed symbols drawn randomly 
construct compositional language comp set meanings dictionary sub signals 
sub signal represent feature value 
simplicity assumption feature value feature unique entry dictionary refers feature value feature 
meaning signal constructed basis dictionary signal formed concatenating corresponding sub signal feature value meaning 
simplicity order sub signals occurring complete signal re ects order feature values meaning 
simpli cation language construction restrict class compositional languages account model 
compositional language similar meanings map similar signals 
similar meanings de nition elements common corresponding signals elements common 
mapping meanings signals neighborhood preserving groups similar meanings map groups similar signals 
kind regularity occur chance constructing holistic languages 
learning production language change observing language agent selects hypothesis best describes called express objects appear meanings agent uses hypothesis nd appropriate signal meanings 
linguistic competence agent de ned ability express signals meanings 
set possible hypotheses process hypothesis selected manner appropriate signals chosen meanings collectively termed generalization process 
consider case observed language subset larger language say agent observes subject transmission bottleneck 
situation resembles chomsky termed poverty stimulus 
language impoverished version depending ectiveness generalization process structure possible agent reproduce meaning signal pairs observing learners natural language placed exactly situation learns english observing english sentences 
behavior agent thought function maps language generation language generation 
certain conditions case stable language 
iterated learning process rests fact input agent output agent 
linguistic terms infant learns language observing linguistic behavior adult 
model show language evolution occur agents su er poverty stimulus 
iterated learning model poverty stimulus occurs result transmission bottleneck language learners learn complete exposure language previous generation 
depicts process detail 
rst agent observes language best described hypothesis objects environment random number times 
agent utter signal objects 
process termed production 
agent observes meaning signal pairs 
process repeated agent random series object observations 
random meanings induced hypothesis produce language generation observe 
language evolution model language evolves agent attempts reconstruct language previous generation basis sparse language exposure 
inconsistencies languages introduced structure language generalization impossible generalization signals meanings random produce observe produce observe observe produce language change generation generation generation rst generations iterated learning model 
rst agent chain example utterances meaning signal pairs holistic language 
hypothesis chosen account linguistic evidence 
set meanings correspond objects drawn randomly set objects 
meanings appropriate signal deduced uttered observes utterances forms hypothesis explain set meaning signal pairs 
process repeated utters signal meanings agent observe 
hypothesis corresponds agents linguistic knowledge 
utterances meaning signal pairs represent agents linguistic performance 
procedure inadequate 
article principally interested steady states transitions language passes stability results 
focusing steady states section clarify factors drive language change 
arrive maintain steady state requires speci conditions 
absence transmission bottleneck language structures stable objects observed conjunction signal 
providing hypothesis consistent data uncertainty arise object needs expressed 
soon learner forms hypothesis basis subset language instability result objects may observed lifetime agent 
situation agent knowledge observed postulate solutions observed 
reason set languages structures result stability depend size transmission bottleneck 
unstable states language evolution exactly language generation get transformed language generation 
consider meaning signal pair hm si occurs language mechanisms responsible association signal generation 
memorization 
learner uses signal accompanied meaning observed 
mapping change 
memorization occur hm si observed 

generalization 
case learner observed hm si known relationship observed meanings signals appropriate signal arrived induction 
situation association remains stable progressive induction decision change progressive reinforce structure occurring mapping meanings signals 

invention 
learner observed hm si learner discern structure observed language help nding signal situation production decision required 
random element production alternatively learner choose produce signal 
way assume existing association broken 
case occur chance 
mechanisms de ne association meaning signal change generation 
combinations processes maintain stable language 
language generation transformed language generation combinations production mechanisms invention memory invention generalisation memory generalization memory memory 
illustrates relationship production mechanisms occurrence divergence 
divergence shown invention required 
stability result invention illustrates fact 
invention occurs presence transmission bottleneck serves important purpose 
language unstructured invention occur frequently 
chance invented signal introduce memory memory generalisation memory generalisation invention memory invention ways language evolve 
diagrams show mechanism produce signal ect change language generation generation 
inward arrows signify reinforcement language structure 
outward arrows indicate divergence current language structure 
language persist combinations production mechanism play memory memory generalization 
divergence occur memory generalization invention occur memory invention occur 
invention causes divergence 
structure language generation structure absent language previous generation 
critically structured relation meanings signals survive transmission bottleneck unstructured relation kirby 
consider region mapping meanings signals structured 
parts structured region need observed structured relation survive transmission bottleneck 
contrast unstructured random region 
region represented language generation requires meaning signal pairs representing region observed 
structure compressible 
stochastic inventions introduce structure previously absent drive language evolution regions stability 
structure mapping contains frequently invention occur 
simulated annealing kirkpatrick iterated learning seen search strategy 
providing language unstructured transmission bottleneck place initial temperature high invention occur frequently 
time temperature decreases invention occur frequently 
search follows trajectory language best ts combined biases hypothesis selection production 
iterated learning ect cumulative evolution structure 
process linguistic equivalent tomasello terms ratchet ect cultural evolution 
certain conditions met iterated learning reliably converge stable state 
aim understand formalize conditions 
parameter space parameter groups de ne behavior iterated learning model 
context model developed follows 
meaning space 
space possible meanings 
meaning space de ned number features cardinality set feature values drawn 
signal space 
signals constructed drawing symbols alphabet 
maximum length signal max 
transmission bottleneck 
number random object observations value related degree language exposure 
probability distribution objects speci es object observed 
assume distribution uniform stated 

perceptual bias 
utterances may perceived accurately 
example transmission channel noisy may limits working memory agent restricting set perceivable utterances 
model developed assume utterances transmitted noise free restrictions perception modeled 

learning bias 
learning bias de nes space possible hypotheses hypothesis chosen data 

production bias 
meaning hypothesis production bias de nes signal chosen express meaning 
stable language occurs certain parameter combinations 
recall transmission bottleneck languages stable 
soon transmission bottleneck place certain languages stable rely certain combinations learning production bias 
mapping parameter space consider role learning production 
compression learning generalization agents act conduit language 
agent observes subset language previous generation 
process learning subset producing utterances generation complex 
level agent simply maps language 
detailed level function de nes mapping composed learning mechanism production mechanism 
learning context process arriving hypothesis explains observed language 
production process hypothesis completes mapping meanings signals 
production mechanism de nes hypothesis yield signals 
chosen hypothesis re ect regular structure existing observed language unobserved regions language recovered production mechanism exploiting structure hypothesis 
combination learning production termed generalization 
section propose candidate computational model generalization minimum description length mdl principle 
minimum description length learning ranking potential hypotheses minimum description length principled elegant approach hypothesis selection rissanen li vit anyi 
mdl principle derived bayes rule short states best hypothesis observed data minimizes sum encoding length hypothesis encoding length data represented terms hypothesis 
tradeo exists small hypotheses large data encoding length large hypotheses small data encoding length 
observed data contains regularity best hypothesis represents data verbatim minimizes data encoding length 
regularity exist data smaller hypothesis possible describes regularity making explicit result hypothesis describes just observed data 
reason cost encoding data increases 
mdl tells ideal tradeo length hypothesis encoding length data encoding described relative hypothesis 
formally observed data hypothesis space best hypothesis de ned min fl djh length bits hypothesis optimal coding scheme hypotheses 
similarly djh length bits encoding observed data hypothesis mdl principle nd hypothesis observed set meaning signal pairs passed agent 
regularity exists observed language hypothesis capture regularity justi ed allow generalization observed 
employing mdl theoretically solid justi cation generalization 
section clarify mdl principle introduce hypothesis space coding schemes 
hypothesis space introduce novel model mapping strings symbols meanings term finite state uni cation transducer 
model extends scheme taylor include variable length signals importantly meanings 
observed data hypothesis space consists consistent observed data 
compositional non compositional languages represented model 
fhf hf hf hf hf hf 




compositional language pre tree transducer shown constructed 
performing edge state merge operations outlined transducer compressed 
transducer shown compressed lead generalizations 
speci ed tuple set states transducer alphabet symbols drawn 
de ne structure meaning space 
transition function maps state symbol pairs new state possibly speci ed meaning corresponding part transducer 
states need speci ed initial nal state respectively 
consider agent receives set meaning signal pairs language acquisition 
example observed language set fhf hf hf hf hf hf language compositional 
constructed dictionary value value feature feature feature example sub signal corresponding feature value rst feature 
depicts models term transducer pre tree transducer observed language observed language represented pre tree transducer 
power model apparent consider possible generalizations merging states edges 
state merge 
states merged form new state transducer remains consistent 
edges mention mention new state 

edge merge 
edges merged share source target states accept symbol 
result merging edges new edge new meaning label 
meanings merged nding intersection component meanings 
features values common take value 
wild card matches values 
fragments meanings may lost check transducer consistency required 
consistency check observed meaning signal pairs accounted resulting transducer 
fhf hf hf hf hf hf 





fhf hf hf hf hf hf hf hf compositional language series state edge merge operations shown result compressed transducer shown 
result compression transducer express meanings contained members language expressed 
illustrate possible state edge merge operations 
transducer resulting merge operations show 
depicts fully compressed transducer performing additional state edge merge operations 
fully compressed transducer express meanings language shown contains meaning signal pairs expressed fully compressed transducer 
compressing pre tree transducer structure compositional language explicit result generalization occur 
encoding lengths order apply mdl principle need appropriate coding scheme hypotheses data hypothesis 
schemes correspond introduced equation 
requirement coding scheme machine take encoding hypothesis decode way unique transducer results 
similarly coding data respect transducer describe data uniquely 
encode transducer containing states edges calculate space required bits encoding state state log symbol symbol log feature value log 
number bits required encode meaning relies cost encoding wildcard speci er 
number bits encode am arbitrary meaning ff meaning denotes value ith feature represents number bits required encode feature value wildcard speci er 
initial bit di erentiate possibilities 
denoting meaning associated ith edge encoding length transducer state symbol meaning state corresponds encoding transition function identity accepting state 
transducer uniquely decoded specify lengths constituent parts transducer 
term part encoding pre block prefix state symbol encoding length bits required de ne number features meaning log 
calculate expression prefix recall de nes length encoding hypothesis coding scheme quantity termed grammar encoding length gel taylor 
similarly length encoding data terms hypothesis djh termed data encoding length del 
del far simpler calculate gel 
string composed symbols jsj need detail transition choose accepting symbol respect transducer 
list choices describes unique path transducer 
additional information required transducer enters accepting state transducer accept string continue parsing characters accepting state contain loop transition 
data composed meaning signal pairs djh calculated lc djh js flog ij ij signal ith meaning signal pair ij number outward transitions state reached parsing symbols signal ith meaning signal pair 
state reached parsing symbols signal ith meaning signal pair denoted ij function handles extra information accepting states ij transducer pre tree transducers compressed applying merge operators described 
beam search mitchell 
merge operators chosen random applied random point transducer 
resulting transducer consistent observed data enter beam 
transducers smallest encoding lengths remain beam new transducers formed applying merge operations replace transducer largest encoding length 
smaller transducers exploration hypothesis space 
operator applied search stops transducer smallest encoding length chosen 
mdl hypothesis selection generalization respect observed data mdl provides ranking candidate hypotheses 
identifying hypothesis yields smallest encoding length practical problem addressed employing search strategy 
selection hypothesis solves half problem mapping meaning signal 
complete mapping production mechanism required hypothesis 
respect implementation mdl procedure required takes meaning produces signal meaning 
mechanism proceeds performing depth rst search path transition taken meaning label inconsistent target meaning 
providing set transitions result correct meaning parsed nal transition leads accepting state resulting signal formed concatenating symbols transition 
procedure generalization occur 
model generalization agent simulation reported brighton kirby 
consider structures pre tree machines compressed machines 
machines correspond hypotheses chosen observed language holistic compositional respectively 
model de nes space hypotheses 
application mdl principle space hypotheses conjunction iterated learning model account evolution gradations language type holistic language fully compositional language mixtures structures 
model developed simplify issue consider stability conditions holistic fully compositional language 
optimal generalization assume agent knows observing utterances observed language compositional structure 
recall compositional language totally de ned dictionary construct 
expectation compositional language degree exposure language required dictionary derived 
earliest point dictionary constructed feature values observed 
disregarding details procedure reconstructing dictionary minimum degree exposure reconstruction possible 
term ability express meanings feature values observed optimal generalization bias 
section aim formalize notion optimal generalization bias 
show degree bias paralleled mdl model generalization discussed 
short aim show compositional language circumstances compressed transducer outlined previous section smallest encoding length 
relating optimal generalization bias mdl model allows model language stability need perform lengthy agent simulations 
strongest possible generalization bias results xed compositional language lc expressed input 
scheme inconsistent observed languages lc consider hypotheses consistent observed data 
compositional languages purposes feature values appearing meaning associated unique sub signals 
dictionary relating feature value sub signal totally de nes compositional language 
slight simpli cation problem dictionary de ne order sub signals assembled 
assume ordering feature values meaning re ected construction signal 
express meaning sub signal corresponding feature value meaning located dictionary 
signal formed concatenating sub signals 
evidence build dictionary observed expressivity suboptimal 
objects need expressed represented meanings contain unobserved feature values entry dictionary missing 
optimal generalization bias ability express meanings built observed feature values 
de nition optimal generalization compositional language learner optimal generalization bias meaning expressed providing value observed 
observed optimal generalization bias assumption sub signal corresponding deduced 
optimal generalization bias serves upper bound degree inductive bias compositional language 
mdl optimal generalization bias imagine dictionary construct compositional language act hypothesis language 
circumstances mdl hypothesis selection result hypothesis 
compressed illustrated corresponds notion dictionary 
feature construct meaning space represented separate region transducer 
regions feature value represented 
meanings constructed combinations feature values expressible transducer structure 
depicts general structure compressed transducer 
compositional language circumstances mdl choose compressed transducer 
show certain assumptions distribution meanings compressed machines chosen mdl compositional language input 
bear mind statement concerning ranking hypotheses respect model mdl principle 
performs search space value value value value value value feature value value value feature feature general structure compressed transducer 
feature represented section transducer 
section individual feature values represented unique path 
hypotheses verify fact practical issue 
assume exhaustive search 
claim applying mdl principle space results hypothesis equivalent characterized optimal generalization bias 
demonstrate fact rst analytic argument illustrate argument data extracted simulation 
relationship del gel compressed machine yields smallest grammar encoding length possible 
smaller machines exist consistent observed data 
rule 
hypotheses selected solely basis size compressed machines chosen compositional language analytic demonstration complete 
policy known occam razor see example mitchell 
mdl principle consider size data represented terms hypothesis 
analysis demonstrates assumptions uniform distribution meanings presence transmission bottleneck impact data encoding length negligible 
result mdl hypothesis selection simpli ed policy occam razor pick smallest hypothesis 
hypothesis chosen mdl di er picked occam razor non uniform statistical ects data 
series meaning signal pairs agent 
series represented set distinct meaning signal pairs 
number times arbitrary meaning signal pair observed denoted 
useful specify size severity transmission bottleneck terms expected number distinct objects observed number object observations 
expected object coverage denoted random observations objects de ned log log value represent proportion objects expect observe 
compositional language know maximum number unique meaning signal pairs 
consider pre tree transducer encode language require log bits encode single meaning signal pair pre tree transducer meaning signal pair represented unique non branching path transducer 
total compositional language del prefix fk 
log bits required encode meaning signal pairs consider bits required encode single meaning signal pair compressed transducer 
assuming feature values observed log arrive expression follows 
feature meaning represented di erent section transducer 
path sections take possible routes 
specifying single feature value requires log bits 
features encode meaning signal pair requires 
log log bits 
total compressed machine compositional language require del comp 
log bits encode meaning signal pairs 
di erence del comp del prefix important 
di erence greater size di erence grammar encoding lengths pre tree machines chosen 
formally encoding length pre tree transducer el prefix de ned el prefix gel prefix del prefix similarly compressed transducer el comp gel comp del comp el prefix comp pre tree transducers selected compressed transducer compositional language input 
situation requires di erence grammar encoding lengths di erence data encoding lengths 
situation discussed 
non uniform distribution meanings di erence del prefix del comp usually smaller di erence gel prefix gel comp 
transducer compression results removal transducer states degree loss states increases cost encoding data usually small 
upshot disparity compositional language pre tree transducer preferred compressed transducer number occurrences meaning signal pair large 
assuming uniform distribution meanings situation occur meaning signal pair observed times 
happen language observed 
situation modeled assume transmission bottleneck 
terms mdl justi cation relationship rests assumption evidence set observations novel observations occur 
illustrates compositional language hypothesis selection depends probability distribution meanings objects transmission bottleneck uniform distribution meanings compressed machines chosen el prefix el comp holds irrespective coverage 
contrast situation shown 
low coverage values inequality el prefix el comp longer holds pre tree transducers preferred low coverage values 
practice uniform distribution meanings situation occur 
el el bits object coverage uniform el prefix el comp pre transducers chosen mdl compositional language 
uniform distribution transmission bottleneck occur 
example pre tree transducers chosen compressed machines 
consider distribution resulting zipf law situation occur 
zip distribution context mean frequency meanings decay power function rank zipf kirby 
case increases values huge 
situation rule compressed transducers selected compositional language longer holds 
introducing strong statistical ects data hypothesis selection mdl begins diverge occam razor 
intend investigate zip distributions 
summary short knowing language advance optimal generalization bias describes amount evidence required generalization occur 
demonstrated assuming uniform distribution meanings compressed transducers preferable pre tree transducers compositional languages 
compressed transducers model optimal generalization reduce problem generalization com positional language optimal generalization 
section mathematical model developed relates stability advantage conferred compositional language parameters iterated learning model 
model relies notion optimal generalization bias 
modeling conditions stability understanding conditions stability requires analysis interaction generalization bias transmission bottleneck structure meaning space 
analysis employs mathematical model agent simulation reported brighton kirby 
modeling simulation disadvantages parameter space large 
parts parameter space intractable map monte carlo simulations outlined previous longer intractable 
mathematical models cultural evolution proposed boyd mathematical models language evolution typically focus arguments appealing natural selection nowak 
model developed estimates stability advantage ered compositional language 
varying severity transmission bottleneck degree structure meaning space conditions compositional language ers greatest stability advantage established 
conjecture precisely regions parameter space compositional language emerge 
contrasting language structure type hypothesis pre tree transducers case holistic language compressed transducers case compositional language 
model estimates likelihood language type emerging basis expressivity corresponding hypothesis 
language type appropriate hypothesis structure mdl principle 
expressivity pre tree transducer pre tree transducer meanings observed conjunction signal expressed 
holistic language pre tree transducer best principled way expressing novel meaning basis previously observed meanings 
calculate expected expressivity agent pre tree transducer need calculate probability observing arbitrary meaning drawn meaning space 
order meaning observed agent rst label object guarantee recall 
secondly object labeled need observed agent 
likelihood observing meaning signal pair representing object depends severity transmission bottleneck 
observing series objects represented meaning agent accumulated set observed meanings 
denote set probability observing arbitrary meaning pr determined number objects environment number meanings meaning space number random object observations agents lifetime 
probability observing meaning de ned pr expression takes account rst label object second observations objects observed 
appendix provides explanation equation 
estimate number distinct meanings observed number distinct meanings expressed simply multiply probability number possible meanings expressivity agent pre tree transducer hypothesis denoted prefix prefix pr 
summarize rst calculate probability meaning label object object observed 
probability multiplied number possible meanings yields expected number meanings observed agent 
generalization occur expressivity agent pre tree transducer equivalent number meanings observed 
expressivity compressed transducer compositional language compressed transducer structure maximizes expressivity agent expressivity function number feature values observed function number meanings observed 
recall optimal generalization bias express meaning requires feature values observed 
rate feature values observed rate meanings observed 
result expressivity achieved compressed transducer degree expressivity achieved pre tree transducer 
estimate expressivity agent optimal generalization need calculate probability observing arbitrary feature value 
recall probability greater probability observing arbitrary meaning fewer entities need seen number observations 
formally arbitrary feature take value drawn set possible values require probability arbitrary feature value drawn set observed object observations 
stage process 
take account arbitrary feature value meanings chosen label objects 
feature value may 
arbitrary feature possible values objects labeled random samples objects taken expression similar equation estimate probability observing arbitrary feature value times 
observations arbitrary feature set feature values observed 
denote probability observing arbitrary feature value pr de ne pr 
meaning ability express requires feature value observed 
express probability expressing combined probability expressing feature values denote probability pr de ne pr pr probability able express arbitrary meaning contrast probability represented equation 
meaning equation tells probability able express equation tells probability observing arbitrary important distinction estimate expressivity agent compressed transducer need multiply probability expressing meaning expected number expressible meanings 
expected number distinct meanings labeling objects 
nd expected number objects signal derived multiply probability expressing meaning number expressible meanings generalisation pr 
compressed transducers derived pre tree transducers means state edge merging operations 
compression increase expressivity consistency observed data maintained 
minimum expressivity compressed transducer prefix de ne expressivity compressed transducer compressed max fe generalisation prefix language stability analysis relates transducer structure expressivity 
compositional language hypothesis selection mdl principle results compressed transducer 
expressivity case de ned expression compressed holistic language results pre tree transducer chosen 
expressivity case de ned prefix relate expressivity stability 
language stability degree hypotheses induced subsequent agents maintain mapping meanings signals 
entire set associations meanings signals may externalized utterances 
may exist virtue hypothesis 
correspondence meaning signal generation di erent generation invention occurs 
invention occurs meaning expressed 
stability inversely proportional rate invention 
simply proportional expressivity 
characterize degree stability language type 
degree stability compositional language compositional holistic language holistic de ned compositional compressed holistic prefix important measure employ relative stability 
denote relative stability de ne compositional compositional holistic coverage value meaning space structure de ned respectively value tells degree compositional language stable holistic language 
re ects larger comp holistic 
compositional language stable holistic language 
case occur experiments corresponds situation holistic language stable compositional language 
stability advantage language type 
model analysis key measurement model relative stability compositional language holistic language 
value dependent variables 
number features construct meaning space 
number values feature meaning space 
bottleneck size represented expected object coverage see equation 

number objects environment value indicates just relative stability compositional language holistic language re ects likelihood compositional language emerging 
result invention compositional structure introduced relative stability compositional language tells compositional structure persist recall section 
shows relationship di erent degrees coverage structure meaning space 
parameter real consequence changing just shift landscape away origin larger meaning spaces required represent objects 
contrast model reveals important relationships exist 
maximum small bottleneck sizes 
nears maximum value certain meaning space structures 
result demonstrates compositional language learned exposure limited 
contrast holistic language persist generations small subset observed 
result important demonstrates poverty stimulus important determinant compositional language 
features values relative stability features values relative stability relationship meaning space structure low coverage relative stability 
examples number objects 
surfaces demonstrate relation meaning space structure low coverage values 
highest values occur low coverage medium complexity illustrated 
number features plays signi cant role number values feature 
features values relative stability features values relative stability relationship meaning space structure mid high coverage relative stability 
surface demonstrates relation meaning space structure mid high coverage values 
maximum values decreases rapidly coverage increased 
size region grows coverage increases 
indicates smaller values meaning space structures lead advantage compositionality 

high values occur certain degree complexity meaning space reached 
means conceptual space agent broken multiple features values compositionality option 
meaning spaces features reach high value 
short certain degree feature structure compositional language advantageous 

clearly illustrated largest meaning spaces contain approximately meanings lead small values examples figures high degree structure leads low values 
certain point highly structured meaning space di erent meanings share feature values 
consider extreme case object labeled meaning containing feature values labeling object 

labeling objects tradeo number features number values feature 
features fewer number feature values required represent objects 
meaning observed feature values observed 
means feature values observed sooner objects conceptualized terms features feature values 
relationship strikingly illustrated surface shown 
compositional language far stable features 
relating results language evolution worth considering interactions detail 
indicate meaning space results higher relative stability meanings space 
number features important 
order magnitude meanings looking surfaces shown figures easy discern size di erence meaning spaces 
number meanings ts parameters high number meanings hand large likelihood objects sharing feature values relatively low 
recall feature values observed single object observed 
objects observed likelihood full feature value coverage increases rapidly comparison degree object coverage 
discrepancy feature value coverage object coverage greatest prefix tree spaces compressed compressed object coverage proportion objects expressible rate expressivity increases function object coverage 
compressed transducers increase expressivity faster pre tree transducers 
proportion objects expressed plotted degree coverage object space compressed transducers quickly reach high expressivity 
pre tree transducers express objects observed linear relationship value shown di erent meaning spaces 
features fewer number observations required feature values observed 
small bottleneck values 
compositional language relatively stable comparison holistic language region parameter space 
expressivity function object coverage holistic language 
expressivity function feature value coverage compositional language 
illustrates relationship expressivity achieved represented proportion objects reaches maximum lower coverage values compressed transducer chosen 
degree object coverage increases set meaning spaces increases 
higher coverage greater number objects observed 
relationship allows larger meaning spaces observations occurring previously rare occurrences feature values 
summary pairing compositional language compressed transducer structure holistic language pre tree transducer structure language stability reduced issue transducer expressivity 
model exactly 
relative stability measure re ects degree stability advantage conferred compositional language 
model parameter combinations yield high occupy part parameter space characterized combination low object coverage high high meaning space complexity 
discussion characteristic structure language explicitly coded lad 
neglect pressures language evolution cultural substrate take seriously claim poverty stimulus lad explicitly code language structure 
neglecting role language evolution cultural substrate treat fundamental determinant language evolution 
hypothesis compositional structure function dynamics cultural evolution 
compositional structure emergent property iterated observational learning 
parameters control behavior iterated learning model 
importantly role learning framed search hypothesis smallest encoding length mdl principle 
approach learning motivated need organisms compress observed data ryabko li vit anyi ryabko speci cally linguistic data wol 
mathematical model language stability conjunction view learning compression mapped large part parameter space iterated learning model 
important parameter severity transmission bottleneck denoted model 
parameter controls degree exposure agent language previous generation 
transmission bottleneck approaches maximum value languages equally stable 
introducing bottleneck decreasing value leads set stable languages restricted contain learnable limited exposure 
compositional languages 
draw parallel presence transmission bottleneck situation poverty stimulus 
poverty stimulus take account iterated observational learning results compositional language structure constituting steady state 
structure meaning space de ned determinant compositional structure model 
tradeo low structural complexity meaning space means compositionality offers little advantage holistic language 
occurs feature values objects discriminated primarily terms feature values 
low structural complexity components feature values meanings occur infrequently 
biological evolution semantic complexity assumed model proposed necessary determinant emergence syntax 
model supports 
furthermore complexity meaning space counterproductive 
limit stability payo gained increased structural complexity feature values constructing meanings occur infrequently generalization function 
ndings strengthen compelling argument iterated learning process information transmission observational learning candidate explanatory mechanism emergence syntactic structure 
taken foundational kirby batali establishes ability evolve structured language built identifying key requisite conditions 
focused transmission bottleneck salient model parameter drew parallel poverty stimulus 
poverty stimulus traditionally characterized problem overcome speci ed compositional syntax argue fact fundamental determinant emergence compositional syntax cultural substrate 
acknowledgments author simon kirby mark ellison caroline round kenny smith 
batali 

negotiation acquisition recursive communication systems result competition exemplars 
briscoe ed linguistic evolution language acquisition formal computational models 
cambridge university press 
boyd 

culture evolutionary process 
university chicago press 
brighton kirby 

survival smallest stability conditions cultural evolution compositional language 
kelemen sos eds advances arti cial life vol 

prague czech republic springer verlag 
chomsky 

re ections language 
london temple smith 
chomsky 

rules representations 
london basil blackwell 
dunbar 

coevolution neocortical size group size language humans 
behavioral brain sciences 
kirby 

syntax natural selection compositionality emerges vocabulary population learners 
knight kennedy hurford eds evolutionary emergence language social function origins linguistic form pp 

cambridge cambridge university press 
kirby 

learning bottlenecks evolution recursive syntax 
briscoe ed linguistic evolution language acquisition formal computational models 
cambridge cambridge university press 
kirby 

spontaneous evolution linguistic structure iterated learning model emergence regularity irregularity 
ieee transactions evolutionary computation 
kirkpatrick gelatt jr vecchi 

optimization simulated annealing 
science 
li vit anyi 

kolmogorov complexity applications 
new york springer verlag 
mitchell 

machine learning 
mcgraw hill 
montague 

formal philosophy selected papers richard montague 
yale university press 
nowak niyogi 

evolution universal grammar 
science 
nowak plotkin jansen 

evolution syntactic communication 
nature 
pinker bloom 

natural language natural selection 
behavioral brain sciences 
ryabko 

analysis language ants information theoretical methods 
problems information transmission 
rissanen 

modeling shortest data description 
automatica 
ryabko 

shannon entropy kolmogorov complexity study communicative system cognitive capacities ants 
complexity 


syntax emergent property evolution semantic complexity 
minds machines 
smith 

establishing communication systems explicit meaning transmission 
kelemen sos eds advances arti cial life vol 

prague czech republic springer verlag 
steels 

perceptually grounded meaning creation 
tokoro ed proceedings international conference multi agent systems 
mit press 
steels 

talking heads experiment vol 
words meanings 
antwerpen 
special pre edition taylor 

ects acquisition language evolution 
arti cial life 
tomasello 

cultural origins human cognition 
harvard university press 
wol 

language acquisition data compression generalization 
language communication 
zipf 

psycho biology language 
london routledge 
objects meanings feature values transmission bottleneck part model developed section requires calculation estimates likelihood observing entity 
calculation appears guises 
entities meanings second entities feature values 
gloss problem terms rst interpretation objects meanings random meaning assigned object 
random object observations probability observing arbitrary meaning 
abstracting details model problem generalized follows balls colors rst balls assigned color 
ball color chosen random color equi probable 
second balls sampled random replacement times 
question particular probability observing arbitrary color 
colors assign random color balls 
probability single ball colored 
pr coloured probability ball colored pr coloured denote event balls colored arbitrary color 
allocating colors balls say pr pr general pr consider sampling balls random replacement 
observe pr st ball sampled color balls colored means pr st ball sampled colored balls colored event nth ball sampled color balls colored denoted term color simplify matters write color denote event balls sampled colored interested event ball sampled color denote event represent pr follows pr pr color pr color color pr color color pr color color pr color color formally terms pr express pr manner pr pr 
pr pr 
pr pr 
pr equation express pr follows pr pr 
pr 
summarize balls arbitrary color probability observing ball colored observations equation 

