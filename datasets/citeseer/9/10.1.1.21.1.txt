emergent neural computational architectures neuroscience stefan wermter jim austin david willshaw springer heidelberg new york march preface book result series international workshops organised project emergent neural computational architectures neuroscience sponsored engineering physical sciences research council epsrc 
aim book broad spectrum current research biologically inspired computational systems encourage emergence new computational approaches neuroscience 
generally understood approaches computing performance exibility reliability biological information processing systems 
massive body knowledge regarding processing occurs brain central nervous system little impact mainstream computing far 
process developing biologically inspired computerised systems involves examination functionality architecture brain emphasis information processing activities 
biologically inspired computerised systems address neural computation position neuroscience computing experimental evidence create general systems 
book focuses main research areas modular organisation robustness timing synchronisation learning memory storage 
issues considered part include modularity brain produce large scale computational architectures 
human memory manage continue operate despite failure components 
brain synchronise processing 
brain compute relatively slow computing elements achieve rapid real time performance 
build computational models processes architectures 
design incremental learning algorithms dynamic memory architectures 
natural information processing systems exploited arti cial computational methods 
hope book stimulates encourages new research area 
contributors book participants various workshops 
especially express mark network assistant network put tremendous ort process publishing book 
epsrc james fleming support alfred hofmann sta springer continuing assistance 
march stefan wermter jim austin david willshaw table contents novel neuroscience inspired computing stefan wermter jim austin david willshaw mark modular organisation robustness images mind brain images neural networks john taylor stimulus independent data analysis fmri michael herrmann theo emergence modularity sheet neurons model comparison weber klaus obermayer computational investigation specialization interactions james reggia yuri explorations interaction split processing stimulus types john hicks modularity specialized learning mapping agent architectures brain organization joanna bryson lynne andrea stein biased competition mechanisms visual attention system gustavo deco recurrent long range interactions early vision thorsten hansen wolfgang sepp heiko neumann neural mechanisms representing surface contour features thorsten hansen heiko neumann representations neuronal models minimal bilinear realisations gary green woods collaborative cell assemblies building blocks cortical computation ronan reilly uence threshold variability mean eld model visual cortex bartsch martin klaus obermayer computational neural systems developmental evolution alistair rust rod adams stella george hamid complexity brain structural functional dynamic modules eter tam kiss timing synchronisation synchronisation binding role correlated firing fast information transmission simon schultz golledge stefano segmenting state entities implication learning james henderson temporal structure neural activity modelling information processing brain roman role cerebellum time critical goal oriented behaviour anatomical basis control principle guido synchronous oscillations excitatory inhibitory groups spiking neurons david temporal coding neuronal populations presence axonal dendritic conduction time delays david halliday role brain chaos eter andr neural network classi cation word evoked brain activity ramin uller simulation studies speed recurrent processing stefano edmund rolls francesco ruth learning memory storage dynamics learning memory lessons neuroscience michael biological grounding recruitment learning algorithms long term potentiation shastri plasticity resolution apparent paradox gary marcus cell assemblies intermediate level model cognition christian modelling higher cognitive functions hebbian cell assemblies marcin spiking associative memory scene segmentation synchronization cortical activity andreas palm familiarity discrimination algorithm inspired computations cortex malcolm brown christophe giraud carrier linguistic computation state space trajectories hermann robust stimulus encoding olfactory processing hyperacuity ecient signal transmission tim pearce paul joel white john finite state computation analog neural networks steps biologically plausible models 
rafael carrasco investigation role cortical synaptic depression auditory processing sue michael role memory anxiety hebbian learning hippocampal function novel explorations computational neuroscience robotics john amanda sharkey time delay actor critic neural architecture dopamine reinforcement signal learning autonomous robots andr es erez uribe connectionist propositional logic simple correlation matrix memory reasoning system daniel jim austin analysis synthesis agents learn distributed dynamic data sources adrian honavar connectionist neuroimaging stephen jos hanson catherine hanson authors index novel neuroscience inspired computing stefan wermter jim austin david willshaw mark hybrid intelligent systems group university sunderland centre informatics st peter way sunderland sr dd uk email stefan wermter mark sunderland ac uk www sunderland ac uk department computer science university york york yo dd uk email austin cs york ac uk institute adaptive neural computation university edinburgh forrest hill edinburgh email david anc ed ac uk 
approaches computing performance exibility reliability neural information processing systems 
order overcome conventional computing systems bene various characteristics brain modular organisation robustness timing synchronisation learning memory storage central nervous system 
overview incorporates key research issues eld biologically inspired computing systems 
generally understood approaches computing performance exibility reliability biological information processing systems 
massive body knowledge regarding processing occurs brain little impact mainstream computing 
response epsrc sponsored project entitled emergent neural computational architectures neuroscience initiated universities sunderland york edinburgh 
workshops held usa scotland england 
book response workshops explores computational systems bene inclusion architecture processing characteristics brain 
process developing biologically inspired computerised systems involves examination functionality architecture brain emphasis information processing activities 
biologically inspired computerised engineering physical sciences research council 
systems examine basics neural computation position neuroscience computing experimental evidence create general neuroscience inspired systems 
various restrictions limited degree progress biological inspiration improve computerised systems 
biologically realistic models limited terms attempt achieve compared brain 
despite advances understanding neuronal processing level connectivity brain known happens various systems levels 
disagreement large amount information provided brain imaging techniques means computational systems 
decade seen signi cant growth interest studying brain 
reason expectation possible exploit inspiration brain improve performance computerised systems 
furthermore observe bene ts biological neural systems child brain currently outperform powerful computing algorithms 
biologically inspired computerised systems growing belief key factor unlocking performance capabilities brain architecture processing lead new forms computation 
architectural information processing characteristics brain included computing systems enable achieve novel forms performance including modular organisation robustness information processing encoding approaches timing synchronisation learning memory storage 
key research issues chapter workshops look various key research issues biologically inspired computerised systems critical consider ered computer science researching biological computation biological neural computation computer science 
considering architectural information processing forms inspiration possible identify research issues associated 
modular organisation knowledge build arti cial neural networks real world tasks little knowledge bring systems solve larger tasks associative retrieval memory 
may hints studying brain give ideas solve problems 
robustness human memory manage continue operate despite failure components 
properties 
current computers fast brittle memory brains slow robust 
learn properties conventional computers 
timing brain synchronise processing brain prevent known race conditions com 
brain schedule processing 
brain operates central clock possibly 
asynchronous operation achieved 
brain compute relatively slow computing elements achieve rapid real time performance 
brain deal real time 
exploit time properties special scheduling methods 
natural systems achieve learn methods may 
learning memory storage evidence neuron network brain levels internal state neurobiological system uence processing learning memory 
build computational models processes states 
design incremental learning algorithms dynamic memory architectures 
modular organisation modularity brain developed thousands years evolution perform cognitive functions compact structure takes various forms neurons columns regions hemispheres 
regional modularity brain viewed various distributed neural networks diverse regions carry processing parallel fashion perform speci cognitive functions 
brain described group collaborating specialists achieve cognitive function splitting task smaller elements 
cerebral cortex biggest part human brain highly organised regions responsible higher level functionality possible regional modularity 
feature regional modularity division activities required perform cognitive function di erent hemispheres brain 
instances volume hicks show split character visual processing di erent brain hemispheres improves visual word identi cation producing modular architecture 
brain imaging techniques successfully provided great deal information regions associated cognitive functions 
oldest techniques mainly involves examination brain lesions held responsible observed cognitive de cit 
lesion approach criticised identify regions involved cognitive function produces misleading results due naturally occurring lesions alternative imaging techniques contradict ndings 
due di culties observed lesion approach technical developments alternative techniques known positron emission tomography pet functional magnetic resonance imaging fmri eeg meg received attention 
pet fmri examine precisely neural activity brain indirect manner create image regions associated cognitive task 
pet done identifying regions greatest blood ow fmri brain map blood oxygen levels 
pet fmri spatial attainment temporal competence limited 
contrast eeg measures voltage uctuations produced regional brain activity electrodes position surface scalp 
meg uses variations magnetic eld establish brain activity exploiting sophisticated quantum devices 
temporal properties eeg meg signi cantly better pet fmri sensitivity millisecond 
major issue currently investigated biological inspired computer system researchers manner modules brain interact 
volume taylor establishes approach examine degree association regions identi ed responsible subtask considering correlation coecients 
approach incorporates structural modelling linear associations active regions accepted path strengths established correlation matrix 
bridging gap brain image information underlying neural network operations activity described coupled neural equations basic neurons 
outcomes brain imaging single cell examinations lead identi cation new conceptions neural networks 
related cortical approach taken kiss volume 
kiss develop model interaction cortical regions 
cortical regions connection strengths delay levels connection matrix devised dynamically outlined model cortex 
reilly identi ed feedforward feedback routes linking modules performing particular cognitive function 
concept regional modularity brain develop various computing systems 
example bryson stein point volume robotics modularity time produced means developing coordinating modular systems 
authors show means functioning models modular decomposition 
deco volume regional modular approach visual attention object recognition visual search 
system modules match principal visual pathways visual cortex performs modes learning recognition modes 
biological inspired computer model contour extraction processes devised hansen 
hansen neumann modular approach 
approach involves long range links feedback feedforward processing lateral competitive interaction horizontal longrange integration localised receptive elds oriented contract processing 
model depends simpli ed representation visual cortex regions interaction regions layers 
large number cortical regions description mutual connectivity complex 
weber obermayer devised computational models learning relationships simpli ed cortical areas 
paradigm maximum likelihood reconstruction arti cial data architecture adapts data represent best 
columnar modularity cerebral cortex turning detailed interpretation brain modular construction kiss 
mari built fact cerebral cortex composed completely blocks repetitive modules known cortical columns basically layer structure 
variations cognitive functionality achieved columnar organisations diverse start connections cortical neurons regional speci integrative registering features 
reilly columns mm diameter include neurons 
columns provide reliable distributed representations cognitive functions creating spatio temporal pattern activation particular time millions active 
development result evolutionary need better functionality bandwidth sensory system 
extreme views form representation takes cerebral cortex 
older view sees representation context independent compositionality kind linked formal linguistics logical depiction 
new view holds brain dynamic system predicate calculus relevant describing brain functionality 
model cortex columns designed doya points layered structure recurrent processing approach 
system provides inhibitory excitatory synaptic connections types neurons pyramidal neurons neurons inhibitory neurons 
pyramidal neurons responsible passing excitatory signal various cells column including cells kind 
inhibitory neurons restrict spiking pyramidal neurons near pyramidal neurons inhibitory neurons control cells column 
volume related columnar model devised bartsch 
considering visual cortex 
prominent character neurons primary visual cortex preference input classical receptive eld 
model combines various structured orientation columns produce full 
orientation columns mutually coupled lateral links gaussian pro les driven weakly orientation biased inputs 
research create multi models cells linked compartments higher degree biological plausibility 
diculty high processing time due channel processing elements compartments 
certain extent overcome lie series solutions lie algebra create restricted hodgkin huxley type model 
general brain consists distributed recurrent interaction billions neurons 
lot insight inspiration computational architectures gained areas regions column organisation 
robustness second important feature brain robustness 
robustness human brain achieved recovery certain functions defect 
brain compensate loss neurons neuron areas functional networks constant basis 
degree recovery robustness dependent various factors level injury location size lesion age patient 
recovery felt best patient younger maturation period approaches recovery complicated variable 
approaches recovery repair damaged neural networks reactivation networks damaged due close proximity injury stopped functioning ii redistribution functionality new regions brain 
mixed evidence time normally takes repair injured tissue 
researchers redistribution functionality new regions brain take longer repair left superior temporal gyrus occurs numerous months injury 
restoration cortex regions critical recovery functionality region known inhibit degree reallocation functionality new regions 
reggia 
volume reorganisation brain regions responsible cognitive function explains remarkable capacity recover injury robust fault tolerant processing 
computerised models recovery regeneration possible model recovery tissue regeneration considering neural network performance various degrees recovery 
instance martin 
examined recovery regeneration tissue deep considering attainment subject naming repetition tests 
model examine robustness associated interaction activation competition neural network recovery comes decay rate returning normal levels 
wright ahmad developed modular neural network model trained perform naming function damaged varying degrees examining recovery 
model incorporates method achieve robustness recovery closer technique employed brain rust 
volume considers creation neural systems dynamic adaptive 
computational model produces recovery allowing adaptability achieving self repair axons dendrites produce new links 
computerised model robustness functional reallocation second form robustness reallocation 
considering recovery functionality reallocation reggia 
volume devise biologically plausible models regions cerebral cortex responsible functions phoneme sequence creation letter identi cation 
model recurrent unsupervised learning unsupervised supervised learning 
sections models represent hemisphere cerebral cortex left contributed recovery functionality particularly level injury hemisphere signi cant 
general graded forms dynamic robustness go current computing systems 
timing synchronisation neurophysiological activity brain complicated diverse random experimental data indicates importance temporal associations activities neurons neural populations brain regions 
timing synchronisation features brain considered critical achieving high levels performance 
volume alterations synaptic ecacy coming pairing pre postsynaptic activity signi cantly alter synaptic links 
induction long term alterations synaptic ecacy pairing relies signi cantly relative timing onset excitatory post synaptic potential epsp produced pre synaptic action potential 
disagreement importance information encoding role played interaction individual neurons form synchronisation 
schultz 
consider synchronisation secondary ring rates 
research questioned temporal organisation spiking trains 
critical feature timing brain performs real time fast processing despite relatively slow processing elements 
instance points role cerebellum line planning achieve real time processing 

commonly held view fast processing speed cerebral cortex comes entirely feedforward oriented approach 

able contradict view producing model layers excitatory inhibitory integrate re neurons included layer recurrent processing 
importance timing synchronisation brain computational modelling architectures achieve various cognitive functions including vision language 
instance examined brain schedules processing considering olfactory system 
desert olfactory system neural activity interesting spatiotemporal synchronisation coding features 
olfactory system receptor cells connect projection neurons inhibitory local neurons lobe projection neurons inhibitory local neuron groups interconnected 
projection neurons appear depict spatiotemporal code second principal elements slow spatiotemporal activity fast global oscillations transient synchronisation 
synchronisation system re ne spatiotemporal depiction 
biologically inspired computerised model attention considers role played formulated 
central oscillator linked peripheral oscillators feedforward feedback links 
approach hippocampal area acts central oscillator peripheral oscillators cortical columns sensitive particular characteristics 
attention produced network synchronisation central oscillator certain peripheral oscillators 
henderson devised biologically inspired computing model synchronisation segment patterns entities simple synchrony networks 
simple synchronisation networks enlargement simple recurrent networks units 
period units diverse activation levels phrases period 
related biologically inspired model addresses ects axonal dendritic conduction time delays temporal coding neural populations halliday 
model uses cells common independent synaptic input morphologically detailed models dendritic tree typical spinal 
temporal coding inputs carried weakly correlated components common input spike trains 
temporal coding outputs manifest tendency synchronized discharge output spike trains 
dendritic axonal conduction delays ms alter sensitivity cells temporal coding input spike trains 
growing support chaotic dynamics biological neural activity individual neurons create chaotic ring certain conditions 
new approach brain chaos andr states volume stimuli brain represented chaotic neural objects 
chaotic neural objects provide stability characteristics superior information representation 
neural objects dynamic activity patterns described mathematical chaos 
uller able identify importance spatio temporal depiction information brain 
performed looking representations single words kohonen network classify words 
sixteen words lexico semantic classes brain responses represent diverse characteristics words length frequency meaning measured meg 
learning memory storage additional structural characteristic brain central nervous system manner learns stores memories 
argues character neural connections approach learning memory storage brain currently major impact computational neural architectures despite signi cant bene ts available 
school known neo constructivism lead elman argue learning underlying brain structure come particular organisation available birth modi cations results experiences faced time 
model certain appeal marcus points various limitations learning mechanism certain degree infants months old ability learn rules developmental exibility necessarily entail learning relies greatly learning neural activity 
marcus holds neo lack toolkit developmental biology put forward approach developing neural networks grow er self organising experience 
toolkit includes cell division migration death gene expression cell cell interaction gene hierarchies 
years computational scientists attempted incorporate learning memory storage arti cial intelligent computer systems typically arti cial neural networks 
systems computational elements gross simpli cation biological neurons 
little biological plausibility indication brain constrains incorporated better way 
hanson 
volume outlines arti cial neural network recurrent ones perform emergent behaviour close human cognitive performance 
networks able produces structure situation sensitive hierarchical extensible 
performing activity learning grammar valid set examples recurrent network able recode input defer symbol binding received sucient string sequences 
synaptic alteration achieve learning memory storage regions brain fundamental learning memory storage cortex hippocampus 
areas involved shown uribe describes basal ganglion model role trial error learning 
hippocampus system cortical subsystem temporal lobe fundamental role short term memory storage transferring short term memories ones 
cortex nal location memories 
rst accounts learning occurs hebb devised model brain stores memories simple synaptic approach cell assemblies cortical processing 
alterations synaptic strengths approach learning persistence memories repeated activation memory retrieval 
determinant assembly connectivity structure neurons lends support ring greater probability activated reliable fashion 
cell assemblies working long term memory storage interact cell assemblies 
substantial amount learning memory 
long term potentiation ltp growth synaptic strength caused rapidly short periods synaptic stimulation close hebb notion activity reliant synapses 
approach ltp strengthening links synapses device reducing synaptic strength known long term depression 
shastri volume computational abstraction ltp greatly simpli ed representation processes involved creation ltp cell represented idealised integrate re neuron spatio temporal integration activity arriving cell 
certain cell kinds ring modes supra active normal 
neurally model relates high frequency burst reaction normal mode relates basic spiking reaction isolated spikes 
ltp identi ed shastri critical episodic memory role binding detection 
shastri model structure fast production cell responses binding matches areas role entity bind 
areas role entity felt primary cells bind cells 
role entity areas match subareas entorhinal cortex bind area gyrus 
devised biologically inspired model cell assemblies known cant system 
cant system network neurons may contain cell assemblies linked neurons 
neural network models connection strengths altered local hebbian rule learning hebbian unsupervised approach 
models learning models learning arti cial systems particularly interesting neuroscience learning methods 
instance mcclelland goddard examined role hippocampal system learning devising biologically inspired model 
forward pathways association regions neocortex entorhinal cortex create pattern activation entorhinal cortex maximises preservation knowledge neocortical pattern 
entorhinal cortex gives inputs hippocampal memory system recoded gyrus ca manner suitable storage 
hippocampus computerised model split main subsystems structure preserving invertible encoder subsystem ii memory separation storage retrieval subsystem iii memory decoding system 
learning process outlined volume simple biologically inspired computational model 
model requires determination epsp synapse back propagating action potential 
learning rule produced relies integration product potentials 
epsp synapse determine ective synapse current equation passive membrane mechanism 
biologically inspired computerised systems learning included robots shows systems improve existing technology 
sharkey developed model hippocampus combines memory anxiety produce novelty detection robot 
robot ers knowledge learning approach making alterations anxiety behaviourally explicit 
learning robot devised erez uribe uses biologically inspired approach basal ganglion learn trial error 

devised biologically plausible algorithm familiarity discrimination energy 
information processing cortex hippocampus system 
approach need assumptions related distribution patterns discriminates certain pattern keeps knowledge familiar patterns weights hop eld networks 
related biologically inspired computerised system devised compositionality context sensitive learning founded group hop eld networks 
inspiration comes cortical column dimensional grid networks basing interaction nearest neighbour approach 
model individual network states discrete transitions synchronous 
state alteration grid carried asynchronous fashion 
considering biological inspired computerised systems natural language understanding proposes sequential processing freeman brain intentionality meaning 
proposed approach include processing components output pulse trains nonlinear reaction input ii modules excitatory inhibitory neurons create oscillatory actions iii feedforward feedback links modules foster chaotic behaviour iv local learning mechanism hebbian learning achieve self organising modules 
pearce 
argues olfactory system ers ideal model examining issues robust sensory signal transmission ecient information representation neural system 
critical feature mammalian olfactory system large scale convergence spiking receptor stimulus thousands olfactory receptors fundamental information representation greater sensitivity 
typically information representation approaches olfactory cortex action grading potentials rate codes particular temporal codings 
study considered rate coded depiction input restricts quality signal recovered olfactory bulb 
done looking outcomes models uses probabilistic spike trains uses graded receptor inputs 
auditory perception various characteristics brain having capability detect loudness di erentiating clicks close 
ndings result manner information representation primary auditory cortex cortical synaptic dynamics 
synapses repeatedly activated react manner incoming impulse synapses produce short term depression facilitation 
great deal activity synapse amount resources available reduced followed period recovery synapse 
leaky integrate re neuron model input neuron model gained summing synaptic 
examination model reaction features neuron incorporated dynamic synapse close primary auditory cortex 

proposed set biologically inspired approaches knowledge discovery operations 
databases domain normally large distributed constantly growing size 
need computerised approaches achieve learning distributed data processed data 
techniques devised 
distributed incremental algorithms attempt determine information needs learner devising ective approaches providing distributed incremental setting 
splitting learning activity information extraction hypothesis production stages allows enhancement current learning approaches perform distributed context 
hypothesis production element control part causes information extraction component occur 
long term aim research develop founded multi agent systems able learn interaction open ended dynamic systems knowledge discovery activities 
models memory storage section provide outline various biologically inspired computerised models connected memory storage brain 
palm took similar approach autoassociative networks willshaw extend biological neurons synapses 
particular palm added characteristics represent spiking actions real neurons addition characteristics spatio temporal integration dendrites 
individual cells modelled spiking neurons time potential level particular threshold pulse action potential created 
palm model associative memory included model reciprocally connected visual areas comprising areas various neuron populations 
region retina input patterns matching input objects visual eld depicted bitmap 
area primary visual cortex spike neurons inhibitory gradual neurons 
area central visual area modelled ssi variant spiking associative memory 
biologically inspired model episodic memory shastri known outlines transient pattern rhythmic activity depicting event altered swiftly persistent robust memory trace 
creation memory trace matches recruitment complicated circuit hippocampal system includes required elements 
order analysis characteristics model performance examined plausible values system variables 
outcomes display robust memory traces produced assumes episodic memory capacity events bindings 
carrasco argue nite state machine modelled discrete time recurrent neural network discrete time processing elements biological networks perform continuous time methods synchronisation memory postulated 
ought possible produce natural biologically plausible approach nite state computation founded continuous time recurrent neural networks 
inputs outputs functions continuous time variable neurons temporal reaction 
possible encode indirect manner nite state machines sigmoid changing integrate re network 
considerable amount research carried austin associates york memory storage 
example volume correlation matrix memories cmms austin simple binary weighted feedforward neural networks various tasks ers indication memories stored human cerebral cortex 
cmms close single layer binary weighted neural networks complex learning recall algorithms 
doubt current computer systems able perform cognitive functions vision motion language processing level associated brain 
strong need new architectural information processing characteristics improve computerised systems 
interest biologically inspired computerised system grown signi cantly approaches currently available simplistic understanding brain system level limited 
key research issues biologically inspired computer systems relate fundamental architectural features processing related associated brain computer science learn biological neural computing 
characteristics brain potentially bene computerised systems include modular organisation robustness timing synchronisation learning memory storage 
modularity brain takes various forms abstraction including regional columnar cellular central biologically inspired computerised systems 
robustness comes brain ability recover functionality despite injury tissue repair re allocation functionality brain regions 
conventional computer systems presently synchronised clocked processing systems potentially enriched basing information processing encoding timing synchronisation approaches brain 
furthermore seen chapter volume particular fertile research area development biological inspired computing models learning memory storage perform various cognitive functions 
move eld biological inspired computing systems forward consideration architectural features brain modular organisation robustness timing learning memory bene performance systems 
suitable level neuroscience inspired abstraction producing systems identi ed 
ndings ered neuroscience research taken serious need understand constraints ered computer hardware software 
greater concentration dynamic network architectures alter structure experience elaborate circuitry constant network modi cation 
comprehensive understanding brain central nervous system critical achieve better biologically inspired adaptive computing systems 

andr 
role brain chaos 
wermter austin willshaw editors emergent neural computational architectures neuroscience volume 
springer verlag heidelberg germany 

uller 
neural network classi cation word evoked brain activity 
wermter austin willshaw editors emergent neural computational architectures neuroscience volume 
springer verlag heidelberg germany 

austin 
adam distributed associative memory scene analysis 
proceedings international conference neural networks page san diego 

austin 
matching performance binary correlation matrix memories 
sun alexandre editors connectionist symbolic integration uni ed hybrid approaches 
lawrence erlbaum associates new jersey 

austin keefe 
application associative memory analysis document fax images 
british machine vision conference pages 

austin 
associative memory image recognition occlusion analysis 
image vision computing 

bartsch obermayer 
uence threshold variability mean eld model visual cortex 
wermter austin willshaw editors emergent neural computational architectures neuroscience volume 
springer verlag heidelberg germany 


role right hemisphere recovery aphasia case studies 
cortex 

binder 
functional magnetic resonance imaging language cortex 
international journal imaging systems technology 

brown giraud carrier 
familiarity discrimination algorithm inspired computations cortex 
wermter austin willshaw editors emergent neural computational architectures neuroscience volume 
springer verlag heidelberg germany 


temporal structure neural activity modelling information processing brain 
wermter austin willshaw editors emergent neural computational architectures neuroscience volume 
springer verlag heidelberg germany 

bryson stein 
modularity specialized learning mapping agent architectures brain organization 
wermter austin willshaw editors emergent neural computational architectures neuroscience volume 
springer verlag heidelberg germany 


role cerebellum time critical goal oriented behaviour anatomical basis control principle 
wermter austin willshaw editors emergent neural computational architectures neuroscience volume 
springer verlag heidelberg germany 


pet follow study recovery stroke acute 
brain language 

honavar 
analysis synthesis agents learn distributed dynamic data sources 
wermter austin willshaw editors emergent neural computational architectures neuroscience volume 
springer verlag heidelberg germany 


modelling higher cognitive functions hebbian cell assemblies 
wermter austin willshaw editors emergent neural computational architectures neuroscience volume 
springer verlag heidelberg germany 

chawla lumer 
relationship synchronization neuronal populations mean activity levels 
neural computation 

deco 
biased competition mechanisms visual attention system 
wermter austin willshaw editors emergent neural computational architectures neuroscience volume springer verlag heidelberg germany 


dynamics learning memory lessons neuroscience 
wermter austin willshaw editors emergent neural computational architectures neuroscience volume 
springer verlag heidelberg germany 


investigation role cortical synaptic depression auditory processing 
wermter austin willshaw editors emergent neural computational architectures neuroscience volume 
springer verlag heidelberg germany 

herrmann 
stimulus independent data analysis fmri 
wermter austin willshaw editors emergent neural computational architectures neuroscience volume 
springer verlag heidelberg germany 

doya 
computations cerebellum basal ganglia cerebral cortex 
neural networks 

elman 
origins language conspiracy theory 
macwhinney editor emergence language pages 
lawrence associates hillsdale nj 

kiss 
complexity brain structural functional dynamic modules 
wermter austin willshaw editors emergent neural computational architectures neuroscience volume 
springerverlag heidelberg germany 

carrasco 
finite state computation analog neural networks steps biologically plausible models 
wermter austin willshaw editors emergent neural computational architectures neuroscience volume 
springer verlag heidelberg germany 


developmental cognitive neuroscience language new research domain 
brain language 

friedman kenny wise wu miller lewin 
brain activation silent word generation evaluated functional mri 
brain language 

mari 
modular auto achieving proper memory retrieval 
wermter austin willshaw editors third international workshop current computational architectures integrating neural networks neuroscience pages 



cognitive neuroscience biology mind 
norton new york 

green woods 
representations neuronal models minimal bilinear realisations 
wermter austin willshaw editors emergent neural computational architectures neuroscience volume 
springer verlag heidelberg germany 

otto 
neural network models cortical functions computational properties cerebral cortex 
journal physiology paris 

halliday 
temporal coding neuronal populations presence axonal dendritic conduction time delays 
wermter austin willshaw editors emergent neural computational architectures neuroscience volume 
springer verlag heidelberg germany 

hansen neumann 
neural mechanisms representing surface contour features 
wermter austin willshaw editors emergent neural computational architectures neuroscience volume 
springerverlag heidelberg germany 

hansen sepp neumann 
recurrent long range interactions early vision 
wermter austin willshaw editors emergent neural computational architectures neuroscience volume 
springer verlag heidelberg germany 

hanson hanson 
connectionist neuroimaging 
wermter austin willshaw editors emergent neural computational architectures neuroscience volume 
springer verlag heidelberg germany 

hebb 
organization behaviour 
wiley new york 

henderson 
segmenting state entities implication learning 
wermter austin willshaw editors emergent neural computational architectures neuroscience volume 
springer verlag heidelberg germany 

hicks 
explorations interaction split processing stimulus types 
wermter austin willshaw editors emergent neural computational architectures neuroscience volume 
springerverlag heidelberg germany 

hodgkin huxley 
current carried sodium potassium ions membrane giant axon 



cell assemblies intermediate level model cognition 
wermter austin willshaw editors emergent neural computational architectures neuroscience volume 
springer verlag heidelberg germany 

sharkey 
role memory anxiety hebbian learning hippocampal function novel explorations computational neuroscience robotics 
wermter austin willshaw editors emergent neural computational architectures neuroscience volume 
springer verlag heidelberg germany 

palm 
spiking associative memory scene segmentation synchronization cortical activity 
wermter austin willshaw editors emergent neural computational architectures neuroscience volume 
springer verlag heidelberg germany 

thiel weber 
brain plasticity aphasia contribution right hemisphere 
brain language 

austin 
connectionist propositional logic simple correlation matrix memory reasoning system 
wermter austin willshaw editors emergent neural computational architectures neuroscience volume 
springer verlag heidelberg germany 

marcus 
plasticity resolution apparent paradox 
wermter austin willshaw editors emergent neural computational architectures neuroscience volume 
springer verlag heidelberg germany 

martin sa ran dell 
recovery deep evidence relation auditory verbal stm capacity lexical error repetition 
brain language 

matsumoto kawato 
organisation computation systems 
neural networks vi 

mcclelland goddard 
considerations arising complementary learning systems perspective hippocampus neocortex 
hippocampus 

mcclelland mcnaughton reilly 
complementary learning systems hippocampus neocortex insights successes failures connectionist models learning memory 
review 

kato kato kojima 
prospective retrospective studies recovery aphasia changes cerebral blood ow language functions 
brain 


linguistic computation state space trajectories 
wermter austin willshaw editors emergent neural computational architectures neuroscience volume 
springer verlag heidelberg germany 

burton 
evidence chaos spiking trains neurons generate rhythmic motor patterns 
brain research bulletin 

keefe austin 
application adam associative memory analysis document images 
british machine vision conference pages 

palm sommer 
neural associative memory 
editors associative processing processors 
ieee computer society los alamitos ca 

rolls 
simulation studies speed recurrent processing 
wermter austin willshaw editors emergent neural computational architectures neuroscience volume 
springerverlag heidelberg germany 

pearce white 
robust stimulus encoding olfactory processing hyperacuity ecient signal transmission 
wermter austin willshaw editors emergent neural computational architectures neuroscience volume 
springer verlag heidelberg germany 

erez uribe 
time delay actor critic neural architecture reinforcement signal learning autonomous robots 
wermter austin willshaw editors emergent neural computational architectures neuroscience volume 
springer verlag heidelberg germany 

reggia 
computational investigation specialization interactions 
wermter austin willshaw editors emergent neural computational architectures neuroscience volume 
springer verlag heidelberg germany 

reilly 
collaborative cell assemblies building blocks cortical computation 
wermter austin willshaw editors emergent neural computational architectures neuroscience volume 
springer verlag heidelberg germany 



editor cognitive neuroscience pages 
psychology press hove east sussex 

rust adams george 
computational neural systems developmental evolution 
wermter austin willshaw editors emergent neural computational architectures neuroscience volume 
springer verlag heidelberg germany 

schultz golledge 
binding role correlated ring fast information transmission 
wermter austin willshaw editors emergent neural computational architectures neuroscience volume 
springer verlag heidelberg germany 

shastri 
computational model episodic memory formation inspired hippocampus system 
wermter austin willshaw editors third international workshop current computational architectures integrating neural networks neuroscience pages 


shastri 
biological grounding recruitment learning algorithms long term potentiation 
wermter austin willshaw editors emergent neural computational architectures neuroscience volume springer verlag heidelberg germany 

song miller abbott 
competitive hebbian learning dependent synaptic plasticity 
nature neuroscience 

spitzer 
mind net models learning thinking acting 
mit press cambridge ma 


locus synchronous oscillations excitatory inhibitory groups spiking neurons 
wermter austin willshaw editors emergent neural computational architectures neuroscience volume springer verlag heidelberg germany 

taylor 
images mind brain images neural networks 
wermter austin willshaw editors emergent neural computational architectures neuroscience volume 
springer verlag heidelberg germany 

treves rolls 
computational analysis role hippocampus memory 
hippocampus 

turner austin 
matching binary correlation matrix memories 
neural networks 

price wise 
mechanisms recovery aphasia evidence positron emission tomography studies 
journal psychiatry 

weber obermayer 
emergence modularity sheet neurons model comparison 
wermter austin willshaw editors emergent neural computational architectures neuroscience volume 
springerverlag heidelberg germany 

huber muller bier woods noth 
recovery aphasia positron emission tomographic study 
ann 

wennekers palm 
cell assemblies associative memory temporal structure brain signals 
miller editor time brain conceptual advances brain research vol 

academic publisher 

willshaw 
non symbolic approaches arti cial intelligence mind 
philos 
trans 
soc 


willshaw buckingham 
assessment marr theory hippocampus temporary memory store 
philos 
trans 
soc 
lond 


willshaw buneman longuet higgins 
associative memory 
nature 

willshaw dayan 
optimal plasticity matrix memories goes come 
neural computation 

willshaw hallam lau 
marr theory neocortex self organising neural network 
neural computation 

willshaw von der malsburg 
patterned neural connections set self organization 
philos 
trans 
soc 
lond 


wright ahmad 
connectionist simulation aphasic naming 
brain language 

zhou austin kennedy 
high performance nn classi er binary correlation matrix memory 
advances neural information processing systems vol 

mit 
