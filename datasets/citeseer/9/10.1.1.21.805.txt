foster provost bell atlantic science tech avenue white plains ny foster com case accuracy estimation comparing induction algorithms analyze critically classi cation accuracy compare classi ers natural data sets providing thorough investigation roc analysis standard machine learning algorithms standard benchmark data sets 
results raise serious concerns accuracy comparing classi ers draw question drawn studies 
course presentation describe demonstrate believe proper roc analysis comparative studies machine learning research 
argue methodology preferable making practical choices drawing scienti 
substantial research devoted development analysis algorithms building classi ers necessary part research involves comparing induction algorithms 
common methodology evaluations perform statistical comparisons accuracies learned classi ers suites benchmark data sets 
purpose question statistical tests dietterich salzberg question accuracy estimation 
believe primary scienti methodologies eld important scienti community cast critical eye 
reasonable justi cations comparing accuracies natural data sets require empirical veri cation 
argue particular form roc tom fawcett bell atlantic science tech avenue white plains ny fawcett com ron kohavi silicon graphics shoreline blvd mountain view ca ronnyk sgi com analysis proper methodology provide veri cation 
provide thorough analysis classi er performance standard machine learning algorithms standard benchmark data sets 
results raise serious concerns accuracy practical comparisons drawing scienti predictive performance concern 
contribution fold 
analyze critically common assumption machine learning research provide insights applicability discuss implications 
process describe believe superior methodology evaluation induction algorithms natural data sets 
roc analysis certainly new machine learning research applied principled manner geared speci machine learning researchers 
hope signi cant progress goal 
justifying accuracy comparisons consider induction problems intent applying machine learning algorithms build existing data model classi er classify previously unseen examples 
limit predictive performance intent accuracy machine learning studies consider issues comprehensibility computational performance 
assume true distribution examples classi er applied known advance 
informed choice performance estimated data available 
di erent methodologies arriving estimations described kohavi dietterich 
far commonly performance metric classi cation accuracy 
care comparisons accuracies benchmark data sets 
theoretically universe induction algorithms algorithm superior possible induction problems wolpert scha er 
tacit reason comparing classi ers natural data sets data sets represent problems systems face real world superior performance benchmarks may translate superior performance real world tasks 
eld collection data sets wide variety classi er applications merz murphy 
countless research results comparisons classi er accuracy benchmark data sets 
argue comparing accuracies benchmark data sets says little classi er performance real world tasks 
accuracy maximization appropriate goal ofthe real world tasks natural data sets taken 
classi cation accuracy assumes equal misclassi cation costs false positive false negative errors 
assumption problematic real world problems type classi cation error expensive 
fact documented primarily elds statistics medical diagnosis pattern recognition decision theory 
example consider machine learning fraud detection cost missing case fraud quite di erent cost false alarm fawcett provost 
accuracy maximization assumes class distribution class priors known target environment 
unfortunately benchmark data sets know existing distribution natural distribution strati ed 
iris data set exactly instances class 
splice junction data set dna donor sites acceptor sites sites natural class distribution skewed dna codes human genes saitta neri 
knowledge target class distribution claim maximizing accuracy problem data set drawn 
accuracy maximization appropriate accuracy estimates compare induction algorithms data sets 
believe best candidate justi cations 

classi er highest accuracy may classi er minimizes cost particularly classi er tradeo true positive predictions false positives tuned 
consider learned model produces probability estimates combined prior probabilities cost estimates classi cations 
model high classi cation accuracy produces probability estimates low cost target scenario 

induction algorithm produces highest accuracy classi ers may produce minimum cost classi ers training di erently 
example breiman 
suggest altering class distribution ective building cost sensitive decision trees see cost sensitive classi cation turney 
criticize practice comparing machine learning algorithms accuracy su cient merely point accuracy metric real world performance measured 
necessary analyze candidate justi cations founded 
justifications reasonable 
rst discuss commonly cited special case second justi cation arguing untenable assumptions 
results empirical study leads conclude justi cations questionable best 
define away problem 
principle class problem stratify classes target costs class distribution 
done maximizing accuracy transformed data corresponds minimizing costs target data breiman 
unfortunately strategy impracticable conducting empirical research benchmark data sets 
transformation valid class problems 
approximated ectively multiclass problems open question 
second know appropriate costs data sets noted applied researchers bradley catlett provost fawcett assigning costs precisely virtually impossible 
third described generally know class distribution natural data set true target class distribution 
uncertainties claim able transform cost minimization problems accuracy maximization problems 
cases specifying target conditions just virtually impossible impossible 
real world domains true target costs class distribution 
change time time place place situation situation fawcett provost 
ability transform cost minimization accuracy maximization justify limiting comparisons classi cation accuracy class distribution 
may comparisons classi cation accuracy useful indicative broader notion better performance 
roc analysis dominating models investigate algorithm generates high accuracy classi ers generally better produces low cost classi ers target cost scenario 
target cost class distribution information order conclude classi er higher accuracy better classi er show performs better reasonable assumptions 
limit investigation class problems analysis straightforward 
evaluation framework choose receiver operating characteristic roc analysis classic methodology signal detection theory common medical diagnosis begun generally ai swets provost fawcett 
brie review basics roc analysis 
roc space denotes coordinate system visualizing classi er performance 
roc space typically true positive rate tp plotted axis false positive rate fp plotted axis 
classi er represented space corresponding fp tp pair 
models produce continuous output estimate posterior probability instance class mem statistics vary threshold output varied extremes threshold value de ning classi er 
resulting curve called roc curve illustrates error tradeo available model 
roc curves describe predictive behavior classi er independent class distributions error costs decouple classi cation performance factors 
purposes crucial notion model dominates roc space meaning roc curves beneath equal 
dominating model model nb models possible cost class distributions 
dominating model exists considered best model terms predictive performance 
dominating model exist models represented best target scenarios cases exist scenarios model maximizes accuracy single number metric minimum cost 
shows test set roc curves uci domains study described 
note roc curves largest domains bumpy roc curves 
typical induction studies roc curves generated hold test set 
accuracy estimates single hold set roc curves may misleading tell observed variation due particular training test partition 
di cult draw strong expected behavior learned models 
conduct roc analysis cross validation 
bradley produced roc curves fold cross validation similarly bumpy 
bradley generated curves technique known pooling 
pooling ith points making raw roc curve averaged 
unfortunately discussed swets pickett pooling assumes ith points curves estimating space doubtful bradley method generating curves 
study important tohave approximation expected roc curve 
generate results fold cross validation adi erent methodology called averaging 
averaging procedure recommended swets bradley acknowledges fact germane study 
problematic 
true positives nb mc bagged mc ib ib ib false positives adult true positives nb mc bagged mc ib ib ib false positives satimage raw un averaged roc curves uci database domains pickett assumes normal tted roc curves roc space average roc curves manner 
fold cross validation roc curve folds treated function tp fp 
done linear interpolations points roc space multiple points fp maximum tp chosen 
averaged roc curve function fp mean fp 
plot averaged roc curves sample points regularly spaced fp axis 
compute con dence intervals mean tp common assumption binomial distribution 
standard methods produce dominating models 
state precisely basic hypothesis investigated standard learning algorithms produce dominating models standard benchmark data sets 
hypothesis true generally conclude algorithm higher accuracy generally better regardless target costs priors 
note classi cation performance line segment connecting roc points achieved randomly selecting classi cations weighted interpolation proportion classi ers de ning endpoints 
problems 
accuracy comparisons may select non dominating classi er indistinguishable point may worse 
hypothesis true di erent justi cation 
provide experimental study hypothesis designed follows 
uci repository chose datasets contained instances accuracy decision trees roc curves di cult read high accuracies 
domain induced classi ers minority class road chose class grass 
selected inducers mlc kohavi decision tree learner mc naive bayes discretization nb nearest neighbor values bagged mc breiman 
mc similar quinlan probabilistic predictions laplace correction leaves 
nb discretizes data entropy minimization dougherty builds naive bayes model domingos pazzani 
votes closest neighbors neighbor votes weight equal distance test instance 
averaged roc curves shown figures 
vehicle domains absolute dominator 
general runs performed data sets crossvalidation folds dominating classi ers 
cases close example adult waveform 
cases curve dominates area roc space dominated 
refute hypothesis algorithms produce true positive true positive mc nb ib ib ib bag mc false positive vehicle false positive dna mc nb ib ib ib bag mc statistically signi cantly dominating classi ers 
true positive true positive mc nb ib ib ib bag mc false positive waveform mc nb ib ib ib bag mc false positive adult smoothed roc curves uci database domains draws question claims algorithm better algorithm accuracy comparison 
order draw absence target costs class distributions roc curve algorithm signi cant dominator algorithm obvious implications machine learning research 
practical situations weaker claim su cient algorithm choice algorithm accuracies signi cantly di erent 
clear type justi ed 
domains curves statistically indistinguishable dominators area space signi cantly dominated 
practical situations typically comparisons wealth classi ers considering 
classi ers compared 
considering general pairwise comparisons algorithms cases model pair clearly better di erent regions roc space 
clearly draws question single number metrics practical algorithm comparison metrics precise target cost class distribution information 
standard methods coerced yield dominating roc curves 
second justi cation accuracy compare algorithms subtly di erent rst 
speci cally possibility coercing algorithms produce di erent behaviors di erent scenarios cost sensitive learning 
done accuracy comparisons justi ed arguing domain algorithm higher accuracy algorithm lower cost reasonable costs class distributions 
con rming refuting justi cation completely scope best coerce algorithms di erent environmental conditions open question 
straightforward method samples evaluated satisfactorily 
argue roc framework outlined far minor modi cation evaluate question 
er rming evidence 
algorithms may produce di erent models di erent cost class distributions roc methodology stated quite adequate 
able evaluate performance algorithm individual model 
characterize algorithm performance roc analysis producing composite curve set generated models 
done pooling convex hull roc curves produced set models described detail provost fawcett 
form hypothesis second potential justi cation standard learning algorithms produce dominating roc curves standard benchmark data sets 
con rming hypothesis important step justifying common practice ignoring target costs class distributions class er comparisons natural data 
unfortunately know con rming evidence 
hand rming evidence 
consider results 
naive bayes robust respect changes costs produce roc curve regardless target costs class distribution 
furthermore shown decision trees surprisingly robust probability estimates generated laplace estimate bradford 
result holds generally results previous section rm hypothesis 
second bradley results provide rming evidence 
speci cally studied real world medical data sets uci repository sources 
bradley plotted roc curves classi er learning algorithms consisting neural nets decision trees statistical techniques 
bradley uses composite roc curves formed training models di erently di erent cost distributions 
previously criticized design study purpose answering question 
results replicated current methodology strong statement 
data sets dominating classi er 
implies domain exist disjoint sets conditions di erent induction algorithms preferable 
recommendations limitations designing comparative studies researchers clear want able draw results 
argued comparisons algorithms accuracy unsatisfactory dominating classi er 
presenting case accuracy goals 
want show precise comparisons target cost class distributions known 
dominator 
single number metric strong domain speci information 
possible look ranges costs class distributions classi er dominates 
problems cost sensitive classi cation learning skewed class distributions analyzed precisely 
knowledge target conditions precise concise robust speci cation classi er performance 
described detail provost fawcett slopes lines tangent roc convex hull determine ranges costs class distributions particular classi ers minimize cost 
speci target conditions corresponding slope cost ratio times reciprocal class ratio 
domains optimal classi ers di erent target conditions table 
example road domain see table naive bayes best classi er target conditions corresponding slope bagged mc best slopes greater table locally dominating classi ers uci domains domain slope range dominator domain slope range dominator adult nb pima nb bagged mc bagged mc nb nb breast nb bagged mc cancer ib nb ib bagged mc ib nb bagged mc satimage nb crx bagged mc bagged mc nb ib bagged mc ib nb ib german nb ib bagged mc bagged mc nb waveform nb ib bagged mc road nb ib grass bagged mc bagged mc dna nb vehicle bagged mc bagged mc 
perform equally 
admit elegant single number comparison believe research practice 
summary dominating classi er exist cost class distribution information unavailable strong statement classi er superiority 
able precise statements superiority speci regions roc space 
example know false positive errors tolerated may nd particular algorithm superior far left edge roc space 
limited investigation classes 
ect results negative 
recommending analytical framework extending multiple dimensions interesting open problem 
completely satis ed method generating con dence intervals 
intervals appropriate neyman pearson observer egan wants maximize tp fp 
appropriateness questionable evaluating minimum expected cost set costs contours roc space lines particular slope 
area fundamental drawback methodology 
ered debate justi cation accuracy estimation primary metric comparing algorithms benchmark data sets 
elucidated believe top candidates justi cation shown realistic specify cost class distributions precisely supported experimental evidence 
draw 
justi cations accuracy compare classi ers questionable best 
second described believe proper roc analysis applied comparative studies machine learning research 
roc analysis simple comparing single number metric 
believe additional power delivers worth ort 
certain situations roc analysis allows strong general positive negative 
situations strong general roc analysis allows precise analysis conducted 
roc analysis new machine learning research applied principled manner geared speci machine learning researchers 
hope signi cant progress goal 
discussed justi cations accuracy comparisons roc analysis applied classi er learning 
rob holte provided helpful comments draft 
bradford kunz kohavi brunk brodley 
pruning decision trees misclassi cation costs 
proceedings ecml pages 
bradley 
area roc curve evaluation machine learning algorithms 
pattern recognition 
breiman friedman olshen stone 
classi cation regression trees 
wadsworth international group 
breiman 
bagging predictors 
machine learning 
catlett 
tailoring rulesets misclassi costs 
proceedings conference ai statistics pages 
dietterich 
approximate statistical tests comparing supervised classi cation learning algorithms 
neural computation 
appear 
domingos pazzani 
independence conditions optimality simple bayesian classi er 
machine learning 
dougherty kohavi sahami 
supervised unsupervised discretization continuous features 
prieditis russell eds proceedings icml pages 
morgan kaufmann 
egan signal detection theory roc analysis 
series perception 
academic press new york 
fawcett provost 
adaptive fraud detection 
data mining knowledge discovery 
available www net fawcett dmkd ps gz 
kohavi sommer eld dougherty 
data mining mlc machine learning library 
international journal arti cial intelligence tools 
available www sgi com technology mlc 
kohavi 
study cross validation bootstrap accuracy estimation model selection 
mellish ed proceedings ijcai pages 
morgan kaufmann 
available robotics stanford edu ronnyk 
merz murphy 
uci repository machine learning databases 
available www 
ics uci edu mlearn mlrepository html 
provost fawcett 
analysis visualization classi er performance comparison imprecise class cost distributions 
proceedings kdd pages 
aaai press 
provost fawcett 
robust classi cation systems imprecise environments 
proceedings aaai 
aaai press 
appear 
available www net fawcett papers aaai dist ps gz 
quinlan 
programs machine learning 
morgan kaufmann san mateo california 
saitta neri 
learning real world 
machine learning 
salzberg 
comparing classi ers pitfalls avoid recommended approach 
data mining knowledge discovery 
scha er 
conservation law generalization performance 
icml pages 
morgan kaufmann 
swets andr pickett 
evaluation diagnostic systems methods signal detection theory 
new york academic press 
swets 
measuring accuracy diagnostic systems 
science 
turney 
cost sensitive learning bibliography 
available ai iit nrc ca bibliographies cost sensitive html 
wolpert 
relationship pac statistical physics framework bayesian framework vc framework ind wolpert ed mathematics generalization 
addison wesley 
true positive true positive true positive mc nb ib ib ib bag mc false positive breast cancer false positive german mc nb ib ib ib bag mc false positive mc nb ib ib ib bag mc true positive true positive true positive mc nb ib ib ib bag mc false positive crx mc nb ib ib ib bag mc false positive pima mc nb ib ib ib bag mc false positive satimage smoothed roc curves uci database domains cont 
