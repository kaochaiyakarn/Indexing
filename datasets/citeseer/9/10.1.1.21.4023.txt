algorithms digit recognition lecun jackel bottou denker drucker guyon simard vapnik bell laboratories holmdel nj usa email yann research att com compares performance classifier algorithms standard database handwritten digits 
consider raw accuracy rejection training time recognition time memory requirements 
comparison leaning handwritten digit recognition international conference neural eds 
fogelman gallinari ec cie publishers paris pp 
le gun bottou 
denker guyon jackel muller 
sackinger simard comparison learning algorithms handwritten digit recognition lecun jackel bottou cortex denker drucker guyon simard vapnik bell laboratories holmdel nj usa email yann research att com simultaneous availability inexpensive powerful computers powerful learning algorithms large databases caused rapid progress handwriting recognition years 
compares relative merits classification algorithms developed bell laboratories purpose recognizing handwritten digits 
recognizing individual digits problems involved designing practical recognition system excellent benchmark comparing shape recognition methods 
existing method combine handcrafted feature extractor trainable classifier study concentrates adaptive methods operate directly size normalized images 
database database train test systems described constructed nist special database special database containing binary images handwritten digits 
training set composed patterns sd patterns sd 
test set composed patterns sd patterns sd 
pattern training set contained examples approximately writers 
sure sets writers training set test set disjoint 
images size normalized fit pixel box preserving aspect ratio 
experiments images moments inertia 
experiments centered larger input field center mass grayscale pixel values reduce effects aliasing 
methods lenet tangent distance subsampled versions images pixels 
classifiers section briefly describe classifiers study 
complete descriptions readers may consult 
baseline linear classifier possibly simplest classifier consider linear classifier 
input pixel value contributes weighted sum output unit 
output unit highest sum including contribution bias constant indicates class input character 
experiment images 
network free parameters 
deficiencies linear classifier documented duda hart included simply form basis comparison sophisticated classifiers 
test error rate 
various combinations sigmoid units linear units gradient descent learning learning directly solving linear systems gave similar results 
baseline nearest neighbor classifier simple classifier nearest neighbor classifier euclidean distance measure input images 
classifier advantage training time brain part designer required 
memory requirement recognition time large complete pixel training images megabytes byte pixel megabytes bits pixel available run time 
compact representations devised modest increase recognition time error rate 
previous case images 
test error 
naturally realistic euclidean distance nearest neighbor system operate feature vectors directly pixels systems operate directly pixels result useful baseline comparison 
pairwise linear classifier simple improvement basic linear classifier tested guyon 
idea train unit single layer network classify class class 
case layer comprises units labelled 
unit trained produce patterns class patterns class trained patterns 
final score class sum outputs units labelled minus sum output units labelled error rate test set slightly better linear classifier 
principal component analysis polynomial classifier preprocessing stage constructed computes projection input pattern principal components set training vectors 
compute principal components mean input component computed subtracted training vectors 
covariance matrix resulting vectors computed diagonalized singular value decomposition 
dimensional feature vector input second degree polynomial classifier 
classifier seen linear classifier inputs preceded module computes products pairs input variables 
error test set 
radial basis function network lee rbf network constructed 
layer composed gaussian rbf units inputs second layer simple linear classifier 
rbf units divided groups 
group units trained training examples classes adaptive means algorithm 
second layer weights computed regularized pseudo inverse method 
error rate test set large fully connected multi layer neural network classifier tested fully connected multi layer neural network layers weights hidden layer 
network trained various numbers hidden units 
de slanted images input 
best result test set obtained network approximately weights 
remains somewhat mystery networks large number free parameters manage achieve reasonably low testing errors 
conjecture dynamics gradient descent learning multi architecture lenet 
plane represents map set units weights constrained identical 
input images sized fit pixel field blank pixels added border field avoid edge effects convolution calculations 
layer nets self regularization effect 
origin weight space saddle point attractive direction weights invariably shrink epochs theoretical analysis confirm sara solla personal communication 
small weights cause sigmoids operate quasi linear region making network essentially equivalent low capacity single layer network 
learning proceeds weights grow progressively increases effective capacity network 
better theoretical understanding phenomena empirical evidence definitely needed 
lenet solve dilemma small networks learn training set large networks design specialized network architectures specifically designed recognize dimensional shapes digits eliminating irrelevant distortions variability 
considerations lead idea network 
convolutional net unit takes input local receptive field layer forcing extract local feature 
furthermore units located different places image grouped planes called maps units constrained share single set weights 
operation performed feature map shift invariant equivalent convolution followed squashing functions 
weight sharing technique greatly reduces number free parameters 
single layer formed multiple feature maps extracting different features types 
complete networks formed multiple convolutional layers extracting features increasing complexity abstraction 
sensitivity shifts distortions reduced lower resolution feature maps higher layers 
achieved inserting subsampling layers convolution layers 
important stress weights network trained gradient descent 
computing gradient done slightly modified version classical backpropagation procedure 
training process causes convolutional networks automatically synthesize features 
convolutional network architecture lenet shown trained database 
lenet small input field images sampled pixels centered input layer 
ooo multiply add steps required evaluate lenet convolutional nature keeps number free parameters 
lenet architecture developed version usps database size tuned match available data 
lenet achieved test error 
lenet experiments lenet clear larger convolutional network needed optimal large size training set 
lenet designed address problem 
expanded version lenet input layer images centered center mass includes feature maps additional layer hidden units fully connected layer features maps output units 
lenet contains connections free parameters 
test error 
previous experiments zip code data replacing layer lenet complex classifier improved error rate 
replaced layer lenet euclidean nearest neighbor classifier local learning method bottou vapnik local linear classifier retrained time new test pattern shown 
improve raw error rate improve rejection 
lenet lenet architecture similar lenet feature maps larger fully connected layer uses distributed representation encode categories output layer traditional code 
lenet total connections free parameters layers 
images centered center mass training procedure included module distorts input images training small randomly picked affine transformations shift scaling rotation skewing 
achieved error 
boosted lenet theoretical schapire drucker 
drucker developed boosting method combining multiple classifiers 
lenet combined trained usual way 
second trained patterns filtered net second machine sees mix patterns net got right got wrong 
third net trained new patterns second nets disagree 
testing outputs nets simply added 
error rate lenet low necessary artificially increase number training samples random distortions lenet order get samples train second third nets 
test error rate best classifiers 
glance boosting appears times expensive single net 
fact net produces high confidence answer nets called 
cost times single net 
tangent distance classifier tdc tangent distance classifier tdc nearest neighbor method distance function insensitive small distortions translations input image simard 
consider image point high dimensional pixel space dimensionality equals number pixels evolving distortion character traces curve pixel space 
taken distortions define low dimensional manifold pixel space 
small distortions vicinity original image manifold approximated plane known tangent plane 
excellent measure closeness character images distance tangent planes set distortions generate planes includes translations scaling skewing error rate test set 
uncertainty quoted error rates 
squeezing rotation line thickness variations 
test error rate achieved pixel images 
prefiltering techniques simple euclidean distance multiple resolutions allowed reduce number necessary tangent distance calculations 
storage requirement assumes patterns represented multiple resolutions byte pixel 
optimal margin classifier polynomial classifiers methods generating complex decision surfaces 
unfortunately impractical high dimensional problems number product terms prohibitive 
particularly interesting subset decision surfaces ones correspond hyperplanes maximum distance convex hulls classes high dimensional space product terms 
guyon vapnik bow realized polynomial degree maximum margin set computed computing dot product input image subset training samples called support vectors elevating result th power linearly combining numbers obtained 
finding support vectors coefficients amounts solving high dimensional quadratic minimization problem linear inequality constraints 
version procedure known soft margin classifier cortes vapnik suited noisy problems th degree decision surface test error reached 
number support vectors obtained 
discussion summary performance classifiers shown figures 
shows raw error rate classifiers example test set 
boosted lenet clearly best achieving score closely followed lenet 
compared estimate human performance 
shows number patterns test set rejected attain error 
applications rejection performance significant raw error rate 
boosted lenet best score 
enhanced versions lenet better original lenet time required spare recognition single character starting size normalized image milliseconds 
raw accuracies identical 
shows time required spare method recognize test pattern starting size normalized image 
memory method slower neural networks 
single board hardware designed lenet mind performs recognition characters sec graf 
cost effective hardware implementations memory techniques elusive due enormous memory requirements 
training time measured 
nearest neighbors tdc essentially zero training time 
single layer net pairwise net pca quadratic net trained hour multilayer net training times longer days lenet days fully connected net weeks lenet month boosted lenet 
training soft margin classifier took days 
training time marginally relevant designer totally irrelevant customer 
shows memory requirements various classifiers 
figures bit pixel representations prototypes nearest neighbors byte pixel soft margin tangent distance 
taken upper bounds clever compression data elimination redundant training examples reduce memory requirements memory requirements classification test patterns mbytes 
numbers bit pixel nn byte pixel soft margin tangent distance byte pixel rest 
methods 
memory requirements neural networks assume bytes weight bytes prototype component lenet memorybased hybrids experiments show byte weights significant change error rate 
high accuracy classifiers lenet requires memory 
snapshot ongoing 
expect continued changes aspects recognition technology remain valid time 
performance depends factors including high accuracy low run time low memory requirements 
computer technology improves recognizers feasible 
larger recognizers turn require larger training sets 
lenet appropriate available technology years ago just lenet appropriate 
years ago recognizer complex lenet required months training data available considered 
quite long time lenet considered state art 
local learning classifier optimal margin classifier tangent distance classifier developed improve lenet succeeded 
turn motivated search improved neural network architectures 
search guided part estimates capacity various learning machines derived measurements training test error function number training examples 
discovered capacity needed 
series experiments architecture combined analysis characteristics recognition errors lenet lenet crafted 
find boosting gives substantial improvement ac relatively modest penalty memory computing expense 
models increase effective size data set data 
optimal margin classifier excellent accuracy remarkable high performance classifiers include priori knowledge problem 
fact classifier just image pixels permuted fixed mapping 
slower memory hungry convolutional nets 
improvements expected technique relatively new 
convolutional networks particularly suited recognizing rejecting shapes widely varying size position orientation ones typically produced heuristic real world string recognition systems see article jackel proceedings 
plenty data available methods attain respectable accuracy 
neural net methods require considerable training time trained networks run faster require space memorybased techniques 
neural nets advantage striking training databases continue increase size 
guyon vapnik training algorithm margin classifiers proceedings fifth annual workshop computational learning theory pittsburgh 
bottou vapnik local learning algorithms neural computation 
vapnik soft margin classifier machine learning appear 

duda hart pattern classification scene analysis chapter john wiley sons 
drucker schapire simard boosting performance neural networks international journal pattern recognition artificial intelligence 
guyon personnaz dreyfus denker lecun comparing different neural net architectures classifying handwritten digits proc 
ijcnn ii washington dc 
ieee 
lecun denker henderson howard hubbard jackel handwritten digit recognition back propagation network touretzky ed advances neural information processing systems morgan kaufman 
lee handwritten digit recognition nearest neighbor radial basis functions neural networks neural computation 
sackinger 
graf system high speed pattern recognition image analysis proc fourth international conference microelectronics neural networks fuzzy systems ieee 
multi font word recognition system postal address reading ieee trans 
simard lecun denker pattern recognition new transformation neural information processing systems morgan kaufmann 
