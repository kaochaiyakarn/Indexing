bioinformatics discovery note cliff clustering high dimensional microarray data iterative feature filtering normalized cuts eric xing richard karp cliff algorithm clustering biological samples gene expression microarray data 
clustering problem difficult reasons particular sparsity data high dimensionality feature gene space fact features irrelevant redundant 
algorithm iterates computational processes feature filtering clustering 
partition approximates correct clustering samples feature filtering procedure ranks features intrinsic discriminability relevance partition relevant features uses ranking select features round clustering 
clustering algorithm concept normalized cut clusters samples new partition basis selected features 
studied problem involving leukemia samples genes demonstrate cliff outperforms standard clustering approaches consider feature selection issue produces result close original expert labeling sample set 
contact cs berkeley edu cluster analysis gene expression microarray data key step understanding activity genes varies biological processes affected disease states cellular environments 
clustering group genes expression set samples eisen wen 
ideally resulting groups coherent expression pattern possibly suggesting modular structure gene regulation system 
type clustering sense difficult curse dimensionality due small sample volume high feature dimensionality valuable clinical mechanistic study cluster samples homogeneous groups may correspond particular macroscopic phenotypes clinical syndromes vol 
pages division computer science university california berkeley berkeley ca usa cancer types golub 
typical biological system clearly known genes sufficient fully characterize macroscopic phenotype 
practically working mechanistic hypothesis testable largely captures biological truth seldom involves dozens genes knowing identity relevant genes just important finding grouping samples induce 
essential formulate problem biological pattern recognition microarray data way involves interplay clustering produce sample partition feature selection identify genes significantly contribute partition interest 
rich literature cluster analysis various techniques developed 
reports shown application techniques cluster analysis gene expression data see somogyi overview 
works address issue feature selection explicitly appear serious problem long number features relatively small features irrelevant redundant 
case objects clustered genes features cluster correspond selected set samples 
situation quite different objects clustered samples features correspond genes 
failure recognize fundamental asymmetry situations may account lack attention feature selection 
typical microarray data sets sample space gene space different dimensionality samples versus genes 
furthermore design sample space gene space different clustering purposes subject different levels quality control 
example usually clear knowledge biological scenario cell cycle wishes analyze gene expression construct sample space accordingly time course data cell cycle hand analyzing sample set patient group usually little knowledge construct oxford university press xing informative gene space genes relevant unclear 
frequent alternative complete list known genes 
sparsity data high dimensionality feature space fact features irrelevant redundant cause difficulties 
may different founded statistically significant ways cluster samples 
clustering algorithm guaranteed capture meaningful partition corresponding phenotype actual empirical interest having having particular type tumor set samples may display gender age disease variability may serve partitioning criteria 

microarrays typically task specific features necessarily related phenotype interest 
phenotype interest tumor type induces strong discriminating pattern feature space distance calculation samples subject interference large number irrelevant features 

goal clustering merely find underlying grouping samples form generalizable cluster representations sample recognition rules novel samples correctly labeled 
vapnik chervonenkis showed generalization risk bound representations rules increases vc dimension exponentially related dimensionality feature space 
large feature sets inevitably increase possibility predictive error clustering results 
approaches taken selecting features microarray sample clustering 
approach domain experts select features 
obviously easily generalizable 
approach clustering algorithm group features coherent sets project samples lower dimensional space spanned average expression patterns coherent feature sets hastie 
approach deals feature redundancy problem fails detect non discriminating irrelevant features 
principal component analysis pca may remove non discriminating irrelevant features restricting attention called eigenfeatures corresponding large eigenvalues basis element new feature subspace linear combination original features making difficult identify important features hastie alter 
main difficulty direct feature selection cluster analysis lack information feature evaluation 
machine learning literature feature selection primarily applied supervised learning paradigm kohavi john langley 
quality feature usually measured respect partition 
relevant feature partition better clustering paradigm information making hard tell feature qualified included analysis 
propose novel algorithm cliff clustering iterative feature filtering combines clustering process feature selection process bootstrap iterative way process uses output approximate input outputs processes improve hand hand course iterations 
cliff requires efficient clustering algorithm filters feature selection 
apply graph partition algorithm known approximate normalized cut shi malik generate dichotomy samples iteration :10.1.1.160.2324
approximate normalized cut avoids pitfalls usual minimum cut approach tends produce highly unbalanced partitions 
efficient algorithm approximate normalized cut ideal module iterative algorithm 
feature selection part mixture feature evaluation experts independent feature modeling information gain ranking markov blanket filtering koller sahami remove non discriminative irrelevant redundant genes respectively original gene set :10.1.1.155.2293
example demonstrate performance algorithm way clustering problem generalization multi way clustering straightforward partitions leukemia samples consisting subtypes groups gene expression profiles 
remainder structured follows section describes approximate normalized cut algorithm object partitioning 
section describes types feature selection techniques feature filtering system 
section full cliff algorithm 
section goes describe experimental results section conclude brief summary discussion algorithm 
clustering algorithm preliminaries consider clustering similarity measure objects 
represent set objects partitioned vertex set complete graph associated object expression vector dimensional feature space 
edge weight corresponding degree similarity objects 
intuitively way partition graph property sum weights edges joining subgraphs small 
accordingly microarray clustering approaches reported click shamir algorithm partition graph minimum cut algorithm minimizes sum weights edges joining parts 
weakness approach little guarantee algorithm go astray generate partitions highly unbalanced sophisticated pruning techniques need developed explicitly enforce cut balance 
approximate normalized cut ncut algorithm applied image segmentation shi malik avoids unbalanced cut difficulty natural efficient way :10.1.1.160.2324:10.1.1.160.2324
normalized cut necessarily disjoint subsets vertex set define minimum cut partition vertex set subsets minimizes contrast normalized cut framework normalize scaling relative example sum vertices total weight edges incident 
scaling eliminates bias highly unbalanced cuts 
specifically define normalized weight cut follows optimal normalized cut cut minimum normalized weight 
unfortunately computing optimal normalized cut np hard edge weights non negative 
effort efficiently compute cut approximately minimum normalized weight shi malik reformulated problem linear algebra notation showed problem computing optimal normalized cut formulated follows subject weight matrix diagonal matrix isa positive constant :10.1.1.160.2324
correspondence cuts feasible solutions value cliff objective function equal normalized weight corresponding cut 
note objective function rayleigh quotient 
relax constraint allow elements take real values rayleigh quotient theorem 
minimize solving generalized eigenvalue system shown eigenvector denoted associated second smallest eigenvalue generalized eigenvalue system optimal solution relaxed problem 
graph partition corresponding approximate solution recovered choosing best possible partitions corresponds separating large components small components 
threshold largest elements smallest elements approximate normalized cut algorithm selects best thresholds normalized cut criterion easy generalize approximate normalized cut algorithm way partitioning multi way clustering 
way perform recursive way cuts resulting subset vertices singleton 
essentially produces binary hierarchy topdown direction cut weight associated branching point hierarchy reflects degree dissociation subtrees directly branching point 
hierarchy choose clustering desired degree granularity cluster corresponding subtree 
way eigenvectors generalized eigenvalue system come simultaneous way cut see shi malik details :10.1.1.160.2324
weight definition difficulties similarity clustering approach quality clustering normalized cut algorithm pairwise similarities fundamentally depends weights provided input 
ways define similarity measure biological objects somogyi 
pearson correlation coefficient exponential kernel expression vector denoted capture similarity objects xing scaling factor controlling sensitivity clustering strength pairwise similarity 
call resulting matrix affinity matrix 
common problem pairwise similarity approach resolution similarity measure scale dimensionality feature space especially measurements noisy majority features irrelevant target partition multiple meaningful partitions possible 
show doing feature selection effectively reduce clustering problem lower dimensional space pairwise similarities mates objects group non mates objects different groups resolve distinguishable distributions 
enhancement significantly improve graph theoretic clustering better able perform probabilistic modeling resulting feature subspace 
mixture feature selection experts preliminary thousands genes simultaneously measured microarray experiment expressions related particular partition samples 
analysis biological system rules thumb regarding gene functions assumed 
gene state subtly neutral genes simultaneously respond single physiological event gene functions highly redundant 
complete knowledge gene regulatory network priori just neglect non discriminative irrelevant redundant genes explicitly lower dimensional feature space 
needs probe relevance different subsets genes target biological event induces partition order choose best feature subspace 
exhaustive search power set feature set intractable number features large case microarray data 
various heuristic feature selection methods developed 
aforementioned assumptions gene activity approaches probe relevance feature sequential fashion independent feature modeling unsupervised information gain ranking supervised markov blanket filtering supervised 
independent feature modeling important empirical assumption activity genes expression generally assume distinct biological states 
combination discrete patterns multiple genes determines sample phenotype 
assumption expect marginal probability measurements individual gene modeled univariate mixture say components includes degenerate case single component 
gene measurement points single state states probability distributions overlap heavily may conclude gene probably contributes little discriminating samples 
formally feature vector measurements samples 
assuming state sample measurements state come form gaussian distribution model likelihood univariate mixture gaussians easily generalizable components sample state 
learning mixture model easy em 
model derive classification hypothesis state feature value drawn 
bayes error probability misclassifying sample drawn mixture gaussians error best achieve classifying samples intuitively reasonable measure discriminability 
highly discriminative features anticipate small 
may rank features discriminability order determined 
note mixture model quantizer allowing discretization measurements feature 
simply replace measurement associated binary value quantization scheme adopted rest feature selection sections specified 
discretization allows bring information theoretic techniques bear determining degree agreement feature partition samples 
reuse quantization partitions samples case hierarchical clustering 
information gain ranking mentioned supervised learning paradigm feature quality easier assess explicitly measure degree agreement feature sample partition 
standard measure purpose information gain 
partition probability part empirical proportion 
suppose test feature induces partition training set information gain due feature respect original partition entropy function entropy discrete probability distribution defined 
information gain measure applicable partition consistent target concept learn 
simplicity deal way partition parts denoted note need decision rule feature order generate partition induced feature 
turn classification rule obtained independent feature modeling described section 
induces subjective partition measurements 
naturally higher information gain suggests induced partition consistent partition relevant feature 
markov blanket filtering natural assume features gene transcription levels redundant secondary responses biological experimental conditions distinguish different samples 
desirable retain non redundant directly relevant features 
formalize goal follows select feature subset distributions close just projection variables 
computational purposes convenient replace original feature values binary values result discretization mixture gaussians 
replacement done subsection noted 
example denotes binary value resulting discretization original real valued measurement 
define distance feature subset asthe expectation discrete values feature subset cross entropy denoted cliff conditional distribution conditional distribution projection want find small feature set small 
intuitively information contributed feature subsumed small subset features called markov blanket graphical models literature able neglect compromising accuracy class prediction 
naturally suggests filtering approach deleting bad features conducting combinatorial search power set feature set 
koller sahami proposed technique sequentially identifying features non existence markov blanket candidate feature set :10.1.1.155.2293
cases features markov blanket limited size look features approximate markov blanket 
purpose define markov blanket fortunate case occur relax condition seek set small 
goal find small irredundant feature subset features form approximate markov blanket feature strongly correlated construct candidate markov blanket collecting features highest correlations small integer 
computing correlations original real values features discretized values 
algorithm goes proposed koller sahami initialize iterate feature set features correlations highest :10.1.1.155.2293:10.1.1.155.2293
compute choose minimizes define heuristic sequential method far efficient methods conduct extensive search subsets feature set 
heuristic method xing requires independent feature modeling discretize case data gene followed computation quantities form full algorithm far described types modules analysis gene expression data 
approximate normalized cut ncut algorithm takes affinity matrix defined objects certain feature space input outputs partition objects 
mixture feature selection experts feature modeling stage supervised information needed take partition output ordering features terms relevance respect partition 
outline procedure combines modules interactive way alternating computing new partition currently selected features selecting new set features current partition 
bootstrapping step select initial set features entirely independent feature modeling 
specifically unsupervised independent feature modeling technique rank features terms discriminability 
generate initial partition discriminative features specified advance 
partition treat feature selection roughly supervised learning problem information gain ranking markov blanket filtering applied newly determined feature subset generate new partition turn improve feature selection 
scenario know exact target partition apriori respect optimize feature subset iteration expect obtain approximate partition close target allows selection approximately feature subset hopefully draw partition closer target partition iteration 
algorithm similar spirit em algorithm dempster searches parameter space local minima coordinate descent type approach improving objective function direction time assuming invariance directions 
approach lead local minimum point basin cyclic set attraction points 
lists details algorithm 
experiments results section report results cliff microarray clustering problem 
data collection initialize independent feature modeling compute bayes error feature features corresponding smallest bayes errors generate partition ncut iterate partition compute information gain feature features corresponding smallest values generate partition ncut order features markov blanket filtering features corresponding smallest values generate partition ncut converge iteration fig 

cliff algorithm leukemia patient samples reported golub 
sample measured genes 
pathological criteria samples include type called type ii called aml 
want see cliff able generate partition matches pathological categorization samples gene expression profiles 
feature relevance analysis expression data prelude partitioning samples help partition correct partition actual subtype labeling leukemia samples gauge possible value feature selection 
evaluated gene measures 
degree discriminability measured bayes error mixture gaussians model 
information gain correct partition 
value feature redundant 
great variation genes respect measures 
shows bayes errors defined eq 
genes ascending order 
seen small percentage genes resolve states error rate significantly better bayes error predictive power genes gene information gain information gain genes respect partition gene divergence removal gene mb cliff gene fig 

feature selection stage procedure 
genes ranked eq 
indicates discrimination power 
genes ranked eq 
indicates degree relevance 
genes ranked eq 
indicates degree redundancy 
random guess 
gene expressions successfully modeled mixture gaussians practice example due presence outliers possibly multiple states spare discussion issue sake simplicity 
plots information gain individual genes respect partition 
note small fraction genes induce significant information gain relevant 
take top genes list set sake computational efficiency proceed approximate markov blanket filtering 
shows value defined eq 
measures extent subsumes information carried renders redundant 
genes removed increasing order small value quantity indicates complete 
experiment choose size markov blanket small avoid fragmenting small sample set 
real biological regulatory network system gene expected directly influenced small markov blanket assumption plausible 
separate xing mixture feature selection experts supervised classification setting 
small subset selected features concept learning resulting classifier significantly outperforms takes full feature set account 
reasonable expect cluster analysis feature selection lead error prone result 
clustering cliff compared algorithms partitioning leukemia samples classes 
approximate ncut feature selection 
means feature selection 
cliff approximate ncut iterative feature selection partitioning 
means feature selection 
quality measures applied shamir minkowski measure homogeneity evaluate quality resulting partitions 
minkowski measure disagreement computed partition correct partition defined number pairs samples part partitions number pairs lie part true partition 
quantities refer computed partition true partition average minimum correlations expression vector sample mean expression vectors samples cluster 
shows results approximate ncut cliff 
graphic display input affinity matrix upper left panel see feature selection contrast strong pairwise affinity poor pairwise affinity low eigenvector ncut bases partition exhibit sharp separation small large values 
lack separation due large number irrelevant features 
ncut feature selection generates partition significantly disagrees original leukemia subtype labeling samples cluster contains aml cluster ii aml 
small number features determined feature selection filters affinity matrix exhibits contrast affinity strengths different pairs samples 
initial partition derived discriminative features best features determined feature filters iterative clustering somewhat close actual labeling samples cluster aml cluster ii aml 
takes full iterations cliff converge invariant clustering solution 
final partition cluster contains samples cluster ii contains samples aml 
expert knowledge provided regarding genes candidates xing input affinity matrix affinity matrix reordered solution vector partition solution vector input affinity matrix affinity matrix reordered solution vector partition solution vector input affinity matrix affinity matrix reordered solution vector partition solution vector fig 

cluster analysis 
clustering approximate ncut feature selection 
upper left input affinity matrix computed full feature set upper right affinity matrix columns rows reordered eigenvector reveal dichotomy structure lower panel elements eigenvector ascending order 
initial partition obtained cliff 
input affinity matrix computed discriminative features 
final partition obtained cliff 
input affinity matrix computed final feature subset consisting qualified features determined iterative passes mixture feature selection experts 
table 
performance comparison clustering algorithms iterative feature selection feature selection feature selection algorithm means cut cut iterative feature selection just cliff relevant leukemia subtype categorization degree agreement partition generated cliff actual leukemia sub categorization remarkable 
noted significant partitions unrelated leukemia may left undetected algorithm 
detailed analyses showed markov blanket filter imposes influence stability correctness clustering information gain filter especially number features small 
comparison clustering result different approaches table see cliff outperforms means feature selection ncut feature selection 
number features selected compute affinity matrix iteration chosen empirically clustering result sensitive different choices number 
choices cliff identified tight separated dichotomies associated small set relevant genes alternated dichotomies 
possible dichotomies may genetically meaningful revealed documented pathological pairwise correlations pairwise correlations pairwise correlations pairwise correlations cluster pairwise correlations alls aml pairwise correlations pairwise correlations cluster pairwise correlations cluster ii cluster ii pairwise correlations pairwise correlations cluster pairwise correlations cluster ii cluster ii pairwise correlations cluster ii cluster ii fig 

change sample affinity distributions cliff iterations 
histograms pairwise sample correlations full feature space 
correlations space spanned discriminative features 
correlations space features determined full iteration cliff 
correlations final feature space determined cliff 
sample records 
recall click algorithm shamir assumes distributions inter intracluster pairwise sample correlations separated gaussian 
shown assumptions hold original features 
observed cliff iterations proceeded significant features retained clear trend gradual separation distributions 
process distributions appear reasonably separable exactly gaussian 
observation suggests data sets assumptions underlying click may valid relevant irredundant features features considered sample affinity measure 
worth mentioning top genes top genes final feature ranking informative genes golub scored relevance measure 
procedure filters redundant genes surprising genes list included 
affinity matrix derived top genes shows strong contrast affinity strengths eigenvector ncut derives final partition sharp distinction values parts final partition 
cliff clustering iterative feature filtering cluster analysis high dimensional gene expression data presence large numbers irrelevant redundant features limited number samples prevent accurate grouping samples 
results show sample labels training information possible feature selection cluster analysis coupling processes way process uses output process approximate input 
dimensionality reduction achieved algorithm enhance performance clustering algorithm eigenvalue ncut algorithm 
cliff fully generalizable arbitrary multi way clustering recursive way cuts simultaneous eigenvectors 
click algorithm cliff needs prior assumptions structure number clusters long proper threshold measure partition quality 
furthermore complex adoption merging steps click dealing singletons unbalanced cuts avoided normalized cut technique assumption mixture gaussians distributions similarity measures may true high dimensional feature space alleviated due iterative feature selection techniques 
summary results suggest cliff algorithm iterative mixture feature filters followed reclustering capable capturing partition characterizes samples masked original high dimensional feature space 
hidden biologically meaningful partitions sample set identified way selected features significant interest represent set causal cliff factors elicit partitions 
information establish causal models connecting quantitative microscopic features gene expression patterns qualitative empirical macroscopic phenotypes disease symptoms pathologies serve basis grouping genes functional clusters 
alter brown botstein 
singular value decomposition genome wide expression data processing modeling 
proc natl acad sci usa 
dempster laird revow 
maximum likelihood data em algorithm 
journal royal statistical society 
eisen spellman brown 
cluster analysis display genome wide expression patterns 
proc natl acad sci usa 
golub tamayo mesirov loh downing bloomfield lander 
molecular classification cancer class discovery class prediction gene expression monitoring 
science 
hastie tibshirani eisen brown ross weinstein alizadeh staudt botstein 
gene shaving new class clustering methods expression arrays 
tech 
report stanford university 
kohavi john 
wrapper feature subset selection 
artificial intelligence 
koller sahami 
optimal feature selection 
proceedings thirteenth international conference machine learning 
langley 

selection relevant features machine learning 
proceedings aaai fall symposium relevance 
aaai press 
shamir 
click clustering algorithm gene expression analysis 
proceedings th international conference intelligent systems molecular biology ismb 
aaai press 
shi malik 
normalized cuts image segmentation 
ieee transactions pattern analysis machine intelligence 
somogyi 
genetic network analysis millennium opening version 
pacific symposium biocomputing tutorial 
vapnik chervonenkis 
uniform convergence relative frequencies events probabilities 
theory probability applications 
wen fuhrman carr smith barker somogyi 
large scale temporal gene expression mapping cns development 
proc natl acad sci usa 
xing jordan karp 
feature selection high dimensional genomic microarray data 
eighteenth international conference machine learning press 

