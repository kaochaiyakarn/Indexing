transductive inference text classification support vector machines thorsten joachims universitat dortmund ls viii dortmund germany joachims ls cs uni dortmund de introduces transductive support vector machines text classification 
regular support vector machines svms try induce general decision function learning task transductive support vector machines take account particular test set try minimize misclassifications just particular examples 
presents analysis suited text classification 
theoretical findings supported experiments test collections 
experiments show substantial improvements inductive methods especially small training sets cutting number labeled training examples twentieth tasks 
proposes algorithm training efficiently handling examples 
years text classification key techniques organizing online information 
organize document databases filter spam people email learn users preferences 
hand coding text classifiers impractical best costly settings preferable learn classifiers examples 
crucial learner able generalize little training data 
news filtering service example requiring days worth training data please patient users 
tackles problem learning small training samples transductive vapnik inductive approach 
inductive setting learner tries induce decision function low error rate distribution examples particular learning task 
setting unnecessarily complex 
situations care particular decision function classify set examples test set errors possible 
goal transductive inference 
examples transductive text classification tasks 
common little training data large test set 
relevance feedback standard technique free text information retrieval 
user marks documents returned initial query relevant irrelevant 
compose training set text classification task remaining document database test set 
user interested classification test set documents relevant irrelevant query 
netnews filtering day large number netnews articles posted 
training examples user labeled previous days wants today interesting articles 
reorganizing document collection advance offices companies start document databases classification schemes 
introducing new categories need text classifiers training examples classify rest database automatically 
introduces transductive support vector machines text classification 
sub improve excellent performance svms text classification joachims dumais 
especially small training sets reduce required amount labeled training data twentieth tasks 
facilitate large scale transductive learning needed text classification proposes new algorithm efficiently training examples 
text classification goal text classification automatic assignment documents fixed number semantic categories 
document multiple exactly category 
machine learning objective learn classifiers examples assign categories automatically 
supervised learning problem 
facilitate effective efficient learning category treated separate binary classification problem 
problem answers question document assigned particular category 
documents typically strings characters transformed representation suitable learning algorithm classification task 
information retrieval research suggests word stems representation units tasks ordering ignored losing information 
word stem derived occurrence form word removing case information porter 
example computes computing computer mapped stem comput 
terms word word stem synonymously 
leads attribute value representation text 
distinct word corresponds feature tf number times word occurs document value 
shows example feature vector particular document 
refining basic representation shown scaling dimensions feature vector inverse document frequency idf salton buckley leads improved performance 
idf calculated document frequency df number documents word occurs 
idf log df total number documents 
intuitively graphics baseball specs hockey car clinton unix space quicktime computer 
xxx sciences edu newsgroups comp graphics subject need specs apple qt need get specs quicktime 
technical articles nice 
verbose interpretation specs unix ms dos system 
quicktime stuff need specs usable magazines books representing text feature vector 
inverse document frequency word low occurs documents highest word occurs 
different document lengths document feature vector normalized unit length 
transductive support vector machines setting transductive inference introduced vapnik see example vapnik 
learning task yj learner hypothesis space functions gamma 
gamma sample train training examples yn training example consists document vector binary label gamma 
contrast inductive setting learner sample test test examples distribution 
transductive learner aims selects function hl train test train test expected number erroneous predictions theta dp delta delta delta dp test examples minimized 
theta zero 
vapnik vapnik gives bounds relative uniform deviation training error train theta test error test theta true probability gamma test train omega gamma confidence interval omega gamma depends number training examples number test examples vc dimension see vapnik details 
problem transductive inference may profoundly different usual inductive setting studied machine learning 
learn decision rule training data apply test data 
solve problem estimating binary values need solve complex problem estimating function possibly continuous space 
may best solution size training sample small 
information get studying test sample 
training test sample split hypothesis space finite number equivalence classes functions belong equivalence class classify training test sample way 
reduces learning problem finding function possibly infinite set finding finitely equivalence classes importantly equivalence classes build structure increasing vc dimension structural risk minimization vapnik 
ae ae delta delta delta ae inductive setting study location test examples defining structure 
prior knowledge nature build appropriate structure learn quickly 
means text classification analyzed section 
particular build structure margin separating hyperplanes training test data 
vapnik shows size margin control maximum number equivalence classes vc dimension 
maximum margin hyperplanes 
positive negative examples marked gamma test examples dots 
dashed line solution inductive svm 
solid line shows transductive classification 
theorem vapnik consider hyperplanes delta bg hypothesis space attribute vectors training sample test sample contained ball diameter exp min ae equivalence classes contain separating hyperplane fi fi fi fi jj delta fi fi fi fi ae fi fi fi fi jj delta fi fi fi fi ae margin larger equal ae 
dimensionality space integer part note vc dimension necessarily depend number features lower dimensionality space 
structure margin separating hyperplanes 
structural risk minimization tells get smallest bound test error select equivalence class structure element minimizes 
linearly separable problems leads optimization problem vapnik 
op transductive svm lin 
sep case minimize jj subject delta delta solving problem means finding labelling test data hyperplane hyperplane separates training test data maximum margin 
illustrates 
able handle non separable data introduce slack variables similarly way inductive svms 
op transductive svm non sep case minimize jj subject delta gamma delta gamma parameters set user 
allow trading margin size misclassifying training examples excluding test examples 
optimization problem solved efficiently subject section 
especially suited text classification 
text classification task characterized special set properties 
independent text classification information filtering relevance feedback assigning semantic categories news articles 
high dimensional input space learning text classifiers deal features stemmed word feature 
document vectors sparse document corresponding document vector contains entries zero 
irrelevant features experiments joachims suggest words relevant 
aggressive feature selection handled care easily lead loss important information mean aggressive feature selection beneficial certain learning algorithms certain tasks see yang pedersen mladeni 
salt basil atom physics nuclear example text classification problem occurrence pattern 
rows correspond documents columns words 
table entry denotes occurrence word document 
arguments joachims show svms especially suited setting outperforming conventional methods substantially robust 
dumais dumais come similar 
inherit properties svms arguments apply 
better 
field information retrieval known words natural language occur strong occurrence patterns see van rijsbergen 
words occur document 
examples asking search engine altavista documents containing words pepper salt returns web pages 
asking documents words pepper physics get hits physics popular word web salt 
approaches information retrieval try exploit cluster structure text see van rijsbergen 
occurrence information exploit prior knowledge learning task 
look example 
imagine document training example class document training example class classify documents test set 
understand meaning words classify class class share informative words 
reason choose classification test data stems prior knowledge properties text common text classification tasks 
want classify documents topic source style 
type classification tasks find stronger cooccurrence patterns categories algorithm tsvm input training examples yn test examples parameters parameters op num number test examples assigned class output predicted labels test examples solve svm qp xn yn classify test examples 
num test examples highest value assigned class remaining test examples assigned class gamma gamma 
gamma gamma small number gamma num gamma loop solve svm qp xn yn gamma loop gammay take positive negative test gammay example switch labels retrain solve svm qp yn gamma gamma min gamma min return algorithm training transductive support vector machines 
different categories 
example analyzed occurrence information test data clusters 
clusters indicate different topics fd vs fd choose cluster separator classification 
note got classification studying location test examples possible inductive learner 
tsvm outputs classification suggested dichotomies achieved linear separators 
assigning class class maximum margin solution solution optimization problem op 
see maximum margin bias reflects prior knowledge text classification 
analyzing test set exploit prior knowledge learning 
solving optimization problem training transductive svm means solving partly combinatorial optimization problem op 
small number test examples problem solved optimally simply trying possible assignments classes 
approach intractable test sets examples 
previous approaches branchand bound search push limit extent lag needs text classification problem 
algorithm proposed designed handle large test sets common text classification test examples 
finds approximate solution optimization problem op form local search 
key idea algorithm begins labeling test data classification inductive svm 
improves solution switching labels test examples objective function decreases 
algorithm takes training data test examples input outputs predicted classification test examples 
parameters user specify number test examples assigned class 
allows trading recall vs preci sion see section 
description algorithm covers linear case 
generalization non linear hypothesis spaces kernels straightforward 
algorithm summarized 
starts training inductive svm training data classifying test data accordingly 
uniformly increases influence test examples incrementing cost factors gamma user defined value loop 
algorithm uses unbalanced costs gamma better accomodate user defined ratio num criterion condition loop identifies examples changing class labels leads decrease current objective function examples switched 
function solve svm qp refers quadratic programs type 
op inductive svm primal minimize jj gamma gamma subject delta gamma delta gamma optimization problem solved dual formulation svm light joachims especially designed text classification svm light efficiently handle problems support vectors converges fast minimal memory requirements 
look algorithmic property algorithm evaluating performance empirically section 
theorem algorithm converges finite number steps 
proof prove necessary show loop exited finite number iterations 
holds objective function optimization problem op decreases iteration loop argument shows 
condition loop requires examples switched different class labels 
write jj gamma gamma available www ai cs uni dortmund de svm light jj gamma jj gamma gamma gamma jj gamma easy verify constraints op fulfilled new values potentially setting negative zero 
inequality holds due selection criterion loop max gamma max gamma means loop exited finite number iterations finite number permutations test examples 
loop terminates finite number iterations gamma bounded experiments test collections empirical evaluation done test collection 
reuters dataset collected reuters newswire 
modapte split leading corpus training documents test documents 
potential topic categories frequent keeping documents 
stemming word removal 
second dataset webkb collection www pages available cmu group 
setup nigam classes course faculty project student 
documents classes deleted 
removing documents just contain relocation command browser leaves examples 
pages cornell university training pages testing 
nigam stemming word removal 
third test collection taken ohsumed corpus compiled william hersh 
documents abstracts training second available www research att com lewis reuters html available www cs cmu edu afs cs project theo www data available ftp edu pub ohsumed bayes svm tsvm earn acq money fx grain crude trade interest ship wheat corn average breakeven point frequent reuters categories training test examples 
naive bayes uses feature selection empirical mutual information local dictionaries size 
feature selection done svm tsvm 
testing 
task assign documents multiple categories frequent mesh diseases categories 
document belongs category indexed indexing term category 
stemming word removal 
performance measures reuters dataset ohsumed collection documents multiple categories precision recall breakeven point measure performance 
breakeven point common measure evaluating text classifiers 
know statistics recall precision widely information retrieval 
precision probability document predicted class truly belongs class 
recall probability document belonging class classified class see raghavan 
estimated contingency table 
high recall high precision exists tradeoff 
breakeven point defined value precision recall equal 
transductive svm uses breakeven point number false positives equals number false negatives 
inductive svm naive bayes classifier breakeven point computed varying threshold confidence value 
examples training set transductive svm svm naive bayes average breakeven point reuters dataset different training set sizes test set size 
examples test set transductive svm svm naive bayes average breakeven point reuters dataset training documents varying test set size tsvm 
results experiments show effect transductive svm inductive methods 
provide baseline comparison results inductive svm multinomial naive bayes classifier described joachims mccallum nigam added 
applicable results averaged number random training test samples 
gives results reuters dataset 
training sets documents test sets documents transductive svm leads improved performance categories raising av bayes svm tsvm course faculty project student average average breakeven points webkb categories training test examples 
naive bayes uses global dictionary highest mutual information words 
feature selection done svm 
due large number words tsvm words occur times sample 
bayes svm tsvm pathology cardiovascular nervous system average average breakeven points ohsumed categories training test examples 
naive bayes uses local dictionaries words selected mutual information 
feature selection done svm 
tsvm uses words occur times sample 
erage breakeven points inductive svm 
averages correspond left points 
graph shows effect varying size training set 
advantage transductive approach largest small training sets 
increasing training set size performance svm approaches tsvm 
influence test set size performance tsvm displayed 
bigger test set larger performance gap svm tsvm 
adding test examples increase performance graph flat 
results webkb dataset similar 
average breakeven points increases transductive approach 
category project tsvm performs substantially worse gain category course large 
look detail 
figures show examples training set transductive svm svm naive bayes average breakeven point webkb category course different training set sizes 
examples training set transductive svm svm naive bayes average breakeven point webkb category project different training set sizes 
formance changes increasing training set size course project 
course tsvm nearly reaches peak performance immediately needs training examples surpass inductive svm project 
happen 
project populous class 
training examples project category 
importantly look project pages reveals give description project topic 
conjecture margin topic dimension large tsvm tries separate test data topic 
project pages different topics training set generalization project topic ruled 
course pages cornell hand give topic information title link assignments lecture notes tsvm distracted large margins topics 
results ohsumed collection complete empirical evidence supporting point 
related previously nigam nigam proposed approach unlabeled data text classification 
multinomial naive bayes classifier incorporate unlabeled data 
problem naive bayes independence assumption clearly violated text 
em showed substantial improvements performance regular naive bayes classifier 
blum mitchell training blum mitchell uses unlabeled data particular setting 
exploit fact problems example described multiple representations 
www pages example represented text page anchor texts hyperlinks pointing page 
blum mitchell develop boosting scheme exploits conditional independence representations 
early empirical results transduction vapnik 
bennett bennett showed small improvements standard uci datasets 
ease computation conducted experiments linear programming approach minimizes norm prohibits kernels 
connecting concepts algorithmic randomness gammerman approach estimating confidence prediction transductive setting 
outlook introduced transductive support vector machines text classification 
exploiting particular statistical properties text identified margin separating hyperplanes natural way encode prior knowledge learning text classifiers 
transductive inductive approach test set additional source information margins 
introducing new algorithm training handle examples empirical results test collections 
data sets transductive approach showed improvements currently best performing method substantially small training samples large test sets 
lot open questions regarding transductive inference svms 
particularly interesting pac style model transductive inference identify concept classes benefit transductive learning 
sample complexity behave training test set 
relationship concept instance distribution regarding text classification particular better basic representation text aligning margin learning bias better 
questions learning theory research algorithms training needed 
algorithm approximate global solution 
results get better invest time search 
transductive classification implicitly defines decision rule 
possible decision rule inductive fashion perform new test examples 
morik comments tom mitchell discussion 
ken lang providing code 
supported dfg collaborative research center statistics complexity reduction multivariate data sfb 
bennett bennett 

combining support vector mathematical programming methods classification 
scholkopf burges smola editors advances kernel methods support vector learning 
mit press 
blum mitchell blum mitchell 

combining labeled unlabeled data training 
annual conference computational learning theory colt 
dumais dumais platt heckerman sahami 

inductive learning algorithms representations text categorization 
proceedings acm cikm 
gammerman gammerman vapnik 

learning transduction 
conference uncertainty artificial intelligence pages 
joachims joachims 

probabilistic analysis rocchio algorithm tfidf text categorization 
proceedings international conference machine learning icml 
joachims joachims 

text categorization support vector machines learning relevant features 
european conference machine learning ecml 
joachims joachims 

making largescale svm learning practical 
scholkopf burges smola editors advances kernel methods support vector learning 

mccallum nigam mccallum nigam 

comparison event models naive bayes text classification 
aaai icml workshop learning text classification 
aaai press 
mladeni mladeni 

feature subset selection text learning 
european conference machine learning ecml springer lnai 
nigam nigam mccallum thrun mitchell 

learning classify text labeled unlabeled documents 
proceedings aaai 
porter porter 

algorithm suffix stripping 
program automated library information systems 
raghavan raghavan jung 

critical investigation recall precision measures retrieval system performance 
acm transactions information systems 
salton buckley salton buckley 

term weighting approaches automatic text retrieval 
information processing management 
van rijsbergen van rijsbergen 

theoretical basis occurrence data information retrieval 
journal documentation 
vapnik vapnik 

statistical learning theory 
wiley 
vapnik vapnik 

structural risk minimization risk problem pattern recognition 
automation remote control 


theorie der 
akademie verlag berlin 
yang pedersen yang pedersen 

comparative study feature selection text categorization 
international conference machine learning icml 
