revue des rev sci techn 
bucharest neural network knowledge extraction cristea alexandra cristea okamoto usage anns safety critical domains include economic financial applications hindered black box type approach difficult verify debug software includes ann components 
significant advantages gained combining symbolic knowledge domain theory dt empirical sub symbolic knowledge stored ann trained examples 
rule extraction adds needed explanation comprehension component ability ann generalise learned set examples 
compiling rules ann provides better initial conditions training network significantly improve speed learning 
mixed approach allows building hybrid systems combine ann ai techniques increasing robustness flexibility 
gives overview bases ann knowledge extraction form logical functions 
ann design relations established 

decade considerable effort dedicated write read symbolic information artificial neural networks anns 
motivation 
primarily anns shown ability represent empirical knowledge contained set examples information expressed sub symbolic form structure weights biases trained ann directly readable human user 
ann behaves black box providing explanation justify decisions taken various instances 
forbids usage anns safety critical domains include economic financial applications difficult verify debug software includes ann components 
hand extraction knowledge contained ann allows portability information systems symbolic ai sub symbolic ann forms 
reasons consider hybrid learning hl systems able exploit simultaneously theoretical empirical data efficient explanation learning ebl systems cristea alexandra cristea okamoto theoretical knowledge form empirical learning el systems handling knowledge empirical form working separately 
ebl el approaches complementary aspects mutually offset weaknesses alleviate inherent problems 
hoped hl systems assist induction scientific theories helping discover salient features input data importance looked 
hybrid systems economic financial applications including stock exchange prediction benefit operative symbolic rule ai modules sub symbolic empirically trained ann modules 
basic steps mixed learning approach compile encode available theoretical knowledge domain theory dt adequate ann empirical data sets examples train network introduce additional knowledge ann extract refined theory symbolic form classical ai system reinserted ann cycle repeated stopping criteria satisfied 

rule implementation 
canonical binary functions simplest way implement discrete rules form binary logical functions 
variable binary logical function attaches truth values false true output dependent consequent variable combination truth values input independent antecedent variables usually numeric coding binary logical values true false representation bipolar representation respectively 
distinct vectors discrete input space distinct vectors output space result distinct mappings logical functions 
fast growing combinatorics main cause complexity problem logical function implementation 
binary logical function seen characteristic function truth set function 
approach readily allows generalisation fuzzy logic connection fuzzy set theory 
binary function constant respect variables granularity binary logical function granularity constant respect variables tautology contradiction family variable binary logical functions comprises identity negation tautology contradiction identity negation represented neural network knowledge extraction instances function exponent conventionally identity negation 
variable canonical binary functions entries truth table different entries 
functions constant respect variables granularity consequently distinct conjunctive product canonical binary functions having single entry truth table entries 


similarly disjunctive sum canonical binary functions single entry truth table entries equal 

role boolean algebra conjunction disjunction denoted product sum respectively 
product sum canonical binary function related de morgan theorems functions completely specified values set symbolic exponents indices canonical binary function related symbolic exponents relation corresponds writing base 

normal forms logical functions binary logical function expressed disjunctive normal form conjunctive normal form characteristic coefficients uniquely define function 
expansion logical function normal forms provides highest analysis truth table 
known veitch diagrams locations correspond canonical product sum functions components entries characteristic coefficients minimise expansions functions combining terms differ values variable 
cristea alexandra cristea okamoto 
ann implementation normal forms fig 
presents standard structure order neuron fig 
gives simplified flow graph representation neuron 
ij ij fig 

standard structure order neuron simplified flow graph representation internal activation potential neuron affine function inputs ij bias threshold ij weights links input neuron external activation neuron output activation function neuron usually sigmoid type fig 
bipolar fig depending chosen representation truth values 
slope parameter sigmoid denoted true false increases true false bipolar increases fig 
sigmoid activation function bipolar sigmoid activation function exp tanh correspondence output truth value neuron output activation internal activation potential corresponding intervals table 
neural network knowledge extraction table bipolar true false case ln ln 
typically chooses results usual numeric choice 
bipolar case ln 
establish state unit implementing rule binary logical function denote set satisfied unsatisfied positive antecedents inputs set satisfied unsatisfied negative antecedents rule 
activation levels inputs shown table 
cardinals sets denoted corresponding small letters total positive negative antecedents respectively number satisfied antecedents total antecedents 
table input ij truth value bipolar jp true jp false jn false jn true denote ij sum absolute values weights inputs set implementation activation unit arbitrary logical input vector falls bonds min max 
implementing rules logical functions absolute value assigned weights links related inputs ax 
output unit certainly true respectively false border conditions min max 
cristea alexandra cristea okamoto ideal case results min max 
relations bipolar implementation min max ax give ideal case min max 
various symmetrical logical functions implemented choosing different values bias see section 
bipolar implementation conjunctive disjunctive canonical functions shown figures respectively 
fig 

conjunctive canonical binary function 
ann implementation bipolar ann implementation 
fig 

disjunctive canonical binary function ann implementation bipolar neural network knowledge extraction implementation disjunctive canonical binary function allow negation inputs inside unit 
necessary negation inputs performed additional step preceding function 
presents ideal true false separation planes input variables case biases generate canonical symmetrical functions bipolar implementation 
fig 

input space equal weights ann implementation logical functions bipolar ann implementation disjunctive normal form logical function shown case variable logical functions 
la ye id la la ju iv fo lo ic fu io ju iv ic fu io 
fig 

ann implementation disjunctive normal form logical function cristea alexandra cristea okamoto network comprises input layer variables hidden layer nodes corresponding conjunctive canonical functions components disposed veitch structure output layer containing logical function implemented 
training network consists case selection conjunctive canonical functions contribute function output unit links layers kept frozen 
training unnecessary nodes hidden layer pruned neighbouring nodes similar outputs combined procedure inspired logical function minimisation 
structure adequate extracting rules subset algorithm intrinsic complexity problem 

shannon symmetrical logical functions symmetrical functions property permutation variables change output value function 
shannon numbers theorem states symmetrical function fully defined set numbers exactly variables true false function true false 
instance numbers 
take values form total number variables distinct symmetrical functions exactly symmetrical canonical functions number 
symmetrical function contains term canonical symmetrical function contain terms canonical symmetrical function 
symmetrical function decomposed disjunction conjunction canonical symmetrical functions written conjunctive disjunctive form similarly normal forms expansions characteristic coefficients uniquely define symmetrical logical function 
ann approach functions type implemented directly 
functions easily expressed terms exactly canonical symmetrical functions 

inverse relation immediate 
canonical symmetrical functions properties neural network knowledge extraction 
ann implementation symmetrical logical functions real world problems symmetries respect groups variables 
group symmetrical functions implemented simpler arbitrary logical functions 
ofn functions implemented weights functions figures changing bias case bipolar case see 
exactly functions conjunctions functions 
presents structure ann implementing symmetrical logical function variables 
training network links output node modified examples define considered logical function 
input layer shannon hidden layer output layer symmetrical functions exactly canonical symmetrical functions fig 

symmetrical functions ann implementation decomposition canonical symmetrical functions resulting network architecture adequate algorithm 

rule extraction algorithms classification rule extraction algorithms main approaches knowledge extraction decompositional structural analysis methods assign unit ann prepositional variable establish logical links variables pedagogical cristea alexandra cristea okamoto input output mapping methods treat network black box analysing internal structure 
algorithms literature kbann kt connectionist scientist game rule neg rule decompositional approach illustrates pedagogical methods 
focused decompositional approach puts correspondence ann sub symbolic structure theory symbolic structure 
subset algorithm method operates relation giving ideal true false border neural unit ann border border relation realistic assumption levels input output signals neurons properly trained network storing rules logic functions correspond true false activation potential determined values weights 
finding subsets incoming links sum weights plus bias internal activation potential high bring output variable true level possible formulate rules form condition proposition condition expresses border relation terms logic meaning proposition statement attached examined neuron 
main difficulty subset algorithm consists combinatorial complexity results exceedingly large number subsets rules redundant 
way alleviate problem setting upper limit number considered subsets 
steps subset algorithm notations defined 
output hidden neuron find subsets positive weight links incoming neuron 
find subsets negative weight links incoming neuron 
state de rule statement attached unit 

remove duplicate rules maximum allowed rules unit algorithm currently rule eliciting systems restriction imposed adversely affect accuracy generates complicated sets rules 
possible improvements keep frozen network correspond initial dt extract additional rules resulted el neural network knowledge extraction construct hierarchical rules logical function minimisation techniques start predefined veitch hidden layers training select connected units 

algorithm real world problems common sets variables playing equivalent roles analysed system inherent symmetry rules extracted 
symmetry greatly reduce complexity extraction procedure 
imposing start symmetry restrictions predicted dt simpler anns learning algorithms result 
method idealised relation bias chosen border border border border border steps algorithm 
output hidden neuron form groups clusters similarly weighted links replace individual weights links group average value group 
step necessary symmetry imposed start training modified bp 

eliminate insignificant groups change true false output state 

keeping weights constant bp biases 
neuron considered construct single rule form border antecedents satisfied statement attached unit 
possible combine rules reduce number 
clustering links efficiently done specialised algorithms existing dt gives necessary hints simpler impose symmetry bp algorithm 

presents overview frequently knowledge extraction algorithms provides design relations give better insight necessary conditions ann properly learn logical functions 
methods extended continuously varying variables locally responsive units approach 
certain problems types knowledge cristea alexandra cristea okamoto computing internal activation potential additive aggregated input replaced multiplicative aggregated input resulting second order units better represent knowledge 
implementing fuzzy logical functions advantageous min max operator evaluate neuron internal activation potential classical affine operator 
efficient training ann able change weights links structure network 
incremental approach balanced step pruning keep complexity low possible problem 
maintaining constant parts network training changing allows refinement dt finding additional rules 
preferred possible changing rules leads loosing meanings initial concepts dt 
implementation symbolic ai rules ann form rule extraction ann sub symbolic form sides potentially powerful tool jointly uses theoretical empirical knowledge solving real world problems 
approach significant advantages stock exchange prediction problem 
received october university electro communications tokyo graduate school information systems university bucharest electrical engineering department 
andrews diederich survey critique techniques extracting rules trained ann neurocomputing research centre queensland university technology brisbane www qut edu au 
andrews geva rule extraction constrained error back propagation mlp proc 
th australian conference neural networks brisbane queensland pp 


multivariate time series modeling financial markets artificial neural networks artificial neural nets genetic algorithms springer verlag wien pages 

craven mw shavlik jw sampling queries extract rules trained neural networks machine learning proc 

eleventh int 
conf san francisco ca 

cristea alexandra okamoto neural networks time series prediction stock exchange forecasting proceedings national conference ipsj chiba japan information processing society japan vol pages 

cristea alexandra okamoto neural networks time series prediction application field stock exchange proceedings national conference ieice artificial intelligence osaka japan institute electronics information communication engineers symposium session vol 
pages sd 
neural network knowledge extraction 
cristea alexandra okamoto stock exchange analysis time series forecasting neural nets proceedings national ieice meeting artificial intelligence chiba japan institute electronics information communication engineers pages 

cristea alexandra okamoto stock exchange forecasting help neural networks proceedings national conference ipsj artificial intelligence osaka japan information processing society japan vol pages 
cristea neural networks tools knowledge eliciting proceedings th seminar nn applications electrical engineering sept belgrade pp 

de dans un systeme intelligence artificielle computer science dea research report imag grenoble france 
herault curvilinear component analysis self organizing neural network nonlinear mapping data sets ieee trans 
neural networks vol 
january 
fu knowledge connectionism revising domain theories ieee transactions systems man cybernetics vol 
pp 

fu rule generation neural networks ieee transactions systems man cybernetics vol 
pp 

gallant connectionist expert systems communications acm vol 
pp february 
gas unsupervised learning temporal sequences neural networks artificial neural nets genetic algorithms springer verlag wien pages 

hayashi neural expert system automated extraction fuzzy rules advances neural information processing systems denver vol 
pp morgan kaufmann 

healy acquiring rule sets product learning logical neural architecture ieee trans 
neural networks vol 
may 

backpropagation neural network study prediction management systems sixth italian workshop ed 
word scientific pages 

chang ko neural network technology stock market index prediction international symposium speech image processing neural networks april hong kong ieee pages 

krogh hertz simple weight decay improve generalization niels bohr institute copenhagen denmark computer information sciences univ california santa cruz ca 

levin leen moody fast pruning principal components advances neural information processing cowan tesauro alspector eds morgan kaufmann san mateo ca 

watanabe kawamura 
systems fuzzy inference structured neural network 
proceedings international conference fuzzy logic neural networks pp 

mcmillan mozer smolensky connectionist scientist game rule extraction refinement neural network proceedings thirteenth annual conference cognitive science society chicago il 
meunier nadal sparsely coded neural networks handbook brain theory neural networks ed 
arbib mit press pages 
cristea alexandra cristea okamoto 
miyano girosi forecasting global temperature variations neural networks massachusetts institute technology ai lab 
center biological computational learning dept brain cognitive sciences ftp publications ai mit edu 

moody prediction risk architecture selection neural networks nato asi series springer verlag 

murata amari network information criterion determining number hidden units artificial nn model dept mathematical eng 
info 
physics fac 
engineering univ tokyo ftp source 

nelson bower simulating neurons networks parallel computers methods neuronal modeling synapses networks ed 
koch segev mit chapt 


giles extraction rules discrete time recurrent neural networks neural networks vol 
pp 

pratt kamm direct transfer learned information neural networks proceedings national conference artificial intelligence anaheim ca pp 

dillon multi layered neural networks learning symbolic knowledge proceedings australian artificial intelligence conference perth australia 
shavlik mooney towell symbolic neural net learning algorithms empirical comparison 
machine learning pp 

shawn davenport continuous time temporal back propagation adaptable time delays ieee transaction neural networks april canada 

prediction pointers pitfalls common errors center cognitive computational neuroscience stirling univ stirling fk la ftp source 

neural network pruning pruning parameters st www nagoya ac jp wsc st online workshop soft computing nagoya japan 

towell shavlik extraction refined rules knowledge neural networks machine learning vol 
pp 


tresp ahmad network structuring rule knowledge neural information processing systems pp 

rumelhart huberman generalization application forecasting advances neural information processing systems denver vol 
pp 

