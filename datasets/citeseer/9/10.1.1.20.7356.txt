university california san diego optimizing ranking functions connectionist approach adaptive information retrieval dissertation submitted partial satisfaction requirements degree doctor philosophy computer science engineering brian theodore bartell professor garrison cottrell chair professor norman anderson professor richard belew professor jeffrey elman professor paul kube copyright brian theodore bartell rights reserved 
ii dissertation examines adaptive methods automatically improve performance ranked text retrieval systems 
goal ranked retrieval system manage large collection text documents order documents user estimated relevance documents user information need query 
ordering enables user quickly find documents interest 
ranked retrieval difficult problem ambiguity natural language large size collections varying needs users varying collection characteristics 
propose empirically validate general adaptive methods improve ability large class retrieval systems rank documents effectively 
main adaptive method numerically optimize free parameters retrieval system minimizing non metric criterion function 
criterion measures system ranking documents relative target ordering defined set training queries include users desired document orderings 
system learns parameter settings better enable rank relevant documents irrelevant 
non metric approach interesting general adaptive method alternative supervised methods training neural networks domains rank order prioritization important 
second adaptive method examined applicable restricted class retrieval systems permits analytic solution 
adaptive methods applied number problems text retrieval validate utility practical efficiency 
applications include dimensionality reduction vector document representations vector space inter document similarity accurately predicts semantic association estimation similarity measure better predicts relevance documents queries estimation highperformance neural network combination multiple retrieval systems single system 
applications demonstrate approaches improve performance adapt varying retrieval environments 
compare methods numerous alternative adaptive methods text retrieval literature positive results 
iii chapter live age vast amount digital information available computers 
tools exist relatively trivial create electronic stream memos articles charts drawings manuscripts archive huge databases 
hundreds megabytes textual information created weekly simply postings electronic newsgroups usenet 
years gigabytes major news sources new york times available online potential access 
called information explosion information produced rate individual able find information interests 
principal concern dissertation dealing information explosion 
goal develop computer methods allow individual find text information interest large collection large searched manually individual 
addressing library problem patron interested finding information useful interesting faced vast sea documents 
find want searching irrelevant material 
note text retrieval significantly different database retrieval documents unstructured permit standard database solutions 
trivial problem computer solve number studies amply illustrated difficulties retrieving documents content 
example blair maron report users mainstream text retrieval system ibm stairs storage information retrieval system able find useful documents collection 
users expectations higher believed useful documents retrieved 
dissertation advances adaptive methods improve automatic retrieval text documents semantic content 
adaptive methods meant complement current approaches field information retrieval ir providing way automatically optimize retrieval systems improve performance 
thesis adaptive methods efficient effective information retrieval leading systems perform better acceptable cost adaptation 
approach model traditional retrieval methods general ranking functions 
chose formalization current retrieval methods implement ranking functions attempting rank documents order estimated preference user 
bookstein states basically people want information retrieval system provide lists documents ordered importance documents 
preferred documents ranked higher preferred ones user need search entire collection find documents interest 
method developed able optimize large class ranking functions optimizing functionality rank documents preferred order 
approach example sessions users retrieval system order provide target rankings system 
demonstrate results indicating adaptive methods able optimize performance number traditional retrieval models leading improved performance 
approach quite general 
addition applicable large class models information retrieval believe approach novel potentially useful adaptive approach fields particularly neural networks 
approach general adaptive algorithm learning rank order objects domain 
approach potentially useful task ranking prioritization important ranking function learnable example 
applications dissertation problems text retrieval approach domain specific assumptions 
chapter particular demonstrates domainindependent application separate ranking functions combined neural network provide ranking function performs 
ranked text retrieval dissertation examines adaptive methods automatically improve performance ranked text retrieval systems 
ranked retrieval system computer system managing collection documents containing text possibly media 
collections typically quite large order megabytes gigabytes text tens thousands documents 
retrieval system number things display documents user manage database documents provide browsing capabilities 
important functions retrieval system focus dissertation provide documents users satisfy information needs 
system estimating degree relevance document user statement need query 
relevance estimates typically real numbers rank documents user relevant relevant order ease user task finding truly useful documents 
theory documents collection ordered retrieval system user easily look ordered list find documents interest 
essential characteristic ranked text retrieval system purposes dissertation estimates relevance documents user queries ranks documents estimates 
definition meant general classic models information retrieval instances ranked text retrieval systems 

example vector space model probabilistic retrieval spreading activation models ranked retrieval systems sense defined 
systems provides estimates document relevance rank documents user 
estimates real values typically positive zero indicating document estimated relevant retrieved non zero scores indicating increasing levels estimated relevance document 
documents ordered user estimates 
boolean model retrieval documents match logic query ranked retrieval systems documents ranked classes absolutely relevant absolutely 
ll review models greater detail chapter 
principal problem addressing develop retrieval systems better predict user ordering documents 
problem difficult brings task number issues variety fields 
foremost solution deal inconsistencies ambiguity natural language 
documents comprised free text unstructured natural language unedited 
ambiguity difficult computer automatically determine conceptual content documents 
example furnas demonstrate difficult identify useful documents simply basis words author document 
widespread synonymy word meaning thing polysemy word having meaning natural language 
second problem complicating text retrieval collections documents quite large containing gigabytes text 
efficient document processing methods practical 
final problem retrieval task ill defined dynamic 
case number reasons 
different document collections widely varying characteristics 
example technical jargon vary collections 
addition needs users may vary 
example users factual queries requiring answers limited scope pose topical queries requiring documents having wide scope 
different user needs require different specialized retrieval techniques employed 
systems designed deal complexities suffer limitations particularly difficulty designing maintaining large systems 
short text retrieval systems deal problems ambiguity scale varying user needs 
thesis adaptive methods alleviate difficulties 
methods automatically train retrieval system better rank documents example set queries documents 
examples represent retrieval system environment infer parameters retrieval system provide improved performance user queries 
overview approach section 
learning document ranking function central problem addressed dissertation learn ranking function example rankings context text retrieval system 
general approach define learning algorithms optimize implicitly explicitly ability ranking function correctly rank examples 
optimized ranking function tested novel examples determine level generalization methodology define criterion function optimize performance system 
criterion function measures system performing respect particular task set training queries 
training queries documents preferred user identified 
criterion function compares desired ordered list relevant documents relevance estimates system 
system learning sample past sessions users 
system optimized best ranks relevant documents irrelevant ones training queries expectation optimized system generalization performance called predictive retrieval ir literature performance training set called retrospective generalize novel queries 
examined number different criteria motivated fields multidimensional scaling mds neural networks statistics rank order 
criteria receive primary attention 
principal emphasis dissertation non metric rank order criterion 
non metric criterion explicitly optimizes retrieval system ability rank preferred documents preferred ones 
second criterion metric criterion 
metric criterion optimizes semantic associations documents represented retrieval system implicitly optimizes ranking ability system 
numerous assumptions retrieval environment system find overly restrictive practical 
approach useful analytic model result significantly improved retrieval performance 
non metric criterion advantages metric criterion restrictive assumptions truly practical general method adaptation ranked text retrieval systems 
metric criterion topic chapter 
non metric criterion topic majority dissertation chapters 
metric criterion optimizing associations metric criterion measures retrieval system estimating association pairs documents 
documents associated semantically related retrieval system output large number system output small number 
metric criterion measures system achieves desired behavior 
criterion metric requires correct associations documents known known associations exact numerical values 
criterion measures different system estimates known values 
minimizing criterion system adapted result better estimates document association 
context ranked text retrieval problem metric criterion example implicit adaptive approach 
explicitly optimize ability retrieval system rank documents 
optimizes characteristics retrieval system ability measure association documents empirically correlated improvement ranking task 
implicit methods common adaptive ir literature review chapter 
primary value metric criterion useful analytic model latent semantic indexing vector space model relatively effective approaches information retrieval 
chapter certain assumptions retrieval model find metric criterion analytic solution leads better empirical performance methods 
addition model provides foundation analysis latent semantic indexing 
metric approach number limitations 
foremost requires exact target semantic association values known 
practice quite difficult acquire principled way 
second requires complete association information known pairs documents training set 
difficult satisfy 
addition requires expensive numerical computations making approach impractical large collections 
application metric method dissertation particular conception document may relevant user 
assumes boolean model document relevant user relevant user ambiguity gradation decision 
chapter see notion relevance common information retrieval 
consider restrictive unrealistic 
non metric criterion optimizing rank order non metric criterion addresses limitations metric criterion 
precise numerical target scores information needed non metric criterion correct ordering document relevance scores number queries training set 
system needs identification documents relevant 
system depend exact numerical value relevance metric criterion 
non metric criterion measures retrieval system ranking documents respect target ranking 
primary benefit non metric approach explicitly optimizes ability ranked retrieval system correctly rank documents 
criterion effective improving ranked retrieval performance 
non metric method reasonably efficient apply 
example applied method learning high performance linear combination different retrieval systems 
collection documents training queries system optimized approximately minutes low sparc ipc 
notion relevance emphasized non metric criterion quite different boolean notion application metric criterion 
non metric model relevance documents stipulates ordering documents 
identifies documents relevant 
contrast boolean notion stipulates sets relevant documents irrelevant documents 
documents fall sets 
note boolean relevance modeled non metric relevance simply relevant documents preferred irrelevant documents reverse certainly true 
dissertation seen empirical exploration differences notions relevance differences metric rank order criteria 
alternative criteria number criteria examined dissertation 
emphasis alternatives compare primary 
criteria include mean squared error mse learning criterion common neural network literature non metric rank criterion neural network perceptron rule number non metric criteria statistics multidimensional scaling literature 
demonstrate dissertation primary non metric criterion better suited learning ranking function context information retrieval alternatives 
issues motivation dissertation addresses number scientific technical issues 
section identify primary issues provide motivation study 
issues motivations efficient effective adaptive ir methods central issue dissertation performance ranked text retrieval systems improved adaptive methods 
interesting worthwhile problem reasons outlined current retrieval systems perform satisfactory level effective new approaches required adaptive problem difficult scale complexity task text collections large require optimized methods natural language component text collections filled difficult ambiguity complexities 
particularly interested general adaptive methods improve performance large class retrieval systems 
contrast adaptive methods text retrieval reviewed chapter specialized applicable certain aspects specific retrieval systems 
efficiency central concern method information retrieval large number documents involved task large number features document words documents 
efficient methods practical gigabyte sized text collections 
learning rank order ir goal dissertation automatically optimize ranking performance retrieval system 
learning ranking function interesting general problem 
issue adaptive method general infer function orders objects example orderings objects 
learning task received attention neural network pattern classification literatures central numerous pattern classification methods 
example nearest neighbor classification novel sample classified comparing known examples classes 
class similar example assigned novel sample 
examples ranked similarity particular similarity value matter rank example 
problem domains benefit adaptive rank order approach example domains process scheduling prioritization essential 
particularly interested general adaptive algorithms training neural network rank objects domain 
knowledge problem previously addressed literature related techniques field multidimensional scaling reviewed chapter 
neural network algorithm useful problem domain requires objects ranked prioritized adaptive manner 
contrast typical supervised learning algorithms exact output neural network interest 
ranking network output value matter determines rank position sample 
adaptive methods facilitating ir system design current text retrieval systems complex due complexities inherent retrieval domain 
complexities fold retrieval systems large number parameters specified system designer 
methods exist setting parameters weights index terms large number parameters adapting specific retrieval environments problematic 
second retrieval systems designed modular way individual modules specialize limited set complexities domain 
example module deal identification indexing phrases text 
help design maintenance system difficult problem deciding combine different modules single system 
seek address issues dissertation system instantiation issue concern current retrieval systems large number parameters systems specified system designer 
values parameters affect system ranks documents relationship parameters system performance obscure making task optimizing parameter values highly non trivial 
examples parameters current systems term weights retrieval thresholds weights combining evidence different components system 
goal adaptive approach general handle large class retrieval systems system parameters 
combining sub systems complexity information retrieval task promoted modularization solutions potential solution specialized deal just small part total complexity 
example specialized approaches dealing phrases documents handling synonyms dealing restricted kinds queries proposed 
specialization useful system designer standpoint specialized approach independently developed maintained 
modularization poses problem different subsystems combined result best retrieval system 
examine dissertation adaptive system appropriate mechanism solving problem 
adaptation preference orders adaptive information retrieval systems proposed relevance judgements source training information 
little making ordering interpretation relevance information strict boolean interpretation wong yao notable exception reviewed chapter 
relevance information retrieval considered boolean characteristic documents document relevant relevant user need 
alternative conception documents relevant user varying degrees 
dissertation examines graded notion relevance employed general adaptive approach 
particular model relevance preference order documents documents relevant document query documents unordered incomparable 
believe generality approach strength allows application number problems information retrieval domains outside text retrieval rank order important 
framework multidimensional ir models multidimensional models quite popular information retrieval 
generally documents queries represented vectors multidimensional space relevance ascribed measuring similarity documents queries 
approach typified salton vector space model vsm review chapter 
multidimensional scaling mds field study concerned related issues similarity multidimensional space accurately represent semantic relatedness 
issue mds provides useful framework vector information retrieval models 
alternative criteria number adaptive approaches information retrieval sufficiently general comparable approach 
methods differ primarily preference information interpreted formalized learning method 
possible compare method alternatives 
obviously important order verify utility methods respect currently available alternatives 
main contributions research dissertation number contributions fields information retrieval neural networks 
main contributions analytically solvable metric criterion analyze squared error criterion employed match model generated associations documents metric target associations 
analysis wellknown results linear algebra 
retrieval model restricted completely linear common information retrieval propose analytic method determining optimal model 
application document representation adaptive method representing documents vector space documents similar semantically related 
method metric criterion uses boolean relevance judgements provided set training queries 
empirically retrieval precision method better smart retrieval system standard benchmark information retrieval performance 
performs better latent semantic indexing conceptually similar retrieval method test collections 
successful application advances claim document relevance information useful indicator semantic association documents 
analysis latent semantic indexing latent semantic indexing lsi retrieval method analyzed framework provided metric criterion 
method relevance judgements available target document associations method equivalent vector similarities documents 
case lsi important special case metric method 
analysis important justifies alternative document representations lsi method result improved target document associations 
general non metric adaptive method propose adaptive method optimizing ability retrieval system rank documents user preferences 
method general adjust large class parameters retrieval function differentiable variety different retrieval models 
method optimize neural network models text retrieval ranking function potentially neural network models tasks prioritization important 
learning similarity measure non metric method learn similarity measure vector space model information retrieval 
method demonstrated searching optimization space similar optimization space standard ir evaluation measure average precision 
similarity measure learned method performs popular similarity measures ir cosine inner product outperforms pseudo cosine number terms similarity 
addition environment retrieval system changes highest ranked documents interest method able automatically adjust new environment 
case method learns similarity measure performs better best compared alternative 
high performance combination experts non metric approach find combination different retrieval systems experts single retrieval system 
application method identifies linear combination experts results improvement best individual 
improvement achieved expert performs poorly isolation 
despite method able find combination uses lesser retrieval expert improve better expert 
second application method identifies linear combination experts performs better best individual retrieval expert 
addition result achieved small amount training data approximately queries highest ranked documents query 
training top documents automatic affect removing troublesome queries ones relevant documents ranked poorly retrieval system cause performance decrements training training set 
non linear network perform better linear network training set 
non linear network performs better linear network trained documents troublesome queries 
novel analysis mixture experts addition learning high performance combination approach analyze performance experts 
analysis experts combination isolation 
novel method analysis important experts perform poorly positively contribute performance 
comparison alternatives compare number alternative adaptive methods non metric approach 
alternatives information retrieval squared error learning algorithm minimizes difference system relevance estimates boolean relevance target values perceptron learning method optimizes retrieval system ranking performance ordered relevance judgements 
demonstrate empirically non metric method far better suited optimizing system ability rank documents correctly 
addition number alternative non metric criteria compared propose 
criterion selected highly correlated typical measure performance information retrieval average precision criterion reasonably efficient naturally models relevance judgements non metric boolean 
dissertation overview issues contributions outlined previous sections elaborated dissertation chapters chapter background concepts related research support proposals dissertation 
primary emphasis major fields information retrieval ir multidimensional scaling mds neural networks 
ir outline ranked retrieval task major approaches 
particular emphasis placed vector retrieval methods definitions relevance adaptive methods 
mds examine number methods representing semantic association concepts discuss relevance ir problem 
discuss relevant neural network pattern classification literature emphasis methods representing semantic association related ir models methods related multidimensional scaling 
chapter metric similarity model method proposed creating vector space representations documents modeling target inter document similarity values 
target similarity values assumed capture semantic relationships associations documents 
vector representations chosen inner product similarities document vector pairs closely match target inter document similarities 
method closely related latent semantic indexing approach fact equivalent target similarities derived directly original document similarities 
method allows external sources inter document semantic constraints indexing greater computational expense 
method applied standard text databases information retrieval literature 
cisi database information science abstracts performance measured precision averaged range recall levels improves compared weighted term vector approach improves compared latent semantic indexing 
similar improvement obtained cranfield database improvement obtained artificial med database medical abstracts 
generally favorable performance suggests interesting potential methods explicitly modify retrieval system meet inter document semantic constraints 
chapter general non metric approach propose novel method automatically adjusting parameters ranked output text retrieval systems improve retrieval performance 
ranked output text retrieval system implements ranking function orders documents placing documents estimated relevant user query relevant ones 
system adjusts parameters maximize match system document ordering target ordering 
target ordering typically user feedback set sample queries generally document preference relation 
chapter validation non metric approach demonstrate utility non metric approach estimate similarity measure scoring relevance documents queries vector space model information retrieval 
approach automatically finds similarity measure performs equivalent better classic similarity measures studied 
performs estimated optimal measure exhaustive sampling similarity measures 
method compared alternative methods perceptron learning rule motivated wong yao query formulation method squared learning rule motivated fuhr buckley probabilistic learning approach 
alternatives useful characteristics demonstrate empirically estimate parameters optimal similarity measure 
chapter mixture experts retrieval performance improved significantly number different retrieval algorithms combining results contrast just single retrieval algorithm 
different retrieval algorithms retrieval experts emphasize different document query features determining relevance 
retrieve different sets documents sum worth parts 
unclear different experts combined yield superior estimate 
non metric learning algorithm learn combination experts results superior retrieval performance 
apply method expert combination tasks 
application demonstrate method provides novel way analyze performance experts allowing determine effectiveness experts conjunction experts isolation 
second application involves combination experts commercial retrieval system 
case method yields combined algorithm performs better average precision single expert ranking algorithm 
chapter outline directions research motivated results achieved dissertation 
direction applying non metric approach dimensionality reduction problem examined chapter 
preliminary results illustrating non metric method scale problems having parameters 
second direction applying non metric approach nearest neighbor pattern classification 
suggest metric find nearest neighbor estimated training examples approach chapter 
third direction examining mixture experts model chapter 
particular suggest making expert adaptive static suggest models closely mirroring methods neural network literature 
chapter chapter background approach employed dissertation result convergence influences number different fields 
goal chapter review pertinent literature different fields provide background definitions necessary remainder 
field information retrieval ir motivates problem addressed dissertation 
problem retrieval system optimized best rank relevant documents irrelevant documents 
background outline common approaches ir 
particular emphasis placed multidimensional vector approaches particularly salton vector space model vsm derivatives estimate document query relevance similarity vector representations documents queries 
outline number popular definitions interpretations relevance justify particular preference interpretation current 
numerous adaptive ir methods emphasis strengths limitations motivating current research 
second provide overview techniques multidimensional scaling mds neural networks pattern classification relevant goal adaptively improving ranking performance retrieval system 
particular intuitive links mds multidimensional vector methods ir identified setting stage analysis chapter motivating research proposed chapter 
differences metric non metric mds approaches examined context ir task 
discuss neural network approaches multidimensional scaling neural net models vector similarity represent predict semantic association class membership 
note chapter give details background material dissertation 
aims discuss core research supports 
background material specific particular model proposed chapter chapter 
examples include details latent semantic indexing approach chapter adaptive approaches wong yao fuhr buckley chapter mixture experts approaches chapter 
information retrieval information retrieval field devoted management large collections information retrieval useful information collection users 
information kinds example text speech graphics video historically primary emphasis information retrieval storage retrieval natural language text documents 
dissertation similar emphasis natural language text applications chapters large collections text 
maron kuhn defined goal text retrieval system order documents collection probability relevance user 
van rijsbergen text retrieval research seen trying satisfy goal 
section oriented discussing possible interpretations maron kuhn statement describing number approaches intend satisfy goal 
review number common approaches information retrieval discuss formalizations goal information retrieval examine definitions document relevance discuss adaptive methods related methods propose chapters 
ranked text retrieval approaches information retrieval instances ranked text retrieval systems 
stated dissertation ranked retrieval system ranks documents collection response user query order documents estimated preference user 
large number approaches ranked information retrieval review common approaches section 
variety reasons large number options building text retrieval system 
foremost system date performed suppresses need alternatives 
failure adequately solve retrieval problem researchers try better 
second important reason information retrieval task exceedingly difficult 
problems natural language ambiguity human interface issues size identified 
exceptionally difficult design system handles different complexities problem 
systems tend general tend specific particular subproblems task 
proliferation approaches corresponding proliferation subproblems solved 
interesting proliferation spawns subproblem number different approaches potential domain expertise combined provide improved retrieval system 
questions interest address dissertation chapter 
vector space information retrieval salton vector space model vsm classic studied methods information retrieval 
approach quite intuitively appealing documents queries represented vectors multidimensional space vector similarity document query vector gives model estimate relevance document query 
pairs documents compared semantic overlap comparing corresponding vectors 
model appealing implements intuitive notion improving retrieval performance relevance feedback relevance feedback information retrieval new experiments relevance feedback retrieval performance relevance feedback information experiment improve document document document vector vector vector vector space model vector space model vsm applied collection titles salton buckley ide rocchio 
similar vectors define semantically related text items 
course efficacy model relies ability retrieval system represent documents queries space vector similarity estimate semantic association point estimate usefulness user 
fact great deal research vsm oriented exactly issue 
simplest form vsm represents text vector counting occurrences terms text 
dimension vector space unique term collection representation text simply vector counts term text 
vector space called term space 
number unique terms collection 
document query vector term space 
th element vector equivalent number occurrences term text 
example application vsm small sample text collection illustrated 
called weight term text 
need simply count term text complicated expression involving collection wide term statistics document statistics 
known variations include inverse document frequency idf weighting takes account frequently term occurs document collection noise scheme incorporates distribution terms documents 
variations possible studied 
note considerable freedom definition term 
typically term alphanumeric string delimited whitespace far advanced term identification methods proposed terms morphological roots words stemming sequences words phrase analyzer chunks words substrings words text 
documents represented vectors space vector similarity measure chosen estimate associational similarity vectors 
quite number different similarity measures examined typical candidates cosine inner product 
retrieval performed representing query vector space ranking documents similarity query illustrated 
document document query term retrieval vector space model document ranked retrieval system higher document similar query vector space 
vsm model number qualities making useful model retrieval 
mentioned intuitively appealing consequently easily understood 
second efficient 
vectors defined similarity measure standard vector measure cosine query non zero similarity documents matching terms 
perform retrieval system need check similarity query documents query just documents containing query term 
result substantial efficiencies conceived indexing methods inverted indexes 
general vsm method defined supports different architectures potential improved performance 
vsm starting point quite alternative models 
variant model examine chapter 
model represents documents vectors vector space smaller dimension term space 
documents represented reduced space explicit goal making semantically related documents similar unrelated documents 
advanced vector space models limitations vsm outlined document similar query shares terms 
document similar document vector orthogonal query vector similarity common similarity measures zero 
problem language highly ambiguous number synonymous terms describing single idea numerous polysemous terms describe multiple ideas depending context 
ambiguity causes documents similar vector space semantically related certain documents orthogonal semantically related 
approach dealing problem alter way documents queries represented vector space document query vectors similar semantically related 
large number proposals lines outline number section 
approach chapter example approach dealing ambiguity problem 
proposed approach involves just slight modification traditional vector space model 
documents represented term space defined vsm vectors altered examples queries relevant documents 
vectors relevant documents similar respective queries adding query terms documents 
method adaptive approach ir defer discussion section adaptive methods chapter 
approach gallant represent term collection multidimensional feature vector 
dimensions vector correspond semantic features human light engineered system designer 
concepts represented feature space term star sky positive weight light feature negative weight human feature concept star movies opposite feature weights 
term star article feature representation sum different concepts 
documents represented sum representations constituent terms 
approach limitations feature space set concepts hand engineered 
time cost 
may acceptable results technique demonstrated 
schutze framework deriving multidimensional representations texts terms 
schutze approach essentially neural network approach review context neural network research 
method outline latent semantic indexing lsi approach 
approach attempts represent documents vector similarity better indicator semantic relatedness typical vsm 
lsi mapping document vectors term space smaller dimension space 
reduced space intended represent major semantic features entire term space 
idea method terms occur documents result similar representations reduced space 
occurrence indication semantic relatedness idea subject debate reduction cause semantically related items represented similarly share terms 
documents sharing terms may mapped similar representations pairs terms documents strongly occur documents 
provide depth analysis approach chapter find lsi optimal special case metric algorithm 
results lsi applied real document collections mixed generally encouraging 
med collection medical abstracts lsi performed better raw term matching better salton smart vector space information retrieval system 
second cisi collection science abstracts average performance lsi raw term matching equivalently disappointing precision lsi performed worse smart average precision 
authors attribute poor results lsi methods general relative homogeneity document collection discrimination difficult 
chapter report better performance application lsi collections 
improved performance result alternative weighting strategies deriving original term space document representations 
lsi appears promising method 
probabilistic model probabilistic retrieval alternative vsm extensively researched 
goal probabilistic retrieval system estimate probability user find document relevant query 
estimate denoted rel rel event document relevant query features document 
rel probability document relevant features document 
probability estimates rank documents user order decreasing probability relevance 
possible associate costs retrieving irrelevant document failing retrieve relevant document 
costs affect ordering documents 
affect cut point list documents retrieved estimated cost high 
major limitation probabilistic model difficult come practical definition rel making major restrictive assumptions 
common assumption assume terms statistically independent 
terribly unrealistic assumption interestingly allow probabilistic model reduce vector space model proper choice term weights 
assumptions allow derivation inverse document frequency weighting scheme principles 
model provides useful mathematical foundation variety weighting mechanisms information retrieval 
restrictive unrealistic assumptions model requires calculation large number joint probabilities terms 
model generated alternatives standard vsm significantly successful 
spreading activation alternative vector space probabilistic models spreading activation sa approach 
sa approach localist connectionist model document term association clear roots models memory neural networks cognitive science 
basic sa model graph nodes weighted interconnections 
nodes represent features text 
example belew adaptive information retrieval air model node represents single document term document author document 
links nodes represent association items 
node level activity 
activity propagated weighted links node node 
typically links exist documents terms contain associational links may exist 
process query query terms activated followed propagation activity nodes period time 
propagation documents may ranked user order decreasing node activity 
appeal sa model derived number features 
model intuitively attractive especially close relationship models memory 
second model able quite naturally retrieve documents terms common query semantically related 
occurs query terms activate set related terms documents turn activate semantically related document interest 
addition activation paths query terms activated documents potentially rich source information 
analysis paths may allow system identify important influences document activation 
spreading activation approach natural framework incorporating adaptive methods 
examine chapter 
directly pursuing probabilistic model spreading activation experiments reported dissertation 
methods propose chapter meant applicable wide variety ranked retrieval systems 
probabilistic sa models important examples systems 
relevance 
retrieval models described previous section thing common interested estimating relevant user find documents collection 
accurately predicted documents high estimated relevance listed documents relevant user need look long list find documents interest 
turns concept relevance information retrieval clear unambiguous great deal discussion literature trying pin elusive concept 
main areas contention 
defining document relevant 
example document relevant topic stated user query 
alternatively document relevant satisfies user query 
second area contention relevance boolean feature 
document relevant relevant user possible middle ground degrees relevance 
address issue briefly section 
interesting philosophically may bearing interpretation results 
primary emphasis remainder section second issue 
second issue crucial main approaches dissertation metric approach non metric approach motivated necessarily dependent particular choice relevance boolean boolean 
chapter examine application metric approach assumes boolean model relevance 
chapters examine approach alternative model relevance quasi ordered model 
definitions relevance certainly shortage suggested definitions relevance 
example cooper defined logical relevance document relevant query part minimal set documents called premiss set sentences logically prove query sentence 
alternative wilson situational relevance 
situational relevance similar logical relevance user oriented premiss set modulated user concerns user individual view world 
saracevic gives historical evolution concept relevance ir identifying number different philosophical perspectives relevance defined follows fact universally appropriate definition relevance 
apparently stable distinction concerning relevance distinction objective subjective relevance 
objective relevance covers notion topicality aboutness see definition maron states factor contributes usefulness document 
document objectively relevant query refer common topic 
subjective relevance user oriented 
document subjectively relevant query useful user issued query 
subjective relevance query simply intermediary user system 
documents relevant irrelevant query relevant irrelevant user issued query 
robertson provides excellent discussion differences objective subjective kinds relevance 
objective relevance relation single query single document 
document topic query determined essentially isolation 
subjective relevance complex relation query document documents user seen document 
documents user seen useful document may directly topic query 
robertson suggests basic idea retrieval system ranks documents order probability relevance query known probability ranking principle may adequate context complex subjective relevance 
probability relevance relies list documents user far 
subjective relevance document computed respect sets previously seen documents leading unwieldy retrieval approach 
contention proper definition relevance troubling especially considering generally unanimous goal retrieval systems correct estimation relevance documents 
simply ir systems trying retrieve relevant documents retrieve irrelevant documents 
relief difficulty empirical demonstration different notions relevance different 
little difference objective subjective kinds relevance terms documents relevant situation 
experiment designed test differences objective subjective notions judges asked rate documents contexts subjective objective 
objective contexts documents relevant somewhat relevant relevant 
subjective context documents useful somewhat useful useful 
judges instructed objective context rate documents query topic subjective context rate documents useful judge doing research topic 
statistically significant differences types relevance 
dissertation take pragmatic approach definition relevance 
prefer relevance user oriented subjective interpretation believe job ir system satisfy user 
simply possible acquire sufficient examples subjective relevance assessments experiments dissertation 
adaptive methods require non trivial number queries relevance judgements available training system 
experiments training queries needed subsequently test trained system 
pragmatically decided relevance judgements available subjective objective defer empirical findings argue results potent regardless type relevance employed 
boolean relevance relevance considered boolean notion 
document relevant relevant user 
degrees relevance typically considered 
relevance binary orthogonal controversy mentioned previous section considering objective relevance document topic query considering subjective relevance document useful useful user 
job retrieval system best job deciding class relevant relevant document 
case ranked retrieval systems documents ordered probability relevant document relevant 
boolean notion relevance number strengths 
foremost supports set oriented notion retrieval task retrieval system classify documents classes relevant irrelevant 
typical set oriented retrieval boolean retrieval systems document retrieved iff satisfies boolean logic query 
addition boolean notion necessary assumption classic probabilistic retrieval model estimating probability user find document relevant 
classic probabilistic model allow shades relevance 
limitations notion relevance clear 
real documents relevant user need different reasons differing degrees 
boolean notion mandates arbitrary decision point apparent relevance continuum side marginally relevant documents irrelevant side somewhat relevant documents relevant 
shades subtleties relevance lost 
multi valued relevance researchers objected boolean notion relevance argued general conception 
particular bookstein offered multi valued relevance 
generalized probabilistic retrieval model finite set possible relevance scores relevance classes 
example classes relevant relevant indifferent irrelevant irrelevant set relevance classes 
set totally ordered meaning possible relevance class relevant relevant class relevance document described retrieval system set probabilities probability document belongs relevance classes 
associated classes cost 
class cost cost user incurs system retrieves document user claim belongs particular class 
typically higher cost associated relevant classes lesser cost associated relevant classes 
retrieval rule retrieve document smallest expected cost expected cost retrieval system estimate probability document relevance class query considering history previously retrieved documents 
retrieval systems ignore history bookstein formalism allows 
cost user retrieving document class bookstein approach certainly allows greater range possible relevance assessments user 
choosing relevant irrelevant user chooses finite set 
second strength approach reduces classic boolean model case relevance classes formalism limited number reasons 
possible relevance classes stipulated advance set finite 
arbitrary distinctions documents assuming arbitrary collection size 
addition relevance document comparable document 
relevance classes totally ordered document probabilistically relevant document 
overly restrictive documents may relevant different aspects user need may orderable respect 
difficulty multi valued formulation cost critical aspect model 
boolean formulation retrieval costs affect ranking documents multi valued framework costs essential 
complicates model great deal actual costs user difficult estimate 
practical limitation goal ranking documents relevance fall naturally multi valued model 
document probability belonging relevance class clear information presentable user 
quasi ordered relevance alternative bookstein approach document preference relation proposed wong yao 
document preference relation dpr suffer disadvantages boolean relevance bookstein multi valued relevance 
dpr method relies quasi order documents 
quasi order serves mathematical formalization intuitive notion pair documents user prefer opinion 
preferences serve valid quasi order long don contradict contradiction example document preferred document simultaneously document preferred document user 
mathematically relation quasi order anti reflexive transitive english states document preferred document preferred document preferred document document preferred document dpr formalism suffer limitations boolean multivalued formalisms 
documents need orderable respect documents 
preferences simply contradictory 
consequently documents relevant user need different reasons user need artificially decide document relevant 
case user indifferent choosing documents preferred 
second great freedom number distinctions documents 
example document differentiated completely ordering documents case indifferent choosing documents 
alternatively documents fall classes preferred preferred documents 
case user indifferent choosing large number documents preferred set referred set 
case mimics boolean relevance discussed earlier 
recap research dissertation boolean notions relevance wong yao preference relation 
boolean relation chapter basis metric adaptive method 
boolean notion naturally interpretable metric constraints retrieval system relevance score document depending user decision 
see metric approach examined chapter number limitations stemming boolean interpretation metric criterion 
chapter alternative non metric approach preference relation formalism 
remainder dissertation seen validation approach comparisons boolean interpretation alternatives 
believe preference order natural formulation ranked retrieval systems objective system rank documents violate user document preferences 
note richer interpretation relevance allowing arbitrary quasi orders limited 
documents relevant varying degrees different reasons true relevance formalized quasi order 
example possible user quite perfectly prefer document document document document document document preferences valid user define valid quasi order 
deficiency dpr account documents user observes document interest 
argued earlier document useful useful user depending redundant documents listed prior 
claiming preference order said relevance document user need necessarily sure valid 
claim preference relation expressive formulation standard boolean notion natural complement ranked retrieval systems fairly rich way represent document preferences users 
addition incorporated adaptive method chapters see leads positive improvements retrieval performance 
performance evaluation important issue dissertation performance retrieval system evaluated 
important reasons 
main goals develop adaptive methods text retrieval allow systems improve performance 
obviously need way measure performance order know succeeded 
second adaptive methods optimizing measures system performing 
evaluation measure exactly may simply able optimize standard performance measure solve task 
section examine number approaches performance evaluation 
chapter examine measures candidate criteria optimization 
common methods evaluating retrieval performance notions recall precision 
measures boolean model relevance document relevant query 
specific set oriented retrieval documents retrieved retrieved query ranked 
set oriented assumption relaxed see moment 
boolean assumption relaxed certainly difficulty consider alternative multi valued quasi ordered notions relevance 
recall percentage relevant documents retrieved system 
measures exhaustive search documents user want retrieved 
defined recall rel ret rel rel ret sets relevant documents retrieved documents respectively 
best recall worst recall 
note measure recall know set relevant documents generally user expert depending subjective objective notion relevance decide documents entire collection relevant 
precision percentage retrieved documents relevant 
measures accurate search documents user relevant 
defined precision rel ret ret recall best precision worst 
objective information retrieval simultaneously maximize precision recall empirically precision tends decrease recall increases 
words retrieval system casts wider wider net tends lose accuracy 
measures specific set oriented retrieval evaluate ranked retrieval systems 
achieved sliding threshold rank ordered list documents 
documents threshold considered retrieved recall precision measured set documents 
sliding threshold set recall precision scores generated 
recall precision reported graph precision vs recall illustrated 
downward slope characteristic inverse relationship recall precision 
recall precision measured query separately evaluation separate query scores averaged 
average precision composite evaluation measure derived recall precision 
simply precision averaged range recall levels usually evenly spaced levels 
average precision useful summary statistic retrieval system obviously hides interesting query specific recall specific differences collection 
precision recall arguably commonly measures retrieval system evaluation 
example government sponsored text retrieval conference trec precision recall primary evaluation measures 
recall precision precision vs recall typical plot precision vs recall 
precision characteristically decreases recall increases 
numerous alternatives 
van rijsbergen provides excellent discussion alternatives fallout percentage irrelevant documents retrieved composite measures proposed swets brookes 
argued incorporation retrieval costs evaluation measures 
van rijsbergen admits decades research area basically precision recall 

final point evaluation regardless particular measure set measures evaluation concerned results statistically significant 
case researchers examined significance reporting comparisons methods 
dissertation test report significance possible typically analysis variance anova 
possible refrain concluding methods significantly different 
adaptive methods principal goal dissertation demonstrate effective adaptive methods information retrieval 
course adaptive methods ir new great deal research ir directed optimizing system performance adaptive methods 
section outline number approaches point strengths weaknesses serve valuable motivation approach 
emphasize characteristics retrieval models generality adaptive method adjust aspects retrieval system performance specialized adjust particular components 
set parameters identified determine system ranking functionality method adjust large class parameters just specialized 
performance orientation goal adaptive method explicitly improve performance system ranking relevant documents irrelevant ones adaptive method trying achieve goals 
size search space adaptive method adjust system performance considering large space search space possible system functionalities method select small range possible system behaviors 
clear features necessarily define bad adaptive methods 
adaptive method classified large number different ways taxonomy quite useful 
goal dissertation stated chapter demonstrate adaptive system general explicitly oriented improving ranking performance handle large infinite search spaces 
see review numerous adaptive methods ir variety 
compared explicitly approach chapters 
feature adaptive ir methods requires separate discussion 
feature time frame adaptation occurs 
possible time frame brief finite long potentially open ended 
form adaptation occurs completely time course user interaction system 
typical time course involves user issuing query system responding set documents estimated relevant user identifying system documents relevant 
system information called relevance feedback described change system behavior retrieve new set documents user 
sequence events may iterate user satisfied walks away key system discard knowledge acquired user dealing new user 
system identical new user sits 
single session system change conform user immediate needs 
second form adaptation occurs user sessions 
users requests identify relevant irrelevant documents experience accumulated time 
form adaptation system tomorrow system today 
hopefully adaptive methods correctly better 
adaptive mechanism relevance information acquired number user sessions modifies system performance goal improved performance new users 
dissertation emphasize second form adaptation entirely 
apparent algorithms time frame may effective time frame believe may case methods describe dissertation 
requirements adaptive forms sufficiently different example efficiency required types system features adapted additional research effort needed evaluate methods shorter time course adaptation 
leave 
relevance feedback adaptive system needs feedback environment 
adaptive mechanism uses feedback order change adapt functionality system 
information retrieval feedback usually relevance information users retrieval system 
issuing query user may identify retrieved documents relevant relevant 
process know relevance feedback 
katter described pp 
relevance judgements affected factors 
factors include ffl type document judged example subject matter style difficulty ffl conditions judgement example amount time available judgement order presentation documents number documents judged ffl query example level specificity ffl judge example level experience domain background attitude 
relevance judgements unstable 
different users consider different documents relevant query user find different documents relevant different times 
suggests relevance judgements may highly suspect volatile source feedback adaptive retrieval methods 
fortunately variability relevance judgements averaged large set relevance judgements adaptive system able chart reasonable course noise 
supported findings jones examine effect evaluating system performance relevance judgments single judge average number judges 
demonstrate variabilities due different judges affect decision system better averaged number queries 
appears variability factored assuming sufficient queries evaluating performance 
vector modification vsm common adaptive approach vector space model adjust document query vectors similar depending user relevance judgements 
single session ide proposed adjust query contains terms relevant documents term weights single highly ranked irrelevant document removed 
number query modification methods reviewed salton ide method appears perform best 
retrieval performance improve greatly technique average precision increasing second retrieval pass user relevance feedback top documents retrieved pass 
ide update method new old gamma irrelevant relevant docs query document vectors irrelevant single highest ranked irrelevant document 
update procedure tendency increase inner product similarity new set relevant documents reducing similarity similar irrelevant document 
rocchio crestani proposed related methods query modification :10.1.1.50.2329
similar adaptive approach modify document representations 
approach documents judged relevant particular query adjusted similar query 
achieved 
adding term query document new document representation 
increasing weight term document occurs query 
decreasing weight term document occur query 
performance new document representations slightly better unmodified representations 
vector modification approaches share number common features 
general adaptive methods modify weights terms documents queries 
feature retrieval system need adaptation kind vector similarity measure methods solve problem 
second explicitly oriented improving ranking performance retrieval system certainly highly result adaptation 
modify similarity document query 
modifying similarity may cause ordering documents change retrieval highly dependent number factors particular amount change term weights different documents queries 
final commonality size search space essentially infinite 
infinite number ways weight document query 
addition dimensionality search space quite large equal number unique terms term space 
adaptation spreading activation models adaptation central part connectionist spreading activation approaches discussed earlier 
recall model documents terms authors represented node connectionist network connected weighted links 
weights links represent association levels nodes affect propagation activation nodes 
weights links main features adapted spreading activation networks 
adaptive approach increase weight nodes contribute retrieval relevant item documents terms authors judged relevant user 
specifically weight item item increased proportion correlation item activation item judged relevant 
associational links increased item activated supports activation linked node judged relevant 
addition weights simply increase bound sum outgoing weights node constant 
increasing association effect decreasing 
terms taxonomy adaptive methods ir spreading activation adaptive algorithm shares features vector space algorithms 
general specific modification weights nodes applicable optimization system characteristics 
second goal explicitly improve ability system rank relevant documents irrelevant documents 
weights items increased decreased inferred association 
effect changing weights rank order documents entirely clear considering highly non trivial activation dynamics retrieval system 
size search space infinite space weights nodes 
exhaustive search widely method improving retrieval performance exhaustive search 
method exhaustive possible choices system parameter values explicitly enumerated exhaustively evaluated 
parameter values result best ranking performance chosen optimal parameters 
excellent comparative studies harman dumais belkin examples methodology 
addition suggest exhaustive search small number system parameters system collection competing retrieval algorithms 
relevance feedback query select retrieval algorithm performed best initial retrieval documents query 
best algorithm formulate new retrieval request theory perform finding additional documents relevant user need 
exhaustive search best parameters typically takes place small discrete set possibilities reasonable enumerate cases 
example harman searches restricted set term weighting algorithms find algorithms rank documents average 
alternatively search space may naturally discrete continuous space systematically sampled 
example belkin search real valued weights combine multiple experts relevance estimates result best possible ordering documents 
small number candidate weights evaluated determine best 
exhaustive method typically constraints kinds system parameters optimized long possibilities enumerated evaluated 
furthermore possibilities examined best parameters assured optimal enumerated set 
exhaustive method characterized differently previous adaptive methods 
general method 
kinds system parameters choice term weighting algorithm choice term stemming algorithm optimized method 
large class parameters easily optimized 
continuous parameters individual weight term 
range values parameters infinite explicitly evaluated 
finite set sample values evaluated optimal value taken set 
second method directly oriented goal improving ranking performance system 
typically criterion evaluation average precision related measures directly measure system retrieval performance 
search space typically small certainly finite 
search space explicitly enumerated large simply impractical exhaustively evaluate possibilities 
course subset possible parameter values gets evaluated order save computational effort exhaustive method able guarantee best solution truly optimal 
addition bias sampling parameter values may corrupt results 
genetic adaptation alternative exhaustive search method optimize explicit measure system ranking performance heuristically 
heuristic search methods allow adaptive method operate infinite search spaces having evaluate possible parameter values 
example heuristic method genetic algorithm ga general optimization method genetic adaptation optimize retrieval system performing 
examples include gordon application adaptively determining best document representations yang query optimization 
genetic algorithm modifies parameters documents queries order optimize measure retrieval system performing 
cases authors variations standard ir measures measure system performance ga example yang experiment different measures performance rel ret gamma ret rel gamma rel ret performance theta rel ret gamma ret rel ga approach appears general applied document query representation 
theoretically approach genetic algorithm optimize variety different system features 
critical issue application particular features interest encoded naturally genetic algorithm framework 
second clear approaches oriented explicitly optimizing ranking performance system 
approaches explicitly optimizing standard performance measures ir expect resulting optimized system perform quite respect measures 
particular measures somewhat non standard clearly measure system ranking documents 
size search space quite large practically infinite 
course characteristic heuristic methods ga approaches guaranteed find optimal system parameters typically examine subset possibilities 
considering space possibilities prohibitively large exhaustive search impractical necessary concession 
wong yao adaptive term associations wong yao proposed adaptive method learning query document term weights best rank relevant documents irrelevant ones 
wong yao gradient descent numerical local hill climbing search algorithm heuristically search infinite space linear term weights 
numerical method explicitly optimizes measure ranking performance system 
measure neural network perceptron learning rule 
application method small text collection database documents queries method learned document term weights resulted average improvement average precision performance 
defer examination wong yao method chapter 
see closely related non metric adaptive approach 
time able sense details method compare carefully alternative 
terms characterization adaptive methods wong yao approach potentially quite general optimize large class parameters just document term associations demonstrated 
look closely generalization chapter 
method clearly oriented improving ranking performance system optimization criterion directly measures documents ranked respect target ranking 
size search space infinite space term associations 
furthermore dimensionality space quite large number terms documents queries 
fuhr buckley probabilistic learning fuhr buckley proposed method learning combine different term indexing strategies result improved performance 
approach weight term occurring document number different standard term weighting methods term frequency tf inverse document frequency idf picking single 
method learns combination weighting methods best estimates probability document relevant query shares terms document 
adaptively weight methods fuhr buckley optimize measure combined term weights estimating probability relevance document query 
method specific boolean relevance judgements probabilistic retrieval models 
environment system performing optimally estimated probability relevance truly relevant documents probability irrelevant documents 
particular measure system performing squared error criterion measuring deviation system relevance estimates depending document true relevance 
squared error criterion common neural networks pattern classification 
wong yao approach described fuhr buckley adaptive method closely related approach advocate chapters 
defer detailed examination fuhr buckley approach 
terms taxonomy adaptive methods fuhr buckley method wong yao 
potentially quite general squared error criterion generalized large class parameters system optimized 
look closely generalization chapter 
addition criterion clearly directly optimizing ability system estimate probabilities relevance 
see chapter estimating probabilities way explicitly improve ability system rank documents order relevance 
method applicable infinite domains space combinations term weighting algorithms effectively infinite 
summary adaptive methods examined wide variety adaptive methods information retrieval 
methods characterized respect features general optimize large class system parameters adaptation explicitly optimizes ranking performance system search space potentially large required small 
stated goal adaptive methods improve system ability rank documents general handle variety parameters applicable non trivial domains 
methods reviewed potentially share characteristics genetic algorithm methods wong yao term association method fuhr buckley probabilistic learning approach 
chapters compare non metric method methods fuhr buckley wong yao 
genetic algorithm complementary approach offers alternative method optimizing non metric criterion 
experiments gradient descent optimization methods global search technique genetic algorithm quite fruitful optimization problems local minima 
leave interesting topic research 
multidimensional scaling multidimensional scaling mds set statistical methods representing similarity association data multidimensional space 
goal represent known similarities objects domain example associations terms similarities points multidimensional space 
object corresponds point space points positioned inter point similarities closely match inter object associations 
section outline number common methods performing multidimensional scaling 
clear distinction common approaches metric approach non metric approach 
difference methods information known inter object similarities 
exact similarity information known example object similarity object metric approach suitable 
metric approach attempts best represent exact similarities inter point similarities multidimensional space 
exact similarity information ordering information may known 
example may know object similar object object object case non metric approach suitable 
dissertation adaptive ir approaches metric algorithm non metric algorithm 
believe mds formalism powerful information retrieval 
provides foundation analysis vector space model assuming documents queries objects mds inter object similarities known associational relationships items 
furthermore suggests adaptive methods optimizing retrieval performance 
identify strengths section elaborate fully chapters come 
metric mds metric mds class algorithms representing metric inter object similarity information multidimensional space 
similarity information metric form exact similarity values 
metric mds attempts find configuration points multidimensional space inter point similarities close possible exact metric similarities 
context information retrieval metric similarities natural interpretation boolean relevance vector space model 
vector representations documents queries points multidimensional space 
metric similarity document query point relevance document query 
document relevant metric similarity 
document irrelevant metric similarity 
apply metric mds problem algorithm give representation documents queries inter point similarities closely equivalent metric similarities 
chapter examine alternative interpretation vsm model metric mds 
alternative interpretation knowledge documents relevant query relevant define metric similarities 
formulation lead analytic solution finding optimal configuration documents support formal analysis latent semantic indexing approach 
solution gradient methods typical method solving metric mds problems define measure particular configuration points represents similarity information 
measure called literature measure misfit configuration error stress 
find configuration points best satisfies similarity constraints gradient descent minimize configuration error 
typically configuration error perfectly minimized similarities multidimensional space perfectly match metric similarities 
impossible perfectly match similarities gradient procedure tries find best match allowing similarity constraints violated 
example configuration error normalized squared error measuring squared difference inter point similarities metric similarities 
oe gamma range set objects multidimensional representation object oe measures similarity points multidimensional space metric similarity objects denominator equation serves normalize configuration error roughly percent misfit 
value constant error just normalized sum squared differences target similarities similarities configuration 
numerous variations configuration error possible emphasize example fractional errors oe gamma largest errors done 
monte carlo studies robust error measures performed 
local minima particularly troublesome non euclidean distance measures minkowski metrics 
analytic solutions certain situations possible find optimal configuration points analytically performing iterative minimization error 
consider idea set metric similarities represented matrix th entry matrix metric similarity objects matrix 
matrix positive semi definite similarity measure inner product similarity measure configuration analytically optimally minimizes sum squares error measure 
particular problem interest chapter take care point examine solution 
critical step finding optimal configuration involves finding singular value decomposition svd svd matrix unique decomposition matrix component matrices multiplied matrix multiplication result original matrix 
svd real matrix product matrices denotes transpose matrix matrices special characteristics 
column orthonormal columns orthogonal inner product different columns unit length 
diagonal matrix singular values diagonal elements zero diagonal elements singular values non negative real values typically ordered decreasing value 
rows columns denoted theta rank allow theta theta theta zero singular values 
svd important properties 
foremost purposes exposition provides best lower rank approximation matrix terms euclidean matrix norm frobenius norm calculated square root sum squared entries matrix 
precisely theta matrix removing gamma columns columns remaining correspond largest singular values similar versions defined 
minimizes sum squared differences corresponding elements matrices rank matrices 
returning metric mds problem svd matrix similarities yields decomposition product real matrices matrices properties defined 
turns positive semi definite positive real values diagonal definition svd define square root diagonal values define matrix identical rows corresponding largest singular values retained 
columns give coordinates optimal configuration points multidimensional space dimensions 
points optimal sense inner product similarities points best possible match target similarities method scaling similarity information derived inner products known metric scaling literature pp 
exploited metric ir method chapter 
methods finding configurations points represent similarity information require explicit iterative minimization equation 
particular method principal ordinate analysis see 
technique principal components similarity matrix principal component analysis point representations derived data 
related technique goldfarb allows real symmetric distance matrix represented points multi dimensional space distances perfectly preserved 
multi dimensional space may euclidean may large dimensionality property positive semi definiteness means factored product theta matrix real valued elements 
decomposition unique certain trivial re arrangements columns sub space rotations 
principal component analysis pca method finding axes maximum variance set vector data 
pca close relative svd calculated svd 
approaching number objects represented 
non metric mds non metric mds class algorithms representing non metric ordinal similarity information multidimensional space 
similarity information ordinal form ordering constraints similarity values 
example ordering constraint similarity objects larger similarity objects critical point know precise values similarities know correct ordering similarities 
non metric mds attempts find configuration points multidimensional space inter point similarities closely possible meet ordinal similarity constraints 
just metric information natural interpretation boolean relevance nonmetric information natural interpretation quasi ordered relevance 
recall quasi ordered relevance stipulates set ordering constraints documents relevant query respect documents 
vector space model vector representations documents queries points multidimensional space 
non metric model relevance ordering defines target ordering similarities points multidimensional space 
example document preferred user document query objective adjust document query vectors similarity query vector vector larger similarity query note care actual similarities properly ranked 
applying non metric mds problem type results representation documents queries inter point similarities closely meet non metric ordering constraints 
chapter non metric adaptive algorithm partly nonmetric mds 
approach generalization non metric mds stipulate documents queries represented points multidimensional space 
quantity interest estimated relevance value document query 
relevance value calculated variety retrieval methods examples saw previous section 
certainly relevance score similarity document query vectors vsm making approach non metric mds limited model 
chapters examining retrieval models motivated standard information retrieval methods neural network techniques 
approach share commonalities non metric mds techniques 
particular configuration error measures non metric mds examined possible optimization criteria adaptive approach 
provide overview number criteria discuss issues mds related approach 
guttman point alienation variety different methods performing non metric mds vary definition configuration error 
configuration error measures set points multidimensional space meeting ordinal constraints inter point similarities 
standard technique regardless particular definition minimize configuration error gradient methods 
guttman proposed statistical measure useful determining random variables rank correlated 
statistical measure modified configuration error non metric mds see moment 
guttman measure rank correlation paired variables defined gamma gamma gamma jj gamma variables index samples variables 
rank ordered identically gamma sign gamma product positive 
consequently numerator denominator identical measure evaluates 
opposite case rank ordered identically opposite gamma opposite sign gamma product negative 
consequently numerator denominator identical magnitude opposite sign measure evaluates gamma 
values gamma indicates varying degrees match rank order rank order 
fairly straightforward change statistical notion rank correlation configuration error optimized solve non metric mds problem 
similarity multidimensional space th pair points index pairs points individual points 
rank index target similarity th pair 
example non metric similarity constraints require similarity th pair points larger similarity th pair points essentially approach mds technique known smallest space analysis 
guttman point alienation fields outside multidimensional scaling 
particular similar criterion perform non metric discriminant analysis 
discriminant analysis general pattern classification approach goal identify function best separates data samples class samples class 
uses point alienation measure samples ranked class ranked samples class 
kruskal algorithm kruskal algorithm non metric mds algorithm clearly define minimize configuration error solve non metric mds problem 
algorithm seen formalization refinement automatic computer method nonmetric mds developed shepard 
just overview algorithm details algorithm quite tedious 
refer reader written guide implementation 
algorithm essentially consists iterative minimization gradient descent configuration error called stress kruskal gamma oe oe oe delta similarity measure points multidimensional space multidimensional representation th object 
kruskal original oe distance measure 
minkowski metrics oe gamma vector multidimensional space 
criterion distances similarities 
furthermore equation target similarities computed ordinal ranking constraints 
describe process determine note essentially squared error criterion measuring deviation true similarities oe delta values set target similarities 
algorithm able adjust configuration points similarities match target similarities stress minimized problem solved 
equation quite squared error criterion normalization term 
normalization serves essential role negates effect configuration scale measure fitness 
points multidimensional space scaled arbitrarily changing criterion value 
desirable measure trying determine configuration similarities ordered independent scale configuration 
turns possible normalization term 
alternative suggested oe gamma oe oe mean similarities 
normalizes variance similarities squares similarities 
critical feature algorithm metric target similarities ordinal similarity data available 
key step minimization procedure new calculated target ordering current configuration points 
sense provide moving target metric optimization procedure step target similarities different 
understand algorithm calculates consider plot 
points plot corresponds similarity current configuration oe 
similarities spread vertical dimension order matches desired ordering similarities 
observe current similarities correctly ordered monotonically non decreasing set line segments drawn points graph 
key kruskal approach provides algorithm efficiently computes best monotonically non decreasing fit data terms horizontal deviations points example best fit illustrated line 
set target similarities fs values monotonic fit 
value value line vertical level point oe plot 
theoretical foundation rests finding best monotonically non decreasing fit data fit interpretation structure imposed ordinal similarity data 
actual algorithm kruskal follow true gradient stress function approximation 
target similarities assumed constant differentiation fact function current configuration 
details efficient algorithm available avoided brevity 
similarity configuration target ordering example kruskal algorithm ordinal similarity data kruskal mds example best monotonically non decreasing fit target similarities denoted line current similarities oe horizontal axis 
point corresponds similarity objects current configuration 
points distributed vertical direction desired similarity ordering 
similarities correctly ordered respect target ordering monotonically non decreasing graph 
rank image alternatives number alternative criteria mds rank statistics literature 
number variations kruskal criterion target similarities derived current configuration 
criteria guttman monotonicity coefficient pearson correlation applied rank order target similarities derived rank image current similarities 
describe approach criteria 
idea rank image procedure target similarity current configuration similarity derived similarity similarities correctly ordered 
accomplished creating ordered lists current similarities 
list current similarities ordered decreasing value 
second list current similarities ordered primarily desired ordering secondarily decreasing value break ties primary ordering 
th element second list serves target similarity similarity list 
example procedure 
note current similarities proper order lists identical target similarity simply current similarity cases 
similarities order rank image method tend generate target similarities effect increasing similarities small decreasing similarities large 
target similarities generated number criteria measure current similarities match targets 
examples literature id current similarity oe desired rank id sorted current similarity id sorted desired rank id current similarity oe target similarity rank image mds example derivation target similarities rank image method 
guttman monotonicity coefficient oe oe oe current similarity corresponding rank image target 
pearson correlation statistic oe gamma oe gamma pp oe gamma oe gamma recap non metric criteria outlined number different criteria solve non metric mds problem 
criteria differ way measure current configuration points matches non metric similarity constraints 
differences criteria quite subtle borg suggested selecting different criteria quite domain dependent particular specific needs task 
defer comparing different criteria chapters able examine domain specific context 
examine criterion efficacy specifically improving ranking performance text retrieval 
number characteristics common criteria described 
quite important consider criteria optimization measures text retrieval 
complete information ordering constraints typically required 
typically need know target ordering pairs similarities order apply non metric mds achieve reasonable results 
study spence monte carlo methods demonstrate large chunks target orderings omitted configuration recovered accurately 
green pp 
demonstrate important omission patterns row conditional structure describe section strong degrading effect recoverability configuration 
useful structure available 
context text retrieval important method applicable useful similarity constraints omitted 
similarity constraints derived user relevance judgements difficult get complete judgements documents 
small subset 
demonstrate experimentally chapters non metric method works limited information 
miyano examined situation similarity constraints available sequentially time goal approach adapt ongoing stream fixed set similarity constraints 
demonstrate artificial data metric mds task configuration recovered subset similarity constraints sampled time 
miyano approach important text retrieval user relevance judgements generated time 
dissertation assume relevance judgements available time static 
miyano results suggest interesting consider sequential adaptation case 
row conditional mds discussion non metric criteria assumed similarities objects multidimensional space ordered 
spence green illustrated similarities need comparable non metric methods applied 
particular kind missing data known row conditional similarity constraints particularly important case text retrieval 
method dealing known multidimensional unfolding 
row conditional constraints particular subset possible inter object similarity constraints 
considering possible constraints pair inter object similarities comparable 
example text retrieval general constraint document similar query document query row conditional constraints hand require similarity comparison object common similarities 
constraint document similar query document query general constraint including row conditional constraints particularly suitable text retrieval relevance judgements different queries 
user indicating documents relevant single query 
multidimensional unfolding sets objects represented multidimensional space single set 
typical application set known judges objects 
judge typically person psychological experiment ranks object example liked liked 
multidimensional space created having objects judges points 
particular judge rank distances objects represent judge rank preferences objects 
unfortunate characteristic unfolding row conditional data lack determinacy 
row conditional data dependencies proximities unconditional data 
loss dependencies quite severe borg state unconditional case objects dependencies unfolding jj jxj set judges set objects 
example jj jx jj jx 
unconditional case dependencies compared row conditional case 
lack determinacy manifests large amount freedom configuration 
example may alternative solutions equally low stress 
solutions may quite different configurations initially generated proximity data 
fortunately lack determinacy major problem applications 
believe primary reason limited number parameters estimated adopting constrained approach 
discuss approach 
constrained mds generic multidimensional scaling operates points multidimensional space known configuration 
solve mds problem points moved inter point similarities match similarity constraints best possible 
generic application constraints positions points move position multidimensional space subject trying minimize configuration error 
important variation generic model know constrained mds 
constrained mds constraints placed valid positions point take multidimensional space 
typically constraints functional underlying representation objects 
function postulated mapping objects representational domain points multidimensional space 

parameterized adjusting parameters causes points move multidimensional space 
task constrained mds minimize configuration error adjusting function directly moving points 
number benefits constrained approach 
impose domain specific model problem 
second mapping function greatly reduce number free parameters estimated method 
general unconstrained mds number parameters proportional number objects domain object contributes coordinates parameters 
constrained case parameters limited number model grow increasing number objects training set 
lastly optimized map new objects representable training multidimensional space 
generalize new objects 
important context information retrieval objective adjusting document query vectors order satisfy relevance feedback constraints really main interest 
able map novel queries documents multidimensional space compare vectors relevance 
number examples constrained mds approaches 
carroll chang consider case set similarity orderings subject psychological experiment 
goal find set configurations subject linearly related single universal configuration 
formally seek set linear functions subject map universal configuration points points multidimensional space 
configuration points subject function fit particular similarity constraints 
method universal configuration individual configurations free parameters estimated 
computational procedure estimate parameters alternating squares fast provable convergence properties 
important restriction algorithm metric data 
furthermore efficient squares algorithm limited case linear operators diagonal matrices arbitrary weight matrices 
chang approach exciting potential text retrieval problem user modeling 
text retrieval different users different relevance judgements query recall katter results discussed earlier 
chang approach possible method modeling user specific variation 
pursued direction dissertation consider interesting possibility research 
provides alternating squares algorithm incorporate nonmetric data missing observations row conditions 
algorithm aimed similar problem previous finding linear relationship subject configuration single universal configuration 
algorithm gains efficiencies restricting relationship configurations 
specifically function configurations restricted diagonal linear matrix 
considers essentially problem carroll chang single universal configuration replaced set configurations subjects 
means subjects unique representation objects domain 
parameters mapping points base configuration free parameters adjusted 
parameters configurations fixed particular values restricted equal parameters 
model carroll chang assumes metric data 
approach generalized de leeuw heiser allow arbitrary restrictions allowable solution configurations 
particular suggest arbitrary linear operator operator diagonal matrix map base configuration points points meeting desired similarity constraints 
specifically mention base configuration may contain physical characteristics stimuli multidimensional psychophysics 
de leeuw heiser suggest possibility non linear models mapping function particularly combination linear sub models topic suggested pursued detail 
non metric approach examine chapters similar constrained mds approaches 
differences 
full linear models diagonal linear models non linear neural networks mapping function 
addition generalize approach independent representing points multidimensional space 
examine general parameterized similarity functions measure similarity objects domain objects need represented multidimensional space 
similarity artificial neural networks artificial neural networks class general multivariate functions 
partially motivated actual biological neurons typical neural network collection neurons called nodes weighted directed inter node connections 
node activity level gets propagated weighted connections nodes 
varieties neural networks feedforward networks recurrent networks topological maps see overview 
dissertation feedforward networks limit discussion networks section 
feedforward network implements mapping vector real valued inputs vector real valued outputs 
network feedforward directed links flow input nodes intermediate nodes output nodes 
cycles interconnections node activity propagated forward 
feedforward network implements function function non linear meaning outputs simply linear combination inputs include complex combinations inputs 
function typically parameterized varying parameters changes function computed neural network 
adaptation central part field neural networks numerous adaptive algorithms automatically set parameters neural network computes desired function 
review adaptive algorithms perceptron learning algorithm backpropagation 
algorithms direct relevance results chapters 
neural networks serve number purposes dissertation 
important purpose functional forms constrained mds problem 
neural network perform mapping text objects initial representation multidimensional mds space similarity objects key 
mds problem solved adjusting parameters neural network leading text representations better meet similarity constraints 
second purpose neural networks dissertation field suggests techniques theory adaptation parametric models 
able exact neural network learning algorithms approach similar benefit general adaptive strategies common field 
third neural network literature provides motivation separate information retrieval examining issues similarity vector representations 
particular vector similarity frequently imply semantic similarity neural network models cognition language understanding 
research pursued dissertation provides framework examining issues closely 
section review classic neural network approaches perceptron feedforward neural network 
examine neural network model metric multidimensional scaling proposed todd rumelhart 
lastly examine importance representational similarity connectionist theories language cognition 
perceptron field neural networks long colorful history dating back neurocomputer minsky called perceptron rosenblatt 
hecht nielson perceptron neurocomputer carry useful function classification patterns classes 
despite age perceptron algorithm important adaptive approach today 
example wong yao adaptive information retrieval method reviewed briefly chapter examined chapter employs slight variation perceptron 
perceptron classifies vector input classes 
computing linear weighted sum input classifying input class sum surpasses threshold 
perceptron computes outputs greater equal 
vector input vector linear weights 
adjusted perceptron learning rule output class output 
rule new old correctly classified incorrectly placed second class gamma incorrectly placed class 
learning rule repeatedly applied training samples training complete 
important aspect learning algorithm guaranteed find perfectly solves problem finite number steps assuming solution exists 
result known perceptron convergence theorem 
true regardless initial weights 
limitation algorithm limited linear weighting functions 
classes linearly separable perceptron solve classification problem 
backpropagation feedforward network current surge popularity neural networks due part identification adaptive learning method non linear neural networks 
adaptive algorithm called backpropagation examined detail widely disseminated rumelhart hinton williams general approach developed earlier number researchers see pp 

backpropagation algorithm adaptively adjusting weights feedforward network training examples 
supervised algorithm requires correct output target known training sample 
training network output compared target determine network behavior changed 
non linear multi layer feedforward network importance backpropagation adapt large variety neural networks contrast limitation perceptron algorithm linear networks 
backpropagation typically applied non linear multi layer feedforward neural networks 
non linear means functionality network linear 
typically non linearity arises sigmoidal function node nonlinearly transforms weighted linear sum inputs node output 
multi layer means network nodes links structured layers 
layers collections nodes typically having homogeneous interconnectivity layers having connections nodes layer 
layer input layer output layer called hidden layer 
feedforward means layers sequentially ordered links connect layer earlier layer formally input layer output layer hidden layer input vector output vector sigmoid feedforward neural network example non linear multi layer feedforward neural network 
network left layers total nodes 
right single node computes sigmoid sum inputs times weights 
interconnections form directed acyclic graph 
example non linear multi layer feedforward network provided 
backpropagation backpropagation operates minimizing criterion measures network outputs matches targets 
squared error criterion gamma vector output network single input vector target vector 
range elements output vector 
essentially implementation gradient optimization multi layer non linear networks 
minimized performing gradient descent weights network 
weights updated proportionately negative gradient new old gamma ff old gamma ff gamma ff constant determining weights changed result training sample training iteration 
definition weights changed training sample 
changes weights accumulated training samples applied time result batch training 
importance information retrieval number possible applications backpropagation feedforward network information retrieval 
approach suggested crestani neural network implement mapping vsm term space representation documents queries better vector representation :10.1.1.50.2329
network maps query vector non linear hidden layer output layer 
target output vector representation document relevant query 
repeated relevant documents 
result network outputs new query vector average vector representation relevant documents 
new query find documents collection may relevant 
application backpropagation information retrieval training retrieval system output correct estimates probability relevance documents 
recall boolean notion relevance documents relevant irrelevant query 
backpropagation train retrieval system assuming differentiable parameters relevance estimate relevant document irrelevant document 
fact similar approach fuhr buckley probabilistic learning method 
examine greater detail chapters compare supervised learning method alternative non metric method 
third application backpropagation feedforward networks model learning algorithm multidimensional scaling 
approach pursued todd rumelhart examined section 
neural network mds recall constrained multidimensional scaling functional model typically implements constraints configuration 
positions points multidimensional space constrained function mapping object representations mds space 
natural neural networks serve role constraining function 
addition supervised learning approach described appears share similarities metric approaches mds 
metric mds similarities objects trained match target similarities 
supervised learning output network trained match target values 
possible solve metric mds problem supervised learning approach 
todd rumelhart pursued approach 
propose neural network model implements constrained unconstrained metric mds 
idea neural network maps objects multidimensional space second neural network compares pairs object representations generates similarity score 
architecture illustrated 
depending input representation objects constrained unconstrained mds implemented 
input localist orthogonal linear representation network position object arbitrarily space leading unconstrained mds 
input feature representation objects network constrained represent objects functionally features 
backpropagation todd rumelhart networks trained output correct metric similarity score pair inputs 
effect training modify networks better represent compare pairs objects 
empirically object object similarity object object representation network similarity network duplicate networks todd rumelhart neural network mds representation network maps objects multidimensional representation similarity pairs object representations measured similarity network 
quite difficult adapt similarity network representation network simultaneously 
similarity network engineered setting weights hand implement similarity function representation network trained quite effectively 
todd rumelhart suggestion adaptive similarity measure interesting implications context adaptive ir approach 
essentially adaptive similarity measure free compute function satisfy similarity constraints mds problem limitations functional model course 
learned similarity measure need act similarity measure fact long solves problem 
adaptive approach chapter allows similar freedom function estimates relevance retrieval system 
relevance function need estimate relevance document query typical notions similarity vsm approach allows arbitrary differentiable estimator 
see chapter allows interesting application method learning high performance combination estimators 
addition chapter examine application non metric approach specifically learning similarity measure documents queries 
representational similarity neural networks vector similarity important concept neural networks particularly models cognition connectionist natural language processing cnlp 
example frequently argued features neural network models cognition vector representations allow objects similar dissimilar graded continuous way different semantic dimensions 
furthermore similarity essential part representation allowing similarity sensitive processing 
neural network process different objects way represented similarly 
stark contrast symbolic methods representation 
symbolic representation object represented composition atomic symbols 
symbols internal structure intrinsic similarity symbols 
consequently symbolic processes sensitive graded similarities symbols axiomatically define similarity 
researchers taken view neural network representations 
example smolensky argues connectionist representations just implementations classical symbolic representations 
key point argument vector representations encode concepts single points activation space clouds points point representing different possible contextual nuance core concept 
course points clustered respect similarity measure vector similarity glue holding argument 
van gelder advances smolensky argument suggesting neural processor similarity patterns 
vector similarity equated semantic relatedness 
analysis network models elman cottrell bartell cottrell extensive explicit proximity measurements internal representations analysis models 
analysis leads understanding semantics internal representations models similarity 
appears task centered semantics nearby points representation space represent things environment dealt similar ways demanded task 
number researchers cnlp connectionist cognitive modeling proposed approaches learn representations useful similarity structure 
usually structure emergent property side effect performing task 
examples include model hinton family trees model elman recurrent network model sentence processing 
empirically demonstrated neural networks learn representations task dependent similarity structure 
similarity structure target training procedure 
schutze proposed method automatically constructing representations useful similarity structure 
goal method called word space represent natural language terms multidimensional vectors 
vectors useful representation terms vectors similar semantically related roughly term occurrence 
fact schutze states vector similarity information word space semantically related words close unrelated words distant 
schutze uses letter fourgrams atomic constituents words performs singular value decomposition extract occurrence relationships 
fourgrams included analysis large require cray supercomputer svd 
alternative schutze computationally expensive approach proposed scholtes 
approach nen map represent similarity large collection terms 
representations variety tasks information retrieval 
approach dissertation advancement methods identified 
chapters adaptive methods construct vector representations satisfying similarity constraints 
methods incorporate alternative kinds similarity constraints examined far field 
particular non metric constraints satisfied approach proposed chapter 
chapter metric similarity modeling adaptive method optimally represents documents vectors multidimensional space 
document vectors chosen inter document similarities best match set metric target constraints 
target constraints method represent inter document semantic associations 
derived relevance judgements set training queries documents relevant query considered semantically related 
multidimensional representation documents semantics documents inferred relevance assessments simply terms documents 
addition representing documents space method produces linear mapping represent arbitrary document query multidimensional space 
documents queries compared multidimensional space order estimate relevance documents queries 
method implicitly improves retrieval system ability rank documents 
true objective adaptation represent documents semantic relationships explicitly encoded inter vector similarities 
method applied standard text databases information retrieval literature 
cisi database information science abstracts performance measured precision averaged range recall levels improves compared weighted term vector approach improves compared latent semantic indexing 
similar improvement obtained cranfield database improvement obtained artificial med database medical abstracts 
generally favorable performance suggests interesting potential methods explicitly modify retrieval system meet inter document semantic constraints 
ambiguity vsm known critical problem document retrieval systems failure individual keywords identify conceptual content documents 
difficult determine document relevant query documents conceptually related simply examining terms share 
chapter method representing documents explicitly semantic relationships documents addition individual term usage documents 
allows documents retrieved semantically related simply related term occurrence 
demonstrate experimentally improve document retrieval performance relative current popular retrieval algorithms 
problems associated document terms indices retrieval partly result ambiguity inherent terms natural language text 
example furnas observed individual keywords adequate discriminators semantic content 
indexing relationship word document content number concepts indexed single term polysemy number terms index single concept synonymy 
retrieval solely matching terms query documents performance suffers 
relevant documents missed indexed keywords query synonyms 
conversely irrelevant documents retrieved indexed unintended senses keywords query 
vector approach currently great deal interest solving problem term ambiguity wide variety approaches proposed 
approaches include building thesauri automatically manually modifying document queries semantically similar terms automatic term disambiguation term senses approaches document representations augmented explicit associations 
approaches avoid simple matching corresponding keywords documents determine relatedness attempt additional semantic extraction augmentation text 
theme common approaches ambiguity problem posit semantic space documents represented 
familiar widely representation documents queries represented semantic space vectors vector similarity determine semantic relatedness text objects 
document vectors similar interpreted evidence documents semantically related documents dissimilar related 
typical application salton approach dimension vector space term corpus 
document vector non zero elements corresponding term occurs document 
typical vector similarity measures inner product cosine vectors similar share nonzero elements documents share terms considered semantically related 
limitation typical application salton method term ambiguity problems discussed 
number enhanced methods proposed potential overcome limitation 
feature unifies methods represent documents vector space document vectors may similar share terms 
methods include latent semantic indexing gallant context vector method adaptive document vectors yang chute canonical concept mapping reviewed chapter 
metric similarity modeling approach propose metric similarity modeling msm uses multidimensional semantic space represent documents 
semantically related documents represented similar vectors related documents may similar share terms 
difference approach identified method semantic representations generated 
method critical features ffl semantic relatedness documents explicitly modeled technique similarities vectors semantic space ffl result method function represent arbitrary document query semantic space original application method 
numerous techniques modify document representations implicitly alter inter document similarities change documents meet specific constraints derived available semantic relatedness information 
msm allows semantic associations directly affect representations text interpretation terms 
addition techniques derive new representations set documents available time method applied semantic map resulting msm subsequently represent novel documents queries semantic space 
proceed overview theoretical development metric similarity modeling technique 
comparison msm related techniques demonstrate latent semantic indexing important optimal special case msm framework 
algorithm discuss implementation followed application method set text collections standard information retrieval literature 
conclude discussion strengths weaknesses proposed method 
derivation method msm approach places primary emphasis inter document similarities 
actual document representations adjusted similarities document representations take appropriate desired values 
seen chapter kind approach typical broader field multidimensional scaling mds mds objects represented points multidimensional space points chosen inter point similarities meet set externally imposed constraints similarities 
msm approach derived directly known techniques metric mds 
sections provide overview msm concepts multidimensional scaling theory formally msm problem solution 
overview metric similarity modeling addresses known problem occurrence terms documents limited estimate semantic relatedness documents 
problem addressed msm explicitly modeling additional semantic relatedness information may available 
semantic information modeled similarities vectors multidimensional space 
vectors similar correspond semantically related documents vectors similar semantically related 
addition msm generates representation function represent original documents arbitrary document query vector semantic space 
representation function takes input document query represented vector terms produces appropriate semantic vector 
main value method related documents represented similarly tend retrieved 
documents relevant query retrieved share terms query represented similarly relevant documents 
result significant improvement precision especially high levels recall 
similarity constraints perform representation task msm requires kinds information ffl term vector encoding documents represented ffl separate indication semantic relatedness documents 
call semantic relatedness information similarity constraints constrain representation documents 
msm explicitly satisfies similarity constraints deriving document representations having inter vector similarities close possible similarity constraints 
similarity constraints metric similarity modeling algorithm metric exhaustive 
metric similarity constraint pair documents provides specific target value real number matched similarity documents new representation 
example metric similarity constraint documents collection 
similarity vectors documents semantic space constrained 
note ways vectors documents positioned semantic space satisfy constraint possible configuration illustrated 
similarity constraints exhaustive target similarity value required pairs documents automatically indexed method 
method applied representation function derived arbitrary documents queries represented semantic space 
target similarity constraints needed novel cases 
similarity constraints non standard component typical document indexing strategies making msm applicable information available 
constraints available msm provides technique providing semantic document representations satisfy 
discuss section approach automatically generate target similarities document relevance data 
sources target similarity data may available suitably derived 
problem statement analytic solution section formally define representation problem give solution provided msm procedure 
casual reader may omit mathematical details section continue section 
problem find linear function maps term vector documents vectors new vector space semantic space inner product similarities semantic space closely match target inter document similarities 
formally real matrix rows columns denoted theta number terms term vector representation number documents corpus 
th doc doc doc doc doc doc doc doc doc doc doc doc document collection document vector document vector document vector document vector training representation function target inter document multi dimensional semantic space term space semantic space msm representation function msm generates representation function maps term space document vectors vectors multi dimensional semantic space 
representation function chosen inner product similarities semantic document representations closely match set target similarity constraints 
column corresponds th document collection th row corresponds weight th term document 
contains complete term vector representation documents collection 
theta square symmetric positive semi definite matrix containing target inter document similarities 
th element target similarity th th documents 
dimensionality target semantic space 
representation function map documents dimensional term space new representations dimensional semantic space 
theta real matrix implementing linear representation function 
result msm procedure 
superscript emphasizes free parameter method 
results reported chapter fixed yields consistently satisfactory results 
dimensionality deerwester applying related latent semantic indexing technique number databases 
product theta matrix documents represented semantic space 
goal inner products representations match target constraints 
inner product similarities documents semantic space matrix transpose operator 
problem find minimizes error criterion gamma symmetric positive semi definite matrix real matrix exists measure error simply sum squared differences corresponding elements target similarity matrix similarities documents re represented semantic space 
demonstrate section details critical point equation form cx singular value decomposition svd pseudo inverse 
pseudo inverse calculated svd svd gamma gamma denotes diagonal matrix having reciprocals singular values diagonal 
provided singular value decomposition product caa caa sigman svd derived retaining columns corresponding largest singular values sigma discarding rest 
section provides algorithmic interpretation result 
able prove solution equation provides actual global optimum just critical point 
empirical evidence indicate solution fact global optimum 
performed number experiments error equation numerically minimized numerical techniques conjugate gradient large number random matrices cases having better linear solution equation 
provides evidence proposed solution globally optimal cases 
solution provided may cryptic essentially solving familiar squares problem solving denotes theta matrix keeping rows corresponding largest eigenvalues removing rest 
known data analysis literature provides optimal representation documents inner products objects best match target similarities squares problem interpreted finding best linear map term vector representation optimal dimensional representation 
unfortunately detail solution somewhat complex details provided section 
details solution fortunate squares solution give optimal solution metric similarity modeling problem optimal document representation squares solution give best linear map optimal representation 
linear squares problems may equality satisfied 
example linear algebra know row space entirely subspace row space solve equality 
complicating problem fact rows discarded constructing may fact intersect row space case singular vectors correspond smaller singular values information correlate target similarity information 
example consider extreme case row spaces orthogonal row spaces gammak identical gammak denotes gamma thetad matrix constructed gamma smallest singular vectors values rank 
pathological case squares solution 
exact solution gammak provides better solution error minimizing 
solution equation reflects complication simple squares approach 
matrix projected row space spanning vectors singular vectors matrix place squares problem 
special case subset row space projection solution simple squares 
general case solve modified squares problem keeps significant dimensions optimal representation projected row space proof main result prove cx critical point configuration error equation 
consider configuration error gamma gamma sk trf gamma gamma trf wx wx wx wx gamma wx wx ssg trace matrix computed sum diagonal entries equivalent definition squared frobenius norm kzk calculating setting partial find critical point satisfy wx wx demonstrate matrix rm cx solution equality equation full rank theta rotation matrix 
letting demonstrated solution cx critical point configuration error 
consider left side 
recall svd decomposable sigman svd caa letting projection matrix derived aa solve left side wx wx wx rm cp rm cp rm sigman sigman sigma sigma derivations facts cp sigman definition 
consider right side equation rm cp cx rm sigman cx sigma cp sigma sigman sigma thetar gammak thetar sigman thetar denotes matrix retaining orthonormal columns columns zero 
gammak thetar similarly denotes matrix retaining gamma orthonormal columns columns zero 
obviously thetar gammak thetar derivation comparing final expressions derived equations see verify equality equation rm cx need show sigma gammak thetar sigman 
noting sigma gammak thetar sigman sigma sigma gammak thetar sigman gammak thetar 
rm cx solution cx critical point configuration error 
indexing retrieval algorithm solution metric similarity modeling problem section constructive amenable implementation 
section details actual application msm document indexing retrieval 
assume target similarities inter document semantic information provided section discuss method deriving values relevance feedback samples 
phases application msm indexing retrieval 
indexing training phase document representations learned term space representations target similarities 
primary product training phase linear map represent arbitrary term space document query vector semantic space 
trained map semantically represent queries matched collection semantic document vectors 
computing document indices result msm procedure linear map re represents objects space objects dimensional semantic space 
linear map represented matrix having rows columns number terms 
matrix product weight matrix term space document represented column vector gives representation document dimensional semantic space 
parameter analogous deerwester latent semantic indexing number dimensions new semantic space 
equal rank original document space equal minimum number documents 
application msm derive involves uses singular value decomposition svd technique 
theta matrix documents columns represented terms rows 
furthermore theta square symmetric positive semi definite matrix containing target similarity values entry target similarity documents 
main steps msm procedure 
compute svd note identical computation required lsi 
column orthonormal matrices diagonal matrix singular values 

compute svd positive semi definite guaranteed exist 
recall provides optimal dimensional representation data respect modeling target similarities 

compute svd caa sigman step determines space spanned terms documents overlaps optimal semantic space solution calculated directly results preceding svd steps cx derived matrix step retaining columns corresponding largest singular values sigma removing columns 
pseudo inverse gamma earlier gamma derived reciprocal non zero singular values diagonal new document representations trivial compute 
recall columns term vector representations documents 
compute semantic representations document collection compute product linear mapping function column turn 
columns resultant matrix dimensional representations documents 
comparing documents queries estimate relevant documents particular query query mapped semantic space documents represented 
query document vectors compared measure vector similarity 
similar documents query vector returned user relevant query 
simple efficient map query semantic space 
assume query represented dimensional term vector space original documents 
label query vector column vector matrix rows single column 
new semantic vector matrix product linear mapping function computational cost costs msm algorithm cost paid indexing training phase second paid retrieval new representations 
indexing cost essentially cost calculating determined cost computing singular value decomposition 
algorithms svd typically order number rows columns matrix 
case msm number documents compute svd symmetric theta matrices 
certain steps indexing algorithm require matrix multiplication document matrices operations order number terms 
large number terms may svd cost 
cost retrieval time burdensome fortunately 
computing semantic representation query takes kt time number terms query 
finding documents similar query vector requires kd time addition computation easily done parallel 
latent semantic indexing similar indexing retrieval costs computes singular value decomposition document matrix 
indexing lsi similarly expensive comparison fast methods term weighting 
msm algorithm lsi benefit certain optimizations due limited number dimensions resulting semantic space due typical sparseness document matrix 
deerwester report algorithm number dimensions semantic space 
alternative lanczos algorithms optimized sparse form typical document matrices 
latent semantic indexing msm latent semantic indexing lsi document indexing technique closely related msm method advocated 
recall chapter latent semantic indexing lsi attempts represent documents new multidimensional space vector similarity better indicator semantic relatedness 
latent semantic indexing method mapping documents high dimensional term representation lower dimension semantic representation 
idea approach map documents alternate representation similarity structure reflects associations terms 
example documents share terms may mapped similar representations pairs terms documents strongly occur documents 
approach perform singular value decomposition svd term matrix yielding column orthonormal real diagonal 
reduced document representation creating matrices theta retaining rows largest singular values documents encoded columns lsi documents encoded columns matrix plays role latent semantic indexing analogous linear mapping function msm 
novel documents queries represented semantic space assuming available vectors dimensional term space 
query encoded column vector dimensional semantic representation query similarity document corpus query measuring similarity rows vector similarity measured example cosine measure done original 
results method applied real document collections mixed generally encouraging 
med collection medical abstracts lsi performed better raw term matching better salton smart vector space information retrieval system 
second cisi collection science abstracts average performance lsi raw term matching equivalently disappointing precision lsi performed worse smart average precision 
authors attribute poor results lsi methods general relative homogeneity document collection discrimination difficult 
chapter report better performance application lsi collections 
improved performance result alternative weighting strategies deriving original term space document representations 
lsi appears promising method 
lsi optimal case msm consider restricted case metric similarity modeling problem 
case metric target similarities derived directly documents 
show optimal msm solution special case identical document indexing solution provided latent semantic indexing 
latent semantic indexing generating semantic space modeling similarity structure full term space 
implications result latent semantic indexing msm discussed section 
detail equivalence lsi special case msm 
theta matrix documents represented terms singular value decomposition similarity pair documents represented columns inner product similarity measure matrix theta pairwise similarities apply msm documents target similarities similarity matrix satisfies condition positive semi definite definition applying msm find optimal linear weighting function cx matrices defined section 
special relationship target similarities documents matrices easily solved terms decomposition matrices particular definition pseudo inverse gamma properties orthonormal matrices solve directly cx xx msm yields linear function solution special problem target similarities inner product similarities documents 
function identical solution latent semantic indexing applied documents 
result surprising 
known data analysis literature dimensional representation data best matches target matrix inner product similarities svd similarity matrix 
case targets trivially derived documents particularly simple linear relationship documents optimal representation 
implications optimality lsi optimality latent semantic indexing respect special case msm important implications lsi msm 
msm provides practical foundation application technique 
similarities original term vector documents targets method yield favorable results reported applications lsi 
addition foundation starting point method determining target similarities collection documents term vector similarities serve default target similarities augmented additional available semantic information 
section provides detailed example approach relevance assessments augment default target similarity matrix 
observation lsi optimally solving metric mds problem complements analysis researchers 
previous illustrated beneficial properties technique reduction noise orthogonalization vector space incorporation associational relationships representation equivalence metric similarity modeling detract results 
current analysis adds new terminology alternative perspective discussion 
major insight analysis importance inner product similarities original term space 
similarities noisy estimate relatedness documents lsi reduction yield results 
analysis suggests certain term weightings original term space improve performance lsi 
known alternative weightings terms term vector representation improve retrieval performance 
authors lsi permit alternative term weightings 
essentially alternative weightings yield new document matrix similarity structure inner products 
better measure relatedness analysis predicts lsi perform better operating dumais empirical study lsi various term weightings agree suggestion term weightings tend improve inner product retrieval original term space tend improve retrieval performance semantic space 
past applications latent semantic indexing similarity semantic space measured cosine measure inner product 
result appear generalize cosine shown cosines original term space preserved terms cosines semantic space 
certainly shown cosine inappropriate measure 
analysis demonstrates inner product natural measure desirable preserve inner products original space 
cosine may useful appropriate measure document vectors generated latent semantic indexing unit length original term vectors normalized 
cosine measure ensure document retrieved simply semantic representation longer 
addition possible preserve cosine similarities original space terms inner products semantic space 
achieved normalizing documents term space resulting new document matrix 
cosines inner products identical unit length vectors cosines preserved inner products lsi 
analysis preclude typical similarity measures pseudo cosine dice 
analysis indicates measures applied semantic representation common original space terms inner product similarity structures 
experiments experiments standard text bases information retrieval literature validate metric similarity modeling approach 
experiments test retrieval performance improved representations generated msm compared benchmark retrieval methods weighted term vector approach weighted smart retrieval system latent semantic indexing 
standard collections cisi collection information science abstracts cranfield collection med collection medical abstracts test methods 
chosen commonly test ir system performance literature standard ir approaches mixed success collections size collection approximately documents tractable svd operations 
included test collections set evaluation queries 
performance arbitrary retrieval method estimated queries documents collection identified relevant irrelevant query 
experiments queries accompanying relevance assessments distinct purposes 
relevant sets documents associated query basis inter document similarity information required msm method 
need queries basis performance evaluation remains 
issue common machine learning research typical solution partition available data sets initial training set construct representation remaining test set evaluate 
case query relevance sets experiments training set provide document similarity constraints test set evaluate resulting msm representation 
unfortunately procedure performance evaluation sensitive queries selected training 
experiments iterate procedure multiple random partitions query data average results 
resulting procedure test databases 
apply benchmark weighting methods generate weighted term vector representations documents queries smart 
generate lsi representations smart weighted vectors lsi procedure section 
iterate number trials randomly partition set queries training set test set 
training queries derive target similarity constraints msm procedure 
generate msm representations smart weighted vectors msm procedure 
measure performance msm lsi smart test queries 

compute mem lsi smart performance averaged random query partitions 
indexing methods initial document data term vectors generated smart retrieval system 
ensures bias different term sets weighting algorithms input different methods 
addition application msm uses queries relevance tagged documents derive target similarity constraints 
performance methods measured set queries test set indexing 
ensures bias evaluating msm algorithm set queries order determine document representations 
total query set randomly partitioned test set training set number times 
ensures bias introduced single poorly randomized partition 
proceed discussion major steps test procedure 
section provides results experiments 
benchmark weighting methods smart retrieval system generate weighted term vector representations 
med database weights calculated tfidf term frequency inverse document frequency method cisi cranfield databases augmented tfidf weighting method 
collections document query vectors normalized unit norm inner product cosine similarity measures identical representations 
weighting methods chosen default settings smart system databases result performance relative possible weightings 
document representations latent semantic indexing method calculated procedure reported deerwester smart weights document matrix raw term frequencies 
experiments lsi different term weights indicate lsi performance increased weightings better performance complete term space 
theoretical justification provided section 
dimensionality semantic space chosen similarity metric cosine original lsi research 
partitioning query set query set partitioned disjoint sets training set test set 
training set construct target similarity constraints msm procedure 
test set reserved evaluation 
query set partitioned randomly queries placed training set remaining placed test set 
med collection tfidf weight term frequency multiplied log quotient number documents collection frequency term 
augmented tfidf identical tfidf term frequency normalized 
sample queries partition training queries test queries 
cisi collection sample queries partitioned training test queries 
cranfield queries partitioned training test queries 
multiple partitions created collection different partitions created cisi collection cranfield med 
results reported averages partitions 
msm weighting document representations msm method calculated procedure section 
latent semantic indexing smart term weights populate document matrix dimensionality msm semantic space lsi avoid bias due differences representational capacity 
lsi cosine measure determine similarity document query vectors 
performance cosine measure superior inner product measure 
cosine appropriate document vectors generated msm unit length cosine normalizes vectors prior likelihood retrieval document uniform 
apply msm matrix target inter document similarities required 
data requirement additional relative requirements term vector weighting lsi 
matrix allows msm incorporate sources information term occurrence affect retrieval performance 
numerous potential sources inter document relatedness information citations hierarchical structures collection 
topic discussed section 
experiments relevance information derive target similarity matrix 
documents relevant classified relevant query 
relevance indication documents related respect query consider reasonable source information indicate documents may similar base rates term cooccurrence may indicate 
construction target similarities procedure calculate matrix target similarities involves steps generating baseline similarity matrix directly term occurrences augmenting entries matrix relevance information normalizing matrix 
baseline similarity matrix generated simply calculating similarity pairs documents inner product measure 
provides similarity matrix identical theoretical target matrix latent semantic indexing 
note documents normalized unit length maximum similarity pair documents occurs documents identical 
minimum similarity occurs share terms 
resultant similarity matrix symmetric diagonal elements corresponding similarity documents equal 
relevance data training queries augment entries baseline matrix 
pair documents relevant query th entry similarity matrix incremented real value ff 
increment ff typically range vary depending characteristics collection confidence relevance data 
general larger ff emphasis doc doc doc doc doc doc doc doc doc doc doc doc doc doc doc doc doc doc doc doc term space doc doc relevant training query target inter document initial term similarities generating target similarities generation target similarities original term space representations training set queries relevance tagged documents assuming increment ff 
placed relevance information greater target similarity matrix deviate baseline matrix 
effect adding ff th entry similarity matrix require documents represented similarly simple term occurrence 
note ff added th entry similarity matrix time documents relevant query 
entry augmented multiple times documents frequently relevant 
addition diagonal th entry incremented time document relevant query 
ensures document similar target matrix document 
illustrates construction target inter document similarity matrix relevance data 
cisi database ff cranfield ff 
med database values ff reported 
ff smaller cisi database average number increments cisi baseline matrix number relevant documents training query larger cranfield med baseline matrices 
selected ff cumulative effect increments baseline matrices roughly 
particular cisi database average number documents relevant training query documents cranfield documents med 
increment baseline matrix pairs relevant documents relationship number increments baseline matrix number documents relevant training query 
estimated number increments cisi baseline matrix order magnitude larger number increments cranfield database increments med database increments 
ff cisi order magnitude smaller ff cranfield med resulting comparable cumulative increment baseline matrices 
small number ff values near reported values examined resulted substantial improvement values chosen 
final step normalization similarity matrix 
purpose normalization target similarity document equal 
normalization important ensure bias introduced representations frequently relevant documents longer retrieved frequently relevant documents 
addition making similarity document equal normalization similarities pairs documents greater 
similar document document 
normalization method divide row column square root diagonal element row column 
elements row column divided square root th element row column 
normalization results similarity matrix diagonal elements equal 
target inner product similarity document constraining documents represented semantic space unit length vectors 
note normalization interesting effect resultant target similarity matrix effect documents relevant slightly similar documents relevant slightly similar target document lengths remain identically 
emphasize method derive target inter document similarity scores relevance data number possibilities 
example large amount relevance data available may necessary seed target matrix raw inter document similarities addition normalization step omitted frequency relevance document reflected representation 
methods outside msm framework incorporating relevance feedback document representations investigated considerable success 
selection method motivated primarily theoretical equivalence latent semantic indexing special case msm 
performance lsi relative competitive methods suggests inter document similarities starting point target similarity matrix 
furthermore research suggests normalization document representations quite helpful 
results performance indexing retrieval algorithms metric similarity modeling msm latent semantic indexing lsi smart compared 
performance systems compared training queries test queries training queries useful illustrate effects msm test queries estimate expected performance benefit technique novel queries 
performance measured precision recall levels averaged set example queries 
ranked retrieval systems recall systematically varied relevant documents retrieved relevant documents retrieved level precision measured recall level details see 
averaging queries graph precision vs recall gives concise portrait ability retrieval system accurately rank relevant documents irrelevant ones 
recall precision msm incr msm incr lsi smart average training set performance med database recall msm lsi smart average training set performance cranfield database msm training set performance performance metric similarity modeling msm comparable indexing retrieval methods smart lsi training query sets 
performance databases med cranfield shown 
surprising performance msm superior relevant documents training queries constrained clustered msm method 
presentation results performance averaged different partitions query set training test sets 
training set performance effect applying msm best illustrated examining performance msm training queries 
displays precision recall graphs databases cranfield med 
immediately apparent msm retrieving relevant documents far better lsi smart 
particularly true med database relevant documents retrieved irrelevant ones 
expect performance msm training set superior benchmarks 
msm represented documents documents relevant training queries highly similar simple term occurrence 
msm enforcing clustering related documents documents self similar separate irrelevant documents 
long training query represented near corresponding cluster retrieve relevant documents baseline methods 
med database particularly useful illustrating point 
med atypical database queries training testing partition collection documents disjoint sets 
document relevant queries disjoint classes documents query garbage class documents relevant query 
term occurrence apparent classes documents form convenient clusters respect queries precision extremely high cluster relevant documents retrieved ahead irrelevant documents 
effect msm reconfigure representations documents explicitly clustered relevance 
med msm able tightly cluster different classes overlap relevant sets force documents represented classes 
illustrated class clusters perfect msm ff 
course med artificial documents partitioned queries 
collections user queries structured 
typical training set performance illustrated cranfield database 
msm performance far superior benchmarks strict clustering created 
fact clustering possible documents belong query class 
relevant documents simply similar base rates term occurrence remain related non relevant documents 
test set performance test effectiveness msm indexing retrieval algorithm msm compared benchmark methods test set queries 
test provides expectation level performance benefit benchmark methods msm representations retrieve documents novel queries 
precision recall performance databases depicted 
databases cranfield cisi msm outperforms lsi smart benchmarks 
summary msm precision better average lsi better smart cisi database 
cranfield database msm precision better average lsi better smart 
results statistically significant query partition random factor recall level indexing method experimental factors 
indicates msm able usefully extract meaningful semantic relationships target similarity data 
relationships exploited document representations result generalization performance 
performance med database positive 
msm precision lower lsi precision average higher smart performance 
result artificial structure med database 
documents relevant test queries completely distinct documents relevant training queries useful semantic relationships learned training set transfer test set 
interestingly performance training set significantly improved structure documents relevant test queries adversely affected decrease performance relative lsi statistically significant increment ff ff query partition random factor recall level indexing method experimental factors 
benefit high recall methods rely matching terms query documents smart term vector approach difficult retrieve relevant documents 
relevant documents contain query terms 
inability mean inter document association index query set method relevant relevant irrelevant separation training set smart lsi msm test set smart lsi msm table inter document association comparison average levels inter document association pairs relevant documents pairs documents relevant 
support cluster hypothesis relevant documents tend highly associated relevant irrelevant pairs regardless indexing method 
msm distinction training test set queries 
index separation mean query set difference association means 
retrieve relevant documents contain query terms results apparently low precision high levels recall remaining relevant documents retrieved remaining set non matching documents essentially random order 
metric similarity modeling latent semantic indexing rely term matching retrieve documents 
retrieval performed measuring similarity semantic space documents similar query degree 
documents ordered respect relevance query important distinctions highest levels recall 
illustrates feature improvement precision gained lsi msm compared smart greatest higher levels recall averaged queries training partitions 
suggests peak improvement msm lsi compared smart peak occurs different levels average recall different databases cisi cranfield 
interestingly recall performance raw term matching similarly varied 
example cisi database approximately relevant documents retrieved smart retrieval system average queries 
cranfield database smart system able retrieve relevant documents 
characteristic peak improvement occurs recall levels maximum attainable recall level smart 
supports hypothesis greatest benefit msm lsi retrieving documents contain query terms 
worth noting apparent depression performance lowest levels recall lsi msm compared smart 
suggests methods best employed specifically retrieve documents identify replace term matching lowest levels recall 
cluster hypothesis cluster hypothesis posits closely associated documents tend relevant queries 
cluster hypothesis generally true collection association measure documents indicator semantic relatedness 
cluster hypothesis valid varying degree document collection depending characteristics collection set queries 
example collection may partitioned sets relevant documents may 
application msm conceptualized procedure enforcing cluster hypothesis set documents 
association level pairs documents constrained msm representation low high depending documents tend relevant 
set documents relevant training queries statement true definition msm explicitly forces relevant documents similar queries 
interesting constraints training relevant documents exhibit positive transfer test documents 
table illustrates phenomenon cranfield collection 
columns data table give mean inter document association level pairs documents relevant query pairs documents relevant query 
difference theses values column indicates degree cluster hypothesis valid collection larger difference assuming reasonable distributions associations highly clustered relevant documents relative non relevant documents 
set values illustrates expected result training queries msm greatly increases separation relevant clusters documents unrelated documents 
second set values illustrates positive transfer clustering test set queries 
documents better separated unrelated documents relative smart lsi benchmarks 
discussion section discuss major strengths proposed method highlight certain assumptions restrict applicability 
directions extension generalization method identified 
metric similarity modeling interesting indexing algorithm creates document representations reflect actual semantic organization documents simply reflecting vagaries term occurrence 
experimental results previous section demonstrate msm representation documents utilized improve retrieval precision especially high levels recall methods known fail 
importantly semantic organization documents important semantic relations generalize novel situations new queries 
improvements possible method models semantic associations explicitly document representations 
msm additional useful properties addition performance benefits 
foremost msm algorithm generates explicit functional relationship basic features documents terms semantic space 
allows post application analysis term contribution document representation similar analysis proposed authors latent semantic indexing 
functional relationship allows arbitrary new document query seen system mapped semantic space basic features 
critical feature order method generalize novel queries documents 
additional value msm approach link establishes indexing problem multidimensional scaling mds techniques 
mds class data analysis methods represent objects case documents queries multidimensional space inter object similarity matches semantic similarity objects 
linking indexing problem mds highlights potentially valuable generalizations msm indicates number assumptions msm critically affect method applicability may relaxed 
major assumptions target similarity constraints msm metric exhaustive 
assumptions require precise target similarity pairs documents provided algorithm 
kinds semantic constraints may difficult formulate information metric form 
example difficult model relevance associations experiments metric similarity values 
experiments demonstrate raw term vector similarities initial estimate perturbed relevance information application completely natural 
numerous possible semantic constraints equally difficult interpret metric constraints citation relationships documents assumption documents related citation may related term vector similarities suggest possible document classification information may available legal database 
increase similarity associated documents may readily clear 
metric exhaustive assumptions relaxed 
subset pairwise similarities available example error measure equation section appropriately modified 
squared error target actual similarity values restricted applicable subset available similarity values 
error measure minimized iterative function minimization methods gradient descent explicit method singular value decomposition chapter 
alternatives metric assumption plentiful depend characteristics available similarity constraints 
alternative examine detail dissertation ordinal constraints similarities documents queries 
ordinal constraints specify specific target similarity metric constraints specify ordering set inter object similarities 
example ordinal constraint requires document similar document third document specify exact target similarity documents 
wong yao argued ordinal constraints may natural interpretation document query relevance assessments 
ordinal constraints quite common multidimensional scaling applications 
approach examined chapters 
assumption msm mapping term vectors semantic representation linear 
emphasize finding best linear mapping data dimensional semantic representation strongly constrains quality 
class linear functions may functionally expressive relate term space representation documents optimal semantic space representation representation best models target similarity data inner product similarities non linear functions hand potential accurately model target semantic structure 
non linear functions example evidence occurrence terms document meaning linear sum individual word meanings 
quite useful phrases mathematical details limitation provided section 
home run systems theory highlight 
non linear msm mapping function learned minimizing error criterion iterative methods greater difficulty due local minima error surface 
note additional parameters non linear mapping function increase susceptibility model fitting training data failing generalize novel queries 
final critical assumption msm target similarities interpreted inner product similarities vectors 
inner product similarity effect applicability technique assumptions discussed important theoretical significance 
affect applicability msm alternative similarity metrics essentially inner product form additional normalization compared vectors 
possible normalize document vectors applying method inner products normalized vectors 
approach taken experiments reported section 
theoretically significance inner product measure remains heavily debated psychology 
unclear semantic similarity accurately represented space measures inner product similarity euclidean distance 
example definition inner product similarity similarity equivalent similarity semantic similarity need necessarily symmetric 
tversky provides example countries china north korea 
human subjects reliably consider north korea similar china china north korea 
axioms metric spaces addition symmetry may violated arbitrary real semantic relationships 
questions multidimensional representational methods msm quite useful modeling interpretation large amount psychological data 
document representations msm derive best approximations underlying semantic structure document collection expect approximation useful determining relevance ir task 
hypothesis supported experiments 
summary proposed method metric similarity modeling msm indexing documents modeling explicit target inter document similarity scores 
documents represented vectors semantic space low dimension relative original term space representation 
vectors chosen similarities vectors closely match target similarities squares sense 
target similarities signify semantic associations documents results document representations similar vectors accurately identify related documents dissimilar vectors identify related documents 
shown latent semantic indexing alternative dimensionality reducing approach optimally solving specific metric similarity modeling problem target similarities exactly inter document term occurrence scores 
method applied text databases standard information retrieval literature 
generally favorable performance technique compared weighted vector space approach latent semantic indexing suggests interesting potential methods explicitly modify retrieval system meet document similarity constraints 
recall precision msm lsi smart average test set performance cranfield database recall precision msm lsi smart average test set performance cisi database recall msm incr msm incr lsi smart average test set performance med database msm generalization performance performance metric similarity modeling msm smart lsi novel set queries 
msm successfully generalized cisi cranfield databases 
msm generalize med test queries due artificial characteristics database 
recall improvement precision average improvement smart cranfield database msm vs smart lsi vs smart recall msm vs smart lsi vs smart average improvement smart cisi database relative improvement smart improvement precision relative smart precision varies significantly recall level 
msm lsi retrieval methods greatest improvement higher levels recall 
high recall levels performance retrieval methods smart poor query terms occur relevant documents 
indicated improvement averaged test set partitions error bars indicate standard deviation mean 
chapter general non metric approach propose novel method automatically adjusting parameters ranked output text retrieval systems improve retrieval performance 
method explicitly optimizes retrieval system ability rank preferred documents preferred documents 
method involves iterative optimization non metric criterion 
approach derived techniques multidimensional scaling discussed chapter 
addition non metric criterion explicit optimization system ranking performance approach chapter differs chapter number ways 
general method applicable wide range models information retrieval 
method optimize parameters ranked retrieval system long system output differentiable respect parameters 
chapters provide illustrative applications method different problems information retrieval demonstrate method utility generality 
second approach assumes relevance judgements documents arbitrary quasi orders 
allows richer description relevance possible strict boolean interpretation relevance previous chapter 
method assume complete relevance judgements documents available 
document preference judgements available may incorporated optimization 
demonstrate chapters approach works available relevance judgements 
state art document retrieval systems large number free parameters weights terms documents parameters similarity metric retrieval thresholds goal design retrieval system set adjust parameters tune system better retrieval performance 
example ranked output retrieval systems system implements ranking function orders documents estimated relevance user query 
adjusting system parameters typically results alternative orderings system 
goal adjust parameters possible range possible queries ordering documents system corresponds actual ordering document relevance user need 
extremely difficult task large number parameters obscure relationship parameter values system performance 
system may designed improve performance small sample set queries especially difficult adjust parameters confidence adjustments result equally superior performance documents queries considered system designer 
addition characteristics system may change time parameter settings initially useful 
propose computational framework explicitly optimizes ability retrieval system rank documents finite set users queries 
set queries users judgments desired ordering retrieved documents system optimized match desired ordering 
retrieval system optimized automatically adjusting certain free parameters system 
parameters adjusted numerically optimizing measure system ranking documents respect users target ranking set queries 
optimizing criterion automatically adjusts parameter values system ranks documents similarly users preference order 
immediate goal method match users document preferences ranking relevant documents relevant ones finite set labeled queries 
goal may may achievable depending retrieval system expressive generate desired ranking users preferences correctly interpreted ordering retrieved documents 
case desired document ranking completely satisfied method finds solution superior average set queries 
accomplished soft satisfaction competing document preference constraints 
larger goal method find system parameter settings result improved performance novel queries ones considered optimization 
ability system generalize new queries optimized system greater value unoptimized system 
general difficult problem determine system generalize 
depend optimized queries representative general class user queries underlying retrieval model useful 
approach allow query samples available contribute determining high performance parameter settings increase likelihood generalization 
addition empirical evidence method generalizes applications learning similarity measure vector space model retrieval learning high performance combination different retrieval systems described chapters 
parameter adjustment document rank optimization propose method optimizing system parameters explicit optimization document preference relation sample set queries 
heuristic search methods identified chapter method searches subset large infinite space system parameter values 
method guaranteed find optimal parameter values considering possible values 
method suffer limitations methods chapter 
particular method applicable optimizing large class system parameters method explicitly exist values parameter allow system rank documents perfectly 
optimizes correctness system document ordering method sufficiently efficient useful large set problems information retrieval 
overview approach define criterion function measures system performs 
system ranking documents respect user desired ordering criterion function evaluates low value system ranking documents poorly criterion evaluates higher value 
objective adjust system parameters minimize criterion improving system ability rank relevant documents relevant ones 
criterion function selected motivated statistics rank order techniques field multidimensional scaling mds reviewed chapter 
key characteristic criterion differentiable respect relevance scores generated retrieval system 
differentiability exploited heuristic search method employed local hill climbing gradient descent 
criterion optimize system parameters system ranking function differentiable 
ranking problem text retrieval recall dissertation critical feature ranked retrieval system user query system assigns score document indicating system estimate relevance document query 
scores collection documents ranked response user query order decreasing relevance estimate 
retrieval systems scores positive real values zero indicating document estimated relevant retrieved non zero scores indicating increasing levels estimated relevance document 
goal set parameters system scores generated system rank documents closely possible user actual document preferences 
theta real valued ranking function implemented text retrieval system optimized 
theta provides single relevance estimate document query theta called ranking function scores determine order retrieved documents user 
large values theta imply retrieval system estimates relevant user query small values imply relevant 
course particular values generated theta interest rank order documents implied values important 
assume theta generates real values example binary scores relevant irrelevant 
difficult assumption meet ranked retrieval systems generate real valued scores 
theta theta set parameters ranking function 
theta weights terms documents parameters vector space similarity function weights combined give score main limitation parameters theta differentiable respect 
theta theta defined 
system parameters optimized assumed held constant optimization included theta 
definition ranking function theta meant quite general common models ranked output text retrieval vector space model probabilistic retrieval spreading activation models included instances theta 
example consider known vector space model vsm 
model documents queries represented weighted vectors dimensional space 
relevance document query estimated vector similarity vectors 
relative estimate course comparison similarities determine document estimated relevant query ranked system 
vsm theta calculates exactly estimate relevance measuring similarity document query vectors derived parameters vsm model include things term weights parameters similarity measure features term stemming algorithm 
parameters ranking function differentiable term weights features stemming algorithm included theta 
note ranking function may information retrieval environment order estimate document relevance 
example robertson bookstein observed truly estimate usefulness document user system knowledge documents user 
current document may useful redundant previously seen documents 
defined theta function theta necessary features retrieval system estimates adaptive method applicable 
preclude features 
satisfying document preference relation goal find values theta theta best ranks documents set queries 
notion best ranking respect desired ordering documents query 
desired ordering formalized document preference relation wong yao earlier chapter 
document preference relation denoted binary relation document pairs query quasi order document pairs defines preference structure documents query documents collection documents user prefers quasi order model wide range user preference structures 
example familiar boolean classification documents relevant irrelevant sets trivial quasi order relevant documents preferred irrelevant documents 
quasi order place strict requirements user example order documents respect documents case total ordering documents 
user need order documents way necessarily addition documents collection need appear relation 
partial incomplete relevance feedback information 
theta successfully ranked documents query ordering implied values theta correspond user preference partial ordering 
method heuristically search space parameters seeking theta theta theta note particular value theta query document irrelevant relative order theta values concern 
assume set training queries provided realistic assumption feedback users system frequently automatically captured translated preference constraints 
document relevance theta high medium high low low inferred preference relation theta gamma delta gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma delta gamma gamma delta gamma example evaluation point alienation example evaluation point alienation criterion theta collection documents single training query 
documents perfectly ranked theta evaluates relatively low value 
case theta exists provide perfect ranking 
equation may perfectly attainable 
case method attempt find theta matches target ordering possible 
main criterion approach optimize criterion measures ordering implied retrieval function theta corresponds target document preference relation criterion derived guttman point alienation measure earlier chapter 
statistical estimate rank correlation variables 
criterion theta gamma dq theta gamma theta theta gamma theta set training queries collection documents function average queries rank match theta documents note theta perfectly orders elements respect numerator denominator equivalent sum differences theta gamma theta equivalent absolute value sum differences 
case ratio criterion takes minimum value gamma 
theta completely numerator negative denominator ratio gamma criterion maximized 
goal minimize criterion 
notation theta denote ensemble relevance scores calculated queries documents notation theta denote subset theta pertaining query provides small example evaluation assume documents collection highly relevant single query training set somewhat relevant relevant 
point alienation measure system ranking documents respect target ranking 
retrieval system set documents queries gives single number indicating system value collection 
point alienation different traditional measure average precision evaluate retrieval performance 
point alienation generally alternative average precision performance evaluation measure 
advocate point alienation way unfortunately clear interpret value generated point alienation 
average precision know average range recall levels half documents retrieved set relevant 
useful knowledge 
point alienation nonmetric criteria intuitive interpretation 
furthermore clear value constant interpretation number documents changes target relevance judgements change 
measure average precision interpretation regardless changes 
point alienation range values varies quite dramatically features change 
course relative comparisons quite effectively chose different systems training data 
method effective relative judgements systems 
final note point alienation turns point alienation fact relevance information valid quasi order 
equation preference relation explicitly assume valid quasi order 
interesting point suggests method contradictory relevance judgements users 
example user state prefer document prefer document constraints incorporated optimization 
course may difficult method satisfy constraints short making relevance scores documents identical 
point alienation certainly oriented relevance judgements valid quasi orders 
performed experiments investigate behavior ambiguous data advocate situations 
alternative criteria placed primary emphasis point alienation criterion optimized saw chapter number criteria 
criteria proposed literature multidimensional scaling statistics rank order 
include pearson correlation rank images guttman monotonicity coefficient kruskal stress functions different normalizations 
chapter examine performance simple task meant validate approach 
see point alienation reasonable choice criterion 
alternative criteria derived mds methods statistics rank order 
example perceptron learning rule known criterion neural network literature measure rank correctness done wong yao adaptive information retrieval approach 
addition squared error measure fuhr buckley option applicable boolean relevant irrelevant relevance assessments general quasi orders advocate 
examined measures results reported chapter 
numerical optimization method gradient optimization theta respect theta provides method optimize parameters system 
chain rule theta theta theta theta delta theta theta equation provides blue print optimizing arbitrary theta respect theta determine expression theta theta gradient methods theta theta determine optimum 
blueprint applications reported chapters dissertation 
differentiable theta critical points derivative degenerate purposes optimization 
critical point occurs theta equivalent extreme condition limiting factor simulations 
prevalent critical point occurs theta theta derivative required 
case define gamma effect forcing theta strictly greater theta approach dealing discontinuity motivated rosenblatt perceptron learning rule 
learning rule described chapter discontinuity linear output network equivalent network threshold 
rosenblatt defines gradient pathological point network weights tend move away point solution region 
theta differentiable respect theta number standard numerical optimization methods function gradient information 
gradient descent conjugate gradient great success 
initially concerned conjugate gradient optimization criterion differentiable 
conjugate gradient optimize similar criterion function non metric discriminant analysis task reported difficulties 
experience criterion optimized quite effectively conjugate gradient 
optimizations reported dissertation conjugate gradient reported 
conjugate gradient advantages known available publically purposes parameter free method 
limitation numerical optimization methods advocated may find global optimum criterion function 
essentially local hill climbing find optimal parameters local starting point 
may get caught local hill finding true optimum 
problem general significant problem applications dissertation 
event local minima cause problems optimization methods alternatives available simulated annealing increase likelihood finding global optimum typically greater computational expense 
additional limitation optimization methods may fit training samples 
optimization may find parameters provide performance training samples brittle generalize novel queries 
problem may especially manifest model theta overly expressive able readily fit samples 
techniques exist alleviate problem cross validation set samples determine terminate optimization adding noise optimization 
fitting problem experiments reported dissertation due limitations retrieval model chosen 
chapter examine possible involving retrieval models parameters generally dissertation 
models fitting may factor 
training efficiency optimization theta numerical methods involves repeated evaluation theta number different parameter settings 
critical theta theta reasonably efficient evaluate 
course theta actual retrieval system 
speak efficient evaluation say may necessary invoke entire retrieval engine time theta evaluated 
case experiments reported dissertation example 
result considerable practical savings 
criterion chosen point alienation reasonably efficient evaluate 
number training queries number documents evaluation worst delta delta denotes worst case time complexity algorithm 
worst case better scenarios possible 
example may limit imposed system design number equivalence classes frequent case documents relevant relevant query preference relations documents 
case evaluation delta rel delta rel maximum number documents relevant query maximum number documents relevant query 
number documents relevant query typically small relative size collection rel log rel constant evaluation delta delta log delta respectively 
addition number retrieval models investigated vector space similarity measure reported chapter set retrieved documents remains fixed regardless parameter values 
changing theta affects ordering retrieved documents change 
case optimization take place smaller set retrieved documents collection resulting time savings 
number criteria examined addition point alienation measure favorable evaluation bounds 
particular measures rank image relevance scores calculate goodness variation pearson correlation coefficient guttman monotonicity coefficient worst case order delta delta log worst case delta 
measures apparently applicable restricted class document ordering relations total order 
emphasize point alienation applicable wider range situations 
analysis useful determine bounds approach computation time scale large document collections 
useful consider raw computation times particular problems get understanding practical cost method real problems 
provide characteristic computation times apply method retrieval problems chapters come 
summary precision system parameter theta theta avg retrieval precision vs hypothetical system parameter precision vs hypothetical system parameter plot precision versus hypothetical system parameter weight term document fixed recall level 
true precision values horizontal line segments discrete system parameter changes precision discontinuous jumps values rank order retrieved documents changes 
particular parameter value difficult determine increase decrease parameter value result improved performance 
precision difficult optimize numerical techniques 
computation time quite reasonable 
short seconds long number hours database documents low workstation 
variation depends primarily relevance information available complexity model optimized 
little relevance information optimization time takes seconds results terms improved retrieval precision novel set queries 
optimize average precision 
general alternative optimizing point alienation directly optimize standard measure performance information retrieval average precision 
approach involve sampling number different parameter values accepting settings result highest level system performance 
parameter values sampled randomly resulting monte carlo search parameter space hypothesis optimization algorithm suggests particular settings 
approach valid alternative method proposed 
note average precision measure amenable gradient numerical optimization proposed criterion 
reason amenable illustrated 
difficulty arises typical measures system performance precision recall values discontinuous step functions 
result discrete form precision recall measures 
example precision defined precision takes values integer multiples 
gradient particular parameter setting provides information direction parameters changed order improve system performance 
course gradient optimization game town 
variety techniques available directly optimize criterion gradient information 
genetic algorithm methods discussed chapter exemplify approach downhill simplex method direction set methods 
average precision step measures optimized directly 
press suggest gradient information helpful speeding optimization examined available 
arbitrary problem extra cost calculate gradient result savings computation 
gradient method propose practical alternative methods demonstrate chapters certainly replace methods 
number different methods examined particular task 
discussion section discuss number strengths proposed optimization method 
addition explicit number assumptions technique may limit applicability certain environments 
general approach foremost strength method general purpose automatic approach finding high performance settings system parameters 
feature result explicitly formulating optimization problem solely task improving ranking performance system 
general framework allows wide variety retrieval systems parameterizations manipulated long system ranking function differentiable respect parameters 
suggested potentially large class possible system parameterizations including things document query term weights parameters similarity measure weights combine retrieval sub experts 
addition general notion document ordering allowing arbitrary quasi order set documents provides framework general handle typical preference structures advocated information retrieval example boolean relevant irrelevant classifications total preference orderings documents 
quasi ordered relevance ir pointed non metric method chapter assumes relevance judgements quasi ordered boolean 
argued previously chapter quasi ordered relevance expressive formulation boolean relevance 
allows documents relevant query graded continuous way 
strength documents need comparable documents quasi ordered 
example user may unable pick documents preferred quasi order force arbitrary distinction 
documents may relevant doc doc doc doc doc doc doc doc doc doc doc doc quasi ordered preferences total ordered preferences quasi ordered relevance judgements quasi ordered relevance judgements illustrated directed graph left 
arrow represents preference document 
quasi order easily mapped real number line retrieval system 
ordering constraints added number line original quasi order 
user different reasons user need arbitrarily score scale 
point retrieval system able fully support quasi ordered notion relevance 
reason retrieval system providing single score document order documents single list 
linearly ordered output retrieval system implies documents comparable quasi ordered constraints require 
illustration dilemma 
limitation fact retrieval model adding inter document constraints may exist 
reason clear quasi order trivially mapped totally ordered set real number line violating ordering constraints ordering constraints added partial order result process 
limitation derived definition ranked retrieval systems rank documents order estimated relevance user 
definition ranked retrieval system constrained quasi ordered notion relevance 
important implication limitation demonstrates shortcoming standard ranked text retrieval approach 
shortcoming ordered list restricted unambiguously represent possible preference relationships query documents 
quasi ordered approach allows documents respect standard text retrieval model 
number alternative retrieval models proposed represent retrieved documents simply ordered list necessarily suffer limitation 
examples include belew adaptive information retrieval air model chapter rose derived model relationships retrieved documents query terms displayed user 
example visualization example vibe system semantic influences inter relationships documents displayed graphically user 
remains interesting topic research 
soft constraint satisfaction numerical methods combination criterion scores possible parameter settings allowed soft satisfaction competing document preference constraints 
particularly important perfect solution exists retrieval model unable satisfy constraints 
case method finds best solution ranks documents average 
addition numerical method advocated conjugate gradient shelf essentially parameter free application 
contrast number alternative adaptive approaches require system designer specify values number adaptation parameters 
importance document rank order potentially limiting assumption method document rank order critical feature interest regards scoring documents valuable adjust system parameters optimally rank documents respect target ordering 
contrary assumption retrieval systems proposed emphasize alternative methods displaying retrieved documents linear list 
retrieved documents displayed user sorted list proposed method may applicable system differentiation system generated relevance estimates 
addition method remains natural way incorporate user relevance feedback adaptive system 
emphasis rank order departs number typical methods adaptively incorporating relevance assessments system operation 
exemplified ide rocchio numerous methods relevance assessments absolute measure relevance 
example documents similar query time relevant irrelevant 
modification performed irrespective relationship documents query 
approach advocate hand considers relevance assessment relative statement document relevant absolute sense query relevant query certain documents relevant query certain 
relevance assessments interpreted full context retrieval multiple documents user relevant user need 
practically allows user specify far richer preference structure retrieved documents possible binary relevant irrelevant scheme 
average performance queries assumption optimizing average agreement target rank order useful 
criterion assumes query contributes equal constraints system ranking function 
soft satisfaction potentially conflicting constraints optimization may find parameters best average quite unsatisfactory individual query 
extreme example case completely identical queries training set identical users identified opposite document preference relations queries 
obviously case best average parameters satisfy users 
number possible responses criticism 
value averaging utility producing parameter settings generalize new queries 
grateful optimized system responds better novel queries system respond better average collection new queries 
desirable learn parameters optimized subset training queries perform average training queries representative novel queries system performance worse sufficiently large collection novel queries 
second response specialized performance required single query parameters optimized single query 
general approach known enhance performance 
third response averaging problem avoided cases increasing expressiveness retrieval model 
model powerful differentiate queries require alternative parameter settings 
differentiation parameterized part model system adaptively discovers strategy best query 
remain interested investigating possible pathologies real user relevance data 
minimum non metric method proposed chapter provides computational formalism explore issues 
restriction differentiable models final assumption approach interesting models information retrieval parameterized 
proposed model technique search optimal instance model necessarily constrained expressiveness model 
model technique search takes place scope particular model 
may fact places important emphasis task defining adequate model 
achieve truly exceptional retrieval performance model rich distinctions necessary rank relevant documents relevant ones 
optimization method attempts set parameters achieve goal 
chapters come examine number different retrieval models 
model adaptive similarity measure examined chapter quite limited result greatly improved performance comparable methods 
combination experts models examined chapter result improvements retrieval precision 
summary general method optimizing system parameters ranked retrieval system 
approach methods statistics rank order multidimensional scaling involves numerically minimizing differentiable criterion function 
criterion function measures degree retrieval system mis ranked retrieved documents respect preference constraints derived relevance assessments sample set queries 
minimization criterion effect improving ranking performance system 
numerous adaptive techniques information retrieval approach applicable optimizing large class system parameters document ranking function differentiable 
chapter validation non metric approach non metric adaptive approach previous chapter examined greater detail applying practical illustrative problem information retrieval 
problem estimate similarity measure vector space retrieval system measure relevance document query vectors 
goal chapter primarily validate technique simple interesting problem compare numerous alternative techniques 
validation involves comparing criterion typical measure performance text retrieval average precision verifying criterion remains effective face varying retrieval conditions 
conditions include varied document collections varied numbers documents training varied numbers queries training 
find method able adapt changing conditions quite 
example able find similarity measure particularly suited correctly ranking relevant documents ranking documents training environment calls 
comparison number alternative non metric criteria common multidimensional scaling 
addition method compared alternatives motivated current adaptive methods text retrieval wong yao perceptron adaptive method fuhr buckley probabilistic learning method squared error criterion 
cases approach compares quite favorably 
determining similarity measure chapter apply method problem finding best similarity measure vector space retrieval 
problem interesting historical pragmatic reasons 
historically large number similarity measures proposed mcgill jones furnas contain lengthy enumerations single similarity measure gained universal acceptance 
fact jones furnas suggest certain characteristics retrieval environment size database type database nature user community affect similarity measure results best performance 
experiments reported example choice documents simply highest ranked documents need correctly ordered user significantly affect measure best 
single theta theta standard similarity measure inner product cosine pseudo cosine number terms table classic similarity measures certain values theta parameterized similarity measure theta identical certain classic vector similarity measures typically text retrieval 
note theta finite value theta yields inner product measure 
measure best situations measure chosen particular characteristics retrieval environment 
pragmatically selection similarity measure significant effect performance system 
ranking performance vary significantly switching different measures example harman demonstrated improvement average precision switching different normalized inner product measures similar differences observed experiments reported 
factors suggest method automatically learns similarity measure relevance feedback quite useful 
model vector space model vsm text retrieval documents queries represented vectors vector similarity measure determine relatedness 
typically measure retrieval system selected empirically set standard vector measures inner product cosine 
depending measure documents ranked differently query results differing levels performance 
demonstrate proposed parameter optimization method learn similarity measure 
apply optimization method retrieval model defined theta specified 
theta parameterization standard inner product measure theta theta theta inner product document length normalization transpose operator query document column vectors derived samples experiments derived query document term frequency inverse document frequency weighting scheme implemented smart retrieval engine 
vectors length normalized parameterized similarity measure perform task 
different values theta instantiate different vector normalizations 
note important term weights negative model summation positive roots defined 
addition summation taken non zero term weights case theta handled naturally 
particular parameterization similarity measures chosen number reasons 
number common similarity measures realizable varying parameter values 
example setting theta cosine measure implemented 
table illustrates number different classic similarity measures realizable parameterized model 
addition model easily amenable analysis having parameters 
allows visualization structure search space dimensional plots 
model non trivial having significant non linearities 
important appreciate search high performance similarity measure take place search space defined particular model equation 
method look absolute optimal measure ranks relevant documents irrelevant ones queries similarity measure realized model chosen 
emphasize importance selecting model optimized model defines space possible solutions expressive result desired level performance 
validation validate usefulness non metric criterion examining performance number varied retrieval conditions 
perform validation different methodologies 
involves examining optimization surface criteria varied conditions 
analysis gives provides visualization performance system parameters varied 
second involves numerical optimization criterion determine actual system performance 
characteristics search space analyze form search space determine shape peculiarities 
method analyze space evaluate theta large number systematically sampled values theta 
value theta plotted sampled value 
analysis possible small number parameters model 
complicated models number parameters foil clear visualization systematic evaluation theta prohibitively expensive 
plot theta versus theta provided 
data calculated cisi collection information science abstracts 
documents collection sample queries 
sample query set relevant documents provided information construct preference relation plot queries included set equation 
theta values theta evaluated give plot adequate level resolution 
plot illustrates general shape optimization surface generally concave highest point theta broad basin low values middle values theta theta 
alternative way view data remainder chapter contour plot 
contour plot view parameter space 
regions constant height constant values theta illustrated iso altitude lines drawn parameter space 
contour plot data shown provided 
contour plot illustrates relative high values theta broad basin low values 
addition classic measures listed table identified contour plot help ground illustration 
recall objective find theta minimizes theta 
starting set parameter values gradient descent procedure iteratively change theta move hill search space 
minimum find near theta surface criterion height denotes criterion value cisi dataset theta theta surface criterion surface illustrated sampling theta different values theta 
perspective highlights general concave bowl shape lowest identified area contour plot 
note classic measures region cosine appears close minimum 
comparison standard ir performance perform surface visualization standard measures retrieval performance compare results surface produced method 
performance measure average precision calculated averaging precision evenly spaced levels recall 
criterion average precision measures ranking performance system 
average precision differentiable directly criterion numerical optimization average precision commonly reasonably accepted method measuring retrieval performance information retrieval literature witnessed text retrieval conference trec evaluations 
contour plot average precision surface provided 
note average precision preferable large values small 
maximum theta optimum average precision minimum theta 
immediately apparent visually comparing figures structure performance surfaces similar 
similarity clear comparing surfaces directly examining correlation pairs contour criterion cisi dataset theta theta pc ip ip nt cos opt contour criterion contour plot surface illustrated 
parameter settings corresponding classic similarity measures identified nt number terms pc pseudo cosine ip inner product cos cosine 
minimum surface optimal parameter setting identified opt 
parameter samples construct surfaces 
scatter plot criterion versus average precision provided 
parameter settings previous contour plots included plot 
measures highly correlated spearman rank correlation gamma perfect correlation gamma correlation 
note measures negatively correlated low values correlated high values average precision vice versa 
importance high level correlation optimizing criterion result positive changes average precision 
furthermore correlation stronger separation single versus average precision curve curves near gamma plot 
separation result differences way sub optimal parameter values scored average precision 
sub optimal regions near upper lower right corners figures contribute curves 
comparison point alienation average precision demonstrates optimization parameters point alienation result improved average precision performance 
real world application technique problem information retrieval level effort requires expensive exhaustive evaluation parameter space need duplicated 
contour average precision cisi dataset theta theta pc ip ip nt cos opt contour average precision average precision surface structurally quite similar surface previous 
optimization generalization having determined optimization useful endeavor high correlation average precision examine automatic optimization conjugate gradient 
compare performance optimized similarity measure classic measures measure exhaustive search estimated optimal measure 
comparison performed set queries separate optimization parameters 
comparison provides estimate optimized system perform deployed handling novel user queries 
cisi document collection comparison 
recall collection queries relevance tagged documents 
set partitioned subsets training set test set 
training set set queries optimize parameters 
test set reserved subsequent comparison generalization performance 
training set randomly selected queries test set remaining queries 
different similarity measures compared measure optimizing criterion measures classic measures instantiated similarity model cosine inner product pseudo cosine number terms 
fifth measure measure exhaustive search set training queries 
different parameter settings corresponding theta grid evaluated average precision avg precision criterion vs avg precision spearman rank correlation cisi database correlation average precision samples average precision surfaces highly correlated 
correlation negative maxima average precision surface correspond minima parameter values best average precision reported 
sixth measure estimated optimum set test queries 
measure exhaustive sampling test set queries training set 
measure provides theoretical upper bound best performance achievable similarity model test set 
similarity measures compared measure optimizing training set 
optimization performed conjugate gradient starting single non optimal point parameter space 
conjugate gradient required evaluations terminating optimum 
attempt improve efficiency optimization cost exhaustive sampling 
optimization required approximately minutes computation sparc station ipc low workstation 
summary comparison methods provided table 
similarity measure learned method estimated optimal performance test set 
measure outperforms compared similarity measures including measure expensive exhaustive search training set 
limitation comparison potentially dependent particular selection training set test set 
different partition queries training test sets result different comparison 
tested effect test set percent metric theta theta avg precision optimal number terms pseudo cosine inner product exhaustive search training set cosine optimization exhaustive search test set table learning similarity measure results learning similarity measure single random partitioning query set training queries test queries 
learned similarity measure outperforms classic similarity measures studied 
performs estimated optimal similarity measure exhaustive sampling parameter space search best test set average precision performance 
duplicating comparison different random partitions query set 
measure exhaustive search included comparison computational expense calculating 
replications experiment optimize system times starting different parameter settings 
done event local minima search spaces different partitions 
resulting parameter sets parameters result best average precision performance training set selected solution 
measure way optimization query partitions significantly performs pseudo cosine number terms measures test set 
performance statistically differentiable performance cosine inner product indicating similarity measures terms average precision performance change retrieval environment find cosine inner product close optimal measures 
optimization demonstrated automatically finds parameter values result performance better competitive measures common information retrieval 
effect training set size section examine effect training set size varying number training queries evaluate feature training environment important understand large numbers queries relevance tagged documents may available real world situations 
general expect larger number queries optimize parameters parameter values useful generalize new situations involving new queries new documents 
cisi collection total queries corresponding relevance tagged documents 
optimization preceding section queries train test 
fewer compare retrieval performance system trained fewer queries trained 
training set sizes contour criterion number queries training set cisi dataset theta pc ip ip nt cos opt contour training set queries queries training set search structure remains similar search contour training queries 
minimum changed dramatically 

queries chosen random set duplicates 
training set includes queries smaller training set training set size query set size plus randomly selected query 
illustrates performance surface training set size 
visually quite clear surface shares general shape seen queries 
true training set sizes shared general contour having broad region reasonably parameter values surrounded regions poor solutions actual minima training set sizes vary greatly 
performance system varies greatly depending number queries training set 
illustrate train systems different training set sizes 
compare performance performance cosine measure training set 
cosine selected classic measure best average precision performance 
performance optimized systems cosine compared test set documents particular training set 
example training set queries optimized system compared cosine performance queries systems compared remaining gamma queries 
attempt factor bias replicating experiment times averaging results 
complete sequences training sets sets training sets generated 
optimize training improvement average precision cosine set size training set test set table performance limited training set query training set system able highly optimize training set performance expense generalization test set 
queries added training set generalization performance improves 
system different starting points alleviate possible local minima problems select solution system best average precision training set 
results optimization provide table 
queries training set approach estimates parameters perform better cosine trained queries 
somewhat expense optimized system ability generalize test queries 
queries added training set performance training set decreases system able find parameters high performance training set 
expected additional queries place additional constraints target ordering system making harder system specialized high performance larger set queries 
system training set performance declines system ability generalize improves 
interestingly performance differences reported table statistically significant 
really surprise considering small performance differences observed test set large training sets noteworthy considering large performance improvement training queries trained small training sets 
reason variance performance system high small training sets 
example standard deviation training performance sets size sets size order magnitude smaller 
high variance result high volatility performance scores individual queries homogeneous performance queries performance query varies greatly 
performance improvements statistically significant high variance 
clear method able find highly specialized parameters fewer queries fewer constraints 
highly specialized parameters perform better single fixed set parameters cosine parameters queries optimized 
suggests method may useful optimizing parameters single query user relevance feedback order improve subsequent retrieval performance query 
pursued direction current research demonstrated general approach productive 
addition expected system ability properly handle novel queries improves additional queries contribute number performance improvement cosine top ranked average precision precision low recall documents training test training test table performance top ranked documents similarity measure optimizing reduced number top ranked documents generalizes cosine average precision performance 
systems trained top ranked documents perform significantly better low levels recall trained 
parameter estimation 
method able successfully integrate potentially competing constraints provide performance average novel queries 
effect incomplete document information stated earlier point alienation criterion require documents included preference relation algorithm attempts satisfy document ordering constraints available subset documents ordered 
enables algorithm optimize parameters case users examined small subset documents providing relevance feedback 
test effect incomplete document ordering information experiment performed subset document collection ordered training query 
subset documents set documents ranked highest system training query 
top ranked documents simulate retrieval environment users system observe provide relevance information documents estimated relevant system 
set top ranked documents generated finding documents ranked highest system inner product similarity measure 
top ranked documents realistic 
typically difficult get feedback documents users system feedback small number highly ranked documents far users 
evaluating documents query user identifies relevance top ranked documents optimization procedure restricted information calculating parameter values 
course query different set top ranked documents optimization appropriate set query 
number documents examined users 
experiments number varied systematically top ranked documents query top ranked top ranked top ranked documents order examine effect different amounts relevance information 
structure search space smallest case top ranked documents relevance tagged query illustrated 
space general structure space documents tagged actual minima vary considerably 
examine performance systems trained top ranked contour criterion top ranked documents cisi dataset theta pc ip ip nt cos opt contour top ranked documents top ranked documents query search structure remains similar search contour training queries 
minimum changed 
documents referred top ranked systems 
previous experiments different partitions queries training queries test queries order factor bias 
systems optimized top ranked levels system resulting best average precision trained queries 
average precision measure performance top ranked systems somewhat worse performance systems trained documents 
interesting top ranked systems perform considering trained fewer ordering constraints 
training documents average retrieved documents contributing ordering query 
top ranked documents documents accordingly 
average precision measure performance levels recall relevant documents retrieved relevant documents retrieved 
top ranked systems trained small number relevant documents 
corresponds low level recall training 
consider low levels recall top ranked systems trained systems perform significantly better systems trained documents 
illustrated table 
precision recall level measure system performance top ranked documents recall point relevant document appears list retrieved documents 
measure indicates average position highest ranked relevant document 
contour criterion med dataset theta theta pc ip ip nt cos opt contour criterion cranfield dataset theta pc ip ip nt cos opt contours med cranfield databases search contours databases med cranfield similar search contour cisi 
minima identical 
performance improvements recall level system trained top ranked documents test set improvement system trained top ranked documents statistically significant better 
particularly clear example optimized similarity measure vary greatly depending characteristics retrieval environment 
constraints system quite different top ranked documents need correctly ordered opposed complete set documents 
optimization method able adapt different constraints generate similarity measure performs better environment 
case similarity measure optimized provide small higher quality list retrieved documents 
comparison databases analysis search space imposed criterion performed text collections verify earlier results collection dependent 
collections med database medical abstracts cranfield collection 
med database documents test queries cranfield database documents test queries 
illustrates search contours med cranfield collections 
search contours general structure cisi collection minima shifted somewhat 
suggests uniformity considering broad class similarity measures measures generally better 
agreement jones furnas conjecture important differences different collections warrant special similarity measures optimized particular retrieval environment 
addition results point alienation scores highly correlated average precision scores database 
spearman rank correlation med collection gamma cranfield collection gamma 
gives evidence robustness correlation different collections 
comparison alternative methods compare method alternatives motivated approaches proposed wong yao fuhr buckley 
alternatives differ approach choice criterion evaluate performance retrieval system 
wong yao advocated perceptron criterion measure documents ranked 
fuhr buckley suggest squared error criterion measuring deviation document relevance scores irrelevant documents relevant documents 
demonstrate criteria predictors performance parameterized similarity measure examined previous section 
addition compare criterion number alternative non metric criteria multidimensional scaling literature 
criteria identified reviewed chapter 
perceptron criterion objective wong yao method find query document term weights result best ranking documents respect target document preference relation 
application method small text collection database documents queries method learned document term weights resulted average improvement average precision performance 
approach wong yao optimize document preference relation defining differentiable criterion measuring system ranking documents optimizing function gradient methods 
criterion derived perceptron learning rule classic understood learning rules field neural networks 
overview perceptron rule provided chapter 
criterion gamma gammab aq gamma fq gamma aq notation requires clarification 
column vector representing terms query standard vector space methods 
queries taken training set document vectors set documents linear association matrix relating terms queries terms documents 
system estimate relevance document query score matrix product aq transpose operator 
values parameters optimized method 
order interpret compare criterion method derive new criterion theta derived generalizing retrieval model theta theta theta theta gamma theta note generalized specific ranking function wong yao aq ranking function theta 
course simply restrict theta aq theta get strict equivalence theta 
interpretation theta straightforward add criterion ranking function predicts document preferred theta theta user prefers 
amount added error simply discrepancy theta gamma theta 
theta ordered respect criterion minimized 
power wong yao criterion strict linearity document ranking function perceptron learning rule 
provides applicability perceptron convergence theorem guarantees weight matrix exists completely satisfies document preference constraints gradient optimization terminate perfect solution finite number steps 
powerful statement perfect solution exists algorithm guaranteed find global optimum finite amount time 
method proposed involving optimization non linear criterion function known characteristic 
strictly linear ranking function wong yao limitation 
restricted class system parameters specifically query document term weights optimized method 
contrast proposal allows optimization general class system parameters ranking function differentiable 
suggest general optimization system parameters propose plausible criterion define general way 
note generalization perceptron convergence theorem longer known hold arbitrary ranking function parameterization 
investigate utility generalized criterion examine wong yao criterion similarity measure task previous section 
examine theta learning optimal similarity measure ranking function definition provided equation definition generalized criterion equation 
previous section evaluate theta large number systematically sampled parameter settings order determine shape search space 
contour search space illustrated 
theta grid theta values resulting samples search space 
immediately obvious theta useful estimator parameter values model similarity measures surface poorly correlated average precision surface spearman rank correlation recall point alienation correlation gamma best correlation gamma 
minima perceptron criterion relation parameters rank documents 
squared error criterion fuhr buckley proposed method learning combine different term indexing strategies result improved performance 
approach weight contour perceptron criterion cisi dataset theta theta pc ip ip nt cos contour perceptron criterion perceptron error measure useful task learning optimal similarity measure surface shaped steep slide away high performance parameter values 
term occurring document number different standard term weighting methods term frequency tf inverse document frequency idf picking single 
method learns combination weighting methods best estimates probability document relevant query shares terms document 
example assume term occurs particular document user query 
number index weights computed corresponding tf idf schemes 
vector represent set weights weight element vector 
function generates polynomial combinations weights example polynomial combinations correspond combinations original indexing methods idf theta tf 
objective method learn linear coefficients elements delta order best estimate probability document relevant query containing term learning accomplished optimizing squared error criterion gamma column vector representing linear coefficients estimated method 
value single score representing combined evidence different indexing methods goal value estimate probability document relevant query contains term target probability estimate query binary variable value document relevant query 
criterion measures squared deviation system estimate document relevance single term actual relevance document query retrieval document relevance score weighted sum individual index weights terms occurring query 
notation ranking function score theta theta indicates importance defined user inferred query term query 
note perceptron criterion point alienation squared error criterion optimize arbitrary quasi ordered user preferences 
binary relevant irrelevant relevance judgements supported method 
important positive feature method optimal linear coefficients calculated directly solving associated set linear equations numerical minimization criterion 
wong yao perceptron criterion criterion motivates alternative criterion generalized ranking function optimize arbitrary system parameters 
criterion theta gamma theta criterion measures squared deviation retrieval system relevance assessment user relevance assessment 
observe equation generalization equation fuhr buckley criterion system relevance score optimization generally optimizing correct ranking candidate documents 
examine utility generalized squared error measure task learning optimal similarity measure 
perceptron criterion previous section sample theta theta different theta values determine criterion shape 
relevance assessments queries cisi collection binary arbitrary quasi orders difficulty collection criterion 
contour squared error criterion similarity measure task illustrated 
perceptron criterion squared error surface mimic desired performance surface defined average precision 
spearman rank correlation squared error average precision 
squared error useful predictor average precision performance similarity measure task 
analysis immediately clear alternative criteria useful estimating optimal similarity measure criteria directly measure performance system task correctly ranking documents relevant relevant 
case perceptron criterion value theta highly dependent scale values theta assumed chapter rank order documents critical feature interest 
rank order contour squared error criterion cisi dataset theta theta pc ip ip nt cos opt contour squared error criterion squared error criterion useful task learning optimal similarity measure 
surface marginally bowl shaped optimal parameters minima squared error criterion away true average precision optimum 
change theta values uniformly change scale values rearranged respect 
theta increased factor criterion value theta increases factor simply rescaling relevance scores apparent performance system changed fold 
consequently minimize theta optimization attempts reduce scale theta values necessarily improve rank order 
achieved similarity measure model maximizing theta theta parameter values 
clearly solution leads degenerate performance 
squared error criterion fails task similar reasons 
criterion artificially requires relevant scores irrelevant scores 
infinite number different relevant irrelevant scores provide ranking scored quite differently deviation target values 
original application fuhr buckley approach appears difficulty linear model scale translate solution range targets 
case model similarity measures examined chapter model permit arbitrary scaling translation estimated relevance scores 
comparison point alienation criterion function proposed invariant contour pearson criterion cisi dataset theta theta pc ip ip nt cos opt pt avg precision pearson vs pt avg precision spearman rank correlation contour pearson criterion contour pearson criterion correlation average precision 
scale translation ranking function 
addition place arbitrary requirements target relevance scores generated system squared error criterion scores correctly rank documents identically satisfactory regardless particular values 
critical features applying method non linear ranking models parameterized similarity measure examined 
non metric criteria identified number non metric criteria chapter addition point alienation 
alternatives include pearson pearson correlation measuring statistical correlation model relevance scores rank images guttman guttman monotonicity coefficient statistical correlation rank images kruskal norm kruskal stress standard normalization involving sum squared relevance scores kruskal variance kruskal stress variant normalization involving variance relevance scores 
section examine criterion performance comparing average precision previous sections 
results comparisons illustrated figures pearson guttman kruskal norm kruskal variance 
observation measures fairly closely mimic average precision contour 
contours shallow basin contour guttman criterion cisi dataset theta theta pc ip ip nt cos opt pt avg precision guttman vs pt avg precision spearman rank correlation contour guttman criterion contour guttman criterion correlation average precision 
reasonably parameter values left steep slopes upper lower right leading away optimal parameter values 
furthermore correspondence criterion values average precision high cases measured spearman rank correlation illustrated scatter plots 
selected point alienation principal criterion optimization number reasons 
alternatives highly correlated average precision measured spearman statistic 
justification fairly weak criteria highly correlated selection clear cut 
addition places great deal emphasis average precision correct thing discussed earlier question 
reasons prefer point alienation 
point alienation reasonably efficient observed chapter especially relevance judgements constrained common boolean variety 
addition point alienation known statistical measure successfully number different fields smallest space analysis non metric discriminant analysis 
lastly handles arbitrary quasi orders quite naturally 
criteria designed measure total orders documents orderable respect documents query 
argued chapter quasi ordered model appears expressive judging relevance documents queries 
believe criteria may interest application tasks 
particular point alienation may correlated average precision criteria current task may 
addition criteria correlated average precision point alienation may correlated equally reasonable measure performance information retrieval average fallout 
believe criteria ignored research investigate possible utility 
contour kruskal norm cisi dataset theta theta pc ip ip nt cos opt pt avg precision kruskal norm vs pt avg precision spearman rank correlation contour kruskal norm criterion contour kruskal norm criterion correlation average precision 
discussion section discuss number strengths non metric algorithm specifically context application learning similarity measure 
addition explicit number assumptions technique may limit applicability certain environments 
generalization performance importantly method demonstrates generalization performance tested novel queries 
critical optimized system replace unoptimized system expected perform better 
addition high correlation point alienation measure average precision standard measure performance information retrieval 
correlation high test collections examined cisi cranfield med 
feature criterion surface nicely bowl shaped 
particularly exciting non linearities parameterized similarity measure 
specific similarity measure model positive results lend optimism suggestion method may practical wide range non trivial non linear problems information retrieval 
incomplete information positive feature method applicability environments partial relevance information available 
demonstrated application similarity measures criterion surface maintains fairly uniform structure regardless number training queries number documents included preference relation performance improves amount information increases 
contour kruskal variance cisi dataset theta theta pc ip ip nt cos opt pt avg precision kruskal variance vs pt avg precision spearman rank correlation contour kruskal variance criterion contour kruskal variance criterion correlation average precision 
user need specify true relevance document retrieved query method ordering information just subset documents 
correlation average precision assumption demonstrated correlation average precision thing 
assumption central selected different alternative criteria including non metric criteria criteria text retrieval correlates average precision 
clear system evaluated different measures correctness order get complete picture system performance 
example suggests cost theoretic analysis finds different values precision recall depending user cost scheme 
addition number different measures identified van rijsbergen 
approach alternatives 
example may possible augment criterion quantifications alternative costs 
assumed worthwhile goal attempt improve average precision performance 
approaches may necessary significantly different measure performance adopted 
technical assumptions number technical assumptions important identify 
method proposed optimization making gradient information preferred explicit optimization standard non differentiable measure average precision 
press suggest optimization techniques gradient information powerful techniques information 
gradient techniques powerful compensates additional computational cost calculating derivatives 
ultimately depend details particular model optimized 
assumption critical singularities theta theta equivalent documents generally encountered 
experience parameterized similarity measure assumption valid 
summary examined performance novel method optimizing performance retrieval system 
method chapter uses non metric criterion adjust system parameter values result improved ability rank relevant documents irrelevant ones 
criterion examined estimate high performance similarity measure vector model information retrieval 
criterion highly correlated typical measure performance information retrieval average precision 
correlation robust number variations retrieval environment including reduced training set sizes varied document collections 
suggests optimizing criterion improve system average precision performance 
non metric criterion compared number different alternatives including approaches text retrieval literature multidimensional scaling 
criterion compares favorably cases 
chapter mixture experts chapter presents application non metric algorithm introduced chapter 
application combine number different retrieval algorithms single retrieval system 
retrieval performance improved significantly number different retrieval algorithms combining results contrast just single algorithm 
different retrieval algorithms retrieval experts emphasize different document query features determining relevance 
retrieve different sets documents sum worth parts 
number approaches previously proposed combining experts system 
review approaches identify number reasons approaches combine large class retrieval experts 
non metric learning algorithm provides alternative limitations 
apply method expert combination tasks 
application demonstrate method provides novel way analyze performance experts allowing determine effectiveness experts conjunction experts isolation 
second application involves combination experts commercial retrieval system 
case method yields combined algorithm performs better average precision single expert ranking algorithm 
text retrieval heads definitely better 
large body research demonstrated retrieval performance greatly improved number different retrieval algorithms experts combining results contrast just single retrieval algorithm 
expert contributes estimates documents relevant user query combined set typically valuable single expert estimates 
belkin offers reasons improvement individual retrieval system address portion complexities retrieval additional systems increase coverage complexities 
second expert relevance estimates may interpreted sources evidence true relevance 
additional experts provide additional evidence resulting accurate estimate true probability relevance 
principal difficulty combining experts deciding multiple experts combined generate retrieval score 
non metric criterion earlier automatically learn combination training examples performs 
technique applicable combining experts ranked retrieval systems expert estimate relevance documents queries rank documents estimates 
systems method determines combination experts combines individual estimates single estimate ordering 
demonstrate automatically derived combination experts performs better individual experts applied different problems 
additionally demonstrate method useful novel analytic tool allowing system designer determine utility different experts combination isolation 
observation combined evidence multiple retrieval systems outperforms single system numerous researchers 
part performance benefit may arise fact different retrieval systems simply retrieve widely different sets documents query 
experts retrieve documents perceived performance combined system may higher increased levels recall attainable 
numerous examples increased recall compared retrieval performance variety different document representation methods example terms title terms terms chosen indexer different sets documents retrieved different representation methods experts single method greatly outperforms 
results echo findings earlier study mcgill 
study human searchers varying conditions automated methods find relevant documents 
little overlap sets documents retrieved query topic 
harman observed systems text retrieval conference trec performed approximate range retrieved significantly different sets documents set test queries 
different retrieval methods retrieve different sets documents leading higher achievable recall combined 
performance benefit attributed enhanced recall combined system 
example saracevic kantor demonstrated odds document relevant query increase monotonically number experts retrieve document 
landmark empirical study different searches performed number experimental queries 
documents retrieved query retrieved searches retrieved higher probability relevant 
example document retrieved times times relevant document retrieved search 
additional research demonstrated fairly convincingly combining number different retrieval algorithms result performance improvements 
example turtle croft promote method combining evidence different experts inference network 
experts generate document relevance estimates propagated network combined probabilistic inference rules form improved estimates 
model combine retrieval experts probabilistic indexing model boolean model 
approach results improvement average precision probabilistic model depending collection cisi cacm boolean query formulation 
study belkin provides evidence combination retrieval strategies useful 
demonstrated different query formulations combined result better performance single formulation 
addition finding important observation formulation contribute improved performance combined system performs poorly 
method apparently poor job retrieval improve performance system combined experts 
consequences observation significant candidate retrieval method discounted simply perform relative alternative methods 
method enhance performance combined 
poses dilemma system designers candidate method evaluated determine added system simple evaluation method comparison techniques predict actual utility 
approach potentially offer insights dilemma offers method analyzing expert performance combined experts system 
addition performance benefit attained combining experts potential benefits architectural design system 
difficult design maintain single monolithic retrieval system support alternative approaches considerations necessary state art performance 
contrast expert approach allows decomposition monolithic system specialized modules knowledge particular restricted aspects retrieval problem 
example expert specialized linguistic knowledge useful analyzing queries rank small set documents highly meet strict requirements relevance 
second expert traditional vector space retrieval system 
third synonym knowledge expand certain terms query increase recall 
appropriate experts designed maintained quite separately estimates combined retrieval time 
addition design benefit expert architecture allows new retrieval technology seamlessly integrated system outdated technology removed 
convinced combining different experts quite useful problem remains experts best combined generate relevance estimate 
ranked retrieval systems problem different ordered lists documents expert generates ordering best merged result preferable ordering 
ordering clearly simple union different retrieved sets take account information expert past performance contributed system performance 
review number methods section argue current approaches important limitations 
foremost current approaches specific particular kinds experts experts estimating probabilities relevance applicable arbitrary ranked retrieval experts 
addition methods experts past performance explicitly improving system performance improving ability rank relevant documents irrelevant 
approach addresses limitations 
approach example queries optimize system ability correctly rank documents relevant irrelevant 
method non metric adaptive approach examined previous chapters 
approach combined experts past performance order estimate correct combination queries 
approach variety different retrieval experts essential characteristic experts implement ranked retrieval function 
ensemble relevance estimates experts set documents order documents user relevant relevant 
discussing method related approaches greater detail sections apply approach expert combination problems 
applications method automatically finds combined system outperforms individual retrieval experts 
addition design experiments performance improvement result enhanced recall 
improvement due entirely automatically derived combination experts 
applications involve commercial text collections application involves commercial retrieval system 
demonstrates utility method real world applications 
addition performance experiments demonstrate method analyze expert performance 
provide response belkin dilemma individual experts may perform poorly positively contribute system performance 
methods combining experts section review number current approaches information retrieval combining expert estimates single improved system 
emphasis identifying strengths limitations proposed techniques 
review relevant literature neural network pattern classification community 
literature addresses problem combining experts static adaptive supervised learning problems 
generally applicable ranked retrieval task approaches offer certain useful insights 
manual search common approach combining expert opinion information retrieval manual search small set possible combination strategies 
researcher system designer evaluates candidate combination strategies set queries relevance tagged documents 
combination strategy resulting best performance selected 
examples candidate strategies average experts estimates differentially weight contribution expert intuitive random estimate utility 
typical approach belkin analysis alternative query formulations trec database lewis croft comparison term phrase concept clustering approaches kwok weighted unweighted averages retrieval experts fox similarity merge approach document relevance score maximum expert relevance estimates 
addition turtle croft propose inference network approach natural framework combination expert evidence 
experts embedded inference net evidence propagates combined determine relevance estimate 
turtle croft manual search find best weighted average experts 
manual search approach quite reasonable motivation choosing particular strategies examined 
example intuitive appeal fox similarity merge approach kwok unbiased averaging approach 
general manual search appears quite limited 
comparison typically limited possibilities system designer willing examine seldom guidelines selecting potent strategies 
weight performance alternative manual search approach weight experts individual utility 
expert evaluated independently determine relative performance level 
retrieval experts relevance estimates combined proportion performance level 
approach thompson attempt determine upper bound set experts perform combined 
fox complicated variation theme 
weighting experts past performance intuitively reasonable experts perform best contribute relevance estimate 
practice approach limitations 
belkin pointed expert performs poorly positively contribute ranking 
weighting experts individual performance clearly address perform 
thompson combined experts weighted performance level performed better combination uniform weights 
fox report similar failure improve performance best individual expert 
experiments demonstrate striking example situation example combination worse experts weighted marginally higher expert performs better experts individually 
thompson ceo model thompson earlier combination expert opinion ceo model clear potential usefulness task 
ceo bayesian model combination evidence set experts 
experts combined yield reliable estimate document relevance query 
model allows hierarchy experts expert relevance decisions evidence provided sub experts expert prior document relevance 
model potential derives foundation defined probabilistic models combining evidence 
unfortunately model limitations may restrict wide applicability problems ranked text retrieval 
model combine experts generate estimates probability document relevant query general measure utility interpretable probability 
unfortunate considering current models retrieval probabilistic vector space weighting methods spreading activation approaches 
second method numerous assumptions experts 
foremost experts statistically independent difficult assumption satisfy practice 
continues model generally applicable 
probabilistic learning fuhr buckley proposed method learning combine different term indexing strategies result improved performance 
approach specific combining term indexing strategies approach generalized combine arbitrary experts 
approach weight term occurring document number different standard term weighting methods term frequency tf inverse document frequency idf picking single 
alternative weightings experts 
method learns combination weighting methods best estimates probability document relevant query shares terms document 
fuhr buckley approach limited learning algorithm explicitly improve system ability rank documents correctly 
algorithm optimizes term document individually estimates probability document relevant query containing term 
retrieval individual term estimates combined give estimate relevance 
optimization operates individual components system optimize system estimates relevance 
furthermore demonstrated earlier chapter generalization learning algorithm optimizes relevance estimates accurately optimize system ability rank documents effectively 
summary ir approaches summary quite approaches available determining best combine evidence multiple experts 
number manual approaches intuitively appealing expert averaging weighting past performance justified general 
automatic methods proposed thompson fuhr buckley great potential limited 
thompson approach specific probabilistic estimators restrictive assumptions experts particular concerning statistical independence 
fuhr buckley approach explicitly improve system ability rank documents 
neural network mixture experts combination multiple experts solve complex problems received considerable attention neural network pattern classification fields 
essential idea difficult classification problems combination methods best 
typically methods single parametric form method tuned give high performance particular sub task problem 
examples approaches include classification regression trees cart multivariate adaptive regression splines mars 
methods decompose large multivariate problems subregions solved single regression function expert terminology 
classify novel pattern procedure finds expert associated subregion containing pattern uses expert classify pattern 
limitation methods strictly partition input space subregions novel pattern handled single expert 
promote generalization experts 
alternatives cart mars mixture experts approaches neural networks see earlier mixture methods unsupervised distribution estimation tasks 
idea mixture experts multiple experts attempt classify novel pattern separate function differentially weights experts estimated suitability pattern 
weighting function known gating function typically weight small number experts highly open gate suppress rest closed gate 
gating function varies weights inputs experts expert may weighted highly pattern weighted highly different pattern 
learning algorithm estimates parameters experts gating function sample classifications promotes specialization experts gating function allowing subregions form similar cart mars 
cart mars experts contribute classification novel pattern contribution quite small 
multiple experts gating functions combined tree structure lower level experts provide gated classification estimates higher level experts leading classification root tree thompson ceo algorithm discussed earlier 
distill essential features mixture experts methods 
learning algorithm hierarchical structure experts gating functions 
key characteristic learning algorithm supervised pattern classification regression 
means input pattern known output distribution outputs generated network 
goal learning algorithm adjust network parameters possible network generates correct output input patterns training set 
contrast ranked retrieval distinctly different requirements 
assume far mixture experts attempting estimate relevance document query ranked retrieval task 
particular output network document query important 
order outputs documents particular query important 
ranking correctly orders relevant documents relevant documents system succeeding 
appears misguided train experts output particular value document query pair important thing ordering outputs particular values 
approach advocated oriented distinction pursue greater detail section 
note retrieval task posed supervised training task mixture experts approach specifying particular target value document query pair 
obvious way require system output document relevant query 
approach intuitively reasonable assumed documents training set relevant relevant query 
report results applications method compare rank method alternative 
similar comparison performed previous chapter 
earlier comparison supervised method perform 
feature mixture experts approach hierarchical structure experts gating functions 
typical structure depicted 
collection experts computes estimate relevance document query 
estimate th expert 
gating network observes document query generates weights expert estimates 
weighted estimates summed produce estimate system document query pair 
gating network generate softmax outputs preliminary gating value generated gating network normalized softmax 
goal softmax accentuate expert largest computed gating network suppress rest 
experts contribute degree score 
structure experts gating functions appears applicable goal combining experts ranked retrieval task 
ir gating unit identify characteristics query document expert expert gating network sum query document relevance estimate network network neural network mixture experts typical neural network mixture experts model applied estimating relevance document query 
expert computes estimate relevance weighted gating network summed produce estimate 
predict expert accurate 
expert highly weighted computation relevance estimate 
applications follow return model examine performance 
expert combination algorithm automatically combine experts approach optimize ability retrieval system correctly rank documents relevant relevant 
retrieval system combines experts parameterized model 
varying parameters model different combinations experts effected 
optimization method automatically adjusts parameters model providing combination leads preferred ranking documents 
approach application general rank optimization method developed previous chapters 
parameterized mixture experts apply method model defined specifies experts combined 
model takes individual experts estimates document relevance combines single relevance estimate 
model certain free parameters 
modifying parameters model combines experts different ways leading different relevance estimates 
goal approach automatically adjust parameters combined estimates result best ranking documents possible 
expert implement ranked retrieval function 
ranked retrieval function generates single numerical score document query pair 
score interpreted degree relevance document query 
ordering documents respect scores expert provides ranked list documents estimating order relevance documents relevant relevant 
current approaches information retrieval examples ranked retrieval functions vector space model probabilistic models spreading activation approaches boolean retrieval 
requirement functionality experts appear overly restrictive 
model combines individual experts scores single score document query pair 
theta ranked retrieval function combining expert scores 
theta generates single relevance score indicating relevance document query parameterized theta different values parameters lead different combinations experts different relevance scores different document ranking 
simple useful example model linear combination experts 
model score expert scaled scaled scores summed yield score 
example system experts defined theta theta theta theta parameters theta serve scales individual experts scales interpreted importance expert contribution score practice take account features experts range estimates 
critical feature score generates differentiable respect parameters theta 
obviously true case linear model 
models interest particularly neural network models described previous section differentiable respect parameters 
differentiability exploited optimization method discussed shortly 
important feature ignorant internals particular experts combined 
experts black boxes simply generating relevance estimates document query pairs 
allows method applicable wide range experts requiring expert specific modifications 
course limitation 
alternative methods may devised optimize combinations particular experts highly efficient robust way 
generally applicable method propose remain valuable situations individually analyzed 
search algorithm standard gradient numerical optimization techniques minimize point alienation criterion defined chapter 
conjugate gradient worked simulations advantage gradient methods parameter free applications source code readily available see 
performed experiments steepest descent variant steepest descent downward step direction gradient taken criterion value increase 
criterion value increases step size decreased 
steepest descent methods worked 
tend increase variance final results led generally superior results 
typically conjugate gradient 
applications follow conjugate gradient specified 
recall chapter optimize parameters model determine gradient criterion respect parameters 
chain rule theta theta dq theta theta delta theta theta set documents retrieved query experts 
equation provides blueprint optimizing different expert combination models simply derive gradient model output respect parameters model 
linear model example theta theta similar calculations non linear neural networks described previous section mathematics somewhat tedious 
concern method computationally demanding useful real world applications 
case 
example applications reported chapter optimization linear model conjugate gradient requires approximately minutes sparc ipc low workstation database documents experts training queries 
training time remains reasonable non linear models increasing hours 
experiments restrict training set consider highest ranked documents query 
cases training time short seconds yields performance results better full size experiments 
course training time may vary text collections models 
discussion computational complexity provided chapter 
learning weights phrases terms applied method problems involving learning combination experts 
application described section 
emphasis application illustration technique demonstration analysis expert performance 
second application emphasize performance improvements achievable production quality retrieval system topic section 
application involves experts 
expert smart retrieval system implementing standard vector space retrieval approach 
vector space method commonly effective approach information retrieval 
expert called term expert uses simple non phrasal terms provides reasonably robust baseline performance 
second expert phrase identifier 
attempts identify phrases documents queries retrieves documents containing query phrases 
phrase expert retrieve documents retrieved term expert phrase causes document retrieved phrase expert composed terms cause document retrieved term expert 
usefulness phrase expert stems solely ability positively alter document ranking term expert 
phrase expert improve precision combined system improve recall 
improved performance stemming phrase expert result improved reranking enhanced recall 
main point demonstrate phrase expert mediocre performance relative term expert contribute usefully combined ranking generated system 
suggest method may provide approach analyzing different retrieval components allows system designer determine value method added system isolation 
combination function function combine term phrase experts simple linear weighting relevance estimates retrieval systems theta theta term theta phrase theta gives system relevance estimate document query estimates experts 
just parameters optimized weight expert 
turns parameter model replaced model single parameter important characteristic optimization criterion 
examining equation apparent criterion invariant scale translation relevance scores theta scale values theta theta matter ratio theta theta individual signs 
replace theta theta single parameter angle determines weight placed expert placed second 
phi angle theta cos phi theta sin phi 
optimize linear combination experts simply adjusting value phi 
analyze model angular representation 
optimization parameter linear model 
model defined equation quite similar earlier model lewis croft examination syntactic phrase indexing semantic phrase clustering 
lewis croft index documents different experts 
standard vector space indexer 
second syntactic phrase expert indexes documents syntactic phrases constituent terms 
third semantic analyzer finds groups semantically related phrases indexes documents group individual phrases 
third expert allows document contains query terms retrieved contains semantically related phrase 
model experts combined linear combination individual scores 
unfortunately lewis croft automatic method available determining parameter values 
forced manually adjust parameter values unable find parameters result trivial improvements baseline weighting 
test collection performed number simulations term phrase expert retrieval system 
simulations portion britannica edited collection widely regarded premier english language general 
britannica eb consists main structural sections collection articles covering topics broad scope architecture islam collection approximately articles topics limited scope province southwestern sweden epistemological retrieval average precision performance test queries system avg precision phrases terms phrase expert term expert optimized combination table optimized term phrase performance phrase expert low performance relative term expert contributes improved performance combined system 
organization articles eb structured tree index 
experiments subset articles categorized eb editors pertaining part human society 
subset documents 
term expert phrase expert smart retrieval system index documents 
term weighting method normalized term frequency inverse document frequency smart ntc 
phrase expert identifies phrases trivially simple algorithm bigram adjacent word pair containing high frequency noise word crossing sentence boundary accepted phrase 
terms term expert phrasal terms stemmed smart algorithm 
document representations normalized term phrase indices stored single vector normalizing separate term phrase vectors 
queries train system test performance derived semantic tree 
node tree represents refinement topic parent node 
example root node part human society 
children social groups peoples cultures social organization social change politics government tree finds refined topic governmental policies influencing population growth composition 
associated nodes internal leaf nodes set articles discuss aspects topic node 
structure naturally permits interpretation node topics queries associated documents relevant set 
able extract queries manner omitting queries containing phrases affect results 
queries average relevant documents query 
queries stemmed analyzed phrases manner documents 
parameter estimation section briefly examine performance improvements achieved optimizing collection experts 
primary points performance combined system improved greatly method isolation performance phrase expert low identified valuable examined isolation 
experiments examine greater detail process optimizing performance provided second application section 
set queries partitioned subsets training set test set 
training set queries test set remaining 
optimization retrieval truncated precision top ranked documents system phrase expert term expert table individual term phrase expert performance phrase expert performs worse term expert small set highest ranked documents retrieved 
training queries performed times conjugate gradient randomly selected initial parameter settings 
optimized networks perform identically standard deviation average precision mean performance values indicate model performance 
table provides summary performance measured average precision optimized system performance individual experts 
performance reported optimized system mean performance training trials 
term expert performs better phrase expert phrase expert average precision performance 
expect effect term expert retrieves documents achieve higher performance higher levels recall 
example term expert retrieves average documents query phrase expert retrieves 
difference recall complete explanation 
highest ranked documents examine performance term expert outperforms phrase expert 
performance summarized table 
addition levels recall term expert higher precision phrase expert 
light results readily reject phrase expert candidate inclusion retrieval system 
perform small set documents retrieve 
results table give cause consider phrase expert 
performs worse term expert retrieves additional documents combination experts method results improved performance average precision performance term expert 
combined system able experts scores result ranking documents better single expert 
interestingly best combination weights phrase expert slightly higher term expert 
despite phrase expert lower individual performance 
illustrates weighting experts individual performance may best strategy 
analysis expert performance section analyze combined performance experts 
particular examine optimized performance combined system query independently 
methodology optimize criterion separately query 
optimization training set single query 
case optimization typically able find parameters highly specialized single training query 
parameter values provide best possible performance single query optimal term weight distribution optimal weights best pt sample query queries opt analysis term phrase weights optimized weights independent query vary greatly 
point plot corresponds optimized weights queries normalized fall unit circle 
optimized weights opt queries optimized shown 
importantly optimized phrase weight negative indicating phrase expert predicting documents relevant queries trying predict opposite 
generally yield performance remaining test queries 
distribution optimized parameter settings queries illustrated 
shows optimized parameters normalized fall unit circle 
illustrated optimized parameter setting case queries optimized 
striking characteristic graph optimized parameter values widely distributed 
appear consensus individual queries optimized parameter setting 
widely distributed distribution optimized parameters far uniform unit circle 
distribution optimized weights illustrated histogram angle unit circle index recall parameter system trivially converted parameter angular system scale translation invariance criterion 
important feature distribution weight term expert positive weight phrase expert frequently half queries negative 
negative phrase weight indicates optimized performance achieved query phrase theta angle degrees distribution optimal weights best pt sample query queries distribution optimized term phrase weights distribution optimized weights independent query angle 
number queries negative optimized phrase weights queries positive optimized phrase weights queries 
negative optimized phrase weights correspond angles larger ffi expert relevance score subtracted term expert score 
half queries phrase expert usefully predicts retrieved documents relevant opposite trying estimate 
negative weight phrase expert discounting documents contain query phrases 
graphs tell number stories 
example substantial number queries phrase expert appears useful retrieval task 
queries optimized phrase weight larger optimized term weight optimized angle close ffi case optimized ranking strategy rank documents containing query phrase rank remaining documents term expert 
cases phrase expert useful 
queries optimized weights positive slightly varying ratios 
corresponds optimized angles ffi ffi cases term phrase experts positively contribute ranking varying proportions 
final class queries weight term expert negative 
fortunately fans vector space retrieval number queries class small 
class ordering documents opposite order term expert gives superior performance 
analysis tell 
foremost suggests phrase expert hurts retrieval performance helps 
apparent reason phrase expert poor individual performance 
suggests phrase expert room improvement 
simply case phrase expert low average precision performance isolation 
queries phrase expert predicting retrieved documents relevant 
section demonstrated system improve phrase expert combined term expert 
system superior average phrase expert positively contributes detracts 
believe partly attributable high contribution phrase expert queries optimized phrase weight higher optimized term weight 
short phrase expert 
analysis serve basis domain specific analysis 
example current application may possible look correlations characteristics phrases mutual information constituent terms utility combined system measured optimized phrase weight 
recall current phrase expert uses highly suspect phrase identification strategy pairs words including high frequency words phrases 
correlations predict candidate phrase phrase expert indexing 
emphasis section illustrating novel forms analysis optimization method supports 
unaware analysis method short exhaustive search parameters gives similar insights combined operation experts 
typical analysis individual performance experts limited reasons 
collection training queries individual analysis indicate expert perform average combined experts 
second single queries individual analysis provide scale best contribution expert 
additional insights particularly useful deciding particular candidate expert added system improved 
performance commercial system applied method combining experts commercial retrieval system 
section report details results application 
commercial retrieval system validates usefulness technique real systems just research retrieval engines 
able achieve high performance relative system individual experts 
optimized system performs average higher measured average precision best individual expert 
addition compared results alternative method supervised learning methods introduced section 
supervised method compare performs average better best individual expert 
details results provided 
commercial system developed compton new media available pc 
st ranking algorithm consults experts determining relevance document query 
experts provide relevance estimates traditional vector space estimate count number query terms document estimate number query phrases document term proximity count number terms title document 
st retrieves titles separately documents omitted expert concentrated retrieval full text documents remaining experts 
objective find best performing combination experts 
note vector expert utilize state art weighting schemes cf 

performance vector expert high achieved smart system possible weighting mechanisms 
emphasis achieving highest level performance possible combining existing systems optimizing individual experts 
attempt modify vector expert experts achieve better performance 
treating experts black boxes way certainly disadvantages 
discussion section address concerns suggest approach advocating allows automatic modification certain parameters experts pursued 
test collection compton multimedia encyclopedia experiments reported section 
consists documents additional associated multimedia objects short video sound clips 
britannica edited high school level audience 
full text documents experiments 
training test queries derived set questions printed version encyclopedia 
head volumes editors provided set questions answered articles volume 
example question head volume sport may rise myth 
answer article titled civilization 
total queries constructed questions subsets queries training testing 
deriving sample queries editors questions limitations 
particular number cases answer number documents just single document editors identified 
cases answer identified document 
decided alter queries compensate factors 
find perfect noise free training queries real retrieval setting 
noted queries somewhat atypical queries commonly seen text retrieval literature 
particular query single correct answer single document 
contrast topic oriented queries typified queries trec collection 
topic oriented queries defined answers potentially large number documents relating aspect broad query topic may relevant query 
implications somewhat atypical query set discussed discussion section 
large number available queries provides rare opportunity train system adequate number samples validate results large number queries training 
experimental method partition set queries number groups test models different group queries training 
method useful estimating performance optimized network deployed real world accepting novel queries users 
test performance partition query proximity count vector optimized improvement partition expert expert expert system best average table performance linear network optimized system performs marginally better best individual expert count expert improvement varies greatly depending training partition 
groups queries constructed purpose approximately queries group 
query belongs group 
groups correspond queries volumes compton encyclopedia 
experiments reported networks trained group tested second 
repeated times network trained tested pairs training test groups 
tests repeated times order avoid bias due particular queries group 
examining performance number different training test sets hope factor bias 
performance linear model examine number different models combining experts 
model linear combination seen previous section theta theta proximity theta count theta vector gives relevance estimate expert document query goal estimate values theta theta theta result best ranking performance model 
optimized network conjugate gradient 
performance optimized system comparison individual experts illustrated table 
performance optimized system measured training linear networks query partitions 
partition network having highest average precision training queries selected representative network partition 
network evaluated test queries determine performance 
performance scores values reported table 
results table extremely positive 
marginal performance improvement average best individual expert partitions performance worse 
addition improvement statistically significant 
results particularly surprising light preliminary experiments performed 
preliminary experiments trained network queries partition 
performance network test queries high improvement average precision best individual expert 
surprising increasing size training set full partition degrade results extremely 
closer examination observed base retrieval system unoptimized performs quite queries preliminary set 
correct answer query top documents retrieved 
contrasts set queries partition 
larger set relevant documents top retrieved top 
possible extremely difficult queries detrimental effect method making difficult find solutions 
order perform difficult queries parameters learned perform extremely remaining 
hypothesis examined section 
effect outliers test hypothesis difficult queries outliers disrupting solution examine performance method trained queries outliers removed 
outliers defined queries relevant document top documents retrieved unoptimized system 
partition queries removed outliers leaving 
performance network better trained outliers 
query partition network trained queries test set average precision count expert performance 
network trained outliers removed test set performance improvement count expert 
performed experiment query partitions 
observations supported related experiments number documents training set limited highest ranked documents retrieved system query 
limiting training set way desired effect removing outliers 
topic section 
limiting number documents preceding experiments assumed collection training queries know relevance irrelevance document collection 
reasonable assumption real world applications 
scenario queries training derived actual sessions users system 
case typically small number documents identified relevant irrelevant user complete information documents simply available 
may severe consequences method propose effective applied limited information 
section examine case training documents restricted highly ranked documents query 
simulates situation user submits query identifies highly ranked retrieved documents relevant irrelevant 
methodology top ranked documents query training 
top ranked documents system retrieve documents query sorting retrieved set relevance estimate 
test performance partition top ranked documents query count optimized optimized improvement partition expert docs top count expert average table linear performance top ranked documents training highest ranked documents query leads high performance improvements 
improvement calculated respect best individual expert count expert 
top documents query training rest ignored 
system evaluated training documents just top 
networks optimized test partitions order avoid local minima 
network best average precision performance training queries selected representative partition 
results experiment comparison best individual expert previous experiments documents illustrated table 
combined system trained top ranked documents performs better best individual expert count expert network trained documents 
improvement count expert partitions average improvement 
improvements count expert linear combination trained documents significant respectively 
results support earlier removing outliers positive effect performance 
top ranked documents relevant document retrieved th document included optimization 
may fairly extreme definition outlier results indicate leads positive performance 
results encouraging application approach real world tasks 
apparently small number documents queries needed sufficiently constrain optimization lead generalization performance 
small amount relevance information available approach appears quite applicable 
addition top ranked documents optimization computed rapidly 
experiments optimization typically required seconds minutes 
performance non linear models linear model far just possible functional models combining expert scores 
section examine certain non linear neural networks combination models 
experiments traditional multi layer feed forward neural networks mixture experts gating networks reviewed earlier performed 
sum relevance estimate expert expert expert query document relevance estimate expert expert expert query document gating network sum sigmoid hidden units linear hidden units non linear mixture experts models schematics non linear neural networks combine experts 
gating network right feed forward neural network inputs expert estimates unit hidden layer sigmoids softmax output 
non linear neural networks combine experts illustrated 
network left layer neural network common architecture neural network literature 
expert estimate fed pathways left pathway linear transformation right hidden units sigmoid transfer functions 
pathways additively combined 
network right mixture experts model pathways left linear transformation expert scores right non linear gating network inputs expert scores hidden units sigmoid transfer functions softmax output layer 
gating network multiplicatively modifies output left pathway 
number variations network architectures examined 
variations include networks hidden units removed networks alternative hidden unit transfer functions exponential linear sigmoid example 
networks depicted perform best attempted 
number variations perform poorly 
particular optimization networks led degenerate parameters relevance estimates network equivalent 
cases criterion function equation degenerate sum differences numerator denominator zero leading divide zero 
attempted alleviate problems adding noise optimization modifying evaluation criterion near critical points 
difficult avoid critical points general 
easiest simply discard networks degenerate attempt avoid degeneracies 
test performance non linear networks query count linear layer gating partition expert network network count network count average table performance non linear networks non linear neural networks perform significantly better count expert 
network better generalization performance linear network 
measure performance precision top ranked documents average precision documents 
performance systems optimized top ranked documents 
average precision performance qualitatively similar 
results optimizing networks query partitions illustrated table 
previous experiments top ranked documents query networks trained query partitions 
linear network non linear networks perform significantly better count expert cases test queries 
non linear networks outperforms linear network differences non linear networks linear network statistically significant layer network gating network 
clear additional functional power non linear networks resulting benefit 
non linear networks apparently unable extract features expert relevance estimates support better estimates simple linear combination 
non linear networks performed better linear network necessary non linear combinations expert estimates predict experts useful query 
apparently outputs experts support kinds distinctions 
secondary result traditional non linear architecture performs marginally better special purpose gating architecture difference significant 
result supports hypothesis expert estimates provide sufficient information help determine experts highly weighted 
interestingly non linear networks utility handling outliers 
recall average improvement linear model trained documents queries including outliers improvement jumps documents 
suggested outliers disrupt ability linear network find solutions 
story different layer non linear network 
non linear network trained documents queries including outliers average improvement count expert 
improvement slightly performance improvement top ranked documents networks trained documents top significantly different 
demonstrates non linearities exploited isolate degrading effects outliers 
comparison supervised learning algorithm alternatives particular rank order criterion 
goal ordering relevant documents relevant ones vague number options goal formalized terms criterion optimization 
previous chapter identified number alternatives suggested field multidimensional scaling mds rank order statistics 
criterion equation correlate average precision standard measure performance information retrieval 
section examine supervised learning alternative closely 
goal restated retrieval system output estimates probability document relevant query simply estimate order documents supervised learning criterion 
supervised criterion measures system estimate document relevance differs document true probability relevance 
true probability relevance feedback user indicates document relevant query system output value document system output 
supervised criterion popular pattern classification neural networks squared error criterion theta gamma theta document relevant query 
criterion essentially identical squared error criterion optimized backpropagation described chapter 
minimizing criterion system relevance estimates true probabilities relevance 
similar squared error criterion fuhr buckley reviewed earlier chapter 
principle difference criterion optimize retrieval estimate theta individual estimates single terms occurring document query 
fuhr buckley goal explicitly optimize system ability estimate probability relevance document 
squared error criterion optimized conjugate gradient linear combination model 
experimental design preceding sections partitions random networks partition retaining network having highest average precision training set 
top ranked documents order remove outliers results experiments illustrated table 
results suggest squared error version supervised learning useful formulation learning combinations experts 
supervised method results average improvement best individual expert difference significant 
similar experiment layer feed forward neural network results network generalizes worse linear network performance count expert performance difference significant 
experiments documents just top ranked performed 
results qualitatively similar 
performance supervised learning algorithm query count rank order supervised improvement partition expert criterion criterion count expert average table performance supervised training supervised training leads combined system performs worse best individual expert 
possible explanation squared error criterion disappointing performance places constraints system relevance estimates needed ranked retrieval 
reiterate main goal ranked retrieval system order documents relevant relevant 
squared error criterion goes goal requiring system estimate relevant documents 
certainly solution satisfies constraints satisfies rank constraints difficult satisfy squared error constraints practice 
hypothesize squared error criterion forces solution rank solution satisfy constraints foremost 
discussion demonstrated method combining experts effective efficient applicable range common experts information retrieval 
method uses understood numerical optimization techniques optimize combination experts leading system specifically configured rank relevant documents relevant ones 
method alternative number techniques combining experts proposed particular manual search method thompson ceo method fuhr buckley probabilistic learning approach 
alternatives certain strengths foundation probability theory combining evidence potential intuitive appeal 
proposed method certain alternative advantages 
foremost method applicable wide range possible text retrieval approaches 
approach estimates correct relevance ranking documents generating relevance score document potential expert combined method 
addition method explicitly optimizes ability system correctly rank documents 
contrast alternatives supervised learning approach examined previous section 
empirically rank approach compared favorably supervised learning approach 
improvements performance demonstrated encouraging 
clear amount possible improvement depends number problem specific factors 
example particular experts characteristics collection quality training queries affect performance optimized system 
addition factors number documents included training significantly affect optimized performance experiments demonstrated 
level improvement may closely tied representative training queries typical queries system 
training queries representative system may learned performs training queries generalize deployed environment 
example assume training queries trivial pursuit variety questions having brief defined answers 
assume expert system question answerer targeting types questions proposed kupiec 
optimized system may learn highly weight trivial pursuit expert disregard experts valuable kinds queries 
optimized system perform kinds queries 
care taken ensure queries representative kinds seen system optimized peculiarities training set phenomenon known training neural network literature 
certainly results current experiments interpreted context 
experiments training queries actual user queries users relevance assessments constructed secondary sources 
possible derived queries atypical user queries optimized systems specific biases resulting poor generalization true user queries 
optimistic results chapter general discussion suggest 
optimistic experts engineered exploit peculiarities artificial queries 
experts general feature detectors counting number phrases occurring documents queries support specialized performance 
event demonstrated training queries representative test queries current experiments performance improvements quite high 
related concern queries second experiments may heightened observed effect outliers performance linear model 
recall queries second experiments relevant document irrelevant 
concern relevant document query poorly ranked may disrupt solution document number relevant documents ranked 
experiments test collections may needed provide satisfactory answer problem 
demonstrated method provide novel way analyzing performance experts combined experts 
typical approaches evaluating retrieval method performance evaluate method isolation 
allows comparison methods give system designer information needed decide particular method included system 
belkin dilemma methods perform poorly individually may contribute improved system 
phenomenon observed phrase expert experiment performs worse term expert 
despite phrase expert combined term expert result superior performance 
addition weight phrase expert optimized system slightly larger weight term expert 
suggests simply weighting experts individual performance may generally productive approach combining experts 
issue analyzing combinations experts far resolved 
certainly analysis indicates expert included system take account costs including method 
furthermore number different factors may contribute cost cost implementing expert cost maintaining expert computational cost retrieval time including expert 
certainly designer second thoughts expert results marginal performance improvement great cost 
analysis may complicated question subset experts provides best performance improvements acceptable cost 
answering question may require expensive combinatorial examination combinations experts 
possible alternative combinatorial solution add cost terms criterion optimization 
cost terms quantify qualitative costs enumerated provide explicit trade ranking performance cost experts 
adding cost terms changes optimization problem optimal system best trade ranking performance cost 
adding cost terms common approach neural network literature 
example cost terms added optimization favor neural networks lowest complexity order promote generalization 
interesting direction difficult issues remain addressed 
possible limitation method reliance sample queries identified relevant documents training 
environments may quite difficult acquire samples 
hope user interfaces improve include relevance feedback standard easier capture interactions users system generate sample queries relevance feedback data 
fortunately experiments suggest method require large number sample queries large number identified documents order effective 
large performance improvements gained approximately queries training set identified documents query 
real sample queries simply unavailable information relevance feedback user queries 
experiments queries derived secondary sources users 
stated approach drawbacks guarantee derived queries truly representative real user queries 
preceding experiments conclude linear models optimization method 
linear models led solutions easy analyze 
case non linear networks somewhat mixed 
important strength traditional layer non linear network able perform better linear network noisy data 
valuable feature documents training just top ranked subset 
non linear networks able outperform simple linear model 
believe primary reason non linear networks rich input signal 
non linear networks inputs estimates experts discriminate experts information 
sufficient information signal support performance superior simple linear combination 
examine chapter 
summary method finding high performance combination retrieval experts proposed 
method uses examples past queries learn combination may better queries 
method applicable experts compute ranked retrieval scores indicating relevance documents queries 
method combines scores ranking scores closely matches preferred order relevance estimates training queries 
current models retrieval ranked retrieval functions method wide applicability 
validate method applied expert combination tasks 
cases optimized system performs better individual experts 
case linear combination term expert phrase expert learned performs better term expert 
improvement despite low performance phrase expert 
second task linear model combining experts commercial retrieval system learned 
system performs better best individual 
addition demonstrate method provide insights utility method combination methods 
contrast traditional methods analysis examine utility experts isolation 
chapter research dissertation means complete 
believe ideas starting point number interesting projects related information retrieval 
numerous dissertation research 
chapter outline particular projects think rewarding 
projects examining nonmetric rule nearest neighbor pattern classification advancing usefulness non linear networks mixture experts model comparing non metric criterion metric criterion dimensionality reduction task examined chapter 
non metric dimensionality reduction seen number methods performing dimensionality reduction ir task 
method latent semantic indexing lsi performs dimensionality reduction term document matrix singular value decomposition 
chapter demonstrated technique call metric similarity modeling msm performs dimensionality reduction 
msm generalization lsi allowing flexible target similarity structure documents teh reduced dimension space 
alternative ways perform dimensionality reduction example principal component analysis pca related non linear methods 
alternative approaches metric model chapter nonmetric method chapter criterion learning reduced dimension representation documents 
approach similar general taken chapter parameterized model defined maps documents queries term space representation reduced representation similarity measure provides similarity documents queries reduced space 
similarity document query defines relevance estimate theta theta parameters dimensionality reducing mapping 
model optimized non metric criterion just applications chapters 
value non metric procedure numerous 
frequently reiterated non metric procedure attractive natural interpretation relevance feedback data 
addition non metric approach explicitly optimizes ranking performance retrieval system metric procedure optimizes association values documents 
chapter demonstrated improving association values effect improving ranking performance direct non metric method point 
value non metric method require specification system designer increment ff recall ff defines influence relevance occurrence target similarities 
apparent benefit metric approach non metric analytically solvable linear dimensionality reductions 
see non metric procedure faster compute metric approach despite closed form solution 
performed preliminary experiments non metric procedure interesting promising results 
required draw definitive non metric approach 
experiments varieties 
compare performance methods metric non metric linear dimensionality reduction 
examine effect different choices initial parameters non metric procedure 
similar notion initial parameters metric procedure analytically solvable 
look briefly non linear dimensionality reduction model 
linear dimensionality reduction recall chapter performance metric similarity model evaluated deriving target similarities determine dimensionality reduction set queries training set testing performance second set 
case cisi document collection training queries test queries taken total queries available collection 
addition different partitions queries training test sets examined help avoid bias estimate performance 
comparison non metric method msm partitions perform dimensionality reduction experiments non metric criterion 
term representations reduced dimensions linear map 
non metric procedure involves numerical minimization dependent initial parameters model latent semantic indexing solution initial document function 
lsi mapping know gives initial level performance optimization procedure improve 
illustration generalization performance non metric procedure comparison smart msm provided 
results precision scores different levels recall averaged query partitions 
non metric model illustrated uses cosine proximity measure reduced space 
average precision performance just slightly higher msm performance 
difference significant 
non metric model inner product similarity examined performs slightly worse average average precision msm significantly 
msm non metric models outperform smart average performance 
results appears non metric procedure results mapping behaves similarly msm method 
msm non metric methods perform differently training queries 
differences illustrated 
non metric methods able perfectly rank relevant documents irrelevant training queries resulting extraordinarily high average precision performance average partitions cosine measure inner product 
msm result similar level performance training set average performance 
difference causes 
msm explicitly optimize ranking performance recall cos non metric msm smart average test set performance cisi database non metric dimensionality reduction performance generalization performance model trained non metric criterion identical msm model studied chapter iii 
models outperform smart retrieval system substantially 
system expect achieve high performance non metric procedure explicitly optimize ranking 
second linear model large number parameters terms times dimensions reduced space requires linear mapping parameters 
number parameters model certain quite constrained capable satisfying ordering constraints 
amazing high degree fitting training set optimized model generalizes positively test set 
performed limited number experiments training halted fitting occurs generalization better worse 
fitting interfere model ability represent documents reasonably novel queries 
research include comparison methods pertinent adaptive methods information retrieval particularly method 
recall method modifies documents relevant query making document vector similar query vector 
accomplished adding terms document query 
may concern non metric procedure take longer compute metric procedure 
non metric procedure recall cos non metric msm smart average training set performance cisi database non metric training set performance training set performance linear model trained non metric criterion better msm smart models 
non metric criterion explicitly optimizes model ability rank documents correctly linear model able solve problem perfectly 
requires iterative minimization criterion criterion evaluation requires recomputation reduced document representations 
non metric procedure takes amount time slightly faster analytic metric procedure 
non metric procedure requires approximately hours sparcstation metric procedure requires approximately hours 
computation time include initial calculation svd document term matrix common procedures 
computation times rough estimates thorough benchmarks performed 
importance initial parameters previous experiments particular initial parameter values iterative optimization linear model 
latent semantic indexing solution known effective parameter set serve starting point optimization 
alternative lsi initial parameters random initial values 
question system performs better worse important consider non linear dimensionality reduction models 
models may parameters linear model parameters initialized randomly 
performance randomly initialized linear model trial cosine inner product train test train test mean table randomly initialized model performance test set performance randomly initialized linear model worse model initialized latent semantic indexing weights cosine performance inner product performance 
randomly initialized model able perfectly solve training problem 
number optimizations performed randomly determined initial parameter values lsi parameters 
linear parameters initialized simple random process sample univariate normal distribution mean variance model optimized times different initial parameter settings repeated similarity measures inner product cosine 
performance networks partition reported table 
experiments run query partition performance poor feel necessary expend cpu cycles 
fact randomly determined initial parameters perform worse latent semantic indexing parameters important surprising 
surprising large number parameters model 
model highly constrained optimization leads model solves problem perfectly training queries extracted important features domain support generalization 
result important optimization method complex dimensionality reduction models particular non linear neural network models 
non linear models large number parameters added addition linear component 
results suggest simply choosing additional parameters random may best alternative 
care taken initialize parameters initial mapping optimization reasonable 
optimization improvements high quality baseline constructing mapping scratch 
suggests method best fine tuning mapping learning mapping random weights particularly model constrained 
non linear dimensionality reduction performed limited experiments non linear neural network model dimensionality reduction 
results reported section selected non linear model essentially linear limited number new randomly initialized parameters 
model linear mapping term space actual variance irrelevant optimization model invariant scale 
test set performance non linear model model training partition non linear cosine non linear ip linear cosine linear ip training set performance non linear model non linear cosine non linear ip linear cosine linear ip table non linear non metric performance non linear network performs similarly linear network 
apparent benefit gained non linear sigmoids bias units 
dimensional reduced space 
adaptive bias value added dimensions value squashed sigmoid 
squashed values document representations compared analogous query representations cosine inner product 
latent semantic indexing map initial linear map modification 
multiplicatively modified scale lsi map output values range suitable input sigmoid transfer function 
motivation rescaling concern inputs sigmoid large small general 
inputs small sigmoid usefulness linear portion sigmoid 
inputs large sigmoid saturated 
initial weight matrix rescaled inputs sigmoid linear range extreme values non linear range 
elementary summary statistics arbitrary decision led rescale lsi matrix factor 
results maximum input sigmoid minimum input gamma 
furthermore inputs greater gamma rescaling 
remaining bias parameters initialized normal variate mean variance 
different sigmoids examined bounding output bounding output gamma 
preliminary experiment measure unoptimized model performs initial parameters 
performance model sigmoid bound gamma quite reasonable test set average precision partitions cosine measure inner product measure 
sigmoid bounded performance worse cosine measure inner product 
surprising 
gamma sigmoid minimally affects linear outputs inputs non linear region sigmoid 
sigmoid hand greatly modifies linear outputs displacing centered 
inner product cosine invariant kind translation similarity structure greatly warped sigmoid 
considering results gamma sigmoid model optimization 
trials performed query partition different randomly selected sets bias parameters 
average performance trials reported 
able optimize models complete set partitions 
results partitions illustrated table 
results preliminary appears non linear model performs comparably linear model training performance test performance 
addition sigmoids bias parameters appear confer advantage non linear model 
adaptive nearest neighbor classification nearest neighbor classification intuitively simple powerful method classifying objects 
nearest neighbor classification uses database classified objects examples novel objects compared classified 
decision rule simple classify novel object class object database similar novel object 
classify object find nearest neighbor set known objects neighbor class 
rule intuitively appealing formalizes simple idea object class similar objects 
identified number reasons nearest neighbor rule attractive 
assumes little distributional structure classes 
useful parametric form class distributions unknown 
second easy apply 
needed database examples similarity measure 
third attempts mimic optimal bayes rule selecting probable class local sample 
fourth strength identified favorable asymptotic error bounds 
asymptotically number examples database increases infinity probability misclassifying object twice theoretically best error rate bayes rate 
interesting result independent similarity measure assuming certain reasonable restrictions 
matter nearest neighbor euclidean distance cosine metric example 
asymptotic error rate bound error rate encouraging asymptotic result 
absence additional restrictive assumptions extreme case infinite set examples nearest neighbor rule guaranteed converge way 
course real instantiation rule examples 
interesting question nearest neighbor rule performs small number samples 
studied case small number samples available 
case similarity measure important determining performance rule 
suggests cross validation test performance variety metrics select performs best 
obviously limitations finite set candidate similarity measures enumerated hand 
fukunaga colleagues suggested alternative inference procedures optimal similarity measure estimated parametric family measures database intuitive reason particular similarity measure matter infinite number examples neighborhood sample dense known examples 
nearest neighbor arbitrarily close sample regardless measure 
statistics 
limits approach inference requires estimation similarity measure small regions samples interest 
authors state difficult determine metric accurately estimated sample small local region sample 
alternative procedure outline may alleviate problem certainly requires sufficient examples support local estimation 
demonstrate empirically estimated distance metric performs better euclidean metric number cases 
suggest non metric approach developed chapter may applicable problem identifying optimal similarity measure nearest neighbor task 
parallels chapter estimated optimal similarity measure vector space information retrieval model certainly promising 
outline procedure identifying optimal similarity measure non metric technique 
fairly clear ranking function theta defined measure similarity unknown sample known database example similarity measure parameterized theta 
goal adjust theta training samples nearest neighbor training sample correct class 
achieve goal point alienation non metric criterion optimized done previously 
principal difficulty approach defining suitable preference relation candidate theta theta interpreted preferred class different class similar database examples class 
performed experiments test formulation 
mixture experts hopeful may suggest alternatives fully exploit functional power non linear networks mixture experts task 
section discusses directions research take 
alternative allow gating network mixture experts model directly receive descriptions query document receiving relevance estimates experts done 
illustrates distinction 
gating network decision estimates experts 
gating network access actual document query representations denoted dashed line query document gating network 
distinctions gating network limited differentiate experts outputs actual features problem 
neural network literature suggests alternative gating network receive inputs environment 
allows gating network decide actual document query experts best suited handle inputs 
performed experiments clear features documents queries gating network 
obvious option provide vector distance metric similarity measure 
change definition preference relation fairly obvious ways 
sum expert expert expert query document expert expert expert query document gating network sum sigmoid hidden units linear hidden units differences mixture experts models dashed lines indicate possible connections query document inputs neural network models 
mixture experts model differs standard neural network approaches network receives outputs individual experts receive document query input 
representation document query 
limited number reasons 
clear simply terms documents queries useful determining set experts emphasized particular situation 
second vector space representation dramatically increase number parameters model 
cause estimation problems saw previous prior initial parameter values 
promising option extract features documents queries useful predicting experts useful particular situation 
example combining phrase term experts previous chapter calculate summary statistics phrases occurring query 
summary statistics gating network predict phrase expert able positively contribute estimate 
direction research option adaptively modifying internal parameters experts addition finding combination 
certainly suggested neural network literature supported method certain kinds experts 
expert relevance score differentiable respect parameters optimization method modify expert parameters 
difficulty may posed approach expert moving target 
expert dynamically changing model may significantly difficult stabilize combination strategy 
potential benefits approach encouraging 
chapter dissertation principally identifying empirically validating methods adaptively improve performance text retrieval systems 
methods featured 
metric criterion measures retrieval system estimates semantic association pairs documents 
criterion metric precise numerical target value specified researcher document association typically values determined observing users document preferences actual system 
minimization metric criterion results retrieval system better able estimate association documents respect target associations 
optimization explicitly improve retrieval system ability rank documents relevance query tasks empirically related 
second adaptive method non metric criterion 
non metric criterion measures retrieval system ranking documents estimated relevance 
metric criterion non metric criterion require numerical targets known 
target rank order documents need known 
document ranking specifies preferences documents user particular query 
minimizing non metric criterion results retrieval system best ranks relevant documents relevant ones 
chapter identified number scientific technical issues hoped address dissertation 
examining contrasting adaptive methods analytically empirically examine problems text retrieval believe contributed resolution issues 
identify issues review results dissertation pertinent 
efficient effective adaptive ir methods supported contention adaptive solutions important information retrieval demonstrating different retrieval environments require different solutions 
example chapter non metric criterion estimate similarity measure vector model retrieval 
certain retrieval conditions documents collection ordered best possible find similarity measure performs best 
different conditions highest ranked documents interest different similarity measure significantly better 
adaptive methods means effectively deal different environments demonstrated method empirically able solve problem 
effectiveness obviously essential characteristic adaptive methods aim improve retrieval performance 
applying methods number different retrieval problems demonstrated methods effective way significantly improve retrieval performance automatically find system parameters perform hand tuned parameters 
application learning similarity measure optimized measure performs better best comparable similarity measure depending retrieval environment 
application combining retrieval experts combined performance better best individual retrieval system 
applications dimensionality reduction optimized system performs better latent semantic indexing better smart retrieval system 
efficiency critical aspect practical adaptive method large size text collections 
dissertation demonstrated adaptive methods employed reasonable sized text collections order tens thousands documents 
efficiency dependent number factors 
retrieval model optimized reasonably efficient 
case applications dissertation adaptive similarity measure mixture experts third term space dimensionality reduction 
second optimization surface multi modal optimization gradient methods method impractical 
applications conditions satisfied adaptive methods efficient order minutes hours optimize collection thousands tens thousands documents workstation 
highly non trivial dimensionality reduction retrieval models optimization takes hours collection documents 
important finding demonstrated chapters non metric method appears robust small amounts training data 
small subset document preference data needed order result solutions 
small amount information optimization times order seconds tens thousands documents 
learning rank order issue adaptive method infer function capable ordering objects example orderings objects 
seen issue highly relevant ranked text retrieval 
may relevant wide range domains example nearest neighbor classification domains prioritization essential 
dissertation identified general adaptive method optimizing differentiable parameters ranking functions 
optimization results function able best rank training samples correct order assuming optimization pathologies occur 
demonstrated utility approach applying number different problems text retrieval including learning similarity measure learning combination experts learning dimensionality reduction document representations 
cases non metric approach successful 
adaptive methods facilitating ir system design general adaptive methods provide way set tune system parameters training examples 
dissertation useful number different tasks system instantiation current text retrieval systems large number parameters tuned system designer 
example parameters include term weights parameters similarity measure weights combining evidence different components system 
dissertation examined effectiveness adaptive methods instantiating parameters 
parameter values simulated example retrieval sessions users 
parameters generalize resulting high retrieval performance novel queries 
applications demonstrate automatic adaptive methods successfully instantiate free differentiable parameters retrieval system 
combining sub systems argued earlier text retrieval task complex problem requiring highly non trivial solutions 
approach complexity common text retrieval design specialized retrieval systems effectively deal portion total problem 
specialized component may deal natural language phrases text may identify synonymous terms 
specialization support flexible modular system design 
significant problem determining retrieval components combined single retrieval system 
approach problem non metric adaptive method learn high performance combination modules 
seen approach lead large performance improvements applications individual retrieval component 
evaluating sub systems non metric approach evaluate performance subcomponents retrieval system 
evaluating sub components groups differs standard method evaluation text retrieval component examined isolation 
group analysis fruitful 
retrieval component perform quite poorly isolation phrase expert chapter positively contribute performance collection components 
evaluation components isolation provide insight 
adaptation preference orders adaptive methods text retrieval assume relevance boolean feature document relevant irrelevant user shades gradations utility 
non metric approach dissertation takes alternate view relevance boolean multi valued 
modeled effectively preference ordering documents 
formulation useful places emphasis retrieval system task ranking documents order relevance user 
adaptive methods boolean formulations hand tend emphasize task estimating probability relevance document 
demonstrated applications chapters boolean approaches tend constrain estimates retrieval system requiring estimation boolean probabilities relevance 
result non metric approach effective examined boolean approaches improving retrieval system ability rank documents 
framework multidimensional ir models demonstrated multidimensional scaling useful framework vector text retrieval models analyzed 
utility derives shared objectives methods attempt represent objects points multidimensional space inter point similarity estimate conceptual semantic association objects 
mds offers text retrieval computational mechanisms explicitly find multidimensional representation objects best satisfying associational constraints 
mds framework useful dissertation 
able metric mds model analyze latent semantic indexing investigate general method representing documents vector space chapter 
second able non metric mds model investigate representation documents reduced dimension space chapter 
cases document representations generated mds motivated approaches far superior resulting roughly improvement typical text retrieval approaches smart 
alternative criteria non metric adaptive approach examined dissertation novel text retrieval neural networks knowledge 
essential part determining value new approach comparison alternative methods 
examined number alternatives dissertation including non metric method wong yao neural network perceptron learning rule squared error learning rule motivated partly fuhr buckley research number non metric criteria multidimensional scaling rank statistics literature 
cases alternatives appear suited optimizing ranked retrieval performance non metric criterion selected 
summary dissertation proposed novel adaptive methods optimizing performance ranked text retrieval systems 
empirically demonstrated variety applications methods effective achieving significant performance improvements baselines general applicable range interesting problems text retrieval problems domains efficient practical problems involving megabytes text 
believe general adaptive approach advocated serve foundation additional applications information retrieval research adaptive prioritization tasks 
bibliography arabie 
multidimensional scaling measures distance partitions 
journal mathematical psychology 
brian bartell garrison cottrell 
model symbol grounding temporal environment 
proceedings ijcnn seattle wa july 
brian bartell garrison cottrell richard belew 
latent semantic indexing optimal special case multidimensional scaling 
proceedings acm sigir copenhagen 
brian bartell garrison cottrell jeffrey elman 
role input target similarity assimilation 
proceedings cognitive science society meeting chicago august 
belew 
adaptive information retrieval machine learning associative networks 
phd thesis university michigan 
belew 
adaptive information retrieval connectionist representation retrieve learn documents 
proceedings acm sigir pages cambridge ma june 
nicholas belkin cool bruce croft james callan 
effect multiple query representations information retrieval system performance 
proceedings acm sigir pages pittsburgh pa june 
david blair maron 
evaluation retrieval effectiveness fulltext document retrieval system 
communications acm march 
bruce 
constrained multidimensional scaling spaces 
psychometrika 
bookstein kraft 
operations research applied document indexing retrieval decisions 
journal acm 
bookstein swanson 
probabilistic models automatic indexing 
journal american society information science 
abraham bookstein 
relevance 
journal american society information science september 
abraham bookstein 
outline general probabilistic retrieval model 
journal documentation june 
borg 
multidimensional similarity structure analysis 
springerverlag new york 

document vector modification 
smart retrieval system experiments automatic document processing pages 
nj prentice hall 
breiman friedman olshen stone 
classification regression trees 
belmont ca wadsworth international group 
brookes 
measure information retrieval effectiveness swets 
journal documentation 
douglas carroll chang 
analysis individual differences multidimensional scaling way generalization eckart young decomposition 
psychometrika 
kenneth church patrick hanks 
word association norms mutual information lexicography 
proceedings th annual meeting association computational linguistics pages vancouver 
cooper 
definition relevance information retrieval 
information storage retrieval 
cooper 
selecting measure retrieval effectiveness 
part 
journal american society information science 
thomas cormen charles ronald rivest 
algorithms 
mit press 
garrison cottrell brian bartell chris haupt 
grounding meaning perception 
proceedings german workshop artificial intelligence 

garrison cottrell paul munro 
principal components analysis images back propagation 
proceedings society photo optical instrumentation engineers 
ieee spie 
fabio crestani :10.1.1.50.2329
learning strategies adaptive information retrieval system neural networks 
proceedings ieee international conference neural networks san francisco ca march 
katter 
experimental studies relevance judgements final report 
technical report report tm vol 
project summary vol 
description individual studies system development santa monica ca june 
jane cullum ralph 
lanczos algorithms large symmetric eigenvalue computations 
boston birkhauser 
douglass cutting jan pedersen david karger tukey 
scatter gather cluster approach browsing large document collections 
proceedings acm sigir pages copenhagen june 
dasarathy editor 
nearest neighbor nn norms nn pattern classification techniques 
ieee press 
jan de leeuw willem heiser 
multidimensional scaling restrictions configuration 
krishnaiah kanal editors multivariate analysis pages 
north holland 
scott deerwester susan dumais thomas landauer george furnas laura beck 
improving information retrieval latent semantic indexing 
proceedings asis pages 
scott deerwester susan dumais george furnas thomas landauer richard harshman 
indexing latent semantic analysis 
journal american society information science 
david demers garrison cottrell 
nonlinear dimensionality reduction 
giles hanson cowan editors advances neural information processing systems nips 
san mateo ca morgan kaufmann 
du toit 
graphical exploratory data analysis 
new york springer verlag 
richard duda peter hart 
pattern classification scene analysis 
john wiley sons 
susan dumais 
enhancing performance latent semantic indexing lsi retrieval 
technical report technical memorandum bellcore september 
susan dumais george furnas thomas landauer scott deerwester richard harshman 
latent semantic analysis improve access textual information 
proceedings chi pages 
jeffrey elman 
representation structure connectionist models 
proceedings cognitive science society ann arbor 
fisher 
multiple measurements taxonomic problems 
ann 

part ii 
peter foltz 
latent semantic indexing information filtering 
proceedings conference office information systems 
edward fox prabhakar joseph shaw russell rao 
combining evidence multiple searches 
donna harman editor text retrieval conference trec pages march 
nist special publication 
jacob shapiro vladimir 
multiversion information retrieval systems feedback mechanism selection 
journal american society information science 
friedman 
multivariate adaptive regression splines 
annals statistics 
norbert fuhr chris buckley 
probabilistic learning approach document indexing 
acm transactions information systems 
norbert fuhr chris buckley 
optimizing document indexing search term weighting probabilistic models 
donna harman editor text retrieval conference trec march 
nist special publication 
fukunaga thomas flick 
optimal global nearest neighbor metric 
ieee transactions pattern analysis machine intelligence pami may 
furnas landauer gomez dumais 
statistical semantics analysis potential performance key word information systems 
bell system technical journal july august 
george furnas scott deerwester susan dumais thomas landauer richard harshman lyn streeter karen lochbaum 
information retrieval singular value decomposition model latent semantic structure 
proceedings acm sigir grenoble france 
george furnas thomas landauer gomez susan dumais 
vocabulary problem human system communications 
communications acm 
stephen gallant 
context vector representations document retrieval 
aaai natural language text retrieval workshop anaheim ca july 
stephen gallant 
practical approach representing context performing word sense disambiguation neural networks 
neural computation 
jones 
study test methodology laboratory evaluation message retrieval systems 
technical report report esd tr little brown boston august 
lev goldfarb 
new approach pattern recognition 
kanal rosenfeld editors progress pattern recognition pages 
elsevier science publishers 
north holland 
lev goldfarb 
hybrid associative memories metric data models 
editor optical shape representation pattern recognition pages 
proc 
spie vol 

louis gomez carol lochbaum thomas landauer 
right words finding want function richness indexing vocabulary 
journal american society information science 
michael gordon 
probabilistic genetic algorithms document retrieval 
communications acm october 
gower 
distance properties latent root vector methods multivariate analysis 
biometrika 
paul green frank jr multidimensional scaling related techniques marketing analysis 
allyn bacon publishers boston 
paul green frank jr scott smith 
multidimensional scaling concepts applications 
allyn bacon publishers boston 
michael leslie 
scaling data matrix lowdimensional euclidean space 
douglas hawkins editor topics applied multivariate analysis pages 
cambridge university press 
guttman 
statistics 
statistician 
donna harman 
experimental study factors important document ranking 
proceedings acm sigir pages pisa italy 
donna harman 
overview text retrieval conference 
proceedings acm sigir pages pittsburgh pa june 
stephen harter 
psychological relevance information science 
journal american society information science 
robert hecht nielson 
neurocomputing 
addison wesley 
john hertz anders krogh richard palmer 
theory neural computation 
addison wesley 
geoffrey hinton 
learning distributed representations concepts 
proceedings eighth annual cognitive science society conference 
lawrence erlbaum associates hillsdale nj 
holland 
adaptation natural artificial systems 
university michigan press 
david hull 
statistical testing evaluation retrieval experiments 
proceedings acm sigir pages pittsburgh pa june 
david 
statistics usenet volume week nov 
ide 
new experiments relevance feedback 
smart system experiments automatic document processing pages 
nj prentice hall 
robert jacobs michael jordan steven nowlan geoffrey hinton 
adaptive mixtures local experts 
neural computation 
jolliffe 
principal component analysis 
springer series statistics 
springerverlag new york 
william jones george furnas 
pictures relevance geometric analysis similarity measures 
journal american society information science 
michael jordan robert jacobs 
hierarchical mixtures experts em algorithm 
technical report ai memo mit ai lab august 
john justeson slava katz 
occurrences adjectives contexts 
computational linguistics 
mcgill tessier frakes dasgupta 
study overlap document representations 
information technology research development oct 
kohonen 
self organization associative memory 
springer verlag berlin nd edition 
kruskal 
multidimensional scaling optimizing goodness fit nonmetric hypothesis 
psychometrika march 
kruskal 
nonmetric multidimensional scaling numerical method 
psychometrika 
julian kupiec 
robust linguistic approach question answering line encyclopedia 
proceedings acm sigir pages pittsburgh pa june 
kwok 
experiments component theory probabilistic information retrieval single terms document components 
acm transactions information systems oct 
kwok papadopoulos kathy kwan 
retrieval experiments large collection 
donna harman editor text retrieval conference trec pages march 
nist special publication 
david lewis bruce croft 
term clustering syntactic phrases 
proceedings acm sigir brussels sept 
karen lochbaum lynn streeter 
comparing combining effectiveness latent semantic indexing ordinary vector space model information retrieval 
information processing management 
maron 
indexing retrieval meaning 
journal american society information science pages jan 
maron kuhns 
relevance probabilistic indexing information retrieval 
journal association computing machinery 
james mcdonald tony plate roger 
pathfinder extract semantic information text 
roger editor pathfinder networks 
ablex publishing 
mcgill koll 
evaluation factors affecting document ranking information retrieval systems 
technical report school information studies syracuse university syracuse new york october 
risto miikkulainen michael dyer 
encoding input output representations connectionist cognitive systems 
david touretzky geoffrey hinton terrence sejnowski editors proceedings connectionist models summer school 
morgan kaufmann june 
marvin minsky 
neural nets brain model problem 
phd thesis princeton 
hisao miyano 
sequential estimation multidimensional scaling 
psychometrika september 
michael mozer 
inductive information retrieval parallel distributed computation 
technical report technical report tr institute cognitive science ucsd la jolla ca may 
paul nelson 
site report text retrieval conference 
donna harman editor text retrieval conference trec march 
nist special publication 
robert nosofsky 
stimulus bias asymmetric similarity classification 
cognitive psychology 
tore 
optimal values recall precision 
journal american society information science pages march 
kai olsen robert korfhage kenneth michael spring james williams 
visualization document collection vibe system 
technical report lis school library information science university pittsburgh 

nearest neighbor classification rules small sample performance comparison linear discriminant function optimal rule 
phd thesis university california los angeles 
helen peat peter willett 
limitations term occurrence data query expansion document retrieval systems 
journal american society information science 
mark white 
cross validation estimates 
proceedings neural information processing systems nips vail colorado december 
press 
jordan pollack 
recursive auto associative memory devising compositional distributed representations 
proceedings tenth annual conference cognitive science society montreal 
jordan pollack 
implications recursive distributed representations 
david touretzky editor advances neural information processing systems 
morgan kauffman 
william press brian flannery saul teukolsky william vetterling 
numerical recipes art scientific computing 
cambridge university press 
vijay raghavan wong 
critical analysis vector space model information retrieval 
journal american society information science 

likelihood estimation multidimensional scaling 
psychometrika 
adi 
nonmetric approach linear discriminant analysis 
journal american statistical association march 
john 
performance measures information retrieval systems experimental approach 
journal american society information science 
robertson 
probability ranking principle ir 
journal documentation dec 
robertson sparck jones 
relevance weighting search terms 
journal american society information science may 
rocchio jr relevance feedback information retrieval 
smart system experiments automatic document processing pages 
nj prentice hall 
rose 
symbolic connectionist approach legal information retrieval 
phd thesis uc san diego computer science cognitive science 
daniel rose richard mander tim oren salomon yin yin wong 
content awareness file system interface implementing pile metaphor organizing information 
proceedings acm sigir pages pittsburgh pa june 
frank rosenblatt 
perceptron probabilistic model information storage organization brain 
psychological review 
frank rosenblatt 
principle neurodynamics perceptrons theory brain mechanisms 
spartan books washington 
kenneth ross charles wright 
discrete mathematics 
prentice hall nd edition 
david rumelhart geoffrey hinton ronald williams 
learning internal representations error propagation 
nature 
david rumelhart geoffrey hinton ronald williams 
learning internal representations error propagation 
david rumelhart james mcclelland pdp research group editors parallel distributed processing explorations microstructure cognition 
mit press cambridge ma 
david rumelhart james mcclelland 
interactive activation model context effects letter perception part 
contextual enhancement effect tests extensions model 
psychological review 
gerard salton editor 
smart retrieval system experiments automatic document processing 
nj prentice hall 
gerard salton 
automatic term class construction relevance summary automatic 
information processing management 
gerard salton 
automatic text processing transformation analysis retrieval information computer 
addison wesley 
gerard salton chris buckley 
improving retrieval performance relevance feedback 
journal american society information science 
gerard salton christopher buckley 
term weighting approaches automatic text retrieval 
information processing management 
gerard salton lesk 
information analysis dictionary construction 
smart retrieval system experiments automatic document processing 
nj prentice hall 
gerard salton michael mcgill 
modern information retrieval 
mcgraw hill 
saracevic 
relevance review framework thinking notion information science 
journal asis 
saracevic kantor 
study information seeking retrieving 
iii 
searchers searches overlap 
journal asis 
jan scholtes 
unsupervised learning information retrieval problem 
proceedings international joint conference neural networks 
jan scholtes 
neural networks natural language processing information retrieval 
phd thesis university amsterdam 
hinrich schutze 
dimensions meaning 
proceedings supercomputing 
hinrich schutze 
word space 
neural information processing systems nips pages 
shepard 
analysis proximities multidimensional scaling unknown distance function 
psychometrika june 
shepard 
analysis proximities multidimensional scaling unknown distance function 
ii 
psychometrika september 
robert short fukunaga 
optimal distance measure nearest neighbor classification 
ieee transactions information theory sept 
dow 
neural net pruning 
proceedings ieee conference neural networks volume pages san diego 
paul smolensky 
constituent structure connectionist mental states reply fodor pylyshyn 
technical report cu cs university colorado boulder march 
karen sparck jones 
automatic keyword classification information retrieval 
butterworths london 
karen sparck jones 
statistical interpretation term specificity application retrieval 
journal documentation march 
ian spence dennis 
single subject incomplete designs nonmetric multidimensional scaling 
psychometrika december 
ian spence stephan 
robust multidimensional scaling 
psychometrika september 
amy richard belew 
exporting phrases statistical analysis topical language 
proceedings document analysis information retrieval conference las vegas april 
gilbert stewart 
matrix computations 
academic press 
gilbert strang 
linear algebra applications 
harcourt brace jovanovich publishers rd edition 
swets 
effectiveness information retrieval methods 
bolt beranek newman cambridge mass 
forrest young jan de leeuw 
nonmetric individual differences multidimensional scaling alternating squares method optimal scaling features 
psychometrika march 
paul thompson 
combination expert opinion approach probabilistic information retrieval part conceptual model 
information processing management 
paul thompson 
machine learning combination expert opinion approach ir 
lawrence birnbaum gregg collins editors machine learning proceedings th international workshop ml pages 
morgan kaufmann 
paul thompson 
description prc ceo algorithm trec 
donna harman editor text retrieval conference trec pages march 
nist special publication 
peter todd 
abstracting featural dimensions parallel distributed processing approach 
unpublished manuscript department psychology stanford university may 
peter todd 
connectionist multidimensional scaling method allowing semantic generalization 
unpublished manuscript department psychology stanford university december 
howard turtle bruce croft 
evaluation inference network retrieval model 
acm transactions information systems july 
tversky 
features similarity 
psychological review 
tversky gati 
studies similarity 
rosch lloyd editors cognition categorization 
hillsdale nj erlbaum 
valiant 
theory learnable 
communications acm 
timothy van gelder 
compositionality connectionist variation classical theme 
cognitive science 
van rijsbergen 
information retrieval 
butterworths london england second edition 
van rijsbergen karen sparck jones 
test separation relevant non relevant documents experimental retrieval collections 
journal documentation 
ellen voorhees 
effectiveness efficiency agglomerative hierarchic clustering document retrieval 
phd thesis cornell university 
ellen voorhees 
wordnet disambiguate word senses text retrieval 
proceedings acm sigir pages pittsburgh pa june 
yih chen wang james martha evens 
relational thesauri information retrieval 
journal american society information science jan 
wang wong yao 
analysis vector space models computational geometry 
proceedings acm sigir copenhagen 
patrick wilson 
situational relevance 
information storage retrieval 
wong cai yao 
computation term associations neural network 
proceedings acm sigir pittsburgh pa june 
wong yao 
query formulation linear retrieval models 
journal american society information retrieval 
yiming yang christopher chute 
linear squares fit mapping method information retrieval natural language texts 
proc 
th international conference computational linguistics coling pages 
yiming yang christopher chute 
application squares fit mapping text information retrieval 
proceedings acm sigir pages pittsburgh pa june 

