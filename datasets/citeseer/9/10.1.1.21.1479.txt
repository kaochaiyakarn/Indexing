gaussian processes classification mean field algorithms manfred opper neural computing research group department computer science applied mathematics aston university birmingham united kingdom ole winther theoretical physics ii lund university lund sweden connect niels bohr institute copenhagen denmark september appear neural computation vol 
issue derive mean field algorithm binary classification gaussian processes tap approach originally proposed statistical physics disordered systems 
theory yields approximate leave estimator generalization error computed extra computational cost 
show tap approach possible derive simpler naive mean field theory support vector machines svm limiting cases 
mean field algorithms support vectors machines simulation results small benchmark data sets 
show 
may get state art performance leave estimator model selection 
built leave estimators extremely precise compared exact leave estimate 
result taken strong support internal consistency mean field approach 
great deal interest non parametric bayesian approaches regression classification concept gaussian processes mackay williams williams rasmussen neal gibbs mackay barber williams williams barber opper winther 
underlying idea conceptually simple 
defining prior distributions parameters learning machine weights biases case neural net directly defines gaussian prior distribution function machine computes 
regression problems approach allows explicit statistical inference analytical methods 
hand non linear statistical models ones classification high dimensional integrals occur performing posterior averages treated approximative methods 
approximation integrations monte carlo sampling neal large number data may time consuming 
semi analytic approaches laplace methods approximation posterior multivariate gaussian barber williams williams barber tractable variational bound data likelihood gibbs mackay 
deals different approach origin statistical physics disordered media 
thermodynamic properties calculated high dimensional expectations gibbs distributions contain large set random quantities similar input data posterior distribution bayesian analysis 
method tap anderson palmer parisi type mean field approximation goes variational approximation posterior product distribution 
tap approach controlled approximation sense exact simple distributions disorder input data limit number integration variables approaches infinity parisi opper winther 
second advantage method construction automatically computes leave estimate generalization error 
clearly real data distribution disorder unknown assess validity mean field approach directly 
contrast prior applications tap method explicit assumptions distribution 
base approach gaussian assumption called cavity fields parisi believe reasonably large class distributions closed set non linear equations posterior averages derived 
alternative derivation equations previous nips opper winther intuitive approximation gibbs free energy parisi 
show internal consistency mean field approximation checked indirectly comparing leave estimate generalization error provided tap theory exact estimate 
organized follows section gives basic definitions classification gaussian processes 
section derive tap mean field algorithm 
recipe solving mean field equations iteration section 
leave loo estimator generalization error tap mean field algorithm proposed section 
section derive naive mean field algorithm 
support vector machines svms obtained tap mean field theory limiting procedure prior change likelihood probabilistic interpretation 
section simulation results small benchmark data sets sonar mines versus rocks crabs pima indians diabetes 
concluded section 
appendices discuss respectively noise model covariance functions 
gaussian process models consider supervised learning problem training set dn ji ng input vectors associated outputs want infer output new input restrict binary classification sigma 
probability output point tjh assumed depend input scalar activation function inferred learning process 
simplest example likelihood noise free case tjh theta theta theta 
functions sign training label non zero probability 
common choice likelihood corresponds noisy ambiguous classifications replace eq 
sigmoidal function 
approach requires integrations likelihood functions gaussian distributions restrict likelihoods corresponding probit model tjh phi th phi error function precisely gaussian cumulative distribution function phi gamma dy gamma integrations performed exactly 
discussed appendix bayesian classification choice derived model form eq 
simple redefinition amounts modify prior includes noise 
subsequent discussion consider likelihood eq 

possible parametric approaches modeling function 
popular represent neural network try estimate parameters data 
bayesian approach learning prior probability distribution parameters weights network determine specified 
go step may skip parameters introduce prior distribution entire space functions choice gaussian probability measure functions motivated study limiting prior distribution neural neural network case number hidden units grows large neal williams 
case typical function drawn random prior distribution realization gaussian process 
gaussian process family random variables denotes possibly uncountable index set finite collection xn random variables joint gaussian distribution det exp gamma gamma gamma gamma mean set zero hh gamma mm covariance matrix input set elements 
function explicitly determines activations different points correlated reflect prior beliefs variability function 
principle may choose positive semi definite function parametric choices related neural network models discussed appendix formally may represent gaussian process model single layer neural network possibly infinitely weights ae ae spherical gaussian prior distribution ae ae ffi performing karhunen loeve expansion see 
papoulis ae ae ae psi ae psi ae eigenfunctions covariance function kernel eigenvalues ae eq 
get kernel representation ae ae psi ae psi ae 
glance infinite dimensional nature gaussian process bit awkward 
interested making inference single input discrete set training data inputs needs dimensional gaussian prior distribution xn random variable associated new input bayes rule likelihood eq 
may form joint posterior distribution hjt jh tjh shorthand training set outputs tjh dh tjh normalization constant 
find hjt formally need marginalize hjt dh hjt call jt predictive posterior term predictive training set posterior may shown making bayesian predictions 
predictive posterior central concept mean field theory mean field approximation amounts assuming special simple parametric form distribution 
bayesian classification predictive posterior jt summarizes knowledge having observed training set 
may calculate predictive probability output label tjt hp tjh dhp tjh hjt introduced notation posterior average 
bayes algorithm classification selects output highest probability bayes dn argmax tjt order minimize posterior probability error 
binary classification bayes algorithm bayes dn sgn jt gamma gamma jt bayesian framework may quantify uncertainty prediction bayes algorithm 
posterior belief probability output correct predictive probability tjt 
estimate probability bayes algorithm gives wrong answer gammat bayes dn jt 
mean field theory posterior average needed derive bayes algorithm cases interest analytically tractable 
likelihood eq 
resort approximate techniques 
introduce advanced mean field theory approach ideas statistical mechanics 
basic idea mean field theories approximate statistics random variable correlated random variables assuming influence variables compressed single effective mean field simple distribution 
approximation variational product approximation distribution interest neglects correlation random variables parisi 
applications gaussian processes see csat graphical models see jordan 
approach goes simple mean field theory equivalent called tap mean field theory known statistical mechanics disordered systems 
assume data posterior distribution new data point set training inputs predictive posterior approximated gaussian mean hhi variance hh gamma hhi jt exp gamma gamma hhi motivate approximation may look expansion eq 

clearly posterior distribution weight vector jt gaussian non gaussian likelihood 
justify gaussian approximation central limit theorem weights ae sufficiently weakly dependent addition fluctuations sum dominated large number terms roughly magnitude 
may argue condition fulfilled eigenvalues ae rapidly decreasing ae exponentially fast 
dimension input space large assume kernel permutation invariant respect components input vector features linear ones eigenvalues may give similar contributions fluctuations eq 

quality gaussian approximation strongly depend input take point close training inputs consider likelihood eq 
imposes strict inequality distribution different gaussian see subsequent discussion eq 
see moment gaussian approximation applied test points training inputs label corresponding likelihood factor discarded 
case expect approximation fairly input points typically close 
fact specific models opper winther approximation exact dimension input space approaches infinity input vectors drawn random probability distribution independent components 
limit probability inputs vectors come uncorrelated 
theory disordered media similar approximations distribution magnetic field cavity left atom corresponding data label removed system 
speak cavity fields cavity distributions 
detailed discussion validity approximation specific models statistical physics disordered media see parisi 
show means variances gaussian approximation eq 
calculated self consistent way 
doing gaussian approximation calculate predictive probability eq 
tjt phi hh phi error function eq 

bayes classifier bayes dn note result holds distribution symmetric mean value 
approximation error probability may calculated gammat bayes dn jt phi gamma important note gaussian mean field approximation predictive posterior equivalent gaussian approximation full posterior 
approximation commonly bayesian modeling parametric models limit number data larger number parameters 
justified nonparametric case 
second justified non smooth likelihoods eq 

understand difference simpler approximation consider case th example observed 
bayes rule posterior distribution jt tjh jt dh tjh hjt likelihood eq 
posterior distribution far gaussian illustrated 
derive mean field algorithm shall exploit gaussian approximation derive analytical relation mean predictive distribution hhi mean jt 
predictive posterior jt dashed line posterior jt solid line addition th example 

lines crossing axis indicates mean value distributions 
mean field approximation need derive expressions hhi hh gamma hhi bayesian predictions quantify uncertainty prediction 
task remainder section 
start deriving exact expressions moments fields 
exact expressions written terms averages cavity distributions training examples left likelihood 
averages may carried mean field approximation second moments cavity distribution determined self consistently 
final result set non linear mean field equations solved find hhi simulations done iteration 
gaussian assumption distribution cavity field possibility derive tap mean field theory 
derivation perturbation expansions gibbs free energy opper winther intuitive requires somewhat deeper background specific techniques statistical physics 
alternative derivations discussed detail 
exact expressions hhi hh starting point approximative treatment exact relations posterior mean hhi tjh posterior variance 
exact relations derived hp cc gamma hp gammac follows eq 

may find exact relation posterior mean hh arbitrary point extending prior include new point th field applying integration parts shift differentiation prior likelihood hh gamma tjh tjh ff introduced notation defined embedding strength ff dhp tjh introducing shorthand notation ij applying eq 
training data points hh ij ff exact expressions interesting allow express expected activation hh weighted average examples training set case support vector learning approach vapnik map approach gaussian processes williams barber 
calculating activation point training data weighted close appropriate metric kernel function defines relevant neighborhood point input space 
importance training point learning problem measured variable ff may seen eq 
non negative tjh increasing function ht 
cavity derivation useful introduce new set predictive posteriors example dh jh dh jh tnt gamma denotes training set ith example 
denoting average distribution ni dh rewrite eq 
ff jh ni hp jh ni mean field expression ff main reason possible calculate averages fact predictive posterior field input included data set 
apply gaussian approximation eq 
exp gamma gamma hh ni variance defined hh ni gamma hh ni inserting derive explicit expression ff hh ni phi hh ni introduced gaussian measure gammay far accomplished write ff terms new unknown quantities moments predictive distribution hh ni need express terms known quantities 
moments may exploit exact relation average posterior predictive posterior jh ni hp jh ni assumed gaussian get reasoning eq 
hh ni gamma hh hh ni gamma dh jh hp jh ni hh ni jh ni hp jh ni comparing eq 
see hh hh ni ff relation shows adding ith example posterior gives change mean field direction target may seen fig 

note eqs 
equations ff non linear solved numerically 
sec 
exploit fact calculated hh ni training example propose mean field estimate leave estimator generalization error 
mean field expressions complete mean field theory derive expressions second moments hh gamma hh hh ni gamma hh ni derivation appendix final results gamma jk gamma jk gamma ii gamma diag gamma gamma ff hh ni gamma explicit expression ff hh ni algebra obtained eq 
ff hh ni gamma ff hh eq 
eqs 
defines set non linear equations gaussian process regression gaussian noise model finds expressions equal noise variance williams rasmussen 
tempting interpret effective noise variance ith example 
summary mean field equations conclude derivation tap mean field equations summary results mean field prediction new data point follows eqs 
bayes dn sgn ff obtain ff solve set non linear mean field equations ff ff eq 
eqs 
eq 
eqs 

section give recipe solving mean field equations 
solving mean field equations mean field equations solved iteration 
parallel iterative scheme described pseudo code indicated equations 
iterative scheme naive mean field see section obtained omitting statements iteration step 
computationally expensive step tap mean field approach inversion matrix compared operations 
turns requires iterations solve equations ff somewhat insensitive precise value ff choose greedy update th iteration step 
problems studied convergence required accuracy obtained iteration steps takes cpu second alpha au largest problem studied 
initialization 

learning rate fault tolerance gamma important contributing factor learning speed iterative learning rate set error decreases update step 

calculate covariance matrix see appendix 

training example ff ii iterate 
max 
eqs 
hh ij ff phi gamma ff hh gamma ff 
ff ff 
th iteration hh ff gamma 
eqs 

gamma ii gamma 
eq 

leave estimator solved mean field equations cavity average activation function hh ni known 
may define loo estimator generalization error algorithm ni mean field estimate prediction algorithm pattern trained example 
get bonus tap approach leave estimator generalization error ffl loo theta gammat hh ni ffl loo exact equal exact loo estimator ffl exact loo obtained fold cross validation estimator unbiased 
appendix show expression hh ni hh gamma ff may obtained treating perturbation tap equations ith example removed training set linear response approach 
ffl exact loo unbiased estimator generalization error training set size gamma 
may define additional loo estimator error probability eq 
ffl pred phi gamma ni estimator unbiased model assumptions correct cases misleading 
observed simulations 
gives self consistency check result hh ni obtained tap approximation 
appendix furthermore shown technique generalized derivation loo estimators algorithms naive mean field algorithm section support vector machines section 
approximate loo estimators smoothing splines xiang wahba neural networks larsen hansen close spirit linear response approach 
naive mean field theory different ways define derive mean field theories see 
parisi 
simplest derive exact relation expectations random variables interest expectation non linear function variables 
spin systems expressions known identities see parisi field theory book page 
calculation expectation naively exchanged non linearity order derive closed self consistent approximation averages 
problem start definition ff eq 
firstly apply standard gaussian transform introduces imaginary unit gamma confused appearing index ff dhp tjh exp gamma cy iy tjh secondly integration parts applied ff exp gamma cy iy tjh hy equality bracket understood formal average joint complex measure variables separate integrations rest variables show ff dh dy exp gamma ii ij gamma jh dh dy exp gamma ii ij gamma jh dh exp gamma gamma ij ii jh dh exp gamma gamma ij ii jh ii ij ii phi ij ii simply neglect fluctuations field ij move expectation non linearities get self consistent set equations ff hy may call naive mean field theory 
explicit expression ff ff ii ij ff ii phi ij ff ii seen simplified version tap equations cf 
eq 
complicated expression eq 
replaced ii mean field theory lower computational complexity avoid matrix inversion needed tap approach obtain may obtain leave estimator naive mean field theory eq 
eq 
omega eq 
omega ii ff hh gamma observe difference tap exchanged ii compare omega corresponds eq 
tap 
support vector machine digression relation variational problems reproducing kernel hilbert spaces learning support vector machines bayesian gaussian processes known pointed various authors see 
wahba jaakkola haussler 
give simple formulation support vector learning realizable case neglecting bias boser guyon vapnik vapnik scholkopf burges smola decomposition activation function eq 
tries find weights ae examples ae ae minimal 
optimization problem direct probabilistic interpretation 
fits result obtained approximation suitable partition function csat 
gaussian process framework limiting procedure introduce margin replace tjh theta th non normalized likelihood theta th gamma absolute values activations smaller excluded 
second prior rescaled introducing parameter fi fi det exp gamma fi gamma independent variables ae prior simply exp gamma fi ae ae 
limit fi posterior dominated solution sv problem 
interestingly tap equations give general approximations posterior averages shown appendix reduce exact kuhn tucker conditions determine embedding strengths ff svms ff hh ff hh hh ij ff prediction support vector machine svm dn sgn ff appendix show svm limit loo estimator eq 
simple approximation loo estimator svms obtained ffl svm loo sv theta gamma ff gamma sv ii sum goes svs sv denotes covariance matrix sv examples 
alternative derivation eq 
generalizations possible lines appendix opper winther 
similar type loo estimator svm different loss function previously derived wahba 
simulations section study performance tap mean field naive mean field support vector machine svm algorithm publicly available widely tested data sets sonar mines versus rocks gorman sejnowski crabs pima indians diabetes ripley 
svm algorithm bias 
input data pre processed linear rescaling training set input variable zero mean unit variance 
tested different covariance functions 
listed appendix cases gaussian covariance function turned give best performance 
chosen report results covariance function 
hyperparameters 
free parameters algorithms hyperparameters covariance functions described appendix 
determined prior knowledge problem lack training data 
simulations reduced number free hyperparameters setting hyperparameters see appendix common value sonar values previous study crabs pima williams barber 
remaining hyperparameters chosen minimize value leave estimator 
optimization leave estimator simply rough probably non optimal optimization scanning range values 
possible restrict free parameters cases different zero 
noted performance quite robust changes hyperparameters 
section briefly discuss hyperparameter determination automatic bayesian evidence framework possible alternative determining hyperparameters 
test leave estimator 
generalization error estimator different algorithms compared exact ffl exact loo leave oneout cross validation error 
error count get going training example keeping turn example testing running mean field algorithm rest 
results shown tables 
shows values different error measures range values single hyperparameter 
leave oneout estimators precise 
usually get classification wrong compared exact value 
exception extremely short ranged covariance functions shown see 
sonar data sonar mines versus rocks data set gorman sejnowski study classification sonar signals neural networks gorman sejnowski 
task train classifier discriminate sonar signals bounced metal cylinder bounced rough cylindrical rock 
dataset contains patterns obtained bouncing sonar signals metal cylinder various angles various conditions patterns obtained rocks similar conditions 
training test data split gorman sejnowski 
results 
algorithms number hyperparameters strongly reduced setting oe input dimension 
turned performance quite robust choice oe may seen 
simulations results shown table 
comparison taken gorman sejnowski 
may observe performance mean field algorithms slightly better svm best layer network 
doubtful difference significant 
table results sonar data 
algorithm ffl test ffl exact loo ffl loo tap mean field naive mean field svm simple perceptron best layer hidden shows test set significantly easier training set 
solution lowest test error trivial ff thh constant 
exchanging training test set find oe ffl test ffl loo 
may conclude training test split biased 
running algorithms splits gave non trivial solution better correspondence test leave oneout error error rate extremes 
give example embedding strength ff stability hh tap mean field naive mean field svms exactly condition training set covariance matrix 
results normalized squared length weight vector ij ff ij ff cases 
results show clear correlation solutions algorithms 
mean field algorithms give rise smaller larger margins svm 
major effect performance 
may obtained www cs cmu edu benchmarks sonar html 
svm test exact loo loo tap test exact loo loo test error ffl test full line leave estimator ffl loo dashed line ffl exact loo dash dotted line function oe sonar data set 
left plot tap right plot svm 
crabs pima indians diabetes crabs pima indians diabetes data set available ripley 
crabs task classify sex crabs basis anatomical attributes color attribute 
examples available color sex making total examples 
examples color sex data file training making examples total rest testing 
training test split williams barber 
pima indians diabetes data set previous studies training test split examples respectively ripley 
task diagnose diabetes population women pima indian heritage information number plasma glucose concentration oral glucose tolerance test blood pressure skin fold thickness serum insulin body mass index diabetes pedigree function age 
baseline error rate obtained simply classifying test input positive 
results 
hyperparameters chosen values williams barber laplace approximation integral gaussian fields ml ii estimation hyperparameters 
results table 
comparisons taken williams barber laplace gp ripley 
results close state art 
performance may obtained www stats ox ac uk ripley 
pattern index pattern index left embedding strengths ff plotted different algorithms sonar data 
right mean activation hh different algorithms ordering left plot 
triangles support vectors circles naive mean field theory stars tap mean field theory 
sorted ascending order support vector ff value tap naive mean field solution rescaled length support vector solution 
quite sensitive choice hyperparameters may hopefully improve results hyperparameters tuned specific algorithm 
pima indians diabetes difference performance algorithms small contrary data sets covariance functions tested similar performance 
mean field approach binary classification gaussian processes 
method extension tap mean field theory known statistical physics 
avoided making special assumptions distribution randomness input data 
derivation resulting non linear equations posterior means new may applied problems probabilistic modeling 
mean field method applied statistical models nonsmooth functions parameters noise free classification approaches taylor expansions likelihood posterior fail 
naive mean field algorithm considered simplified version tap algorithm lower computational complexity derived 
shown support vector machines svms obtained limiting procedure tap approach 
built table results crabs pima indians diabetes 
crabs pima indians diabetes algorithm ffl test ffl exact loo ffl loo err 
ffl test ffl exact loo ffl loo err 
tap mean field naive mean field svm laplace gp hybrid monte carlo maximum likelihood ii best layer network simple perceptron logistic regression mars degree projection pursuit ridge functions gaussian mixture leave estimator generalization error provided tap mean field method naturally extend svm naive mean field case 
simulation result small benchmark data sets sonar mines versus rocks crabs pima indians diabetes give promising results matching state art performance 
number unresolved questions need addressed new algorithms may regarded practical tools 
discuss 
model selection 
far dealt automatic determination hyperparameters 
having reduced number hyperparameters remaining ones minimizing leave estimate range trial values 
able higher number hyperparameters search automated preferable gradient descent method suitable differentiable function rules discrete leave error count 
established candidate function exploits full power bayesian approach total probability data model 
termed evidence bayesian terminology mackay stochastic complexity rissanen mdl approach 
negative log evidence corresponds called free energy language statistical physics approximations available mean field theory opper winther csat 
preliminary results approach promising reported forthcoming 
assessing validity mean field approximation 
iterative scheme solving mean field equations fails far way telling equations solution numerical method simply appropriate specific choice parameters 
possible way approach problem conversion minimization suitable cost function tap free energy anderson palmer opper winther 
quality mean field approximations 
quality mean field approximations may directly assessed comparison monte carlo sampling neal approximate posterior averages arbitrary accuracy computer time invested 
hand toy models high dimensional input spaces possible derive exact bayesian learning curves replica method statistical mechanics mean field methods tested opper winther 
computational complexity 
computational complexity tap mean field algorithm number training examples requires inversion theta matrix 
algorithm impractical data sets larger thousands 
complexity naive mean field algorithm hand scale worse iteration update requires inner products terms iterative process expected converge finite number steps 
leave estimate requires matrix inversion 
evidence framework csat algorithm naive mean field theory interesting alternative svms larger datasets 
similarity results mean field theory svm algorithm suggests simplifications 
cases svms lead sparse representations small number support vectors mean field algorithms expected converge small number large embedding strengths 
setting remaining ones zero leaving corresponding examples training set early stage iteration may speed algorithm significantly 
may think hybrid approaches combine strengths svm bayesian approaches 
calculate embedding strengths directly svm algorithm exploiting sparseness directly mean field approximation bayesian evidence yardstick estimating hyperparameters kernel 
extension data models interesting try apply tap approach gaussian process models types likelihoods 
natural choice problem classification classes 
likelihood problem simply obtained softmax function class gaussian process 
tap approach requires integration likelihood gaussian distributions task unfortunately longer done exactly 
approximations treating usually low dimensional integrations necessary 
acknowledgments thankful sara solla grace wahba discussions chris williams anonymous referees constructive comments suggestions 
research supported swedish foundation strategic research 
input noise appendix discuss inclusion input noise probit model modification mean field algorithms implied 
define input noise model addition independent random variables activation function likelihood eq 
tjh theta assume noise gaussian zero mean variance probit regression neal 
gaussian noise original step function likelihood modified sigmoidal shaped error function gain variance noise tjh tjh phi th alternatively completely equivalent may shift variables noisy process 
process gaussian modified covariance matrix ij ffi ij likelihood mean field equations process noise free case 
noise simply included extra term covariance matrix 
simulations stick formulation 
importantly formulations equivalent tap naive mean field theory bayes prediction leave estimate hh ni formulations 
quantities theory invariant hh original process equivalent hh ff noisy process ff remain unchanged 
covariance functions thorough discussion covariance functions properties 
tested covariance functions 
simple perceptron 
infinite net arcsin 
gaussian exp gamma gamma 
cauchy gammax ffi obtained feed forward neural network models converge gaussian processes 
cases prior weights thresholds spherical gaussian 
infinite net covariance function obtained layer network linear output unit limit infinite number hidden units sigmoidal error function activation function phi gamma neal williams 
sign activation function may treated infinite net limit simply amounts removing terms denominator infinite net covariance function 
gaussian covariance function known example exponential covariance function 
may obtained radial basis network infinite number hidden units williams 
cauchy rational quadratic covariance function defined ffi 
simulations simply chosen ffi 
properties solution independent scale gaussian fields may fix say 
length scale input dimension defined important inputs defined associated relative short length scale vice versa unimportant length scales 
variance output threshold 
include effect gaussian noise fields term discussed appendix added diagonal covariance matrix 
note map approach williams barber formulation equivalent 
second moments appendix derive mean field expressions second moments hh gamma hh hh ni gamma hh ni linear response method 
derive detail indicate generalize result integrating parts eq 
find hh tjh rewrite term may apply linear response argument expressed derivative known quantity case hh achieved introducing fields added random activations likelihood term derivative wrt 
field setting tjh tjh fi fi fi tj tjh fi fi fi fi fi hh fi fi fi fi fi tj dhp tjh 
note moving derivative left normalization constant gives rise additional factor compared eq 

calculate linear responses hh gaussian approximations tap approach approximation assume adding small fields need consider changes means fields neglect changes variances 
eq 
get solve terms shifts ff related shifts means fields eq 
ij shift eq 
gamma defined eq 

substituting eq 
solving arrive eq 

repeat steps derive equations difference fact posterior distributions replaced corresponding posteriors ith pattern absent 
analogous eq 
find ii gamma jk ij ni ni gamma jk ki ni ni denote gamma theta gamma reduced matrices ith row column deleted 
assumed change negligible ith pattern removed 
identity partitioned inverse see 
press teukolsky vetterling flannery gamma gamma ii ii gamma jk ij ni gamma jk ki expression may simplified final result eq 

leave linear response subsequent derivation approximate leave loo estimator closely spirit similar procedures rely order expansions 
may motivated assumption small changes appropriate variables example removed training set 
see xiang wahba develop approximate loo estimator smoothing splines 
applications neural networks see 
larsen hansen 
simplicity equality expressions expect reader understand approximations 
assume remove example training set algorithm retrained remaining ones mean fields remaining inputs changed eq 
ik gamma ij ff general leave estimator simply ffl loo theta gammat hh set equal eq 

derivation general possible assume algorithm produces embedding strengths ff solution equation ff hh ff 
examples eqs 
respectively tap naive mean field theory 
may dependencies quantities assume response change hh ff negligible 
change expressed assuming changes small linear response hh ff analogous eq 
get gamma omega definition omega hh ff gamma may solve omega nj nj gamma ki ij ff order find example removed training set set eq 
eq 

matrix identity eq 
get gamma omega gamma ii gamma omega ff self consistent check tap mean field theory note equal hh ni gamma hh gamma ff case compare eq 
omega eq 
identified eq 

support vector machines appendix consider svm limit tap mean field equations 
tap equations svm learning simple perceptron spherical input distribution large input dimensionality previously derived wong 
firstly re derive kuhn tucker condition tap equations secondly show leave loo estimator especially simple limit 
show self consistently limit fi eq 
equivalent sending variances tap approach 
having factor fi prior rescaling covariance matrix factor fi eq 
hh ij ff introduced rescaled finite embedding strength ff ff fi 
asymptotic expression error function phi gammaz theta jzj eq 
extra factor minus margin obtain ff gammat hh ni fi theta gamma hh ni 
inserting back eq 
get hh hh ni gamma theta hh ni gamma 
putting expressions ff hh arrive kuhn tucker condition eq 

loo estimator eq 
need compute cavity average activation hh ni hh ni see immediately hh hh ni non support vectors contribute loo error 
need consider support vectors hh hh ni gamma fi ff determine fi done limit expressions eq 
expression eq 
giving fi gamma sv ii hh ni ii fi hh ni sv denotes covariance matrix support vector examples 
result proves self consistent assumption fi 
inserting result hh ni eq 
gives loo estimator svms eq 

review gaussian random fields correlation functions technical report norwegian computing center oslo norway 
nd edition 
adaptive perceptron algorithm europhys 
lett 

barber williams gaussian processes bayesian classification hybrid monte carlo neural information processing systems mozer jordan petsche eds 
mit press 
berger statistical decision theory bayesian analysis springer new york 
boser guyon vapnik training algorithm optimal margin classifiers th annual workshop computational learning theory pittsburgh acm 
cortes vapnik support vector machines machine learning 
csat opper ole winther efficient approaches gaussian process classification submitted nips 
gibbs mackay variational gaussian process classifiers preprint cambridge university 
gorman sejnowski analysis hidden units layered network trained classify sonar targets neural networks 
jaakkola haussler probabilistic kernel regression models appear proceedings conference ai statistics 
jordan ed learning graphical models mit press 
larsen hansen linear cross validation advances computational mathematics 
mackay bayesian interpolation neural computation 
mackay gaussian processes replacement neural networks nips tutorial may obtained wol ra phy cam ac uk pub mackay 
parisi spin glass theory lecture notes physics world scientific 
neal bayesian learning neural networks lecture notes statistics springer 
neal monte carlo implementation gaussian process models bayesian regression classification technical report dept statistics university toronto 
opper winther mean field approach bayes learning feed forward neural networks phys 
rev lett 

opper winther mean field algorithm bayes learning large feed forward neural networks neural information processing systems mozer jordan petsche eds 
mit press 
opper winther mean field methods classification gaussian processes advances neural information processing systems nips kearns solla cohn eds mit press cambridge ma 
opper winther gaussian processes svm mean field results leave large margin classifiers bartlett scholkopf schuurmans smola eds mit press 
papoulis probability random variables stochastic processes mcgraw hill series electrical engineering nd edition 
parisi statistical field theory frontiers physics addison wesley 
parisi mean field equations spin models orthogonal interaction matrices phys 
math 
gen 
convergence condition tap equation infinite range ising spin glass phys 

press teukolsky vetterling flannery numerical recipes cambridge university press 
ripley flexible non linear approaches classification statistics neural networks friedman wechsler eds springer 
ripley pattern recognition neural networks cambridge university press 
scholkopf burges smola editors advances kernel methods support vector learning mit press 
anderson palmer solution solvable model spin glass phil 
mag 

vapnik nature statistical learning theory springer verlag new york 
williams computing infinite networks neural information processing systems mozer jordan petsche eds 
mit press 
williams barber bayesian classification gaussian processes ieee trans pattern analysis machine intelligence 
williams rasmussen gaussian processes regression neural information processing systems touretzky mozer hasselmo eds mit press 
wahba spline models observational data cbms nsf regional conference series applied mathematics siam philadelphia 
wahba support vector machines reproducing kernel hilbert spaces randomized technical report rr dept statistics wisconsin published scholkopf burges smola 
wong microscopic equations stability conditions optimal neural networks europhys 
lett 

xiang wahba generalized approximate cross validation smoothing splines non gaussian data statistica sinica 

