active learning causal bayes net structure kevin murphy department computer science university california berkeley ca cs berkeley edu propose decision theoretic approach deciding interventions perform learn causal structure model quickly possible 
interventions impossible distinguish markov equivalent models nite data 
perform online mcmc estimate posterior graph structures importance sampling nd best action perform step 
assume data discrete valued fully observed 
essentially kinds approaches learning structure bayesian networks bns data 
rst approach tries nd graph satis es constraints implied empirical conditional independencies measured data pea sgs 
second approach searches space graphs uses scoring metric evaluate ch hec hec typically returning highest scoring model 
approaches structure learning su er fundamental problem nite data identify model markov equivalence 
adequate density estimation goal just predict observations inadequate goal causal discovery graphs markov equivalent imply set conditional independencies 
example markov equivalent represent zjy general graphs markov equivalent structure ignoring arc directions structures vp 
structure consists converging directed edges node 
bns markov equivalent different predictions consequences interventions markov equivalent di erent assertions ect changing 
way distinguish members markov equivalence class perform experiments 
experiments mean ideal interventions sense pearl pea learning agent clamp subset variables xed values 
example genetics domain experiment consist gene think clamping xed value 
manipulation theorem sgs pea says compute consequences interventions cutting arcs coming nodes clamped intervention doing probabilistic inference graph usual way 
assuming nodes observed hidden nodes pearl calculus compute consequences interventions pea 
straightforward modify existing bayesian scoring methods handle data obtained interventional studies addition usual passive observational data 
speci cally simply refrain updating parameters nodes clamped cy 
intuitive justi cation observing clamped node certain value tell value occur forced 
studied notable exception tk discuss section way decide interventions perform learn causal structure quickly cheaply possible :10.1.1.26.1873
goal 
adopt standard decision theoretic approach problem 
basic idea compute posterior probability distribution graph structures data gjd possible experimental action perform compute ex pected utility action respect current beliefs model 
update beliefs outcome experiment repeat 
basic framework discussed sections 
number directed graphs dags grows super exponentially number nodes form online mcmc approximate belief state gjd data seen time step number examples seen far 
addition computing expected utility action requires enumerating possible observations takes time assuming binary nodes 
approximate importance sampling 
discuss approximations section 
section show experimental results approximations di erent bayes nets 
diving technical details controversy concerning attempts learn causality data 
claim merely statistical signatures data suggest causal hypotheses checked experiment 
active learning basic idea approach illustrated uence diagram 
de ne value expected utility performing action yjg gjd 
utility function yjg probability generating observation performed action graph intervention action need 
set hypotheses dags considering set possible observations generate action past data 
best myopic action de ned arg max set possible actions set values clamp settable nodes 
goal infer model structure actions cost utility function log gja 
section show experimentally inferring correct model structure necessary predict ect interventions necessary simply predict observations 
closed form formula known number dags nodes rst values coo 
crude upper bound number boolean matrices 
uence diagram shot experiment design 
dotted line informational arc speci es data observed action square node chosen 
random variable observed action unknown graph parameters shown integrated 
diamond node utility 
going details show maximizing log gja equivalent maximizing expected kl divergence posterior prior equivalently minimizing conditional entropy ber familiar criteria 
max kl gja jjp gjd max yja gja log gja gjd denominator gjd independent may drop get min gjd yjg log gja optimal algorithm nite set dags considering 
nodes discrete nite sets compute optimal action exhaustive enumeration follows 
compute yjg compute gja yjg gjd log gja completeness brie summarise compute various equations algorithm 
standard techniques sequential bayesian structure learning fully observed discrete domains see sl bun hec details 
expression yjg predictive density graph modi ed action past data evaluated equation yjg ijk ijk ijk jg ijk ijk ijk ijk indicator function event occurs case 
number values take number values parents take 
nodes de ne ijk kj 
clamped nodes de ne ijk clamped ijk 
yjg contradicts values clamped nodes implied 
assume parameters node independent dirichlet priors hyperparameters ijk posterior mean parameters ijk ijk jg ijk ijk ijl ijl ijk ijk 
priors ijk hgc call bdeu metric 
ensures markov equivalent models marginal likelihood observational data ijk 
graphs share families cache eciently compute yjg 
example di er single edge addition deletion yjg yjg di er term 
marginal likelihoods yjg priors gjd compute posterior probability graph follows gja yjg gjd yjg jd uniform structural priors 
sampling algorithm optimal algorithm assumes exhaustively evaluate typically expensive 
discuss approximate turn 
sampling graphs basic idea metropolis hastings mh algorithm draw samples gjd 
algorithm summarized jg probability proposing move burn period number samples want draw 
see details 
choose random sample 
jg compute jd jg jd jg sample unif minf rg return gb gb proposal distribution sample uniformly neighborhood de ned set dags di er single edge addition deletion reversal 
way quickly checking proposed graph acyclic ancestor matrix described gc 
words jg jg ndb jp djg jp djg marginal likelihood ch djg ij ij ij ijk ijk ijk main advantage proposal distribution ecient compute bayes factor djg djg case edge reversal terms marginal likelihood ratio cancel 
mapv suggested searching smaller space markov equivalence classes dags inappropriate interventional data 
fk suggested searching smaller space total orderings nodes marginalizing actual structure 
converges faster chose pursue technique nd easier design priors heuristic proposal distributions space graphs orderings see section 
mh algorithm described appropriate ine batch computation 
need compute gjd online sequentially 
combine ideas particle ltering see ddfg mh follows 
belief state gjd represented set weighted particles samples 
new observation arrives apply small number mh moves particle get approximation gjd 
justi cation new belief state similar old new observation 
initialise sampling prior 
domains prior knowledge construct small set hypothetical models feasibly enumerate needing mcmc 
sampling observations nd small set probable candidate models size model set large making expensive sum possible observations 
nodes binary possible observations continuous valued nodes nite number 
case importance sampling 
basic idea approximation ys gjd log gja set sampled drawn proposal distribution 
normalized importance weights yjg 
combining approximations modify active learning algorithm follows 
sample 
jd mcmc sample 
ja compute yjg compute gja set wy gjd log gja simplest proposal distribution uniform assumed nodes binary action clamped nodes 
optimal proposal course yjg 
case algorithm sample 
jd mcmc sample 
jg set jy compute ja gjd log gja jg typically expensive 
cheaper approximation sample mixture distribution yjg gjd 
just sample uniform distribution 
sampling actions consider max computation 
clamp nodes simultaneously nodes binary jaj 
dicult expensive clamp nodes simultaneously small low order polynomial 
jg 
jy computations action expensive 
currently working heuristic methods avoid brute force enumeration 
continuous valued nodes number actions principle nite practice reasonable discretize action space 
example genetics domain able clamp node wildtype mean value clamp standard clamp 
see bmi interesting mcmc approach selecting continuous valued actions 
samples 
question samples need take interesting 
key insight action selection relative values matter 
idea exploited ok reduce number samples see hoe ding races approach mm 
just xed number samples 
results compare behavior active learner algorithms passive observation random interventions 
assume clamp nodes simultaneously legal values 
various ways compare algorithms 
posterior graphs big easily visualize way compressing tk compute edge error induced belief state time experiments performed matlab bayes net toolbox available www cs berkeley edu bayes bnt html :10.1.1.26.1873
number examples edge error number examples cancer network random cpd parameters uniform distribution 
lines top bottom represent passive random active learning 
sampled observations sampled graphs 
averaged trials 
number examples pas rnd act edge error number examples asia network published cpd parameters 
lines top bottom represent passive random active learning 
computed exactly sampled graphs averaged trials 
number examples pas rnd act edge error number examples car network random cpd parameters beta distribution 
top dotted line passive learner lines random active learners 
sampled observations sampled graphs averaged trials 
prediction performance interventions asia network 
vertical axis negative log likelihood horizontal axis number examples 
dotted line passive learner lines random active learners 
averaged trials 
gjd err indicator variable edge true graph structure means connection 
henceforth shall call true model oracle 
metric evaluate ability models predict observations drawn oracle generated response intervention oracle 
simply generate test set size possibly oracle compute average negative log likelihood current belief state learner err pred log jg gjd consider performing interventions oracle corresponds clamping nodes simultaneously random values sampling nodes 
test set consists cases 
comparable tk test algorithm commonly networks node cancer network fmr node asia network ls node car trouble shooter network :10.1.1.129.4770:10.1.1.26.1873
networks binary nodes multinomial conditional probability distributions cpds 
asia network published parameters 
networks sampled cpd parameters dirichlet distribution 
changing parameters dirichlet control degree determinism 
speci cally ijk cpts deterministic ijk cpt entries chosen uniformly subject sum constraint ijk row cpt tends maximum entropy distribution 
figures show error decreases number examples increases 
cancer asia networks see active learner identi es structure quickly random passive learners 
car trouble shooter network active learner better random learner presumably sampling long 
plan investigate 
plot prediction performance learners asia network 
see passive learner predict observations oracle slightly better learners initially wrong structure 
figures indicate knowing right structure helps predict ect interventions expected especially number clamped nodes increases 
true models especially deterministic parameter regime 
related previous active learning context classi cation regression function optimization 
case linear regression minimum posterior entropy criterion discussed section called optimality maximized closed form 
results derived linear regression model bayesian non bayesian setting see cv review 
active learning non linear regression models neural networks mac locally weighted regression objective minimize expected variance predictor 
works active learner choose point input space 
contrast query ltering paradigm learner choose see label certain items stream inputs see 
pac setting ang showed ability ask questions reduces problem identifying certain kinds boolean functions np complete polynomial time 
tr active learning tree structured boolean functions internal nodes hidden 
results concerning upper lower bounds number experiments necessary learn possibly cyclic boolean networks 
discusses active learning techniques learning boolean networks entropy cost function 
closely related tong koller tk decision theoretic framework bn structure learning :10.1.1.26.1873
shall henceforth refer tk algorithm 
tk algorithm builds fk mcmc total orderings nodes dags 
space orderings size smaller space dags space size 
key insight conditioned ordering parents node chosen independently longer global acyclicity constraint marginalized 
tk extend showing conditioned perform needed expected utility computation variable elimination algorithm 
advantages algorithm tk simplicity fact assumptions form loss function fact incorporate prior structural knowledge 
loss function tk loss gja ij ij ij entropy associated edge distribution nodes fairly intuitive theoretically motivated 
advantage tk algorithm speed 
firstly computes analytically importance sampling 
secondly uses mcmc smaller space orderings 
clear big speedup cost variable elimination depends induced width graph turn depends summation ordering structure graphs induced set possible families induced width quite large 
fk shown mcmc space orderings converges faster space dags believe approach really practical need strong prior knowledge easier specify prior natural gj 
issues pursue main ones continuous variables missing data online learning dynamical systems 
applications biology 
continuous variables plan cpds possibly non linear basis functions discussed 
nonlinearities di erent global jointly gaussian approach hg 
missing data plan sampling data augmentation 
actions consist choosing measure hidden variable classical value information computations 
online learning longer keep dataset store expected sucient statistics possible models 
approach keep statistics just fringe probable models fg 
straightforward adapt techniques learn structure dynamic bayesian network dbn time series data fmr :10.1.1.129.4770
allow arcs time slices diachronic arcs parents node chosen independently feature subset selection 
get advantages tk algorithm need sample node orderings 
eventually hope apply algorithm problem experiment design inferring structure gene regulatory networks 
stuart russell michael jordan comments earlier draft 
miyano 
cation gene regulatory networks strategic gene disruptions gene 
siam symposium discrete algorithms 
ang angluin 
queries concept learning 
machine learning 
ber bernardo 
expected information expected utility 
annals statistics 
hancock hellerstein 
learning boolean read formulas generalized bases 
comp 
systems sciences 
bmi mueller ios 
decision analysis augmented probability simulation 
management science 
bun buntine 
operations learning graphical models 
ai research pages 
cohn ghahramani jordan 
active learning statistical models 
ai research 
ch cooper herskovits 
bayesian method induction probabilistic networks data 
machine learning 
coo cooper 
overview representation discovery causal relationships bayesian networks 
glymour cooper editors computation causation discovery 
mit press 
cv 
bayesian experimental design review 
technical report univ minnesota 
www stat umn edu papers tr html 
cy cooper yoo 
causal discovery mixture experimental observational data 
uai 
ddfg doucet de freitas gordon 
sequential monte carlo methods practice 
springer verlag 
fg friedman goldszmidt 
sequential update bayesian network structure 
uai 
fk friedman koller 
bayesian network structure 
uai 
fmr friedman murphy russell :10.1.1.129.4770
learning structure dynamic probabilistic networks 
uai 
freund seung shamir tishby 
selective sampling query committee algorithm 
machine learning 
gc 
improving markov chain monte carlo model search data mining 
machine learning 
appear 
heckerman breese 
troubleshooting uncertainty 
technical report msr tr microsoft research 
hec heckerman 
bayesian approach learning causal networks 
uai 
hec heckerman 
tutorial learning bayesian networks 
jordan editor learning graphical models 
mit press 
hg heckerman geiger 
learning bayesian networks uni cation discrete gaussian domains 
uai volume pages 
hgc heckerman geiger chickering 
learning bayesian networks combination knowledge statistical data 
machine learning 
karp 
discovery regulatory interactions perturbation inference experimental design 
proc 
paci symp 
biocomputing 
ls lauritzen spiegelhalter 
local computations probabilities graphical structures expert systems 
stat 
soc 

mac mackay 
information objective functions active data selection 
neural computation 
mapv madigan anderson perlman 
bayesian model averaging model selection markov equivalence classes acyclic graphs 
communications statistics theory methods 
mm maron moore 
hoe ding races accelerating model selection search classi cation function approximation 
nips 
madigan york 
bayesian graphical models discrete data 
intl 
statistical review 
ok ortiz kaelbling 
sampling methods action selection uence diagrams 
aaai 
pea pearl 
causality models reasoning inference 
cambridge univ press 
sgs spirtes glymour scheines 
causation prediction search 
mit press 
nd edition 
sl spiegelhalter lauritzen 
sequential updating conditional probabilities directed graphical structures 
networks 
tk tong koller :10.1.1.26.1873
active learning structure bayesian networks 
ijcai 
submitted 
tr tadepalli russell 
learning examples membership queries structured determinations 
machine learning 
vp verma pearl 
equivalence synthesis causal models 
uai 
