detections bounds timelines umass tdt james allan victor lavrenko malin russell swan report presents system university massachusetts participation tdt tasks year detection story detection story link detection 
task discuss parameter setting approach results system test data 
addition tdt evaluation approaches show tracking performance sites achieving expected information retrieval technology 
show story detection system tracking approach sufficiently accurate purposes 
overview automatic timeline generation system developed tdt data 

basic system core tdt system uses vector model representing stories represent story vector term space coordinates represent frequency particular term story 
terms features vector single words reduced root form dictionary stemmer 
system originally developed summer workshop johns hopkins university center language speech processing 
detection algorithms system supports models comparing story previously seen material centroid agglomerative clustering nearest neighbor comparison 
centroid approach group arriving documents clusters 
clusters represent topics discussed news stream past 
cluster represented centroid average vector representatives stories cluster 
incoming stories compared centroid cluster closest cluster selected 
similarity story closest cluster exceeds threshold declare story old adjust cluster centroid 
similarity exceed threshold declare story new create new singleton cluster story centroid 
nearest neighbor second approach nn attempt explicitly model notion topic declares story new story seen 
incoming stories directly compared stories seen 
similar neighbors story similarity neighbors exceeds threshold story declared old 
story declared new 
center intelligent information retrieval department computer science university massachusetts amherst ma 
similarity functions important issue approach problem determining right similarity function 
considered functions cosine weighted sum language models divergence 
critical property similarity function ability separate stories discuss topic stories discuss different topics 
cosine cosine similarity classic measure information retrieval consistent vector space representation stories 
measure simply inner product vectors vector normalized unit length 
represents cosine angle vectors 
note unit length denominator angle calculated simple dot product 
cosine similarity tends perform best full dimensionality case comparing long stories 
performance degrades vectors shorter 
built length normalization cosine similarity dependent specific term weighting performs raw word counts weights 
weighted sum weighted sum operator query retrieval engine developed center intelligent information retrieval ciir university massachusetts 
inquery bayesian inference engine transition matrices restricted constant space deterministic operators sum 
weighted sum represents linear combination evidence weights representing confidences associated various pieces evidence represents query vector represents document vector 
instance centroid model cluster centroids repre sent query vectors compared incoming document vectors 
weighted sum tends perform best lower dimensionality query vector fact devised specifically provide advantage short user requests typical ir 
performance degrades slightly grows 
addition weighted sum performs considerably better combined traditional tf idf weighting discussed 
language model language models furnish probabilistic approach computing similarity document topic centroid clustering documents nearest neighbour 
approach previously seen documents clusters represent models word usage estimate model source generated newly arrived document specifically estimating estimated background model corresponding word usage general english 
making assumption term independence unigram model rewrite represent individual tokens maximum likelihood estimator simply number occurrences divided total number tokens models may sparse words document may zero probability model resulting alleviate problem smoother estimate allocates non zero probability mass terms occur set witten bell estimate tokens model number unique tokens 
note detection tasks online tasks may words smooth similar fashion uniform model unseen words 
total number divergence treating document sample came models view distribution compute information theoretic measure divergence distributions 
measure experimented divergence relative frequencies word 
respectively smoothed appropri represent 
feature weighting important issue weighting individual features words occur stories 
traditional weighting employed ir systems form tf idf weighting 
tf idf tf component weighting represents degree term describes contents document 
idf component intended discount common words collection function words 
particular tf idf scheme inquery engine tf comp component general form tf raw count term occurrences document influences significance attach seeing consecutive occurrences term particular document 
functional form strictly increasing asymptotic tf grows bounds 
effect assign lot significance observing single occurrence term significance consecutive occurrences 
observation documents contain occurrence word contain successive occurrences parameter influences aggressively discount successive occurrences inquery set document length average document length collection 
means shorter documents aggressive discounting longer stories assign lot significance single occurrence term 
idf comp component logarithm inverse probability term collection normalized 
denotes total number documents collection df shows documents term occurs 
particular idf formulation arises naturally probabilistic derivation document relevance assumption binary occurrence term independence 
tf weighting scheme simply actual tf value formula number times term occurs story 
intuition omitting idf component feature selection points process choose medium high idf features discrimination value 
result tf weighting scheme high dimensionality low idf features appear need 
idf weighting scheme simply raw tf component times idf component tf idf scheme 
weighting method boosts importance multiple occurrences feature tf idf scheme 

story detection tuned parameters choice detection algorithms running fsd experiments january june tdt corpus 
months served training corpus months development corpus 
ran experiments entire months data 
detection algorithms centroid nearest neighbor similarity measures cosine weighted sum weighting schemes tf idf idf tf thresholds varied 
parameter selection basis det curves topic weighted fsd norm values 

parameter setting set initial parameters entire months training data 
small margin best det curve generated nn clustering algorithm cosine similarity dimensionality tf idf weighting scheme 
run parameters idf weighting scheme came close second 
tf weighting scheme parameters came third 
alternative clustering algorithms lower dimensionality proved effective 
previous experiments indicated optimal threshold tried promising weighting schemes threshold values specifically 
table shows fsd norm values different thresholds applied nn cosine comparison tf idf weighting feature system months fsd norm decide actual parameter values month split data 
varied weighting scheme tf idf idf promising weighting schemes threshold values 
top runs tf idf weighting scheme 
optimal threshold training collection 
optimal threshold development 
table shows results weights jan apr may jun fsd norm tf idf tf idf tf idf tf idf idf idf idf basis results final runs nn clustering algorithm cosine similarity dimensionality tf idf weighting scheme threshold 
score primary story detection evaluation sr te eng nat boundary def 
graph shows error tradeoffs 
story link detection choice similarity measures weighting schemes thresholds running story link experiments january june tdt corpus 
similarity measures sampled cosine weighted sum language model divergence 
weighting schemes sampled tf idf idf tf 
previous experiments indicated best threshold tried values range 
usual parameter selection basis det curves topic weighted link norm values 
table shows top runs cosine similarity dimensionality idf weighting scheme 
set parameters threshold yielded lowest link norm value 
months link norm easy way divide training collection training development collections verify parameters running month subsets 
score primary link detection evaluation sr te eng nat def 
det plot shows tradeoff false alarm errors mid january nist released new link detection index file performed additional run 
link norm score run order better score 
know link detection subset stories easier larger set 

detection story detection ran wide parameter sweeps month january june tdt corpus 
confirmed settings narrower parameter sweeps finer granularity month month training development corpora 
checked choice parameters different languages 
languages tried eng nat english corpus natural language english mul eng english mandarin mandarin translated english mul nat english mandarin natural language 
month eng nat collection varied clustering algorithm weighting scheme threshold 
clustering algorithms sampled nn centroid 
weighting schemes sampled tf idf tf idf 
thresholds range 
chose threshold range basis previous experiments 
optimal combination months data nn dimensionality idf weighting threshold 
trend showed month data 
table shows sample runs weighting months months det norm idf tf experiments month eng nat training corpus nn clustering algorithms wider range thresholds yield improvement 
duplicated english parameter sweep multilingual collection stories english mul eng 
results indicated parameter choice collection far stable languages 
month collections confirmed parameter choices basis eng nat collection 
runs table show optimal parameter combination nn dimensionality idf weighting threshold 
weighting months months det norm idf tf tried multilingual collection original languages mul nat month collections 
month mul nat training corpus showed idf weighting scheme nn threshold values promising 
strengthened confidence set parameter choices stable languages 
table shows values runs nn weighting det norm months idf idf month mul nat development corpus placed optimal threshold left parameters 
basis results nn clustering algorithm idf weighting scheme threshold official detection runs 
results primary detection evaluations multilingual english task sr te mul eng boundary def 
det curve shows error tradeoffs run multilingual original languages task sr te mul nat boundary def result 
error tradeoff run 
bounds effectiveness section show things 
tracking performance approximately expect state art information filtering systems trec 

fsd system built tracking system extremely fsd effectiveness satisfactory 
suggest fsd unsolvable effective fsd simple matter improving tracking technology 
section described detail rate tracking nt tracking nt filtering nt filtering nt false alarm rate det plot filtering tracking runs query generated stories 

expected tdt performance det curve shows tracking runs tdt evaluation data 
shows runs trec filtering task modified tracking 
thing graph shows tracking performance near performance filtering achieves similar starting information 
tasks run completely different corpora different definitions tracking performance approximately filtering performance predicts 
hypothesize wildly different performance tasks news topics focused oklahoma city bombing trec filtering queries drug benefits 
result single story representative news topic take documents isolate information pertinent hidden query 

bounds fsd possible solution fsd apply tracking technology 
intuitively system marks story corpus high score story topic corpus 
begins tracking story 
second story tracks assigned low fsd score 
track topic story assigned high fsd score system starts tracking 
point system tracking numerous topics fact system fsd false alarm tracking topics multiple ways 
clear perfect tracking system yields perfect fsd system 
tracking systems far perfect 
sort fsd performance expect state art tracking system 
possible derive expected fsd error rates average tdt error rates omitted 
result lower expected fsd performance 
emphasize predictions sense assume fsd system uses approach tracking 
shows rate actual fsd tracking nt predicted upper lower fsd bounds false alarm rate lower left graph tracking det curve upper part graph shows lower upper bound predicted performance tracking fsd error rates grey actual system performance fsd system black 
appropriate det curves 
note fsd error rates fall nicely performance predicted tracking 
result suggests fsd system working expect 

difficulty improving fsd predicted actual error rates tracking fsd system fact unacceptably high applications matter threshold det curve 
assume reasonable fsd performance approximately equal tracking det curve shown lower left curve 
system misses stories generating false alarms acceptable applica rate desired fsd performance needed upper lower fsd bounds tracking false alarm rate shows desired fsd performance black surrounded reasonable confidence intervals 
extreme lower left curve corresponding tracking performance 
monica middle school sexual advance daimler benz industrial merger oregon school shooting terry nichols death penalty super bowl northern ireland easter louise woodward au pair dismiss menu hide names hide places hide orgs oregon springfield kip ap thurston high school ore kip dismiss menu title get histogram show context ad 
january february march april may june overview january june 
topic labeled monica highest ranked topic topic labeled middle school second highest ranked 
pop oregon school shooting shows significant named entities oregon springfield kip ap thurston high school ore pop displays obtaining information kip 
tions 
shows desired fsd curve really just tracking curve lower upper bounds errors encompass 
order achieve bounds improve tracking performance factor 
resulting det curve small line segment lower left 
research tdt tdt tdt resulted tracking det curve substantially better ones 
shown section level effectiveness comparable achieved years filtering research trec 
little reason believe tracking technology improve fold 
shown reduce fsd problem tracking task 
shown error rate tracking results substantially worse error rates corresponding fsd system 
importantly shown little reason believe tracking fsd effectiveness raised point technology widely useful 

automatic timeline generation developed technique determining relative importance occurrence extracted features text 
technique requires explicitly time tagged corpus tdt stories arrive known times 
technique able analyze extracted features named entities noun phrases explicitly rank features high content bearing 
able group features clusters correspond strongly notion topic defined topic detection tracking tdt study 
figures show examples system running 
described detail model tokens emitted random processes assume hypotheses defaults 
assumptions random processes generating tokens stationary meaning vary time random processes pair super bowl monica january tucker execution cable car dismiss menu execution dismiss menu dismiss menu hide names cable car hide places resort hide orgs marine ski resort mountains atlantic ski aircraft marine plane low level pilot ea altitude cable february detail january march 
topics shown monica super bowl tucker execution cable car 
additional phrases displayed tucker execution cable car crash 
tokens independent 
measure look features violate hypotheses 
details omitted 
built system constructed timelines shown 
curious finding reasonable events ran small evaluation 
entire tdt corpus experiment training month development set evaluating held months 
corpus tagged bbn nymble tagger identified unique named entities 
extracted noun phrases running shallow part speech tagger labeling noun phrase groups words length matched regular expression noun adjective noun 
led set unique noun phrases 
final run evaluation portion tdt produced clusters features pairing features value time imposing threshold paired 
believe clusters features indicative major news stories covered news organizations time spanned corpus 
felt clusters highly suggestive major news stories provided sum mation obtained unordered collection features 
test hired students undergraduates graduate student evaluate clusters 
clusters judged times judgments 
evaluators great majority groups indicative single topic groups judged pairwise overlap judgments topics contained group 
overlap expected chance nearly pairwise kappa statistics ranged weighted average value 
kappa statistic measure reliability value indicates overlap expected chance value indicates perfect overlap 
kappa value indicates poor agreement evaluators data reliable 
seen looking scores individual groups 
judged single topic majority assessors assessors agreed 
asked assessors compare generated groups tdt topics indicate agreed 
results stronger 
pairwise overlap topic group matches pairwise kappa statistics ranged average value indicating agreement 
indicates topic defined features system selects sufficient recognizing topic 
groups terms automatically labeled assessors asked rate usefulness label 
assessors asked rank point likert scale 
general assessors felt labels poor average rank poor excellent 
assessors agreement rankings average standard deviation 
feel techniques study significant contribution accessibility information allows automatic generation interactive overview timelines modest cost 
archives news mails historical newspapers memos time corpora increasingly common digital libraries feel system tremendous tool allow broader access electronic information 

results detection tasks acceptable high quality liked 
believe hit limits effectiveness reached simple ir approaches story topic comparison 
spent considerable effort including months summer working fsd unable achieve great improvements system 
major finding workshop extended idea tracking fsd systems effective 
result idea current approaches hit limits 
believe event information organization realized tdt requires substantially different approaches ideas 
briefly automatic timeline generation believe serves example moving tdt ideas new directions 
hope richer set ideas directions yield new approaches techniques addressing existing tdt tasks new tasks arise 
acknowledgments supported part national science foundation library congress department commerce cooperative agreement number eec part national science foundation number iri part sd number part air force office scientific research number 
opinions views findings contained material authors necessarily reflect position policy government official endorsement inferred 

allan jin rajman wayne gildea lavrenko 
topic novelty detection summer workshop final report 
available www jhu edu ws tdt 

james allan victor lavrenko hubert jin 
comparing effectiveness tdt ir 
technical report ir university massachusetts department computer science ciir 
conference submission 

bikel miller schwartz weischedel 
nymble high performance learning name finder 
fifth conference applied natural language processing pages 
acl 

russell swan james allan 
extracting significant time varying features text 
eighth international conference information knowledge management cikm pages kansas city missouri november 
acm 

russell swan james allan 
automatic generation overview timelines 
technical report ir university massachusetts department computer science ciir 
conference submission 

witten bell 
zero frequency problem estimating probabilities novel events adaptive text compression 
ieee transactions information theory 

xu broglio croft 
design implementation part speech tagger english 
technical report ir center intelligent information retrieval university massachusetts amherst 
