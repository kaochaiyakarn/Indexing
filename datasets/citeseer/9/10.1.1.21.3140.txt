draft june learning learn sebastian thrun www cs cmu edu thrun computer science department robotics institute carnegie mellon university pittsburgh pa 
past decades research machine learning data mining led wide variety algorithms learn general functions experience 
machine learning maturing begun successful transition academic research various practical applications 
generic techniques decision trees artificial neural networks example various commercial industrial applications see 
learning learn exciting new research direction machine learning 
similar traditional machine learning algorithms methods described book induce general functions experience 
book investigates algorithms change way generalize practice task learning improve 
illustrate utility learning learn worthwhile compare machine learning human learning 
humans encounter continual stream learning tasks 
just learn concepts motor skills learn bias learn generalize 
result humans able generalize correctly extremely examples just single example suffices teach new thing see 
deeper understanding computer programs improve ability learn potentially enormous practical impact 
years field significant progress theory learning learn practical new algorithms led impressive results real world applications 
book provides survey exciting new research approaches written leading researchers field 
objective investigate utility feasibility computer programs learn learn practical theoretical point view 

definition mean algorithm capable learning learn 
aware danger naturally arises providing technical definition folk psychological term term learning lacks satisfactory technical definition section proposes simplified framework facilitate discussion issues involved 
defining term learning 
mitchell 
task 
training experience 
performance measure computer program said learn performance task expected improve experience 
example supervised learning see various addresses task approximating unknown function experience form training examples may distorted noise 
performance usually measured ratio correct incorrect classifications inverse proportional squared approximation error 
reinforcement learning name second example addresses task selecting actions maximize reward :10.1.1.117.6173
performance average cumulative reward experience obtained interaction environment observing state actions reward 
mitchell definition define means algorithm capable learning learn 

family tasks 
training experience tasks 
family performance measures task algorithm said learn learn performance task expected improve experience number tasks 
put differently learning algorithm performance depend number learning tasks benefit presence learning tasks said learn learn 
algorithm fit definition kind transfer occur multiple tasks positive impact expected task performance 
learning scenarios easy specify algorithm learns learn 
particular learning tasks equivalent training experience individual task just added regular learning algorithm definition fit notion learning learn 
particular interest scenarios learning tasks differ 
example consider tasks learning recognize different faces 
person face looks alike examples learning task blindly augment set examples 
hypothesize face recognition tasks share certain invariances identity person invariant facial expression viewing perspective illumination 
invariances learned transferred different learning tasks algorithm improve performance number tasks 

analysis theoretical research complexity learning shown fundamental bounds performance achievable learning experience see 
algorithms learn learn side step bounds transferring knowledge learning tasks see 
see consider problem learning function noise free examples 
standard result blumer colleagues relates size hypothesis space number noise free training examples required learning function theorem 
function space functions probability hypothesis error larger consistent noise free dataset size gamma jhj 
words gamma ln gamma ln jhj ln ffi training examples suffice ensure probability gamma ffi hypothesis consistent data produce error larger data 
bound independent learning algorithm required produces hypothesis consistent data 
holds independently choice sampling distribution long distribution training testing 
notice equation logarithmic hypothesis set size jhj 
consider problem learning functions noise free examples 
enable learning algorithm improve performance number learning tasks assume target functions share common set properties 
domain face recognition example target function invariant respect translation scaling facial expression 
invariances understood properties target functions obey 
suppose properties initially unknown 
learning algorithm considers pool candidate properties denoted pm key idea identifying right properties hypothesis space diminished yielding accurate generalization data 
simplify formal analysis assume property holds true subset functions denote fraction functions property reasons simplicity assume 
assume properties independent knowing certain properties correct target functions tell correctness property 
assumption algorithm check constant error correctness property training examples learning tasks 
simplistic model allows assertions reduction hypothesis space 
lemma 
set properties consistent learning tasks reduces size hypothesis space factor probability reduction removes target functions hypothesis space considered failure bounded learning tasks common properties correct ones identified probability proof lemma straightforward 
smaller hypothesis spaces yield better generalization contain target function 
applying lemma blumer theorem advantage smaller hypothesis spaces expressed formally reduction sampling complexity learning new function 
corollary 
conditions lemma upper bound number training examples blumer theorem reduced factor gamma ln ln ffi ln jhj probability reduction erroneously removes target function bounded gamma analogous logarithmic lower bound obtained results derived ehrenfeucht colleagues 
similar analysis 
key idea underlying analysis explicit levels learning meta level base level 
base level learning problem problem learning functions just regular supervised learning 
meta level learning problem problem learning properties functions learning entire function spaces 
learning meta level bears close resemblance baselevel learning best exemplified baxter analysis shown standard bounds relating accuracy sample complexity hypothesis space size applied meta level 
important consequence algorithm learns learn possess bias levels just regular learning algorithm 
bias meta level constitutes priori assumptions concerning relatedness learning tasks just regular base level bias brings bear assumptions concerning relation individual data points 
exist uniquely best algorithm general problem learning learn just best algorithm learning se cf 
fact provably best algorithms special cases general problem 

representations key learning learn representation 
improve performance learning algorithm increasing number tasks algorithm change way generalizes capable representing knowledge determines bias 
date appears major families approaches approaches partition parameter space conventional learning algorithm parameters general cross task parameters approaches learn shape constraints superimposed learning new function 

partitioning parameter space conventional learning algorithm transformed algorithm learns learn subdividing parameters set task specific parameters set general parameters tasks 
type approach rests observation 
approaches outlined search parameters space combines task specific parameters general parameters 
principle distinction rigor existing algorithms fact sharply distinguish task specific general parameters 
ffl recursive functional decomposition 
functional decomposition approaches rest assumption task achieved function form ffi alternatively ffi task specific learning specific training examples learn tasks knowledge improve results learning new function 
transfer functional decomposition approach particularly effective complexity larger individual examples functional decomposition ffi popular neural network literature :10.1.1.57.3196
approaches assume represented layered multilayer perceptrons share hidden layer input hidden weights 
examples opposite functional decomposition ffi speaker adaptive speech recognition adaptive filtering see 
example complex module recognizes speech low complexity filter increases understandability signal filter accent non native speaker 
families approaches assumptions nature learning tasks 
baxter shown analytically decomposition assumption correct complexity small compared reduction sample complexity dramatic 
various practical findings confirm results 
ffl piecewise functional decomposition 
related family approaches rest assumption function represented collection functions hm partially defined subspace input space 
number building blocks small compared number learning tasks complexity approach reduce sample complexity 
piecewise functional decomposition popular learning families sequential decision tasks :10.1.1.117.6173
assumption multiple policies selection actions consist building blocks known need combined yield new policy 
various reinforcement learning algorithms described basically learn learn piecewise functional decomposition proposed context 
approaches rely static ways determine appropriate pieces 
example partial functions defined hierarchy sub goals different coarse grained resolution hierarchy control voronoi tessellation specific geometric landmark states pre designed behavioral decompositions 
approach proposed identifies decomposition fly utilizing minimum description argument 
ffl learning declarative procedural bias 
piecewise recursive functional decomposition particularly explored symbolic algorithmic approaches machine learning learn declarative procedural knowledge 
bias represented rules declarative bias straight program code procedural bias representation may employed bias learned functions 
consequently knowledge acquired task may bias 
examples learning systems modify declarative bias soar inductive logic programming theory revision ralph 
procedural bias learned genetic programming approach schmidhuber colleagues 
notice rules program code viewed partially defined functions concatenated recursively learning declarative bias symbolic program code version combines piecewise recursive functional decomposition elegant way 
ffl learning control parameters 
function approximators possess control parameters 
control parameters provide natural way partitioning parameter space groups 
sutton proposed learn transfer control parameters search hypothesis space 
choice inductive learning algorithm described control parameter 
approaches mcs select combine entire learning algorithms pool algorithms 
net effect transferring knowledge multiple learning tasks course bounded richness expressiveness control parameter space 
popular example learning control parameters context memory methods nearest neighbor 
distance metric comparing instances memory learning parameterized 
various algorithms described adjusting parameterized distance metric :10.1.1.27.8372
assuming optimal distance metric metric yields best generalization performance fixed amount data tasks approaches optimize distance metric multiple tasks effectively fit definition learning learn 

learning constraints second family algorithms learns constraints 
constraints usually represented separate data structures constraint learning algorithms obtained partitioning parameter space conventional machine learning algorithm 
number existing algorithms falls category considerably smaller 
ffl synthetic data 
training data imposes constraints function learned 
way impose constraints function approximator generate new synthetic data real data cf 

assumptions underlying approach rules transforming data learning task considerably easy learn learned real data replaced augmented artificially synthesized data 
rules correspond invariances domain 
beymer poggio exploited idea generate virtual views faces single view face generic technique learning pose parameters face recognition 
transformations learned performance system increases number learning tasks 
ffl slope constraints 
second group constraint learning approaches learns slope constraints 
data point previously acquired knowledge constrain slope function learned 
idea incorporating slopes neural network learning due simard colleagues proposed encode certain known invariances character recognition directional slope information 
ebnn algorithm extends approach learns slope constraints 
ebnn rests assumption recursive functional decomposition approaches listed new target functions composed previously learned functions 
constructing new target function previously learned function ebnn derives slope information constrain target function 
ebnn shown empirically robust errors various application domains chess object recognition robot control see 
ffl internal constraints 
constraints learned superimposed internal aspects function approximator 
example edelman proposed context face recognition learn directions submanifolds face images invariant 
done learning changes activations faces rotated translated specific internal representational space 
invariances assumed equivalent faces learned project new faces back canonical frontal view easier recognize 
detailed overview specifically connectionist approaches 
reader may notice approaches listed applied problem learning learn 
number algorithms systematically evaluated context multiple learning tasks considerably small 

issues apart different representations researchers explored various facets general problem 
ffl incremental vs non incremental approaches 
learning tasks attacked incrementally parallel 
methodologies potential advantages disadvantages 
tasks arrive see incremental approaches memorize training data consume memory 
non incremental approaches cf 
discover commonalities different learning tasks difficult find learning tasks processed sequentially :10.1.1.57.3196
ffl vs selective transfer 
approaches weight learning tasks equally transferring knowledge 
shown study transferring knowledge un selectively hurt performance cases learning tasks meet inductive assumptions implicit explicit underlying learning algorithm 
approaches examine relation learning tasks transfer knowledge selectively tc algorithm proposed robust unrelated learning tasks 
ffl data sharing 
scenarios data partially shared different learning tasks 
example suddarth learning hints caruana multitask learning rests assumption input patterns tasks output labels differ 
contrast baxter theoretical analysis architecture assumes training examples generated independently learning task 
ffl initial search bias vs search constraints 
merely technical matter way knowledge tasks incorporated 
approaches previously learned knowledge initial point parameter search see incorporate knowledge constraint search see 
study sullivan compared methodologies empirically characterized key advantages 
ffl performance tasks 
scenarios performance tasks important contain designated performance task task performance matters 
baxter analyzed cases particular architecture pointed commonalities differences 

perspective surprisingly real world learning scenarios naturally give raise multiple learning problems provide opportunity synergy 
example mobile service robot trained find fetch objects fact learn benefit variety tasks perceptual tasks recognize object recognize landmark control tasks avoid collisions obstacles prediction tasks predict location obstacles objects name just 
huge variety application domains naturally contain families learning tasks ffl cursive handwriting recognition ffl speech recognition ffl computer vision face recognition object recognition ffl stock market analysis prediction ffl language acquisition ffl personalized user interfaces ffl software agents internet agents numerous 
imagination expertise reader find 
caruana provides excellent discussion real world problems provide opportunity learning multiple tasks 
traditional machine learning approaches tackled learning problems separately thoughts results book hope learning simultaneously better performance achieved data 
putting technical detail aside ideas algorithms described appear applicable application involves cheap data expensive data 
includes systems trained customer data expensive practice learning task factory data cheap 
believe deeper understanding learning learn long run provide new explanations human learning abilities change bias transfer knowledge appears play important role human generalization see 

outline outline book ffl text suggested schmidhuber modified read chapter schmidhuber approach learning learn 
learner leads single life actions continually executed systems internal state current policy action considered learning algorithm modify system policy 
explicit difference learning meta learning meta meta learning effects learning processes learning processes measured reward time ratios 
occasional backtracking enforces histories policy modifications corresponding lifelong reward accelerations idea putting book sparked exciting workshop entitled learning learn knowledge consolidation transfer inductive systems led richard caruana daniel silver organized jonathan baxter tom mitchell pratt part nips workshops vail december 
contributors book participated workshop 
papers volume appear international journal machine learning special issue edited pratt 
begun benefit discussions various contributors book 

abu mostafa 
method learning hints 
hanson cowan giles editors advances neural information processing systems pages san mateo ca 
morgan kaufmann 


ahn brewer 
psychological studies explanation learning 
dejong editor investigating explanation learning 
kluwer academic publishers boston dordrecht london 


ahn mooney brewer dejong 
schema acquisition example psychological evidence explanation learning 
proceedings ninth annual conference cognitive science society seattle wa july 

atkeson 
locally weighted regression robot learning 
proceedings ieee international conference robotics automation pages sacramento ca april 

barto bradtke singh 
learning act real time dynamic programming 
artificial intelligence appear 

baxter 
canonical metric vector quantization 
submitted publication 

baxter 
learning internal representations 
phd thesis university australia 

beymer poggio 
face recognition model view 
proceedings international conference computer vision 

blumer ehrenfeucht haussler warmuth 
occam razor 
information processing letters 

brodley 
recursive automatic algorithm selection inductive learning 
phd thesis university massachusetts amherst ma august 
available coins technical report 

caruana 
multitask learning knowledge source inductive bias 
utgoff editor proceedings tenth international conference machine learning pages san mateo ca 
morgan kaufmann 

caruana 
algorithms applications multitask learning 
saitta editor proceedings thirteenth international conference machine learning san mateo ca july 
morgan kaufmann 

caruana baluja 
sort rankprop multitask learning medical risk evaluation 
touretzky mozer editors advances neural information processing systems cambridge ma 
mit press 
appear 

caruana silver baxter mitchell pratt thrun 
workshop learning learn knowledge consolidation transfer inductive systems 
workshop held nips vail see world wide web www cs cmu edu afs cs cmu edu user caruana pub transfer html december 

cramer 
representation adaptive generation simple sequential programs 
grefenstette editor proceedings international conference genetic algorithms applications pages pittsburgh pa 

dayan hinton 
feudal reinforcement learning 
moody hanson lippmann editors advances neural information processing systems san mateo ca 
morgan kaufmann 

lavrac dzeroski 
multiple predicate learning 
proceedings ijcai pages france july 
ijcai 
ehrenfeucht haussler kearns valiant 
general lower bound number examples needed learning 
information computation 

franke 
scattered data interpolation tests methods 
mathematics computation january 

friedman 
flexible metric nearest neighbor classification 
november 

geman bienenstock doursat 
neural networks bias variance dilemma 
neural computation 

hastie tibshirani 
nearest neighbor classification 
submitted publication december 

hild waibel 
multi speaker speaker independent architectures multi state time delay neural network 
proceedings international conference acoustics speech signal processing pages ii 
ieee april 

hume pazzani 
learning sets related concepts shared task model 
proceedings eighteenth annual conference cognitive science society 

kaelbling 
hierarchical learning stochastic domains preliminary results 
utgoff editor proceedings tenth international conference machine learning pages san mateo ca 
morgan kaufmann 

kearns vazirani 
computational learning theory 
mit press cambridge ma 

koza 
genetic programming programming computers means natural selection 
mit press cambridge ma 

koza 
genetic programming ii automatic discovery reusable programs 
mit press cambridge ma 

laird rosenbloom newell 
chunking soar anatomy general learning mechanism 
machine learning 

edelman 
generalizing single view face recognition 
technical report cs tr department applied mathematics computer science weizmann institute science rehovot israel january 

langley 
areas application machine learning 
proceedings fifth international symposium knowledge engineering sevilla 


lin 
self supervised learning reinforcement artificial neural networks 
phd thesis carnegie mellon university school computer science pittsburgh pa 

mel 
view approach object recognition multiple visual cues 
mozer touretzky hasselmo editors advances neural information processing systems 
mit press december 

mitchell 
need biases learning generalizations 
technical report cbm tr computer science department rutgers university new brunswick nj 
appeared readings machine learning shavlik dietterich eds morgan kaufmann 

mitchell 
machine learning 
mcgraw hill ny preparation 

mitchell thrun 
explanation neural network learning robot control 
hanson cowan giles editors advances neural information processing systems pages san mateo ca 
morgan kaufmann 

mooney ourston 
multistrategy approach theory refinement 
michalski editors proceedings international workshop multistrategy learning pages 
morgan kaufmann 

moore 
efficient memory learning robot control 
phd thesis trinity hall university cambridge england 

moore hill johnson 
empirical investigation brute force choose features smoothers function approximators 
hanson judd petsche editors computational learning theory natural learning systems volume 
mit press 

moses ullman edelman 
generalization changes illumination viewing position upright inverted faces 
technical report cs tr department applied mathematics computer science weizmann institute science rehovot israel 


inductive logic programming 
academic press new york 

sullivan 
integrating initialization bias search bias artificial neural networks 
internal report january 

poggio vetter 
recognition structure model view observations prototypes object classes symmetries 
memo 

pomerleau 
knowledge training artificial neural networks autonomous robot driving 
connell mahadevan editors robot learning pages 
kluwer academic publishers 

pratt 
transferring previously learned back propagation neural networks new learning tasks 
phd thesis rutgers university department computer science new brunswick nj may 
appeared technical report ml tr 

pratt jennings 
review transfer connectionist networks 
connection science 
special issue transfer appear 

quinlan 
learning logical definitions relations 
machine learning 

rendell 
layered concept learning dynamically variable bias management 
proceedings ijcai pages 

ring 
methods hierarchy learning reinforcement environments 
animals animats second international conference simulation adaptive behavior pages 
mit press 

ring 
continual learning reinforcement environments 
oldenbourg verlag munchen wien 

russell 
prior knowledge autonomous learning 
robotics autonomous systems 

schmidhuber 
learning learn learning strategies 
technical report technische universitat munchen january 
revised version 

schmidhuber 
evolutionary principles self referential learning learning learn meta meta hook 
master thesis technische universitat munchen munchen germany 

schmidhuber 
general method incremental self improvement multi agent learning unrestricted environments 
yao editor evolutionary computation theory applications singapore 
scientific publishing 
sharkey sharkey 
adaptive generalization transfer knowledge 
proceedings second irish neural networks conference belfast 

silver 
meta level inference constrain search learn strategies equation solving 
phd thesis department artificial intelligence university edinburgh 

simard lecun denker 
tangent prop formalism specifying selected invariances adaptive network 
moody hanson lippmann editors advances neural information processing systems pages san mateo ca 
morgan kaufmann 

singh 
transfer learning composing solutions elemental sequential tasks 
machine learning 

stanfill waltz 
memory reasoning 
communications acm december 

suddarth holden 
symbolic neural systems hints developing complex systems 
international journal machine studies 

suddarth 
rule injection hints means improving network performance learning time 
proceedings eurasip workshop neural networks portugal feb 
eurasip 

sutton 
adapting bias gradient descent incremental version delta bar delta 
proceeding tenth national conference artificial intelligence aaai pages menlo park ca july 
aaai aaai press mit press 

sutton editor 
reinforcement learning 
kluwer academic publishers boston ma 

teller 
evolving programmers evolution intelligent recombination operators 
angeline kinnear editors advances genetic programming ii cambridge ma 
mit press 

teller veloso 
pado new learning architecture object recognition 
ikeuchi veloso editors symbolic visual learning 
oxford university press 

thrun 
explanation neural network learning lifelong learning approach 
kluwer academic publishers boston ma 

thrun mitchell 
integrating inductive neural network learning learning 
proceedings ijcai france july 
ijcai 
thrun sullivan 
discovering structure multiple learning tasks tc algorithm 
saitta editor proceedings thirteenth international conference machine learning san mateo ca july 
morgan kaufmann 

thrun schwartz 
finding structure reinforcement learning 
tesauro touretzky leen editors advances neural information processing systems cambridge ma 
mit press 

utgoff 
machine learning inductive bias 
kluwer academic publishers 

utgoff 
shift bias inductive concept learning 
michalski carbonell mitchell editors machine learning artificial intelligence approach volume ii 
morgan kaufmann 

valiant 
theory learnable 
communications acm 

vapnik 
estimations dependences statistical data 
springer publisher 

whitehead karlsson tenenberg 
learning multiple goal behavior task decomposition dynamic policy merging 
connell mahadevan editors robot learning pages 
kluwer academic publishers 

widrow rumelhart lehr 
neural networks applications industry business science 
communications acm march 

wolpert 
training set error priori distinctions learning algorithms 
technical report sfi tr santa fe institute santa fe nm 
