expectation propagation approximate inference dynamic bayesian networks tom heskes snn university nijmegen geert ez nijmegen netherlands snn kun nl describe expectation propagation approximate inference dynamic bayesian networks natural extension pearl exact belief propagation 
expectation propagation greedy algorithm converges practical cases 
derive double loop algorithm guaranteed converge local minimum bethe free energy 
furthermore show stable xed points damped expectation propagation correspond local minima free energy converse need case 
illustrate algorithms applying switching linear dynamical systems discuss implications approximate inference general bayesian networks 
algorithms approximate inference dynamic bayesian networks roughly divided categories sampling approaches variational approaches 
popular sampling approaches context dynamic bayesian networks called particle lters 
examples variational approaches dynamic bayesian algorithms ghahramani hinton switching linear dynamical systems ghahramani jordan factorial hidden markov models 
subset variational approaches methods greedy projection 
similar standard belief propagation include projection step simpler approximate belief 
examples extended kalman lter generalized pseudo bayes switching linear dynamical systems bar shalom li algorithm hidden markov models boyen koller :10.1.1.119.6111
article focus greedy projection algorithms 
expectation propagation minka stands family approximate inference algorithms includes loopy belief propagation murphy improved iterative versions greedy projection algorithms special cases 
section describe expectation propagation dynamic bayesian networks extension exact belief propagation di erence additional projection collapse procedure updating messages 
illustrate resulting procedure section switching linear dynamical systems 
expectation propagation converge minka 
section derive double loop algorithm guarantees convergence minimum bethe free energy 
rephrasing optimization saddle point problem interpret damped expectation propagation attempt perform gradient descent ascent 
simulation results regarding approximate belief propagation applied switching linear dynamical systems section 
section discussion implications approximate inference general bayesian networks 
expectation propagation collapse product rule dynamic bayesian networks consider general dynamic bayesian networks latent variables observations graphical model visualized time slices 
joint distribution latent variables observables written form jx jx jx jx jx jx jx jx jx graphical representation dynamic bayesian network 
convention jx prior 
pay special attention boundaries details worked easily 
assume evidence xed include de nition potentials 
thought supernode containing latent variables time slice include discrete continuous variables switching linear dynamical systems 
convenience stick integral notation 
collapse product rule goal compute slice marginals beliefs form jy probability latent variables time slice evidence 
marginal required em type learning procedures interest especially latent variables direct interpretation 
slice marginals jy data likelihood obtained free 
known procedure computing beliefs general bayesian networks pearl belief propagation pearl 
follow description belief propagation speci case sum product rule factor graphs kschischang 
description symmetric respect forward backward messages 
distinguish variable nodes local function nodes variable nodes message forward called message back referred see 
belief variable node product messages sent neighboring local function nodes jy sum product rule factor graphs implies chain variable nodes simply pass messages receive local function node 
information potentials incorporated corresponding local function nodes 
extend standard recipe computing message local function node neighboring variable node forward message backward message follows 
message propagation 

multiply potential corresponding local function node messages neighboring variable nodes yielding current estimate distribution local function node incoming messages 

integrate variables variable obtain current estimate belief state project collapse belief state distribution exponential family yielding approximate belief 

divide message collapse operation step obtain standard sum product rule slight disguise 
usual de nition excludes step incoming message division 
collapse multiplication marginalization division marginalization essentially gives procedure 
collapse ordering matter multiplication collapse division collapse 
important lesson expectation propagation repeated better sense approximate beliefs derive messages approximate beliefs approximate messages 
exponential family approximating family distributions take particular member exponential family canonical parameters suf cient statistics 
typically vectors components 
examples gaussian poisson wishart multinomial boltzmann conditional gaussian distributions 
initialize forward backward messages example choosing stay form canonical parameters fully specify messages keep track 
exact belief propagation belief de ned product incoming messages form typically kinds reasons making particular choice exponential family 
treated framework 
exact belief exponential family dicult handle 
approximating distribution particular exponential form usually completely free 
examples gaussian nonlinear kalman lter conditional gaussian switching linear dynamical system treated section 
exact belief exponential family requires variables fully specify 
approximate belief part exponential family additional constraints factorized groups variables boyen koller algorithm boyen koller :10.1.1.119.6111
moment matching projection step replace current estimate approximate form closest terms kullback leibler divergence kl jq dx log exponential family solution follows moment matching nd canonical parameters hf dx dx members exponential family link function unique invertible mapping canonical parameters moments 
forward backward working moment matching step division step terms canonical parameters slice marginals arrive forward backward passes 
forward pass 
compute hf hf switching linear dynamical system 
note hf depends messages kept xed solution computed inverting 
translating moment form canonical form hf backward pass 
compute hf hf similar forward pass solution written 
order messages updated free choose 
iterating standard forwardbackward passes logical 
collapse exponential family distribution approximation exact standard forward backward algorithm 
cases easily show independent similarly forward backward messages interfere need iterate 
case lter version kalman smoother forward backward algorithm hidden markov models 
example switching linear dynamical system illustrate operations required application expectation propagation switching linear dynamical systems 
reliable algorithms approximate inference relevant exact inference switching linear dynamical systems nphard lerner parr number mixture components needed describe exact distribution grows exponentially time 
potential corresponding switching linear dynamical system graphically visualized written js ij ij stands gaussian mean covariance matrix shorthand notation messages taken conditional gaussian potentials form potential form normalization fact need normalizable negative covariance 
message combination gaussian potentials switch state written exponential form 
slice marginal consists gaussians fi jg 
bookkeeping involves translation canonical parameters moments rewrite ij ij ij ij dimensional vector ij covariance matrix 
forward pass compute moments follow integrating summing integration done exactly ij ij ij ij ij supposed restricted components corresponding components means covariances 
summation yields mixture gaussians switch state member exponential family 
conditional gaussian closest kl divergence mixture gaussians follows moment matching ij ijj ij ijj ij ijj ij ij ijj ij nd new forward message divide approximate belief backward message 
easily done translating moment form canonical form subtracting canonical parameters corresponding yield new canonical form 
procedure backward pass follows exactly manner integrating collapsing forward ltering pass equivalent method called gpb bar shalom li current popular inference algorithms switching linear dynamical system 
attempt obtain similar smoothing procedure required quite additional approximations murphy 
description forward backward passes completely symmetric smoothing require approximations ones ltering 
furthermore forward backward passes iterated convergence nd consistent better approximation 
similar way apply expectation propagation iteratively improve approximate methods inference dynamic bayesian networks 
iterative version algorithm boyen koller proposed murphy weiss :10.1.1.119.6111
optimizing free energy free energy fixed points expectation propagation correspond xed points bethe free energy minka dx log dx log expectation constraints hf hf hf refers slice marginals slice marginals de nition exponential form 
free energy equivalent bethe free energy loopy belief propagation yedidia stronger marginalization constraints replaced weaker expectation constraints correspond projection step collapse product rule 
speci cally interested minima free energy 
double loop algorithm technical problem free energy may convex qg constraints especially concave log term 
bounding concave part linearly obtain bound old dx log old dx log formulation suggests double loop algorithm 
outer loop step reset bound ensure bound old 
inner loop solve convex constrained minimization problem guaranteeing new new bound new new old bound old satisfying constraints 
constrained minimization inner loop turned unconstrained maximization lagrange multipliers functional log dx de ne log old plus irrelevant constants substitute interpreted di erence forward backward messages sum 
sketch proof 
get rid terms depending substituting convex combination symmetric appears natural hf hf hf leaves constraints forward equals backward hf hf resulting minimization problem convex linear constraints 
introduce lagrange multipliers constraints 
minimization lagrangian respect yields distribution form substitutions 
plugging solution back lagrangian yields 
unconstrained maximization problem concave unique maximum 
optimization algorithm particularly ecient elegant obtained considering xed point equations 
terms standard forward backward updates gradient respect reads setting gradient zero suggests update new update may greedy direction gradient increase guaranteed update new suciently small update loosely interpreted natural gradient ascent step 
update easily check increases lower necessary 
practice keep 
outer loop step rewritten update new saddle point problem minimization free energy constraints equivalent saddle point problem min max log dx sketch proof 
bound outer loop step double loop algorithm written minimization auxiliary variables explained minka 
maximization lagrange multipliers follows explicitly write inner loop 
double loop algorithm basically solves problem 
full completion maximization inner loop required guarantee convergence correct saddle point 
show damped version full updates loosely interpreted gradient descent ascent procedure 
gradient descent ascent standard approach nding saddle points objective function 
convergence fact unique saddle point guaranteed convex concave provided step sizes suciently small seung 
case concave need convex say stable xed points damped expectation propagation local minima bethe free energy 
converse need case minima free energy unstable xed points damped expectation propagation 
sketch proof 
consider parallel application damped versions inner loop update outer loop update 
updates aligned respective gradients combining interpreted performing gradient descent gradient ascent choosing de ning 
write damped update form new 
study local stability update procedure de ne hessian similarly evaluated xed point gradient descent ascent locally stable positive de nite negative de nite 
true construction consider max 
hessian obeys positive de nite gradient locally stable local minimum 
opposite need true positive de nite 
example phenomenon straightforwardly damping updates new new obtain update update 


gradients respect vanish xed point damped expectation propagation local stability properties gradient descent ascent procedure 
simulation results tested algorithms randomly generated switching linear dynamical systems 
generated instances corresponds particular setting potentials 
varied number switches dimension continuous latent variables observations 
give phenomenological description simulation results 
focus quality approximated beliefs jy compare beliefs result algorithm lauritzen strong junction tree yielding conditional gaussian jy 
refer exact beliefs fact probabilities switches mean covariance conditional distribution switches exact 
quality measure consider kullback leibler divergence kl 
cases expectation propagation works ne converges couple iterations 
number iterations kl divergence typical easy instance number iterations typical difficult instance typical examples easy left dif cult right instances switching linear dynamical systems switch states dimensional continuous latent variables dimensional observations sequence length 
kl divergence exact approximate beliefs plotted function number iterations 
damping step size dashed line sucient convergence stable xed point 
typical instance see left kl divergence drops single forward pass equivalent gpb acceptably low value decreases little smoothing step little sweeps signi cant changes seen 
damped approximate belief propagation double loop algorithm converge xed point ecient 
refer instance easy 
ran dicult instance expectation propagation gets stuck limit cycle 
typical example shown right 
period limit cycle iterations consisting forward pass backward pass smaller larger periods 
damping belief updates little say instances sucient converge stable solution 
double loop algorithm converges advantage step size set usually takes longer 
single instance considerable damping lead convergence 
algorithm converge minimum obtained unstable single loop expectation propagation small step sizes 
numerical evaluation hessians solution double loop algorithm con rms analysis explains instability hessian bethe free energy max positive de nite local minimum hessian signi cantly negative eigenvalue gradient descent ascent unstable 
suggested see 
minka loopy belief propagation kl forward pass difficult instances easy instances kl divergences easy dif cult instances single forward pass gpb versus convergence minimum free energy 
histograms visualize distributions kl divergences corresponding axes dashed easy instances solid dicult ones 
converge sense force convergence minimum bethe free energy heavy artillery failure belief propagation converge indicates solution inaccurate 
check hypothesis experiment 
dicult instances generated easy instance structure length time sequence number switch states dimensions 
plotted kl divergences single forward pass corresponding gpb convergence obtained damped expectation propagation double loop algorithm dicult instances 
results suggest 
sense iterate search minimum free energy 
instances easy dicult ones beliefs corresponding minimum free energy closer exact beliefs ones obtained single forward pass 
convergence belief propagation indication clear cut criterion quality approximation 
easy instances typically smaller kl divergence dicult ones 
considerable overlap kl divergences easy dicult instances 
discussion described expectation propagation natural extension exact belief propagation 
crucial ingredients 

description belief propagation symmetric respect forward backward messages 

notion project beliefs derive messages approximate beliefs approximate messages 
derived convergent double loop algorithm similar proposed yuille loopy belief propagation 
bound possible get rid log terms resulting algorithm slightly ecient importantly easier implement 
interpreted damped expectation propagation alternative single loop algorithm solve saddle point problem 
nice property converges stable xed point minimum bethe free energy 
damped versions suggested minka murphy loopy belief propagation slightly di erent may share property 
practical point view expectation propagation works ne cases 
di erent reasons 
innocent reason large step size similar large learning parameter gradient descent procedure resolved straightforwardly damping updates 
serious reason occurred frequently simulations switching linear dynamical systems damping lead convergence small step sizes 
case resort tedious algorithm guarantee convergence 
simulations con rm theoretical ndings suggest sense iterate search minima bethe free energy expectation propagation fails 
obviously solid comparison bene simulations di erent bayesian networks comparing sampling approaches variational techniques 
point promising expectation propagation clearly improves gpb solves smoothing problem switching linear dynamical system hardly extra implementation orts 
issues deserve attention numerical instability see 
lauritzen jensen combination expectation propagation sam pling approaches exact computation required moments intractable 
important question results obtained chains article generalize general bayesian networks 
preliminary results suggest derive similar double loop algorithms guaranteed convergence single loop short cuts correspondence stable xed points local minima 
words results article speci dynamic bayesian networks hold bayesian networks general projection loops 
current interpretation soon messages start interfere take care update messages special way 
example going uphill relative satisfy constraints going downhill minimize free energy 
damped versions approximate loopy belief propagation tend move right uphill downhill directions explain single loop algorithms converge practical cases 
cemgil helpful input acknowledge support dutch technology foundation stw dutch centre competence board 
bar shalom li 

estimation tracking principles techniques software 
artech house 
boyen koller 

tractable inference complex stochastic processes 
proceedings fourteenth conference uncertainty arti cial intelligence pages san francisco 
morgan kaufmann 
ghahramani hinton 

variational learning switching state space models 
neural computation 
ghahramani jordan 

factorial hidden markov models 
machine learning 
kschischang frey loeliger 

factor graphs sum product algorithm 
ieee transactions information theory 
lauritzen 

propagation probabilities means variances mixed graphical association models 
journal american statistical association 
lauritzen jensen 

stable local computation conditional gaussian distributions 
statistics computing 
lerner parr 

inference hybrid networks theoretical limits practical algorithms 
uncertainty arti cial intelligence proceedings seventeenth conference uai pages san francisco ca 
morgan kaufmann publishers 
minka 

ep energy function minimization schemes 
technical report mit media lab 
minka 

expectation propagation approximate bayesian inference 
uncertainty arti cial intelligence proceedings seventeenth conference uai pages san francisco ca 
morgan kaufmann publishers 
murphy 

learning switching kalman lter models 
technical report compaq crl 
murphy weiss 

factored frontier algorithm approximate inference dbns 
uncertainty arti cial intelligence proceedings seventeenth conference uai pages san francisco ca 
morgan kaufmann publishers 
murphy weiss jordan 

loopy belief propagation approximate inference empirical study 
proceedings fifteenth conference uncertainty articial intelligence pages san francisco ca 
morgan kaufmann 
pearl 

probabilistic reasoning intelligent systems networks plausible inference 
morgan kaufmann san francisco ca 
seung richardson lagarias hop eld 

minimax hamiltonian dynamics excitatory inhibitory networks 
jordan kearns solla editors advances neural information processing systems 
mit press 
yedidia freeman weiss 

generalized belief propagation 
leen dietterich tresp editors advances neural information processing systems pages 
mit press 
yuille 

cccp algorithms minimize bethe kikuchi free energies convergent alternatives belief propagation 
neural computation press 
