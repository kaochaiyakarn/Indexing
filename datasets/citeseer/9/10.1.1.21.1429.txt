generic grouping algorithm quantitative analysis amir michael lindenbaum presents generic method perceptual grouping analysis expected grouping quality 
grouping method fairly general may grouping various types data features incorporate different grouping cues operating feature sets different sizes 
proposed method divided parts constructing graph representation available perceptual grouping evidence finding best partition graph groups 
stage includes cue enhancement procedure integrates information available multi feature cues reliable bi feature cues 
stages implemented known statistical tools wald algorithm maximum likelihood criterion 
accompanying theoretical analysis grouping criterion quantifies intuitive expectations predicts expected grouping quality increases cue reliability 
shows investing computational effort grouping algorithm leads better grouping results 
analysis quantifies grouping power maximum likelihood criterion independent grouping domain 
best knowledge analysis grouping process time 
grouping algorithms different domains synthesized instances generic method demonstrate applicability generality grouping method 
keywords perceptual grouping grouping analysis graph clustering maximum likelihood wald performance prediction generic grouping algorithm 
proposes generic approach perceptual grouping studies quality expected grouping result 
proposed approach may serve generate grouping algorithms computer science department technion iit haifa israel cs technion ac il different domains implement test 
analysis domain independent applies specific cases 
visual processes deal analyzing images extracting information 
reason processes hard subsets data items contain useful information relevant 
grouping processes rearrange data eliminating irrelevant data items sorting rest groups corresponding particular object indispensable computer vision gestalt psychologists noticed humans basic properties recognize existence certain perceptual structures scene extract image elements associated structures recognized meaningful objects 
properties called wertheimer laws grouping denoted perceptual grouping cues just cues 
witkin tenenbaum suggested grouping processes part processing levels computer vision 
grouping levels domains starting low level processes smoothness discrimination motion grouping mid level processes high level vision processes object recognition :10.1.1.34.3584
proposed method distinguishes components grouping method grouping cues grouping mechanism combines partition data set grouping criterion 
grouping methods grouping criterion formally defined maximization consistency function group assignments data 
maximization usually done various grouping mechanisms including dynamic programming relaxation labeling simulated annealing graph clustering 
grouping may carried hierarchically 
proposed grouping criterion maximization function likelihood available data relative grouping hypothesis 
crucial difference proposed method previous analysis provide predicts grouping performance reliability data 
perceptual information represented graphs vertices observed data elements edges pixels arcs carry grouping information 
graphs grouping clustering different way 
grouping task divided parts constructing graph applying perceptual cues data finding best partition graph groups 
stages implemented known statistical tools wald algorithm maximum likelihood criterion 
perceptual grouping cues building blocks grouping processes shall treated source information available task 
studied task grouping edge points lying smooth boundary example possible variety grouping cues typically cues proximity linearity circularity curvature length combinations 
domains different assumptions cues motion cues symmetry planarity symmetry invariance :10.1.1.34.3584
cues domain specific functions rely assumed properties sought groups 
choice essentially taste intuition rigorous statistical properties considered 
cues essential successful grouping finding aim 
assume cue function model random variable quantify reliability focus relation reliability expected grouping quality 
suggest general cue enhancement method integrating multi feature cues operating data elements relying higher order statistics provide reliable bi feature cues 
grouping methods suggested tested little theoretical background established 
performance grouping algorithms previously assessed implementing algorithm testing small number simulated real examples visually evaluating results 
methodology shows grouping methods perform examples succeed partitioning image elements seemingly correct subsets 
allow predict performance algorithms images compare algorithms tested examples 
analyze power maximum likelihood criterion provide guaranteed grouping quality 
analysis quantifies expected grouping performance function quality available data cue reliability connectivity graph 
similar analyses exist object recognition 
main contributions fairly general approach grouping applicable domains 
previous algorithms domain specific 
cue enhancement procedure cep capable significantly improving reliability existing grouping cues multi feature cues 
quantification expected quality grouping results 
best knowledge analysis grouping done 
rest organized follows starts formulation grouping task graph representation 
generic graph grouping method described 
cep section including short review wald algorithm 
main theoretical analysis section 
actual implementation briefly described section followed experiments instances generic method 
study research directions considered section 
shorter version 
grouping task graph representation consider grouping task set partitioning problem 
fv set data elements may consist boundary points image previously detected line segments naturally divided groups disjoint subsets data elements group belong object lie straight line associated manner 
sm denote natural true partition sm groups sense described set non important background elements 
context grouping task data set partition unknown inferred indirect information form grouping cues 
grouping cues grouping cues building blocks grouping process 
may described functions operating subset data elements revealing information grouping subset 
formally ae denote subset data elements 
shall say consistent subset single true group denote 
inconsistent 
unfortunately measure consistency subset directly image 
grouping cue scalar function cm tells consistency subset example consider problem grouping points lie approximately straight line plane 
set points 
denote eigenvalues nd order central moments matrix note point set collinear second smaller eigenvalue vanishes 
second eigenvalue may linearity grouping cue see 
possible associated multi feature binary cue cm threshold 
points approximately linear small cm cm 
selection considered appendix apparent example grouping cues domain dependent 
cues discriminative preferably pose invariant robust noise 
grouping cues considered literature functions defined data subsets including data elements 
shall refer functions bi feature cues 
general reliable multi feature cues may defined larger data subsets including data features 
linearity cue convexity cue shape cue examples cues require data elements points line segments 
bi feature cues usually easier calculate straightforward cues test larger data subsets tend reliable accidental linearity example points considered probability decrease slightly case 
generally reliability shape multi feature cue consistent instance particular object clearly increases number data features 
multi feature cues considered section provide cue enhancement procedure cep integrates evidence available reliable bi feature cues 
rest section shall consider bi feature cues may cues common grouping processes result cep 
shall denote bi feature cue just denotes pair data features 
main goals provide general framework grouping processes determine grouping performance domain independent way 
shall consider domain dependent properties cues shall characterise reliability 
reliability quantified considering cue function random variable distribution depends consistency tested pair data features 
cue function modelled probability density functions pdf 
rest section shall consider binary cues return 
binary cues easily obtained general cue setting threshold result see eq 

binary cues pdf may simply described corresponding error probabilities ffl probability cue indicates false negative answer ffl fa probability cue indicates false positive answer false alarm 
ffl ffl fa ideal cue 
important note ffl ffl fa describe different distributions defined disjoint populations pdf defined consistent pairs inconsistent feature pairs 
characterization calculated analytical models approximated monte carlo experimentations see left 
representing groups cues graphs approach grouping process convert partition problem graph clustering problem 
graphs represent perceptual information provided cue graph represent grouping result fourth graph represent ground truth partition unknown grouping process 
nodes graphs observed data elements arcs may take different meanings 
shall arcs natural way describe feature pairs 
arc represents pair nodes data features connects 
denote set possible arcs nodes sub graph defined clique 
graphs illustrated algorithm flowchart 
example task grouping edge points lying smooth curves 
node represents edge point 
ground truth partition determined represented target graph composed disconnected cliques 
clique represents different object group 
connection arcs nodes belong different cliques 
graph characterization called clique graph class graphs denoted nodes graph available grouping algorithm arcs contain grouping information hidden directly observable 
fact arc set target graph set consistent feature pairs 
knowing belongs class clique graphs grouping algorithm provide hypothesis graph close possible perceptual grouping information extracted image binary cue function represented graphs underlying graph measured graph 
underlying graph specifies arcs feature pairs evaluated cue function 
measured graph gm em specifies information provided cues 
arc belongs gm belongs result binary cue function indicates feature pair belongs group 
hypothesis graph set groups underlying create graph data features set grouping graph clustering underlying graph measured graph unknown desired target graph max 
likelihood graph clustering grouping cue cep decide arc generic grouping algorithm 
illustrated graphs case grouping edge points lying smooth curve 
generic grouping method generic grouping method described section consists main stages cue evaluation feature pairs maximum likelihood graph clustering partitioning 
stages general depend particular grouping domain choice domain dependent cue associated decisions process 
essence grouping method described section implementation details left section technical report 
user decisions cue connectivity selection order proposed generic method decisions 
grouping cue naturally depends domain assumed characterization groups chosen 
related decision choice feature pairs cue evaluated 
principle feature pairs corresponding complete underlying graph considered 
cues meaningful near adjacent data elements adequate evaluating feature pair 
cue evaluation restricted subset feature pairs specified spatial extent available cue 
locally connected graph task grouping edge points lying smooth curve 
graph data features connected nearest neighbours see illustration figures 
hand complete graph grouping linear points motion segmentation see 
stage evaluating grouping cues image provides set data features may edge points line segments underlying graph built chosen cue guidelines described 
feature pairs corresponding arcs considered pair time 
cue function decide data features belong group 
show decision reliably multi feature cues see section 
positive decisions represented measured graph gm em 
decisions em estimate projection target graph underlying graph projection graph graph denotes graph 
measured graph carries information accumulated stage graph clustering stage 
second stage maximum likelihood graph clustering recall decision stage modelled value binary random variable statistics depends data features belong group 
likelihood decision depends consistency corresponding arc may look grouping hypothesis best agrees cue information 
note hypothesized grouping graph partitioning implies hypothesis consistency arc underlying graph 
data features hypothesized consistent iff assigned partition part group 
joint likelihood decisions previous stage represented measured graph gm depends hypothesized grouping 
common tradition estimation propose choose clique graph partition maximizes likelihood approximation required unknown target graph arg max gc note complete graph expect find hypothesis projection underlying graph 
hypotheses projection distinguished available cue information obviously likelihood 
context cue decisions assumed independent subject types errors specified uniformly error probabilities ffl rfe em ffl fa rfe em denote set difference operator likelihood measured making probabilities nonuniform associating arc individual pair error probabilities may accurate model requires knowledge error mechanism 
graph gm candidate hypothesis eu likelihood edge table 
note likelihoods arc consistent inconsistent complementary sum 
gamma ffl fa ffl ffl fa gamma ffl table likelihood assigned arcs hypothesis 
maximum likelihood graph clustering criterion defined eq 
specifies grouping result constructive algorithm 
shown problem general form np hard 
address theoretical aspect practical side separately 
considering theoretical aspect section assume necessarily unique hypothesis maximizes likelihood address question relationship result unknown target graph question interesting important concerns prediction grouping performance 
practical point view ask optimization problem solved reasonable time 
similar np hard problems solved simulated annealing methods heuristic algorithms 
implementations heuristic algorithm approximates optimal solution see section 
cue enhancement procedure performance grouping algorithm depends strongly reliability cues available 
situations reliability predetermined grouping algorithm designer prefer reliable cues available variety 
section shows reliability grouping cue significantly improved statistical evidence accumulation techniques 
cep considers pair data features time tries data features order estimate consistency pair 
idea process evidence accumulation random data subset contains data pair additional data feature may consistent necessarily consistent see left 
multi feature cue operating feature subset carries statistical information consistency recall cm denotes raw multi feature cue right denote enhanced bi feature cue 
cep uses cm cues determine value cue 
illustration cep obtaining reliable smoothness cue left image contains edgels belong group smooth curve 
subsets groups denoted consistent 
cep extracts raw cues feature subsets contain feature pair decides consistency cues 
raw feature smoothness cue experiments illustrated right image 
circularity test evaluates consistency edgels directions tangent circle drawn centers 
cm fv oe oe algorithm conceptually simple data pair underlying graph algorithm draws random data subsets size contain pair corresponding multi feature cues cm cm extracted 
cue values deterministic functions subsets may considered instances random variable statistics depend data pair particular consistency 
number random data subsets associated cues required conclusive reliable decision consistency determined adaptively efficiently known method statistical evidence integration wald test 
wald algorithm application cue enhancement consider random variable distribution depends unknown binary parameter omega gamma takes value instance random variable carries statistical information parameter 
integrating information sequence instances random variable eventually lead reliable inference omega gamma efficient accurate procedure integrating statistical evidence sequential probability ratio test suggested wald 
procedure quantifies evidence obtained trial log likelihood ratio function result log fx fx fx rfx 
fx rfx 
probability functions different populations value assigned random variable ith trial 
log likelihood ratio high value instance random variable hypothesis 


negative low situation reversed 
probabilities obtaining hypotheses close carries little information 
trials taken loglikelihood function composite event considered 
trials independent composite log likelihood function equal sum individual log likelihood functions oe 
sum oe serves statistics decision 
wald procedure specifies upper lower limits denoted respectively 
cumulative log likelihood function crosses limits decision 
trials carried 
formally denote decision procedure resulting binary enhanced cue allowed probabilities decision error ffl rf 
ffl fa rf 
context cep probabilities quantify reliability enhanced cue 
notations ffl ffl fa section justified cep result 
cep algorithm 
upper lower limits iterative rule depend allowed error probabilities ffl ffl fa calculate practical approximation proposed wald accurate ffl ffl fa small log gamma ffl ffl fa log ffl gamma ffl fa basic algorithm terminates probability optimal sense uses minimum expected number tests necessary obtain required decision error 
expected number tests 
fa gamma ffl fa 
gamma ffl conditional expected amounts evidence single test efh efh despite average case optimality worst case number trials required algorithm bounded 
deal disadvantage modified truncated uses predefined upper bound number tests 
set times larger 

gg 
order cep user specify distributions raw cue fcm fcm desired reliability ffl ffl fa enhanced cue 
specifying raw cue distributions involves technical details deferred appendix eq 
calculate limits cue enhancement procedure summarized 
cep algorithm arc feature pair underlying graph 
set evidence accumulator oe trials counter 
randomly choose gamma data features 
calculate multi feature cue cm fv 

update evidence accumulator oe oe log cm cm 
oe oe output consistent 
oe oe output inconsistent 
repeat cue enhancement algorithm cep success cep relies validity statistical model particularly assumptions necessary claim claim statistics cue values evaluated data subsets containing consistent inconsistent arc approximately cues extracted random consistent inconsistent subsets including feature pair independent identically distributed iid random variables cue enhancement procedure identify consistency feature pair specified error tolerance ffl ffl fa irrespective reliability raw cue cm 
surprising contradict intuition arbitrarily low identification errors impossible amount data image finite 
arbitrarily high performance possible requires large number trials leading contradiction independence assumption 
reliability basic cue fcm fcm important leads lower expected number trials computationally advantageous important validity statistical independence assumption 
experiments show significantly improves cue reliability achievable error rate arbitrarily small see section 
assumptions may violated practice 
may example assumption fails particular data features 
marked arcs ij example failure assumption claim cue see text 
ulation inconsistent pairs 

random triplets associated falsely colinear closer linear group 
effect violation addition close points groups 
cue function constant specified reliability ffl ffl fa expected run time cep constant 
expected total run time evaluating arcs underlying graph linear number arcs 
emphasize cue enhancement method completely general independent grouping mechanism uses 
may cue satisfies benign assumptions stated relies characterization distribution 
analysis grouping quality proposed maximum likelihood graph clustering criterion analyzed section 
analysis quantifies similarity unknown ground truth grouping hypothesized grouping obtained method 
shall see dissimilarity depends error probabilities individual arcs ffl ffl fa connectivity density general grouping performance groups densely connected underlying graph expected worse loosely connected groups 
example node data feature connected group arc underlying graph may separated group hypothesized partition probability ffl may high 
result shows solutions rejected 
claim em jg max gc provided ffl ffl fa 
proof clique graph jg eu ene ffl gamma ffl eu ne ffl fa gamma ffl fa arcs exist sets affect ratio counted 
borrowing terminology parameter estimation claim shows consistent estimator 
arbitrarily reliable labeling underlying graph associated cues leads correct decision 
realistic case cues disagree hypothesized groups 
shall see grouping performance degrades gracefully quality reliability cues 
assume ffl ffl fa consistency ensured 
turn prove fundamental claim results rely 
necessary condition satisfied partition 
consider disjoint node subsets ae graph denote cut fe denote cut width relative underlying graph 
similarly em denote cut width relative measured graph see left 
claim necessary condition hypothesis satisfying eq 
ff log ffl fa gamma ffl log ffl gamma ffl fa gamma 
bisection group ffl 
groups ffl em eu alpha left cut involved splitting group groups right value ff ffl ffl fa see claim left right 
proof prove part consider likelihood ratio hypotheses denoted constructed separating different groups denote 
jg eu gamma ffl ffl fa ffl gamma ffl fa gammal likelihood ratio non decreasing function greater ffl claim satisfied contradicts assumption eq 
holds 
second part claim proved similar manner 
claim interesting implications 
shows example true group erroneously split smaller groups substantial subset arcs connecting missed 
recalling ffl probability missing arcs reasonably small ffl probability split happen extremely low probability 
implication relates addition errors merging group alien node claim requires substantial fraction edges cut fv included em requires false alarm cues 
parameter ff specifying fraction cut edges required merge subsets reflects expected error types false alarm probability equal probability ff false alarm probability higher ff see right 
necessary condition key claims expected grouping quality 
complete underlying graph complete underlying graph connects data feature provides maximal information graph clustering stage 
may lead excellent grouping accuracy 
hand mentioned useful global grouping cues straight line consistent affine motion model claims consider quality grouping result evaluated measures 
claim denote respectively true group hypothesized group containing nodes probability group contains particular additional alien node addition kmin ffl fa gamma ffl fa gammai min proof claim nv fv note merging requires ffl edges connecting included em event happens binomial distribution 
claim denote respectively true group hypothesized group containing nodes probability contains nodes alien gammak gammak kj kmin kj ffl fa gamma ffl fa kj gammai min proof claim ns find probability particular data subset merges 
take worst case approach sum probabilities subsets particular size sizes higher parts predictions analysis left connected curve group smooth curve break number sub groups 
graph shows upper bound expected number sub groups versus minimal cut size group eq 

group size length elements ffl ffl fa typical values images 
shows increasing connectivity quickly reduces false division type groups 
right upper bound probability adding alien data features group size complete underlying graph claim 
apparently probability negligible ffl ffl fa 
claim denote respectively true group hypothesized group containing maximal number data features probability contains js gamma data features js js gammak kmin js gammak ffl gamma ffl js gammak gammai min dff js gamma proof particular deleted subset ae js claim note js gamma split requires ffl event happens binomial distribution 
find probability subset size deleted sum subsets ignoring dependency events decrease probability 
claims simply state original groups big false alarm probabilities small maximum likelihood partition include group object containing elements 
example crude bound plotted right 
shows probability hypothesizing highly mixed subsets small substantial cue errors provided group large 
locally dense underlying graphs intuitive choice underlying graph dense complete graph connect data feature fixed number closest data features data features certain radius 
type graphs example grouping edge points lying smooth curve 
see section 
specifying graph important keep substantial connectivity data features objects accidental deletion 
connectivity demand quantified requiring projection group underlying graph connected 
gamma nodes eliminated projected subgraph remains connected 
nice property connected graphs cut contains edges 
significant change case complete graph ffk errors cause deletion subgroup containing data feature 
characterize grouping performance measure expected number large subgroups group decomposes 
consider particular cut size projection object underlying graph 
probability object divided cut parts divide kmin ffl gamma ffl gammai min 
suppose estimate number potential cuts denote number cut expected number group separations simply cut divide fortunately estimate may done interesting case curve groups 
typical example grouping edge pixel lying smooth curve 
grouping process contrast area segmentation :10.1.1.34.3584
long connected curve group 
group break number shorter smaller groups cut ks gamma different potential places 
break associated cut size graph 
expected number parts curve decomposes higher cut divide ks gamma kmin ffl gamma ffl gammai number plotted left generally decreases cut size increases due non constant non monotonic nature ratio kmin ff strictly monotonic 
useful choosing practical value implementation algorithm composed main stages see 
underlying graph constructed 
locally connected underlying graph kd tree spatial data structure finding nearest neighbors edge pixel efficiently building graph jv log jv average time 
gm evaluated applying grouping cue arc underlying graph jej time 
stage graph clustering maximum likelihood criterion eq 
computationally demanding 
heuristic algorithm finds partition graph local maxima likelihood criterion implemented domains 
graph clustering algorithm finding seeds clusters groups form clique gm growing modifying iteratively 
random graphs theory implies cliques certain size inside object graph 
seed highest entry square adjacency matrix corresponding nodes gm similar process proposed shapiro haralick shapes decomposition simple parts 
modification stage combined phases greedy phase seeds iteratively modified making small changes moving element group merging groups greedy policy local maximum likelihood function obtained 
constraint phase result corrected small groups trying escape local maxima 
phase usually causes temporary decrease likelihood 
stages repeated iteratively new seeds added population nodes gm algorithm summarized 
heuristic algorithm 
input perceptual information gm ffl ffl fa 

create new group seeds 

calculate best possible move node 

calculate best possible joins neighbor groups 

greedy modification phase 

likelihood improved previous iteration exit partition 

constraint modification phase return 
summary maximum likelihood graph clustering algorithm 
experiments described section algorithm provides results tested grouping domains 
main weakness neighboring groups joined process algorithm unable separate 
prefer start iterations smaller group seeds 
typical runtimes figures 
detailed description algorithm including data structures needed accelerate calculation included technical report simulation experimentation section presents different grouping applications implemented different domains instances generic grouping algorithm described 
best knowledge time generic grouping algorithm multiple domains 
implementation domain data features grouping cue different grouping mechanism computer program see table 
aim examples show useful grouping algorithms may obtained instances generic approach examine performance predictions experimental results 
expect general algorithm perform domain specific algorithm tailored domain 
tested domains got grouping results comparable obtained existing domain specific methods 
reported run times include self tr available www cs technion ac il publications html statistics graphics 
st example nd example rd example data elements points edgels patches affine optical flow grouping cues linearity circularity consistency proximity affine motion cue extent global local global enhanced cue subsets points subsets edgels gamma underlying graph complete graph locally connected graph complete graph grouping mechanism maximum likelihood graph clustering program table instances generic grouping algorithm example grouping points linear sets set points plane algorithm partition data linear groups background set 
remove doubt intend propose grouping approach efficient reasonable method detecting linear clusters 
common solutions hough transform ransac exist particular task 
chosen example case study characteristic example grouping tasks associated globally valid cues complete underlying graphs 
provides convenient way measuring grouping performance quantification prediction main interest 
linearity multi feature grouping cue defined data subsets containing data features described section 
raw cue cep 
cue global underlying graph complete graph 
consider synthetic images containing random object points drawn distribution specified collection straight lines objects additional uniformly distributed 
data source easy automatically create data sets known noise distributions grouping ground truth see typical image grouping result 
linearity example test performance grouping algorithm predictions 
considered cue reliability improvement achieved ground truth perfect due alien points located close objects cep 
reliability raw multi feature cue estimated cue value distributions see appendix eq 

done monte carlo process randomly selected feature triplets 
distributions tend similar shown left 
value threshold eq 
determines error probabilities associated binary cue effects expected number trials eq 

apparent right expected number trials minimized threshold value threshold selected 
note selection effect grouping quality computational time needed reach desired error enhanced cue ffl ffl fa 
described cue enhancement result depends computational efforts invested 
measured average number subsets needed labels different pre specified ffl ffl fa values remarkably agrees predicted average eq 
shown labeled curves graph remains constant 
shown enhanced cue reliability exceeds ffl ffl fa simple cue low discrimination power 
turn grouping quality 
regardless choice ffl ffl fa lines detected largest groups experiments 
selection ffl ffl fa affects grouping quality 
measured counting addition errors deletion errors shown respectively 
note deletion error low expected addition error higher expected groups size 
reason discrepancy alien data features close lines erroneously added 
tradeoff grouping quality computational time cep obtained graphs increases errors decrease figures 
example grouping edgels smoothness starting image edgels data feature edge location gradient direction algorithm group edgels lie smooth curve 
useful grouping task considered researchers see :10.1.1.34.3584
crude circularity cue original image set points 
associated data features original image 
underlying graph complete graph 
pixel gray level indicates number arcs passing 
measured graph gm pixel gray level indicates number arcs passing 
detected groups 
points fall wrong group 
detected groups example grouping linear points 
example images experiments 
image associated lines contains points vicinity distributed additional data features 
grouping quality perfect predicted analysis showing power complete underlying graph 
quantitative results experiments shown 
function operating edgel triples cep 
calculated maximal angular difference gradient direction corresponding normal direction circular arc passing points see right 
underlying graph locally connected constructed connecting edgel nearest edgels constant 
test procedure synthetic real images results cases see 
synthetic images created detecting edges piecewise constant images contain grey level smooth blobs 
synthetic example big blobs splits groups see 
happens places connectivity low minimal connectivity assumption fails split probability increases 
see 
example segmentation optical flow affine motion third grouping algorithm common motion 
data features pixel blocks grouped motion obeys rule optical flow consistent affine motion model 
technically pixel block represented location parameters local affine motion model calculated squares 
grouping cue defined pairs blocks value sum optical flow errors block calculating affine model block 
cue global complete underlying graph 
cue enhancement cue reliable typical error probabilities ffl ffl fa 
results comparable obtained domain specific algorithm 
final clustering result shown obtained post processing stage obtained grouping calculate affine motion model group classify individual pixels image groups 
post processing method 
original image associated data features edgels 
underlying graph locally connected nearest nbrs 
pixel gray level indicates number arcs passing 
brighter areas correspond denser regions 
measured graph gm pixel gray level indicates number arcs passing 
note bright groups measured graph correspond local density smoothness 
byproduct serve saliency map 
detected groups 
detected groups 
example grouping smooth curves synthetic image 
edge detection gradient calculated image 
true edge pixels randomly removed background pixels added uniformly distributed gradient directions 
total number edgels arcs processing time minutes super spark cpu 
original image brain image 
edge detection 
associated data features edgels 
underlying graph locally connected nearest nbrs pixel gray level indicates number arcs passing 
measured graph gm pixel gray level indicates number arcs passing 
largest detected groups 
detected groups superimposed original image 
example grouping smooth curves brain image 
underlying graph edgels arcs 
processing time minutes super spark cpu 
original image flowers sequence 
associated data features optical flow blocks 
underlying graph complete graph 
pixel gray level indicates number arcs passing 
measured graph gm low number edges black pixels clouds area indicates optical flow area match affine motion model 
resulted groups different gray level 
black regions eliminated high error affine model tree border grouped groups 
final grouping post processing stages black pixels classified 
result shows groups visually nice capture correct motion clustering image 
example image segmentation regions consistent affine motion parameters 
underlying graph complete graph nodes arcs runtime minutes 
cue value cue threshold left distribution linearity cue values subsets including consistent feature pair solid subsets including inconsistent feature pair dashed 
similar populations distinguished error shown 
right expected number trials needed cep function selected cue threshold 
optimal cue threshold gamma correspond minima curve 
discussion goal provide theoretical framework grouping processes generic grouping algorithm apply wide variety domains yields predictable performance 
proposed approach relies established statistical techniques sequential testing maximum likelihood 
maximum likelihood principle similar previous grouping approaches densities evaluating evidence certain cues cumulative pairwise interaction score ground discrimination 
provides time analysis relating expected grouping quality cue reliability graph connectivity cases computational effort invested 
limit theoretical studies 
grouping applications different cue implemented instances generic grouping algorithm demonstrating usefulness 
argued judging merits vision algorithms visually comparing results examples indicate results visually similar obtained domain specific methods smoothness grouping motion grouping 
addition grouping graph gm may serve saliency map saliency data element degree gm saliency map visually comparable proposed works fa fa quantitative results grouping linear points see 
point represents complete grouping process labeled resulting number deleted points deletion error lines number added points addition error lines experimental average number trials runtime cep 
ffl efa decrease grouping quality grows 
tradeoff enhanced cue reliability computational effort invested clearly demonstrated 
solid lines show predicted error probabilities lines labels 

proposed graph model fairly general representation perceptual information 
example new grouping hypothesis verification method object recognition system 
consistency grouping information object instance hypothesis served score associated hypothesis 
interesting arise analysis experimentation grouping algorithms apparent higher connectivity provided complete underlying graph high degree locally connected graph enhance grouping quality 
selection cues grouping algorithm maximizing reliability extent 
cue extent determines connectivity valid underlying graph words amount information may extracted cue 
cue enhancement possibility introduced considered grouping task 
shown possible obtain reliable cues predicted reliability relatively low computational effort 
procedure incorporates multi feature cues grouping process 
way look grouping cues quantifiers high order statistical information higher order st order statistics 
straightforward example collinearity cue described examines locations points provides value depends relative locations 
higher order statistical information provides additional source grouping information reliable 
computational requirements cue enhancement stage clearly stated related stage unclear 
worst case theoretical analysis shows global optimization criterion np hard 
shown efficient heuristic algorithm performs experiments practical data 
interesting issue invest computational effort enhancing quality relatively small number cues larger number unreliable cues merge higher connectivity underlying graph 
framework proposed choice explicit providing cue enhancement procedure independent maximum likelihood graph clustering method 
making optimal choice interesting open question consider 
research direction methodology context grouping notion different partitioning hypothesized groups necessarily disjoint 
john wang providing flowers optical flow data anonymous referees helpful comments friends jeff oded ilan help manuscript 
adelson wang 
representing moving images layers 
technical report may 
amir 
quantitative approach perceptual grouping computer vision 
dsc dissertation technion iit dept june 
amir lindenbaum 
construction analysis generic grouping algorithm 
technical report cis technion israel nov 
amir lindenbaum 
grouping non additive verification 
technical report cis technion israel oct 
amir lindenbaum 
quantitative analysis grouping processes 
eccv cambridge volume pages 

eklundh 
seeing obvious 
technical report kth mar 
clemens 
region feature interpretation recognizing models images 
phd dissertation dept june 
cox rehg hingorani 
bayesian hypothesis approach edge grouping contour segmentation 
ijcv 
fridman bentley finkel 
algorithm finding best matches logarithmic expected time 
acm transactions mathematical software sept 
geman geman dong 
boundary detection constrained optimization 
pami july 
gordon 
theories visual perception 
john wiley sons edition 
grimson 
object recognition computer role geometric constraints 
mit press edition 
grimson huttenlocher 
verification hypothesized matches modelbased recognition 
pami december 
guy medioni 
perceptual grouping global saliency enhancing operators 
icpr volume pages 
medioni stein 
extraction groups recognition 
eccv stockholm pages 
herault horaud 
ground discrimination combinatorial optimization approach 
pami sep 
jacobs 
grouping visual object recognition 
phd dissertation dept june 
jacobs 
robust efficient detection salient convex groups 
pami jan 
jacobs 
finding structurally consistent motion correspondences 
icpr jerusalem volume pages 
lindenbaum 
amount data required reliable recognition 
icpr jerusalem volume pages 
accepted pami 
lowe 
perceptual organization visual recognition 
kluwer academic pub 

graph theoretic techniques cluster analysis algorithms 
editor classification clustering pages 
ap 
mohan nevatia 
perceptual organization extract structures 
pami nov 
palmer 
graphical evolution 
series discrete mathematics 
john wiley sons 
parent zucker 
trace interface curvature consistency curve detection 
pami august 
saund 
labeling curvilinear structure scales token grouping 
cvpr pages 
sha ullman 
structural saliency detection globally salient structures locally connected network 
iccv pages 
sha ullman 
grouping contours iterated pairing network 
neural information processing systems nips 
shapiro haralick 
decomposition dimensional shapes graph theoretic clustering 
pami jan 
shapiro 
affine analysis image sequences 
phd dissertation university oxford 
sitaraman rosenfeld 
probabilistic analysis stage matching 
pattern recognition 
tuceryan jain ahuja 
supervised classification early perceptual structure dot patterns 
icpr pages 

relational matching 
lect 
notes cs 
springer third edition 
wald 
analysis 
wiley publications statistics 
wiley third edition 
weiss 
geometric grouping applied straight lines 
cvpr pages 
wertheimer 
laws organization perceptual forms 
ellis editor source book gestalt psychology pages 
witkin tenenbaum 
role structure vision 
beck hope rosenfeld editors human machine vision pages 
academic press feb 
wu leahy 
optimal graph theoretic approach data clustering theory application image segmentation 
pami nov 
zisserman mundy forsyth liu pillow rothwell :10.1.1.34.3584
class grouping perspective images 
iccv mit pages 
zucker :10.1.1.34.3584
computational psychophysical experiments grouping early orientation selection 
beck hope rosenfeld editors human machine vision pages 
academic press feb 
calculating fcm fcm cep context cue enhancement procedure cue value regarded random variable 
apart specifying desired reliability ffl ffl fa eq 
calculate thresholds supply distributions consistent inconsistent feature pairs log likelihood ratio determined 
distributions evaluated carefully distributions multi feature cues taken consistent inconsistent populations denoted respectively con fcm incon fcm usually quite different 
important observe feature pair consistent random set includes may consistent see 
distributions modified follows random set fe fv fv gg containing consistent feature pair fv ae additional gamma randomly selected data features consistent probability def fv fv gamma gamma gamma gamma ksk ks size true group 
modified cue distributions conditioned relative consistency features fcm fv fcm fv distributions estimated monte carlo process calculated fcm con fcm gamma incon fcm fcm incon fcm unfortunately distributions similar con incon difficult distinguish see left pair distributions considered experiments 
shall restrict rest discussion binary multi feature cues distribution specified error probabilities fa def con fcm jt fa def incon fcm jt case conditional distributions fcm fcm cm fa cm gamma fa cm cm cm gamma cm gamma gamma fa note fa 
log likelihood ratio th randomly selected subset cm log fa cm log gammap gammap fa cm 
