empirical analysis predictive algorithms collaborative filtering john breese david heckerman carl kadie may revised october technical report msr tr microsoft research microsoft microsoft way redmond wa collaborative filtering recommender systems database user preferences predict additional topics products new user 
describe algorithms designed task including techniques correlation coefficients vector similarity calculations statistical bayesian methods 
compare predictive accuracy various methods set representative problem domains 
basic classes evaluation metrics 
characterizes accuracy set individual predictions terms average absolute deviation 
second estimates utility ranked list suggested items 
metric uses estimate probability user see recommendation ordered list 
experiments run datasets associated application areas experimental protocols evaluation metrics various algorithms 
results indicate wide range conditions bayesian networks decision trees node correlation methods outperform bayesian clustering vector similarity methods 
correlation bayesian networks preferred method depends nature dataset nature application ranked versus presentation availability votes predictions 
considerations include size database speed predictions learning time 
typically automated search corpus items query identifying intrinsic features items sought 
search textual documents web pages uses queries containing words describing concepts desired returned documents 
search titles compact discs example requires identification desired artist genre time period 
content retrieval methodologies type similarity score match query describing content individual titles items user ranked list suggestions 
complementary method identifying potentially interesting content uses data preferences set users 
typically systems information regarding actual content words author description items usage preference patterns users 
called collaborative filtering recommender systems resnick varian built assumption way find interesting content find people similar interests recommend titles similar people 
increasing commercial interest collaborative filtering technology little published research relative performance various algorithms collaborative filtering systems 
describe various collaborative filtering prediction methodologies including previously published algorithms correlation coefficients algorithms learning bayesian models 
empirical data regarding relative predictive performance various algorithms extensions 
results addressing computational scalability issues involved applying various algorithms primary emphasis accuracy quality recommendations predictive component 
collaborative filtering algorithms task collaborative filtering predict utility items particular user active user database user votes sample population users user database 
examine general classes collaborative filtering algorithms 
memory algorithms operate entire user database predictions 
model collaborative filtering contrast uses user database estimate learn model predictions 
collaborative filtering systems distinguished operate implicit versus explicit votes 
explicit voting refers user consciously expressing preference title usually discrete numerical scale 
example grouplens system resnick uses scale bad users rate netnews articles users explicitly rate article reading 
implicit voting refers interpreting user behavior selections impute vote preference 
implicit votes browsing data example web applications purchase history example online traditional stores types information access patterns 
regardless type vote data available collaborative filtering algorithms address issue missing data typically complete set votes titles 
assume items missing random 
applications users vote items accessed access vote items 
applications interest involve implicit voting algorithms described section rely interpretation vote appearing database indicates positive preference 
show making different assumptions nature missing data performance collaborative filtering algorithms improved 
memory algorithms generally task collaborative filtering predict votes particular user refer user active user database user votes sample population users 
user database consists set votes corresponding vote user item set items user voted define mean vote user ji memory collaborative filtering algorithms predict votes active user indicated subscript partial information regarding active user set weights calculated user database 
assume predicted vote active user item weighted sum votes users gamma number users collaborative filtering database nonzero weights 
weights reflect distance correlation similarity user active user 
normalizing factor absolute values weights sum unity 
distinguish various collaborative filtering algorithms terms details weight calculation 
possible characterizations memory collaborative filtering restrict formulation described 
correlation general formulation statistical collaborative filtering opposed verbal qualitative annotations appeared published literature context grouplens project pearson correlation coefficient defined basis weights resnick 
correlation users gamma gamma gamma gamma summations items users recorded votes 
vector similarity field information retrieval similarity documents measured treating document vector word frequencies computing cosine angle formed frequency vectors salton mcgill 
adopt formalism collaborative filtering users take role documents titles take role words votes take role word frequencies 
note algorithm observed votes indicate positive preference role negative votes unobserved items receive zero vote 
relevant weights ia squared terms denominator serve normalize votes users vote titles priori similar users 
normalization schemes including absolute sum number votes possible 
extensions memory algorithms investigated number modifications standard algorithms improve performance 
describe extensions effectiveness discussed section 
default voting default voting extension correlation algorithm described section 
arose observation relatively votes active user matching user correlation algorithm uses votes intersection items individuals voted 
assume default value vote titles explicit votes form match union voted items default vote value inserted formula appropriate unobserved items 
addition assume default vote value number additional items user voted 
effect assuming additional number unspecified items user voted agree 
cases value reflect neutral somewhat negative preference unobserved items 
instance equation kd gamma kd kd kd gamma kd kd gamma kd summations union items user voted ji applications implicit voting observed vote typically indication positive preference visit web page assigned vote value 
case default vote take value associated visit 
instance default voting takes role extending data user true value missing data 
note calculate weights users match active user item 
inverse user frequency applications vector similarity information retrieval word frequencies typically modified inverse document frequency salton mcgill 
idea reduce weights commonly occurring words capturing intuition useful identifying topic document words occur frequently indicative topic 
apply analogous transformation votes collaborative filtering database term inverse user frequency 
idea universally liked items useful capturing experiments value similarity common items 
define log number users voted item total number users database 
note voted item zero 
apply inverse user frequency vector similarity algorithm transformed vote equation 
transformed vote simply original vote multiplied factor 
case correlation modify equation treated frequency item higher assigned weight correlation calculation 
relevant correlation weight inverse frequency gamma uv gamma gamma case amplification case amplification refers transform applied weights basic collaborative filtering prediction formula equation 
transform estimated weights follows ae gamma gammaw ae transform emphasizes weights closer punishes low weights 
typical value ae experiments 
model methods probabilistic perspective collaborative filtering task viewed calculating expected value vote know user 
active user wish predict votes unobserved items 
assume votes integer valued range pr ijv probability expression probability active user particular vote value item previously observed votes 
examine alternative probabilistic models collaborative filtering cluster models bayesian networks 
cluster models plausible probabilistic model collaborative filtering bayesian classifier probability votes conditionally independent membership unobserved class variable relatively small number discrete values 
idea certain groups types users capturing common set preferences tastes 
class preferences regarding various items expressed votes independent 
probability model relating joint probability class votes tractable set conditional marginal distributions standard naive bayes formulation pr pr pr jc left hand side expression probability observing individual particular class complete set vote values 
straightforward calculate needed probability expressions equation framework 
model known multinomial mixture model 
parameters model probabilities class membership pr conditional probabilities votes class pr jc estimated training set user votes user database 
observe class variables database users employ methods learn parameters models hidden variables 
em algorithm dempster learn parameters model structure fixed number classes 
choose number classes selecting model structure yields largest approximate marginal likelihood data 
method cheeseman stutz approximate marginal likelihood see chickering heckerman 
experiments assume model structure possible number classes equally uniform prior model parameters 
initialize em algorithm marginal plus noise technique described thiesson 

bayesian network model alternative model formulation probabilistic collaborative filtering bayesian network node corresponding item domain 
states node correspond possible vote values item 
include state corresponding vote domains natural interpretation missing data 
apply algorithm learning bayesian networks training data missing votes training data indicated vote value 
learning algorithm searches various model structures terms dependencies item 
resulting network item set parent items best predictors votes 
conditional probability table represented decision tree encoding conditional probabilities node 
example tree television viewing data see section shown 
beverly hills watched watched watched friends watched friends watched beverly hills watched watched watched watched watched decision tree individual watched place parents friend beverly hills 
bar charts bottom tree indicate probabilities watched watched place conditioned viewing parent programs 
details learning algorithm discussed chickering 

remainder term bayesian network refer networks decision tree title 
experiments follow structure prior penalizes additional free parameter probability derive parameter priors prior network described chickering 

particular prior network encodes uniform distribution possible outcomes equivalent sample size 
experiments subsets training data showed parameters produce accurate results little sensitivity 
empirical analysis purpose evaluate predictive accuracy various algorithms collaborative filtering 
section describe evaluation criteria various protocols datasets analysis 
discuss results regarding predictive accuracy computational considerations 
evaluation criteria effectiveness collaborative filtering algorithm depends manner recommendations user 
evaluate algorithms defined metrics type collaborative filtering application interface providing 
basic classes collaborative filtering applications 
class individual items time users rating indicating potential interest topic 
original grouplens system category article netnews interface ascii bar chart indicating system prediction regarding user possible interest article 
piece content associated estimated rating user interface displays estimate link content part display presentation item 
second class collaborative filtering applications user ordered list recommended items 
examples systems recommendation lists include phoaks terveen 
spirit internet search engines systems provide ranked list items web sites music recordings highest ranked items predicted preferred 
types systems user presumably investigate items ordered list starting top hoping find interesting items 
applied scoring metrics evaluations appropriate individual item recommendations appropriate ranked lists 
cases basic evaluation sequence proceeds follows 
dataset users votes divided training set test set 
data training set collaborative filtering database build probabilistic model 
cycle users test set treating user active user 
divide votes test user set votes treat observed set attempt predict votes predict votes shown equations 
individual scoring look average absolute deviation predicted vote actual vote items users test set voted 
number predicted votes test set active case average absolute deviation user pa jp gamma scores averaged users test set users 
metric evaluating grouplens project miller 
ranked scoring story bit complex 
information retrieval research ranked lists returned items evaluated terms recall precision 
number returned items recall percentage relevant items returned precision percentage returned items relevant 
collaborative filtering framework votes binary dislike complete preference judgments set users develop similar metric 
generally wish estimate expected utility particular ranked list user 
expected utility list simply probability viewing recommended item times utility 
analysis equate utility item difference vote default neutral vote domain 
furthermore estimate user visit item ranked list 
posit successive item list viewed user exponential decay 
expected utility ranked list items sorted index order declining max gamma gamma ff gamma neutral vote ff viewing 
number item list chance user review item 
experiments items 
scoring ranked list generated user apply equation observed votes available 
items available apply neutral vote effectively removes items scoring 
final score experiment set active users test set max max maximum achievable utility observed items top ranked list ordered vote value 
transformation allows consider results independent size test set number items predicted experiment 
datasets evaluated algorithm separate datasets follows ffl ms web dataset captures individual visits various areas microsoft corporate web site 
example implicit voting database application 
characterized visited vote vote 
ffl television dataset uses network television viewing data individuals week period summer 
data transformed binary data indicating show watched 
ffl eachmovie explicit voting example data eachmovie collaborative filtering site deployed digital equipment research center 
votes ranged value 
table provides additional information dataset 
protocols classes experiments reflecting differing numbers votes available recommenders 
protocol single randomly selected vote user test set try predict value votes user voted 
term ran set experiments items little sensitivity results 
dataset available study courtesy nielsen media research 
information see www research digital com src eachmovie 
dataset eachmovie total users total titles mean votes user median votes user table number users titles votes datasets testing algorithms 
users votes considered 
protocol 
second set experiments randomly select votes test user observed votes attempt predict remaining votes 
call protocols 
experiments measure algorithms performance data possible test user indicative expected algorithms steady state usage database accumulated fair amount data particular user 
various experiments look users data available examine performance algorithms relatively little known active user 
results experiments reflect performance various algorithms startup period user new particular collaborative filtering recommender 
running tests prospective test adequate votes trial eliminated evaluation 
number trials evaluated protocol vary 
results sections compare algorithms analyze effects individual algorithmic extensions 
randomized block design algorithm run test cases observed votes 
refer comparisons experiment 
analyses uses anova bonferroni procedure multiple comparisons statistics dietrich 
tables follow value row labeled rd required difference 
difference scores column big value rd row order considered statistically significant confidence level experiment 
visual aid score boldface significantly different score directly table 
ms web rank scoring algorithm bn cr bc pop rd table ranked scoring results ms web dataset 
higher scores indicate better performance 
performance tables show performance various major classes algorithms various datasets experiments 
compared best performing variation algorithm dataset different protocols 
scores result presenting user popular items regardless known votes individual 
results baseline performance zero order collaborative filtering system labeled pop tables 
algorithm labeled cr refers correlation technique inverse user frequency default voting case amplification extensions 
refers vector similarity method inverse user frequency transformation 
bn bc refer bayesian network clustering models respectively 
additional details bayesian methods discussed section 
results show bayesian networks decision trees node correlation methods best performing algorithms experiments run 
ran combinations dataset protocol scoring criteria 
bayesian network correlation best statistically equivalent cases 
bayesian clustering best performing cases vector similarity best cases 
see bayesian network performs best protocol 
generally methods perform protocols expected 
vector similarity clustering methods competitive limited data scenarios methods partial information effectively 
table shows data rank scoring microsoft web site dataset 
ranked scoring higher scores indicate better performance 
see bayesian network model results best statistically equivalent best score protocols 
correlation appropriate enhancements designed improve ranked scoring fairly close performance 
note correlation default voting operate binary data implicit voting observed votes value 
vector similarity algorithm slightly worse correlation 
rank scoring algorithm bn cr bc pop rd table ranked scoring results dataset 
higher scores indicate better performance 
algorithms outperform popularity recommender 
dataset table bayesian network outperforms statistically ties algorithms protocol 
correlation extensions vector similarity best performance bayesian clustering performs relatively poorly 
see bayesian network drops performance quite significantly protocol relative correlation vector similarity 
discuss observation 
see somewhat different pattern eachmovie ranked scoring shown table 
correlation algorithm top performer vector similarity performing data 
dataset score bayesian network performs worse algorithms experiments top performer competitive correlation protocol 
bayesian networks decision trees suffer scenarios provided little data relative provided training set 
title held testing appears near top tree value set vote evaluating probability possibly related title 
may result title provided ignored having little impact simply due ordering various predicting titles tree 
various experiments able utilize trees fuller extent perform relative methods partial data 
absolute deviation examined eachmovie dataset results shown table 
dataset vote range making vote prediction relevant task 
examine algorithms previous table correlation algorithm applying extensions inverse user frequency 
extensions effective absolute deviation scoring 
basic correlation algorithm performs best experiments indicating algorithm performs adequate data regarding active case 
bayesian clustered model slightly better bayesian network outperforms correlation cases 
eachmovie rank scoring algorithm cr bc bn pop rd table ranked scoring results eachmovie dataset 
higher scores indicate better performance 
eachmovie absolute deviation algorithm cr bc bn rd table absolute deviation scoring results eachmovie dataset 
lower scores better 
experiment algorithm domain protocol cr eachmovie average average ms web average average table improvements ranked scores due application inverse user frequency correlation vector similarity 
inverse user frequency section describe inverse user frequency modify vote values applying memorybased algorithms 
performed set experiments datasets protocols vector similarity correlation judging effect applying inverse user frequency ranked scoring 
results shown table 
experiments application improved ranked score cases results statistically significant 
average improvement vector similarity algorithm correlation algorithm 
experiments run eachmovie dataset absolute deviation scoring improvement impressive 
results significant experiments 
average improvement vector similarity correlation 
results shown table 
experiment algorithm domain protocol cr eachmovie average table improvements absolute deviation scores due application inverse user frequency correlation vector similarity eachmovie data 
domain average eachmovie ms web average table improvements ranked scores due application case amplification correlation 
case amplification case amplification section modifies weights memory algorithm emphasize higher weights 
performed set experiments datasets protocols applying case amplification correlation 
average improvement ranked score results significant experiments see table 
significant effect case amplification absolute deviation scoring 
ran experiments combining case amplification inverse user frequency benefits additive 
probabilistic methods training set build probabilistic models dataset 
title encoded additional explicit vote value vote complete dataset probabilistic learning 
scoring bayesian networks cluster models vote values explicitly entered network missing ranked absolute deviation scoring 
trees vote values entered tree independently order generate probability title 
absolute deviation scoring expected vote calculated renormalizing output probabilities clamping vote probability zero 
improvement algorithm domain protocol absolute percent ms web eachmovie web table improvements ranked scores due larger smaller decision trees 
roughly movies eachmovie dataset estimate full model reasonable amount time 
bayesian methods trained eachmovie top movies terms popularity 
testing movies 
datasets items training testing 
bayesian networks applied alternate prior specifications resulted trees varying complexity 
priors strongly penalized splits generated bayesian networks nodes approximately parents distributions decision tree representation 
model larger trees predecessors distributions variable 
experiments domain compared larger trees smaller trees 
larger smaller trees built structure prior penalized parameter factor respectively 
experiments produced statistically significant results larger trees doing better smaller tree doing better 
average improvement larger trees experiments 
table displays result various experiments 
larger trees results reported section 
applying clustering datasets identified classes dataset classes ms web dataset classes eachmovie dataset 
classes clustering ms web dataset shown 
entry page area virtual root distinguishes class 
class names left manually generated inspecting resulting classes 
support support desktop knowledge base windows support search nt server support windows products free downloads windows windows support windows family products office products ms office info free downloads ms word news office free stuff ms office developers search training games developer network job openings internet explorer internet explorer free downloads support net meeting international content internet explorer technical search free downloads products internet explorer internet site construction dev 
site builder internet site construction dev web site builders gallery developer workshop network membership jakarta activex technology dev 
probabilistic methods bayesian network decision tree item outperformed cluster models ranked scoring 
comparisons average improvement ranked scores differences statistically significant 
absolute deviation experiments run eachmovie data methods significantly different 
additional issues predictive accuracy probably important aspect gauging efficacy collaborative filtering algorithm considerations including size model sampling runtime performance 
considers size collaborative filtering prediction representation memorybased methods require relatively small algorithm code base plus user database consisting sample user votes 
model methods require representation bayesian network model typically having smaller memory requirements 
example user databases required memory methods eachmovie ms web datasets approximately kilobytes compressed bayesian network model sizes kilobytes compressed respectively 
number items usage database memory methods determined experimenting scoring various sizes training set 
shows increase ranked scoring accuracy function size training set 
training set sizes number users eachmovie ms web 
identical training sets user database model methods database learning probabilistic models 
experiments sample sizes order adequate purposes generating recommendations 
terms runtime performance probabilistic model methods approximately times fast memory methods generating recommendations correlation generating recommendations second bayes net generating recommendations second mhz pentium ii processor eachmovie dataset 
course probabilistic training set size learning curve showing effect sample size ranked scoring correlation method protocol dataset 
models learned 
learning times models experiments ranged hour hours eachmovie ms web 
presents extensive set experiments regarding predictive performance statistical algorithms collaborative filtering recommender systems 
results indicate wide range conditions bayesian networks decision trees node correlation methods outperform bayesian clustering vector similarity methods 
correlation bayesian networks preferred method depends nature dataset nature application ranked presentation availability votes predictions 
see relatively votes correlation bayesian networks advantage techniques 
considerations include size database speed predictions learning time 
bayesian networks typically smaller memory requirements allow faster predictions memory technique correlation 
bayesian methods examined require learning phase take hours results lag changed behavior reflected recommendations 
plan ms web data study available learning community irvine repository 
noted eachmovie data currently available 
hope availability data coupled discussion spurred result additional examination improvement collaborative filtering algorithms 
datasets generously provided digital equipment eachmovie media research microsoft ms web 
max chickering david robert contributed programming algorithms study 
max chickering eric horvitz chris meek useful discussions 
john riedl provided useful comments 
cheeseman stutz cheeseman stutz 

bayesian classification autoclass theory results 
fayyad shapiro smyth uthurusamy editors advances knowledge discovery data mining pages 
aaai press menlo park ca 
chickering heckerman chickering heckerman 

efficient approximations marginal likelihood bayesian networks hidden variables 
machine learning 
chickering chickering heckerman meek 

bayesian approach learning bayesian networks local structure 
proceedings thirteenth conference uncertainty artificial intelligence providence ri 
morgan kaufmann 
dempster dempster laird rubin 

maximum likelihood incomplete data em algorithm 
journal royal statistical society 
dietrich dietrich 

statistics 
publishing san francisco fourth edition 
miller miller riedl konstan 

experiences grouplens making usenet useful 
proceeding usenix annual technical conference pages anaheim ca 
resnick resnick iacovou suchak bergstrom riedl 

grouplens open architecture collaborative filtering netnews 
proceedings acm conference computer supported cooperative pages new york 
acm 
resnick varian resnick varian 

recommender systems 
communications acm 


personalized navigation web 
communications acm 
salton mcgill salton mcgill 

modern information retrieval 
mcgraw hill new york 
terveen terveen hill amento mcdonald 

phoaks system sharing recommendations 
communications acm 
thiesson thiesson meek chickering heckerman 
december 
learning mixtures dag models 
technical report msr tr microsoft research redmond wa 

