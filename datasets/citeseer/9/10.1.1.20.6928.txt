holographic reduced representations tony plate department computer science university toronto toronto ontario canada tap ai utoronto ca associative memories conventionally represent data simple structure sets pairs vectors 
describes method representing complex compositional structure distributed representations 
method uses circular convolution associate items represented vectors 
arbitrary variable bindings short sequences various lengths simple structures reduced representations represented fixed width vector 
representations items right constructing compositional structures 
noisy reconstructions extracted convolution memories cleaned separate associative memory reconstructive properties 
distributed representations attractive number reasons 
offer possibility representing concepts continuous space degrade gracefully noise processed parallel network simple processing elements 
problem representing compositional structure distributed representations time prominent concern proponents critics connectionism neural network style associative memories focussed auto associative memories 
auto associative memories hopfield networks store unordered set items 
recall item distorted version 
hetero associative memories holographic memories matrix memories store set pairs items 
item pair recalled cue 
matrix style memories popular class owing superior storage capacity fewer constraints vectors stored 
artificial intelligence tasks language processing reasoning need arises represent recursive tree structure 
ieee transactions neural networks complex data structures sequences trees 
difficult represent sequences trees distributed representations associations pairs tuples items retain benefits distributed representations 
problem representing compositional structure associative memories items associations represented different spaces 
example hopfield memory matrix style memory items represented unit activations vector associations represented connections weights matrix 
difficult represent relationships recursive structure association items may subject association 
hinton discusses problem proposes framework reduced descriptions represent parts objects part hierarchy frame representation 
framework requires number vectors part forming compressed reduced single vector dimension original vectors 
reduced vector turn part representation greater 
reduction reversible move directions part hierarchy reduce set vectors single vector potential part expand single vector part set vectors 
way compositional structure represented 
essential aspect reduced descriptions systematically related components information components gleaned expansion 
aspect distinguishes reduced descriptions arbitrary pointers 
unfortunately hinton suggest concrete way performing reduction expansion mappings 
researchers built models designed frameworks compositional structure distributed representations 
examples see papers touretzky pollack smolensky propose new method representing compositional structure distributed representations 
circular convolutions construct associations vectors 
representation association vector dimensionality vectors associated 
allows construction representa tions objects compositional structure 
call holographic reduced representations hrrs convolution correlation memories closely related holographic storage provide implementation hinton reduced descriptions 
describe hrrs auto associative item memories build distributed connectionist systems manipulate complex structures 
item memories necessary clean noisy items extracted convolution representations 
convolution correlation holographic memories generally regarded inferior matrix style associative memories associating pairs items reasons concerning capacity constraints see 
matrix style memories problem expanding dimensionality representing compositional structure 
convolution correlation memories problem 
storage capacity sufficient useful restrictions classes vectors coped matrix style associative memories transform unsuitable vectors 
associative memories reviewed section ii 
interpretation convolution compressed outer product 
section iii addition memories reviewed 
need high capacity error correcting associative memories discussed section iv 
representations complex structures discussed section sequence representations reviewed ways doing variable binding representing simple frame structures suggested 
idea holographic reduced representations hrrs falls naturally representations discussed section section vi discuss issues representing features tokens types vectors convolution memories 
describe simple machines hrrs section vii 
various mathematical properties discussed section viii including relationship convolution fast fourier transforms status correlation approximate inverse convolution 
capacity convolution memories hrrs discussed section ix 
section give examples construction decoding hrrs 
ii hetero associative memory matrix convolution implementations hetero associative memories store associations pairs vectors 
vectors usually distributed representations discrete items images convolution correlation memories referred holographic memories matrix memories regarded alternative methods implementing hetero associative memory matrix memories received interest due relative simplicity higher capacity terms dimensionality vectors associated relative lack constraints vectors 
ii associative memories operations non adaptive associative memories encoding decoding trace composition 
encoding operation takes item vectors produces memory trace vector matrix 
decoding operation takes memory trace single item cue produces item originally associated cue noisy version thereof 
memory traces composed addition binary operation 
decoding operation sum individual traces retrieved items may noisier 
models encoding decoding operations bi linear murdock decoding operation non linear hopfield operations non linear willshaw illustrate ibe space vectors representing items tbe space vectors matrices representing memory traces 
constraints vectors nearly orthogonal 
theta theta 
encoding operation theta 
decoding operation delta theta trace composition operation 
item vectors memory traces 
association items represented trace theta recover decoding operation cue gives noisy version 
noisy versions cues depending properties particular scheme retrieved vector similar trace represent number associations theta delta theta delta theta item pair cue recover item pair gives noisy version recovered vector increases number associations stored single memory trace 
number associations represented usefully single trace usually referred capacity memory model 
matrix memories usually symmetric cue transposed 
convolution memories symmetric member pair cue 
matrix memories encoding operation outer product convolution memories encoding operation convolution 
addition operation trace composition operation matrix convolution memories 
matrix convolution memories especially versions linear encoding operations property preserve similarity 
items similar items similar traces theta theta similar 
degree similarity traces related degree similarity items 
property potentially useful allows estimate similarity traces computed decoding 
ii problem complex structure pairwise associations suffice practical representation complex data structures trees 
need represent data structures arises systems higher order predicates predicates cause think believe language processing reasoning systems 
approach representing complex data structures associative memory way associations lisp data structures car cdr address 
touretzky hinton touretzky describe systems idea 
major problem approach access slow pointers followed determine constituents structure 
removes major advantages distributed representations fast determination similarity 
approach associative memory operator applied recursively 
corresponds operator map theta theta major problem implementations approach expanding dimensionality association spaces vectors grow arbitrarily dimension difficult practical systems 
approach number researchers problem expanding dimensionality tackled number ways 
metcalfe murdock describe methods aperiodic convolution 
metcalfe discards outside elements convolution products avoid expanding dimensionality 
murdock uses infinite dimensional vectors 
smolensky proposes tensor product memories generalized outer product associative operator 
memories dimensionality association space exponential depth recursion involved 
smolensky suggests placing hard limit depth recursion order keep size association space tractable structure levels deep 
legendre smolensky describe scheme permits soft limit depth recursion properties limit approached exceeded clear 
pollack recursive auto associative memories raams items associations recursive associations represented vector space 
back propagation network learns encoding decoding mappings 
solves problem expanding dimensionality 
learning slow generalization mappings novel items structures highly variable 
hrrs items associations represented vector space circular convolution approximate inverse encoding decoding operators 
ii convolution correlation memories nearly convolution memory models aperiodic convolution operation form associations 
traces usually composed addition 
aperiodic convolution vectors elements results vector gamma elements 
result convolved vector recursive convolution vector elements result gamma elements 
dimensionality resulting vectors expands recursive convolution 
problem expanding dimensionality avoided entirely circular convolution operation known signal processing see 
result circular convolution vectors elements just elements 
matrix convolution memories provide different instantiations associative memory operators set section ii closely related suggested 
convolution vectors circular aperiodic regarded compression outer product vectors 
compression achieved summing top right bottom left diagonals outer product illustrated figures 
outer product vectors content example location shown 
outer product vectors illustrated intended help understanding subsequent figures 
shows standard aperiodic convolution shows truncated aperiodic convolution metcalfe exception non linear willshaw published 
sake mathematical elegance authors considered vectors infinite number elements centered zero th element indexed gamma 
vectors finite number non zero elements order convolution operation defined usually centered zero th element 
gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma ck gammak gamma gamma gamma gamma gamma gamma gamma aperiodic convolution represented compressed outer product 
indices centered zero vectors grow ends dimensionality repeated convolutions 
gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gammak gamma gamma gamma metcalfe truncated aperiodic convolution represented compressed outer product 
gamma ck gammak gamma subscripts modulo circular convolution represented compressed outer product 
fl gamma ck tk gamma subscripts modulo circular correlation represented compressed outer product 
circular convolution operation illustrated 
elements summed indicated figures 
circular convolution operation straightforward remarkable circular correlation fl illustrated approximate inverse operation 
pair vectors convolved give memory trace member pair correlated trace produce member pair 
suppose trace convolution cue vector correlation allows reconstruction distorted version fl correlation operation aperiodic circular versions approximate inverses respective convolution operations 
multiple associations represented sum individual associations 
decoding contribution irrelevant terms ignored distortion 
example result decoding fl fl vectors chosen randomly second term high probability low correlation sum recognizable distorted version ii distributional constraints elements vectors sufficient condition correlation decode convolution elements vector dimension independently identically distributed mean zero variance results expected euclidean length vector 
examples suitable distributions elements normal distribution discrete distribution values sigma reasons distributional constraints apparent sub section 
tension constraints conventional particular elements vectors represent meaningful features distributed representations discussed section vi 
ii correlation decodes convolution immediately obvious correlation decodes convolution 
hard see example worked 
consider vectors elements independently drawn normal distribution mean zero variance example 
convolution decoding trace retrieve fl provided elements vectors satisfy certain distributional constraints 
treated zero mean noise 
variances inversely proportional distributions normal limit goes infinity approximation small 
typical values convolution associative memory systems hundreds thousands 
central limit theorem assuming independent distributed distributions large gamma independent mean variance gamma gamma terms mean zero variance pairwise covariances terms zero 
useful calculate variance dot product delta fl gives measure similarity original reconstructed versions requires account covariances noise terms different elements 
extensive tables variances dot products various convolution products compiled weber aperiodic convolution 
unfortunately apply exactly circular convolution 
means variances dot products common circular convolution products table section viii ii relationship convolution correlation correlation equivalent convolution involution involution vector gammai subscripts modulo 
example 
writing preferable writing fl simplifies algebra correlation associative commutative convolution 
furthermore analogy inverse matrices convenient refer approximate inverse exact inverse vectors convolution gamma discussed section viii ii information stored convolution trace convolution trace numbers may strange pairs vectors involution general meaning mean particular operation 
stored vectors numbers 
reason vectors stored poor fidelity 
convolution trace stores information recognize vectors reconstruct accurately 
store vector recognition memory need store information discriminate vectors 
vectors represent different equiprobable items log bits information needed represent pairs items purposes recognition 
dimensionality vectors enter calculation number vectors matters 
example items represented different vector number bits required store pairs items slightly theta theta log bits 
convolution memory random vectors elements comfortably able store pairs 
storing bits information floating point numbers efficient storage complex structure critical issue 
iii addition memories simplest ways store set vectors add 
storage allow recall reconstruction stored items allow recognition determining particular item stored 
real world example easy recognition objects multiple exposure photograph 
properties addition memories determine characteristics storage multiple items convolution memories 
principle addition memory stated adding high dimensional vectors gives vector similar similar 
principle underlies convolution matrix memories sort analysis applied linear versions 
analysis capacity addition memories appendix note necessary elements vectors continuous values addition memories 
furthermore capacity improved applying suitable non linear threshold function trace 
touretzky hinton rosenfeld touretzky discussed memories viewed non linear version addition memories 
binary memories model touretzky hinton iv need reconstructive item memories convolution memories share inability addition memories provide accurate reconstructions 
conse slightly log bits required pairs unordered 
applies degree elements vectors randomly independently distributed 
binary memory binary vectors logically ed added 
quently system convolution representations sort recall opposed recognition additional error correcting autoassociative item memory 
needed clean noisy vectors retrieved convolution traces 
reconstructive store items system produce 
input noisy version items output closest item indicate input close stored items 
note convolution trace stores associations items item memory stores items 
example suppose system store pairs random vectors item memory store vectors able output closest item input vector clean operation 
system shown 
trace sum convolved pairs system item input cue task output item cue associated trace 
output scalar value strength high input cue member pair low input cue member pair 
trace cue produce high strength 
cue give low strength 
item outputs unimportant strength low 
clean output item strength input cue trace hetero associator machine 
operand convolution indicates approximate inverse taken 
exact method implementation item memory unimportant 
hopfield networks probably candidate low capacity terms dimension vectors stored 
kanerva networks sufficient capacity store binary vectors 
simulations reported appendix stored vectors array computed dot products order find closest match 
representing complex structure pairs items easy represent types associative memory convolution memory suited representation complex structure 
assumes items represented real vectors convolution memories binary vectors 
sequences sequences represented number ways convolution encoding 
entire sequence represented memory trace probability error increasing length stored sequence 
alternatively chunking represent sequence length number memory traces 
murdock murdock propose chaining method representing sequences single memory trace model large number psychological phenomena 
technique stores item pair information memory trace example sequence vectors stored trace ff fi ff fi ff ff fi weighting parameters ff ff retrieval sequence begins retrieving strongest component trace retrieval chaining correlating trace current item retrieve item 
sequence detected correlation trace current item similar item item memory 
representation sequences properties sequence similar items retrieval start element sequence similar sequences similar representations 
disadvantage sequences repeated items properly represented 
way represent sequences entire previous sequence context just previous item possible store sequences repeated items 
store trace type sequence retrieved similar way previous retrieval cue built convolutions 
retrieval items representations improved subtracting prefix components items sequence retrieved 
way represent sequences fixed cue position sequence 
store trace retrieval storage cues arbitrary generated manner single vector methods representing sequences represent stacks 
example stack items delta delta delta xn top represented delta delta delta xn operations manipulating stack push top clean gamma pop gamma top power vector defined section viii cues form care taken length increase exponentially empty stack noticed clean operation finds similar problem type stack implementation pop push approximately equal approximate inverse 
consequence successive pushes pops level lead continual degradation lower level items 
pair push pop actions stack approximately equal additional push pop pairs corrupt remaining part stack 
possible solutions problem chunking see section restrict vector exact inverse equal approximate inverse case see section viii 
chunking sequences methods soft limits length sequences stored 
sequences get longer noise retrieved items increases items impossible identify 
limit overcome chunking creating new non terminal items representing subsequences second sequence representation method suitable chunking 
suppose want represent sequence create new items representing subsequences abc de new items added item memory 
representation sequence abc abc de abc de decoding chunked sequence slightly difficult requiring stack decisions item non terminal decoded 
machine decode representations described section vii variable binding simple implement variable binding convolution convolve variable representation value representation 
example binding value variable value variable variables unbound convolving binding approximate inverse variable 
binding method allows multiple instances variable trace substituted single operation approximately non recursive variable binding implemented easily types associative memory triple space outer product roles fillers simple frame slot filler structures simple frame structures represented convolution encoding manner analogous cross products roles fillers hinton frames frame consists frame label set roles represented vector 
instantiated frame sum frame label roles slots convolved respective fillers 
example suppose simplified frame eating 
vector frame label eat vectors roles agt eat obj eat frame instantiated fillers mark fish represent mark ate fish eat agt eat mark obj eat fish fillers roles retrieved instantiated frame convolving approximate inverse role filler 
role vectors different frames frame specific agt eat different agt see just similar 
role filler binding agt eat mark uncorrelated role filler expected value delta zero 
desired representation frame somewhat similiar fillers added appropriate proportion 
recursive frames holographic reduced representations vector representation frame dimension vector representation filler filler frame 
way convolution encoding affords representation hierarchical structure fixed width vector 
example instantiated frame previous section filler frame representing hunger caused mark eat fish cause agt cause hunger obj cause cause agt cause hunger obj cause eat obj cause agt eat mark obj cause obj eat fish decoding frames discussed section simulation results 
recursive representations manipulated chunking 
chunking extract agent object convolving obj cause agt eat obj cause agt eat chunking extract object clean extract agent giving noisy result 
tradeoff accuracy speed intermediate chunks cleaned retrievals faster accurate 
slack suggests distributed memory representation trees involving convolution products similar representation suggested uses convolution fixed width vectors 
normalization euclidean lengths frame issue see section commutativity circular convolution operation cause ambiguity situations 
results fact ambiguity greatly alleviated frame specific role vectors generic role vectors generic agent vector 
situation ambiguity arise instantiations frame nested instantiation frame 
case agent object confused object agent 
causes problems remains seen 
case variants circular convolution commutative section viii 
holographic reduced representations provide way realizing hinton hypothetical system physical set units focus attention constituents meaning 
furthermore systematic relationship representations components frames reduced descriptions means frames need decoded gain information components see section 
vi constraints vectors representation features types tokens connectionist systems vectors representing items analyzed terms micro features 
example hinton family trees network learned microfeatures representing concepts age nationality 
requirement hrrs elements vectors randomly independently distributed odds interpretation 
furthermore element vector regarded micro feature unclear large number vectors hrrs provide 
section describes way resolving issues 
vi representation features types tokens requirement single micro features represented single bits distributed representation 
features represented high dimensional distributed representations wide representation object 
item having features partly sum features 
tokens type distinguished addition identity giving vector unique token 
features represented random vectors 
example person mark represented mark person random vector distinguishes mark representations people 
component feature weighted importance salience necessary 
advantages scheme local micro feature representation ffl representation feature item degrade gracefully elements vector representing item corrupted 
ffl number features item loosely related dimensionality vectors representing items 
ffl vectors high dimension desired higher dimensionality give better fidelity representation features 
ffl vectors representing items expressed sums vectors random independently distributed elements 
set vectors representing items constructed distributed features way elements vectors consistent drawn independent distributions 
linear circular convolution construct representations expressions describing recall matching vectors expanded terms random feature vectors 
means variances signals system non random vectors consequently probabilities correct retrieval analytically derived 
done example section idea distributing features entire vector representing item new 
linear transform suggested authors name randomization random maps 
care taken ownership features confused method represent features attributes objects 
ambiguity feature ownership arise multiple objects stored addition memory 
example suppose color shape encoded additive components 
representations red circle blue triangle summed result sum red triangle blue circle 
note representations convolved distinct vectors different role vectors added results ambiguous 
vi constraints vectors authors argued constraints vectors necessary holographic memories perform restrictive holographic memories useful argument entirely valid observation vectors produced sensory apparatus satisfy constraints 
argument context storing associations pairs items entirely applicable task storing types complex structured associations hrrs designed 
matrix memories problem expanding dimensionality task provide clearly superior alternative case storing pairwise associations 
desired interface system uses hrrs system uses vector representations conform constraints perceptual system hetero associative memory translate representations 
combination holographic memory hrrs matrix hetero associative memory mapping conforming representations allows representation complex associations difficult represent matrix memories 
vii simple machines hrrs section simple machines operate hrrs described 
machines successfully simulated convolution calculator vectors elements 
control sequencing second machine done manually 
important understand hrrs representation cohesive chunks 
example hrrs representation structure words suitable representation long unstructured list words 
long list large set best stored type associative memory 
vii role filler selector manipulate frames roles fillers able select appropriate roles fillers convolving 
describe machine extract appropriate role uninstantiated frame particular filler 
appropriate role role frame role combines best filler 
selection criteria combined single mechanism 
uninstantiated frame stored sum roles frame label 
role filler stored separately item memory 
machine demonstrates way high level choices parallel systems hrrs 
uninstantiated frame fi ff ff ff frame label role vectors ff fi scalar constants 
task select role combines best filler 
suppose item item memory containing roles typical role bindings quite similar presence similar binding item memory defines best fitting role roles frame selected best fit ff approximately equal selected ff greater 
selection role done convolving uninstantiated frame potential filler fi ff ff ff 
closest matching vector item memory convolved give vector written fl fl magnitude noise depend similarity result fl added uninstantiated frame give fi ff ff fl ff noise strongest role selected cleaning item memory 
role represented strongly trace depend ff ff fl ff greater 
ff approximately equal quite similar strongest 
ff larger similar role largest ff selected 
machine accomplishes operation shown 
clean clean best role filler frame uninstantiated role selection machine 
operand convolution indicates approximate inverse taken 
possible modify technique approximate bayesian reasoning roles fillers correspond hypothesis evidence respectively 
requires powerful clean memory output blend items strength item blend proportional product strength input value associated item memory 
items blending clean memory evidence convolved hypothesis value associated pr 
evaluate likelihoods set hypotheses evidence evidence sum hypothesis weighted prior probabilities passes vector blending clean memory 
convolving result approximate inverse sum hypothesis weighted approximate relative likelihoods 
scheme suffers drawback possible reverse roles evidence hypothesis compute appropriate filler role 
circular convolution commutative means pr distinguished pr blending clean memory 
drawback overcome non commutative variants circular convolution section viii 
vii chunked sequence readout machine machine reads chunked sequences described section built buffers stack classifier correlator clean memory gating paths 
classifier tells item prominent trace terminal non terminal chunk 
iteration machine executes action sequences depending output classifier 
stack implemented number ways including way suggested earlier simple addition memory 
machine shown 
control loop chunked sequence readout machine loop stack gives signal clean trace recover prominent item clean 
classify terminal non terminal case pop appropriate action appropriate action sequences 
terminal item output 
gates path replace trace follower gamma 
non terminal signal tells stack push follower non terminal push gamma 
signal gates path replace trace non terminal pop signal gates path replace trace top stack top 
signal tells stack discard top stack pop 
stack gives signal empty 
gamma item output current item gate key control value control value path vector data path output signal pop non terminal terminal classifier pop push stack trace clean chunked sequence readout machine 
simple controller shown described text receives classifier output provides boolean control values 
machine trace buffer contains chunk currently decoded stack contains portions higher level chunks decoded 
chunked sequence readout machine example system achieves hinton objectives able focus attention constituents necessary meaning chunk 
viii mathematical properties circular convolution may regarded multiplication operation vectors vectors multiplied convolved result vector 
finite dimensional vector space real numbers circular convolution multiplication usual definitions scalar multiplication vector addition forms commutative linear algebra 
easily proved observation convolution corresponds elementwise multiplication different basis described section viii rules apply scalar algebra associativity commutativity addition multiplication distributivity multiplication addition apply algebra 
easy manipulate expressions containing additions convolutions scalar multiplications 
algebra properties algebra considered poggio aperiodic convolution multiplication operation infinite dimensional vector space restricted vectors finite number non zero elements 
observed representing correlation convolution involution expressions convolutions correlations easier manipulate 
viii distributions dot products distributions dot products vectors convolutions vectors analytically derived 
useful dot products shown table 
variances means shown assumption elements vectors independently distributed 
follows expected length vectors 
dot products sum scalar products individual vector elements normally distributed central limit theorem 
variance dot product term depends number correlated scalar products dot product 
equivalent expressions rows derived identity convolution algebra delta delta delta means variances follows suppose random vectors elements drawn independently 
value delta expected value variance rows table 
value delta expected value variance rows table 
interest distribution elements elements independently distributed mean elements zero variance higher covariance elements zero 
expected length provided elements distributed independently expected length 
variance delta higher delta consequence care taken correlations scalar products sufficient independence central limit theorem apply 
expression mean variance delta delta delta delta delta delta delta delta delta delta delta delta delta delta delta delta table means variances dot products common convolution expressions 
normally distributed 
dimensionality vectors 
storage cue especially case particularly relevant storage capabilities hrrs recursive frames stored convolution products obj cause agt eat storage cues 
viii ffts compute convolution fastest way compute convolution fast fourier transforms fft computation involves transform element wise multiplication vectors inverse transform 
write fi discrete fourier transform inverse discrete fourier transform fi element wise multiplication vectors 
steps take log time compute obvious implementation convolution equation gammaj takes time compute 
shall refer original domain spatial domain domain fourier transform takes frequency domain 
domains dimensional vector spaces forward inverse fourier transforms linear 
discrete fourier transform field complex numbers defined gamma gammai jk gamma jth element 
discrete fourier transform invertible defines relationship vectors spatial frequency domains 
computed log time fast fourier transform fft algorithm 
computing convolution ffts takes time method 
faster slower 
inverse discrete fourier transform similar gamma gamma jk computed log time fft algorithm 
viii identities approximate exact inverses frequency domain convolution spatial domain equivalent element wise multiplication frequency domain easily find convolutive inverses transforming frequency domain 
definition inverse write gamma convolutive identity vector 
transforming frequency domain gives fi gamma transform identity gives independent relationships corresponding elements gamma expressed gamma expressing polar coordinates gives see fourier transform inverse gamma gammai consider approximate inverse 
seen definition fourier transform transform involution gammai difference frequency domain approximate inverse exact inverse elements approximate inverse magnitudes original elements magnitudes elements exact inverse reciprocals magnitudes original elements 
follows involution gives exact inverse 
refer class vectors unitary vectors 
equivalent condition auto correlation convolutive identity vector delta function magnitude 
viii exact inverse useful gamma decode exactly better candidate decoding vector approximate inverse exact inverse results lower signal noise ratio analogy unitary matrices hermitian conjugates complex conjugate transpose equal inverses 
retrieved vector memory trace noisy vectors added 
problem arises vectors elements independently distributed equals aj gamma usually greater aj unitary vectors 
unexpected inverse filter known sensitive noise viii convolutive power vector convolutive power vector exponentiation straightforwardly defined exponentiation elements frequency domain fractional negative exponents vectors defined way complex numbers 
integer powers useful generating types encoding keys cf 
section fractional powers represent trajectories continuous space viii matrices corresponding circular convolution convolution operation expressed multiplication 
matrix corresponding convolution elements ij gammaj subscripts interpreted modulo 
matrices known circulant matrices eigenvalues individual complex valued elements fourier transform corresponding eigenvectors inverse transforms frequency components frequency domain possible mapping computed connections layers feed forward network matrix multiplication correspond convolution fixed vector 
viii non commutative variants analogs convolution commutativity convolution cause ambiguity representations structures 
problem non commutative variants circular convolution computed permuting elements argument vectors spatial frequency domain 
permutations applied right left vectors different 
resulting operation commutative associative bilinear distributes addition preserves similarity easily computed approximate inverse 
alternative operation non commutative associative matrix multiplication 
associate vectors treating vector square matrix 
dimension vectors perfect square 
am unaware scaling interference properties associative memory operation 
vectors corresponding orthogonal matrices simple inverses 
viii partial derivatives convolutions convolution operation feed forward networks values forward propagated log time serial machines 
derivatives back propagated log time 
suppose network values groups units convolved sent third group units 
relevant portion feed forward network shown 
suppose partial derivatives outputs convolution respect objective function partial derivatives inputs convolution calculated follows ak gammak gammai vector elements delta kth element vector 
delta delta delta cn gamma objective function bn gamma gamma delta delta delta delta delta delta inputs delta delta delta delta delta delta convolution operation back propagation network 
means possible incorporate convolution operation feed forward network forward back propagation computations convolution log time 
reason want back propagation network learn vector representations items specific task 
pursued plate ix capacity convolution memories hrrs number associations stored convolution memory approximately linear dimensionality vectors 
appendix shown number pairs vectors stored feed forward networks see 
convolution memory trace ln gamma vector dimension number candidate vectors probability errors decoding 
assumed vector elements independently distributed vector appear trace 
wide range parameter values numerical solutions capacity equation equation appendix 
approximated ln assumptions violated vectors similarity independent vector stored pair convolution memory probability error increase 
effect similarity vectors capacity considered greater length plate size structure stored successfully retrieved hrr increases linearly vector dimension similar constants 
size number terms expanded convolution expression sum convolution products structure 
example hrr section terms 
probability correctly decoding deep structure slightly correctly decoding shallow structure number terms variance decoding long convolution products variance delta slightly higher decoding shorter convolution products 
drop performance deeper structures avoided unitary vectors encoding cues cf 
section viii 
example encoding decoding hrrs example hrr frame construction decoding section 
types tokens representing objects concepts constructed suggestions section vi 
results simulation example dimensional vectors reported 
representation similarity tokens suggestion section vi token vectors representing instance type composed sum features distinguishing vector giving individual identity 
example base vectors representing features elements chosen independently 
base vectors listed table 
token role vectors constructed summing relevant feature vectors distinguishing random identity vector give distinct identity instance type 
scale factors included order expected length vectors equal 
token vectors representative pair role vectors listed table 
similarity matrix tokens shown table 
tokens features common higher similarity mark john tokens features common low similarity john fish 
mark ate fish 
hunger caused mark eat fish 
john ate 
john saw mark 
john saw fish 
fish saw john 
table sentences 
object features role features frame labels food obj cause person fish agt eat state bread see table base feature vectors 
vector elements independently chosen 
mark person id mark john person id john paul person id paul luke person id luke fish food fish id fish bread food bread id bread hunger state id hunger thirst state id thirst agt eat agt id eat agent obj eat obj id eat object table token role vectors constructed base feature vectors random identity giving vectors 
identity vectors chosen way base feature vectors 
denominators chosen expected length vector 
roles agt see constructed analogous fashion 
representation similarity frames sentences listed table represented hrr frames 
expressions hrrs listed table 
scale factors included expected length vectors equal 
similarities hrrs shown table 
similarities instantiated frames detected decoding hrrs similar sentences similar high eat agt eat mark obj eat fish cause agt cause hunger obj cause eat agt eat john see agt see john obj see mark see agt see john obj see fish see agt see fish obj see john table hrr frame vectors representing sentences table mark luke hunger john fish thirst paul bread mark john paul luke fish bread hunger thirst table similarities dot products tokens 
diagonal elements squares vector lengths 
tokens sharing feature vectors see table higher similarity 
table similarities dot products frames 
dot products 
note constituents distinct structures different 
fact higher dot product filler role creates similarity having fillers different roles 
extracting fillers roles frames filler particular role frame extracted follows frame convolved approximate inverse role result cleaned choosing similar vector item memory 
item memory contains feature token role frame vectors vectors listed tables 
extraction various fillers roles shown table 
extraction vectors item memory similar result shown 
cases similar object correct 
shown row expression extract agent agt eat objects item memory similar respective dot products mark john paul 
filler agent role mark extraction performed correctly 
construction determination filler object role row table illustrated 
order enable perception similarities vectors element vectors laid rectangles dimensions permuted vectors simultaneously reduce total sum variance neighboring elements 
done simulated annealing program 
reader take visual vectors seriously dot product similarity important difficult judge merely looking figures 
row illustrates agent extracted generic agent role agt agent role specific eat frame agt eat 
results stronger specific agent 
object role filled frame 
alternative methods extracting components sub frame 
method slower clean sub frame item memory row extract components rows 
second faster method omit clean operation directly convolve result approximate inverses roles expressions fast method shown rows 
method example chunking clean intermediate results gives stronger results expense introducing intermediate cleanup operations 
intermediate cleanup omitted chances error higher row correct vector slightly stronger incorrect 
high scoring incorrect responses similar correct response clear sub frame object role filler person 
row shows happens try extract filler absent role 
frame john ate object 
expected obj eat significantly similar 
food appropriate guess coincidence food strongest response 
possible determine role token filling rows 
row correct role john agt eat obj eat scores quite highly 
john person person filling object role compare object role filler fish similar agent role filler john 
extracted role john similar object role shown row 
probabilities correct decoding expectation variances dot products agt eat delta mark agt eat delta calculated section vector person mark 
allows calculate probability agent extracted correctly row table 
emphasized behavior particular system set vectors deterministic 
particular frame particular system decoded correctly 
probabilities calculated section probabilities randomly chosen system behave particular way 
agt eat delta mark delta person delta delta person id vector generic incorrect person filler 
extraction judged performed correctly delta mark delta item 
limit consideration people vectors item memory extremely vectors similar vectors representing people 
important note dot products correlated share common term delta person 
calculate probabilities accurately necessary take account value term choosing threshold 
xmark delta delta id delta person values xmark regarded uncorrelated variables derived random vectors 
distributed normally 
calculation means variances xmark appendix mean variance std dev xmark lower estimate probability xmark pr xmark delta pr mark threshold chosen maximize probability 
example people pr xmark delta pr maximum value 
probability correctly identifying mark filler agent role 
people probability drop 
primary reason calculating means variances signals estimate vector dimension result acceptable probability error 
necessary calculate means variances possible signal value means discriminated small variances large 
normalization vectors vectors constructed sum components euclidean length 
causes problems results stored memory similarity matching large vector large dot product vector proportion shared components relatively small 
example problem dealt including constant factors designed expected length result equal 
works pair wise expected similarities vectors sum zero 
probably preferable normalize vectors lengths exactly 
done example doing affect validity analysis expectations variances dot products 
alternative cosine dot product measure similarity analysis difficult 
thresholds fixed thresholds helpful analysis probability correct retrieval determining result similarity match practice 
reasons ffl best threshold varies composition hrr frame number terms hrr 
ffl best threshold varies particular objects hrr 
varied value section consequently single fixed threshold appropriate choosing winning match situations 
simpler scheme variable thresholds choose similar match 
situation filler role detected low case xmark xmark yp mark 
chosen knowledge xmark yp threshold 
alternative decision region highest score fixed amount greater highest score result considered unclear decision 
xi discussion circular convolution bilinear operation consequence linearity low storage efficiency 
storage efficiency sufficient usable scales linearly 
convolution endowed positive features virtue linear properties 
preserves similarity hrrs similar fillers similar roles similar 
convolution computed quickly ffts 
analysis capacity scaling generalization properties straightforward 
potential system hrrs retain ambiguity processing ambiguous input 
convolution fixed mapping connectionist network replace usual weight matrix mappings 
forward propagation activations back propagation gradients calculated quickly ffts 
possibility pursued possible calculations hrrs entirely frequency domain 
vectors represented frequency domain necessary ffts operations hrrs done time 
rest system including clean memories able complex vectors 
research creating adapting neural network architectures units complex valued activations 
problems convolution memories noisy results give 
noise reduced encoding vectors uniform power frequency domain 
condition approximate inverse equal exact inverse 
advantages afforded outweigh disadvantages having constraint open question 
possible hrr system vectors conform constraint 
system convolution involution dot product operations straightforward analogue addition operation linear 
properties system remain subject investigation 
xii memory models circular convolution provide way representing compositional structure distributed representations 
implement hinton suggestion reduced descriptions microfeatures systematically related constituents 
operations involved linear properties scheme relatively easy analyze especially compared schemes pollack raams learning entailed scheme works wide range vectors 
systems employing hrrs error correcting autoassociative memory clean noisy results produced convolution decoding 
conversations jordan pollack janet metcalfe geoff hinton essential development ideas expressed 
comments chris williams anonymous reviewers helped greatly improve exposition 
research supported part canadian natural sciences engineering research council 
object extract expression similarity scores dot product agent agt eat mark john paul agent agt mark john person object obj love fish fish food agent agt cause hunger state thirst object obj cause eat agent object obj cause agt eat mark paul luke object object obj cause obj eat fish fish food object obj eat food bread mark role john john agt see agt obj see role john john agt see agt agt eat table results extracting fillers frames 
cases shown item similar result correct 
similarity comparisons entire set features tokens roles frames 
see text discussion row 
hrr agt eat obj eat fish obj eat approx inv 
object role agt eat mark fish filler constructing hrr decoding object hrr noisy decoded agent 
mark filler eat frame id obj eat object role agt eat agent role mark john paul parenthesis contain dot product similarity scores 
closest matches agt eat clean memory 
construction decoding hrr sentence mark ate fish 
section 
instantiated frame labeled sum role filler bindings frame id shown second column 
dimensionality objects may filler frame section 
filler hrr extracted convolving hrr approximate inverse role 
extraction agent role filler sentence shown right see table 
items clean memory actual filler mark similar shown dotted region 
similar items shown dot product match value parenthesis 
high dimensional space items significantly similar actual filler 
see section discussion 
appendix lower bound capacity addition memories addition memory store small set vectors single trace 
easy recognize vector stored trace 
appendix show probability correct recognition related vector dimension number vectors stored 
suppose addition memory trace parameters ffl distinct items vectors stored memory trace selected possible vectors ffl elements vector element independently drawn distribution 
ffl probability making errors determining items stored memory trace 
ffl accept reject signals see 
test item trace compute dot product resulting signal distributions accept distribution trace reject distribution trace 
means variances distributions calculated expanding delta example consider trace signal accept distribution delta delta delta delta recall vector elements distributed follows var var central limit theorem terms delta distributed terms delta distributed 
terms zero covariance add means variances get 
signal delta greater threshold assume accept distribution item trace assume 
decision procedure chosen probability error acceptably low 
cumulative distribution functions probability pr hit pr correctly deciding item stored trace probability pr reject pr correctly deciding item stored trace 
threshold chosen maximize probability pr correct correctly identifying items stored stored particular trace pr correct pr hit pr reject gammak probability density functions pdfs optimal single threshold shown example pr correct 
note optimal scheme deciding signal comes accept reject distributions involves testing signal region distribution smaller variance 
purposes sd sd signal strength threshold distribution accept sa reject sr signals recognition linear addition memory 
threshold shown maximizes pr correct 
small gains scheme single threshold scheme outweighed added complexity 
difficult find analytic expression capacity relating reasonably close lower bound follows 
definitions erfc standard error function tail area normalized normal probability density function tail erfc gammat dt tail gammat dt erfc inequality abramowitz stegun simplification erfc gammax erfc gammax probability correctly identifying items trace simplified chosing applying binomial theorem pr correct max pr pr gammak pr pr gammak gamma pr gamma pr gammak gamma pr gamma gamma pr consider probability errors 
simplified inequality give inequality replacing smaller variances maximum variance give inequality 
inequality give inequality replace square root factor give inequality safe assume factor 
gamma pr correct pr gamma pr tail gamma tail tail gamma tail tail erfc gamman gamman rearranging gives ln ln gamma gamma lower bound capacity reasonably close 
numerical solutions exact expression pr correct equation range gamma gamma reasonably approximated gamma ln analysis treats signal values random variables randomness consequence random choice original vectors 
particular trace particular set vectors signal values deterministic 
style analysis consistently stochastic operations randomly chosen vectors 
appendix lower bound capacity convolution memories appendix show analysis appendix extended convolution memory stores pairs items 
storing items trace store pairs items 
parameters described appendix check cue probe pair stored trace convolving trace approximate inverse cue checking similarity dot product probe 
assume know appropriate cues find pairs trace try combination cue probe 
distributions reject signals distribution accept signals 
distribution reject signal depends cue probe occurred trace cue equal probe 
signals summarized complications avoided permitting repetitions pairs identical items 
assume retrieval process avoids checking probe cue equal 
table examples trace means variances signals shown number signals tested probe exhaustively pair trace 
variances calculated adding appropriate variances table ignoring terms order covariance terms signals delta delta zero 
signal example var number tests delta delta delta gamma delta gamma delta gamma delta gamma gamma gamma table means variances signals paired associates convolution memory 
probability correctly identifying pairs trace pr correct max pr pr pr gamma pr gamma pr gamma pr gamma gamma gamma inequalities appendix get tail gamman ln numerical solutions equation range gamma gamma reasonably approximated ln appendix means variances signal detailed calculation mean variance components signals section appendix 
allowed identical pairs trace true signals delta delta non zero covariance 
expression underestimates pr correct small proportion signal values correlated 
causes clustering errors trace decoding error 
expanding signal xmark gives xmark agt eat delta eat agt eat mark obj eat fish agt eat delta eat agt eat person id mark obj eat fish agt eat delta eat agt eat delta agt eat agt eat delta agt eat person agt eat delta id mark agt eat agt eat delta obj eat fish agt eat delta expectations variances terms consulting table 
necessary expand vectors agt eat obj eat fish components independent vectors appearing terms 
expectation fourth term row table expectation remaining terms zero 
terms independent variance sum sum variances 
variance term row table variance second third terms row variance fourth term row variance fifth term row 
terms uncorrelated expectations variances summed give xmark var xmark expectations variances calculated similar manner 
var var milton abramowitz irene stegun editors 
handbook mathematical functions formulas graphs mathematical tables 
dover new york 
piazza 
complex backpropagation algorithm 
ieee transactions signal processing 
poggio 
convolution correlation algebras 
kybernetik 
brigham 
fast fourier transform 
prentice hall new jersey 
david brian 
key recollection vector effects memory performance 
applied optics 
phillip davis 
circulant matrices 
john wiley sons new york 
douglas elliot 
handbook digital signal processing engineering applications 
academic press san diego ca 
arthur fisher wendy john lee 
optical implementations associative networks versatile adaptive learning capabilities 
applied optics 
fodor pylyshyn 
connectionism cognitive architecture critical analysis 
cognition 
robert richard roberts 
signals linear systems 
john wiley sons 
hinton 
implementing semantic networks parallel hardware 
hinton anderson editors parallel models associative memory 
hillsdale nj erlbaum 
hinton 
mapping part connectionist networks 
artificial intelligence 
hinton mcclelland rumelhart 
distributed representations 
mcclelland rumelhart pdp research group editors parallel distributed processing explorations microstructure cognition volume pages 
cambridge ma mit press 
hopfield 
neural networks physical systems emergent collective computational abilities 
proceedings national academy sciences 
kanerva 
sparse distributed memory 
mit press cambridge ma 
legendre smolensky 
principles integrated connectionist symbolic theory higher cognition 
technical report cu cs university colorado boulder 
stephan bennet murdock 
memory serial order 
psychological review 
janet metcalfe 
composite holographic associative recall model 
psychological review 
murdock 
distributed memory model serial order information 
psychological review 
bennet murdock 
theory storage retrieval item associative information 
psychological review 
bennet murdock 
serial order effects distributed memory model 
david robert hoffman editors memory learning ebbinghaus conference pages 
lawrence erlbaum associates 
gi paek demetri 
optical associative memory fourier transform 
optical engineering 
ray pike 
comparison convolution matrix distributed memory systems associative recall recognition 
psychological review 
tony plate 
holographic reduced representations 
technical report crg tr department computer science university toronto 
tony plate 
holographic recurrent networks 
giles hanson cowan editors advances neural information processing systems nips pages san mateo ca 
morgan kaufmann 
poggio 
holographic models memory 
kybernetik 
pollack 
recursive distributed representations 
artificial intelligence 
rosenfeld touretzky 
coarse coded symbol memories properties 
complex systems 
rumelhart hinton williams learning internal representations error propagation 
mcclelland rumelhart pdp research group editors parallel distributed processing explorations microstructure cognition volume bradford books cambridge ma 

algebraic relations involutions convolutions correlations applications holographic memories 
biological cybernetics 
slack 
parsing architecture distributed memory machines 
proceedings coling pages 
association computational linguistics 
smolensky 
tensor product variable binding representation symbolic structures connectionist systems 
artificial intelligence 
touretzky 
dynamic symbol structures connectionist network 
artificial intelligence 
touretzky geva 
distributed connectionist representation concept structures 
proceedings ninth annual cognitive science society conference hillsdale nj 
erlbaum 
touretzky hinton 
distributed connectionist production system 
cognitive science 
elke weber 
expectation variance item resemblance distributions convolution correlation model distributed memory 
journal mathematical psychology 
willshaw 
associative memory inductive generalization 
hinton anderson editors parallel models associative memory 
erlbaum hillsdale nj 
willshaw dayan 
optimal plasticity matrix memories goes come 

