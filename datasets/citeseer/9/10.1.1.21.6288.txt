learning context free context sensitive languages mikael bod mikael boden ide hh se school information science computer electrical engineering university po box sweden 
janet wiles csee uq edu au school computer science electrical engineering university queensland st lucia australia 
september contrary claims long short term memory neural network learns context sensitive language 
simple recurrent network sequential cascaded network able generalize training data utilizing different dynamics 
differences performance dynamics discussed 
keywords recurrent neural network language prediction 
gers schmidhuber set simulations called long short term memory lstm network learning generalizing couple context free context sensitive language :10.1.1.15.4201
profound reasons gold showed certain assumptions super finite languages learned positive grammatically correct examples 
network environment enforce learning bias enables network capture language 
second network establishes necessary means processing embedded sentences requiring potentially infinite memory stacks 
network relies analog nature state space 
contrary claimed network network architecture shown able learn generalize context sensitive language :10.1.1.15.4201
specifically order simple recurrent networks srn second order sequential cascaded networks scn able induce mechanisms finite fragments context sensitive language processing non finite :10.1.1.29.8351
lstm network clearly superior terms generalization performance 
notably srns process recursive languages qualitatively different manner compared lstm 
difference dynamics highlighted may implications application modelling purposes 
complementary results gers schmidhuber treatment supplied :10.1.1.15.4201:10.1.1.15.4201
focus scn permits simpler analysis 
learning ability similar networks trained set strings called generated :10.1.1.15.4201:10.1.1.15.4201
strings consecutively network trained predict letter selected randomly string 
contrary employ start string string symbols testing showed learning easier :10.1.1.15.4201
crucial test successfully processing string predicting letter string 
scn input output units symbol 
sigmoidal state units sufficient 
consequently scn small bounded state space contrast lstm equipped specialized unbounded units :10.1.1.41.7128
backpropagation time bptt training scn 
noted bengio bptt suffers vanishing gradient prone long term dependencies 
may extent explain low proportion networks successfully learning language see table 
related study low success rate observed instability learning partly explained radical shifts dynamics employed network 
blair trained hidden unit srn predict incremental version hill climbing see table 
network hidden train 
train 
sol best test units set str 
tri 
scn bptt max srn na na lstm na avg table results recurrent networks csl showing left right number hidden state units values training number sequences training number solutions trials largest accepted test set :10.1.1.15.4201:10.1.1.15.4201
generalization infinite languages important note gers schmidhuber training networks non regular context sensitive language :10.1.1.15.4201:10.1.1.15.4201
finite fragment context sensitive grammar generates straightforwardly regular language possible strings finite automaton recognize reject 
test test network employs means expected finite automaton 
networks observed trained regular languages 
claim complete success need proof network solving possible test instances test infinite number strings 
lstm scn srn succeed strict sense admittedly gers schmidhuber test large number strings 
underlying mechanisms need established 
turing machines able process infinitely long strings language access infinite memory 
linear bounded automaton linear input size arbitrary long string generated context sensitive grammar logistic activation function 
assume sufficient memory resources 
establish networks process strings manner similarly scales size input 
processing mechanisms noted gers schmidhuber srns learn simple context free languages 
studied languages tested 
capable inducing mechanisms srns 
successful training exhibit similar dynamics srns processing 
interestingly particular light clear distinction context free context sensitive languages classical linguists qualitatively similar dynamics induced 
observe principal types dynamics generalize training set 
oscillation attractive fixed point counting oscillation repelling fixed point counting bs 
oscillation rate set count matches count 
principal dimension required keep track counter 
fixed point counting near fixed point 
rate set count count match 
dimensions required implement spiral 
majority successful large set simulations employed oscillation 
srn employed oscillation 
observe type dynamics generalizes training data oscillation fixed point counting dual pronged oscillation saddle fixed point counting bs oscillation third fixed point counting cs typical example see 
oscillation rates set count matches count count matches count 
second fixed point requires principal dimensions fixed point repelling matching count fixed point attractive match 
linearizing system fixed points possible characterize dynamics terms eigenvalues eigenvectors 
established linearization dynamics processing qualitatively 
diagrams state units lstm networks count increasing decreasing activation referred cell state monotonically :10.1.1.15.4201
basically increases activation cell decreases activation cell matched rate 
activation level threshold network predict symbol shift 
cells required principle applies 
mechanism rely unbounded activation functions infinitely large state space 
srns monotonic counters generalize 
bounded sigmoidal activation functions state units activation quickly saturates 
discussion monotonic counters obviously stable terms learning lstm network observed loose track solution processing lstm generalized quite 
monotonic counter generalizing natural assume unbounded linear state space required 
counters observed bounded state spaces srns hand degrades quickly lack precision 
needs emphasized scn state trajectory activation trajectory state space scn generated presenting shown solid line 
decision boundaries output space shown dotted lines 
state quickly aligns fixed point system starts oscillating 
state attracted fixed point attraction principal dimension oscillation continues state fixed point principal dimension 
repel rate matches count signal expected 
attract rate fixed point determining final 
fixed point repelling oscillation 
activation crosses decision boundaries time step ahead symbol shift due temporal delay scn 
human memory 
performance profile srns recursive languages correlate psycholinguistic behaviours 
remarkable generalization performance cognitively plausible 
siegelmann proven recurrent network implement universal turing machine 
proof relies fractal encoding data state space opposed digits tape 
fractal encoding advantage apart carrying counting information incorporate contents 
fixing alphabet advance possible adjust length trajectories state space smaller larger fractions respect particular symbol operation push pop 
sequence digits encoded way single value possible uniquely instantaneously identify component 
linear monotonic counter lend added functionality 
analysis separate counter employed lstm network symbol coordinated separately :10.1.1.15.4201
semi fractal counter hand bears similarities siegelmann proposal 
remains seen content carrying oscillation realized automatically induced 
principal approaches scaling increased input size monotonic fractal encoding put different requirements state space 
process infinitely long strings monotonic counters require state space infinitely large fractal counters require state space infinite precision 
brief note scratched surface possible ways processing recursive languages establish precise account dynamics 
bengio patrice simard paolo frasconi 
learning long term dependencies gradient descent difficult 
ieee transactions neural networks march 
alan blair jordan pollack 
analysis dynamical recognizers 
neural computation 
mikael bod alan blair 
learning dynamics embedded clauses 
applied intelligence special issue natural language processing neural networks 
press 
mikael bod janet wiles 
context free context sensitive dynamics recurrent neural networks 
connection science 
mikael bod janet wiles brad alan blair 
learning predict context free language analysis dynamics recurrent hidden units 
proceedings international conference artificial neural networks pages edinburgh 
iee 
stephan alan blair 
hill climbing recurrent neural networks learning language 
proceedings th international conference neural information processing pages perth 
morten christiansen nick chater 
connectionist model recursion human linguistic performance 
cognitive science 
jeffrey elman 
finding structure time 
cognitive science 
felix gers rgen schmidhuber :10.1.1.15.4201
lstm recurrent networks learn simple context free context sensitive languages 
ieee transactions neural networks 
press 
gold 
language identification limit 
information control 
jordan pollack :10.1.1.29.8351
induction dynamical recognizers 
machine learning 
paul rodriguez janet wiles jeffrey elman 
recurrent neural network learns count 
connection science 
siegelmann 
neural networks analog computation turing limit 
birkh user 
brad alan blair janet wiles 
inductive bias context free language learning 
proceedings ninth australian conference neural networks pages 

