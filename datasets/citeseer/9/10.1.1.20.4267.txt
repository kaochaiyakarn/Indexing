efficient training conditional random fields hanna wallach master science school cognitive science division informatics university edinburgh thesis explores number parameter estimation techniques conditional random fields introduced probabilistic model labelling segmenting sequential data 
theoretical practical disadvantages training techniques reported current literature crfs discussed 
hypothesise general numerical optimisation techniques result improved performance iterative scaling algorithms training crfs 
experiments run subset known text chunking data set confirm case 
highly promising result indicating parameter estimation techniques crfs practical efficient choice labelling sequential data theoretically sound principled probabilistic framework 
iii supervisor miles osborne support encouragement duration project 
iv declaration declare thesis composed contained explicitly stated text submitted degree professional qualification specified 
hanna wallach table contents directed graphical models directed graphical models 
hidden markov models 
labelling sequential data 
limitations generative models 
maximum entropy markov models 
labelling sequential data 
label bias problem 
performance hmms memms 
chapter summary 
conditional random fields undirected graphical models 
crf graph structure 
maximum entropy principle 
vii potential functions crfs 
crfs solution label bias problem 
parameter estimation crfs 
maximum likelihood parameter estimation 
maximum likelihood estimation crfs 
iterative scaling 
efficiency iis crfs 
chapter summary 
numerical optimisation crf parameter estimation order numerical optimisation techniques 
non linear conjugate gradient 
second order numerical optimisation techniques 
limited memory variable metric methods 
implementation 
representation training data 
model probability matrix calculations 
dynamic programming feature expectations 
optimisation techniques 
stopping criterion 
experiments 
shallow parsing 
features 
viii performance parameter estimation algorithms 
chapter summary 
bibliography ix chapter task assigning label sequences set observation sequences arises fields including bioinformatics computational linguistics speech recognition information extraction 
example consider natural language processing nlp task labelling words sentence corresponding part speech pos tags 
task word labelled tag indicating appropriate part speech resulting annotated text prp vbz dt jj current nn account nn deficit md vb narrow rb cd cd nnp september 
labelling sentences way useful preprocessing step higher level natural processing tasks pos tags augment information contained words explicitly indicating structure inherent language 
nlp task involving sequential data text chunking shallow parsing 
text chunking involves segmentation natural sentences usually augmented pos tags non overlapping phrases syntactically related words grouped phrase 
example sentence pos tagging example may divided follows chapter 
np vp np current account deficit vp narrow pp np pp np september 
pos tagging preprocessing step tasks text chunking shallow parsing provides useful intermediate step fully parsing natural language data task highly complex benefits additional information possible 
common methods performing labelling segmentation tasks employing hidden markov models hmms probabilistic finite state automata identify sequence labels words sentence 
hmms form generative model assign joint probability pairs observation label sequences respectively 
order define joint probability nature generative models enumerate possible observation sequences task domains intractable observation elements represented isolated units independent elements observation sequence 
appropriate assumption simple data sets real world observation sequences best represented terms multiple interacting features long range dependencies observation elements 
representation issue fundamental problems labelling sequential data 
clearly model supports tractable inference necessary model represents data making unwarranted independence assumptions desirable 
way satisfying criteria model defines conditional probability yjx label sequences particular observation sequence joint distribution label observation sequences 
conditional models label novel observation sequence selecting label sequence maximises conditional probability yjx 
conditional nature models means effort wasted modelling observation sequences 
furthermore specifying conditional model terms log linear distribution free making unwarranted independence assumptions 
arbitrary facts observation data captured worrying ensure model correct 
number conditional probabilistic models developed generative models labelling sequential data 
models fall category non generative markov models define single probability distribution joint probability entire label sequence observation sequence 
expected conditional nature models mccallum maximum entropy markov models memms form state classifier result improved performance variety known nlp labelling tasks 
instance comparison hmms memms pos tagging showed conditional models memms resulted significant reduction word error rate obtained hmms 
particular memm incorporated small set orthographic features reduced word error rate vocabulary error rate 
unfortunately non generative finite state models susceptible weakness known label bias problem 
problem discussed detail chapter arises state normalisation requirement state classifiers probability transitions leaving state sum 
transition distribution defines conditional probabilities possible states current state observation element 
state normalisation requirement means observations able affect successor state selected probability mass passed state 
results bias states low entropy transition distributions case states single outgoing transition causes observation effectively ignored 
label bias problem significantly undermine benefits conditional model label sequences indicated experiments performed lafferty 
experiments show simple memms equivalent hmms observa chapter 
tion representation perform considerably worse hmms pos tagging tasks direct consequence label bias problem 
reap benefits conditional probabilistic framework labelling sequential data simultaneously overcome label bias problem lafferty introduced conditional random fields crfs form undirected graphical model defines single log linear distribution joint probability entire label sequence particular observation sequence 
single distribution neatly removes state normalisation requirement allows entire state sequences accounted letting individual states pass amplified probability mass successor states 
sure simple crfs compared memms hmms demonstrate performance effects label bias problem crfs outperform memms hmms indicating principled method dealing label bias problem highly advantageous 
lafferty propose algorithms estimating parameters crfs 
algorithms improved iterative scaling iis generalised iterative scaling gis techniques estimating parameters nonsequential maximum entropy log linear models 
unfortunately careful analysis described chapter reveals lafferty gis algorithm intractable iis algorithm mean field approximation order deal sequential nature data modelled may result slowed convergence 
lafferty experimental results involving crfs pos tagging indicate convergence iis variant slow attempting train crf initialised zero parameter vector lafferty convergence reached iterations 
deal slow convergence lafferty train memm convergence iterations parameters calculating expected value correction feature necessary enable analytic calculation parameter update values intractable due global nature correction features 
initial parameter vector crf 
convergence iis variant crf parameter estimation converged iterations 
technique enabled lafferty train crfs reasonable time principled technique entirely dependent availability trained memms structurally equivalent crf trained 
additionally study osborne shown iis yield multiple globally optimal models result radically differing performance levels depending initial parameter values 
observation may mean decision start crf training trained parameters memm fact biasing performance crfs reported current literature 
theoretical practical problems parameter estimation methods currently proposed crfs provide significant impetus investigating alternative parameter estimation algorithms easy implement efficient 
interestingly experimental malouf indicated despite widespread iterative scaling algorithms training nonsequential conditional maximum entropy models general numerical optimisation techniques outperform iterative scaling wide margin number nlp datasets 
functional form distribution label sequences observation sequence defined crf similar non sequential conditional maximum entropy model 
functional correspondence suggests general optimisation techniques crf parameter estimation highly result similar performance advantages obtained general numerical optimisation techniques estimating parameters non sequential conditional maximum entropy model 
thesis explores number parameter estimation techniques conditional random fields highlighting theoretical practical disadvantages training techniques reported current literature crfs confirming general numerical optimisation techniques result improved performance lafferty iterative scaling algorithm 
compare performance parameter estimation algorithms considered subset chapter 
known text chunking data set train number crfs different parameter estimation technique 
particular subset data chosen representative size complexity data sets nlp tasks experiments performed indicate numerical optimisation techniques crf parameter estimation result faster convergence iterative scaling 
highly promising result indicating parameter estimation techniques crfs practical efficient choice labelling sequential data theoretically sound principled probabilistic framework 
structure thesis follows chapter generative conditional data labelling techniques directed graphical models introduced thorough description hmms memms label bias problem 
chapter addresses theoretical framework underlying conditional random fields including parameter estimation algorithms described current literature theoretical practical limitations 
chapter number second order numerical optimisation techniques discussed outline techniques may applied task estimating parameters crf 
software implemented perform crf parameter estimation described experimental data compare algorithms 
results experiments implications detailed summary covered thesis chapter 
chapter directed graphical models hidden markov models probabilistic finite state automata maximum entropy markov models may represented directed graphical models 
directed graphical models framework explicating independence relations set random variables variables representing state hmm times independence relations may construct concise factorisation joint distribution states markovian model case hmms observations 
labelling sequential data hmm memm labels represented states markov model defining probability distribution state sequences equivalent defining distribution possible sequences labels 
chapter introduces theory underpinning directed graphical models explains may identify probability distribution set random variables 
description hidden markov models uses natural language processing discussion limitations generative models labelling sequential data 
maximum entropy markov models form conditional state classifier introduced context solution problems encountered generative models segmentation sequence data 
la chapter 
directed graphical models bel bias problem fundamental weakness non generative finite state models described motivating need conditional model provides principled method overcoming problem 
directed graphical models directed graphical model consists acyclic directed graph set nodes belonging set directed edges nodes set nodes direct correspondence random variable denoted correspondence nodes random variables enables directed graphical model represent class joint probability distributions random variables directed nature means node set parent nodes set indices parents relationship node parents enables expression joint distribution defined random variables concisely factorised set functions depend subset nodes specifically allow joint distribution expressed product set local functions node associated distinct function set defined node parents identify functional form turn notion conditional independence 
particular observe structure directed graphical model embodies specific conditional independence assumptions factor joint distribution natural probabilistic interpretation emerges 
non overlapping sets correspondence means ignore distinction nodes random variables terms node random variable interchangeably 

hidden markov models nodes definition conditional independence states nodes conditionally independent nodes probability jv jv relate concept conditional independence structure directed graphical model define topological ordering nodes nodes appear ordering having chosen ordering nodes conditional independence relations random variables expressed statement node conditionally independent set nodes appear topological ordering exclusive parents conditional independence statement allows joint probability distribution random variables directed graphical model factorised probability chain rule giving explicit probabilistic interpretation local function 
precisely fact conditional probability jv enables joint distribution defined jv see method joint distribution random variables may concisely express probability distribution sequence labels look forms markovian model hidden markov models maximum entropy markov models 
hidden markov models hidden markov models successfully applied data labelling tasks including pos tagging shallow parsing speech recogni chapter 
directed graphical models tion 
gene sequence analysis 
revisiting part speech tagging scenario introduced chapter illustrate hmms labelling segmenting sequential data task annotating words body text appropriate part speech tags producing labelled sentences form prp vbz dt jj current nn account nn deficit md vb narrow rb cd cd nnp september 
hmms probabilistic finite state automata model generative processes defining joint probabilities observation label sequences 
observation sequence considered generated sequence state transitions start state final state reached 
state element observation sequence stochastically generated moving state 
context pos tagging state hmm associated pos tag 
relationship tags states necessary simplify matters consider case 
pos tags generate words tag associated word considered account word fashion 
possible find sequence pos tags best accounts sentence identifying sequence states traversed generating sequence words 
states hmm considered hidden doubly stochastic nature process described model 
observation sequence sequence states best accounts observation sequence essentially hidden observer viewed set stochastic processes generate observation sequence 
returning pos tagging example pos tags associated sequence words may identified inspecting process words generated 
principle identifying state sequence 
hidden markov models best accounts observation sequence forms foundation underlying finite state models labelling sequential data 
formally hmm fully defined finite set states finite output alphabet conditional distribution js representing probability moving state state observation probability distribution xjs representing probability emitting observation state initial state distribution returning notion directed graphical model expression conditional independence relationships set random variables hmm may represented directed graph nodes representing state hmm label time observation time respectively 
structure shown 
representation dependency graph structure order hmms sequences 
hmm clearly highlights conditional independence relations hmm 
specifically probability state time depends state time 
similarly observation generated time depends state model time pos tagging application means considering tag recall assuming correspondence states tags word depend chapter 
directed graphical models tag assigned previous word word depend current pos tag conditional independence relations combined probability chain rule may joint distribution state sequence observation sequence product set conditional probabilities js js js labelling sequential data stated labelling observation sequence task identifying sequence labels best accounts observation sequence 
words choosing appropriate label sequence observation sequence want choose label sequence maximises conditional probability label sequence observation sequence argmax yjx distribution defined hmm joint distribution observation state sequences appropriate label sequence observation sequence obtained finding finding sequence states maximises conditional probability state sequence observation sequence may calculated joint distribution bayes rule argmax reading labels associated states sequence 
finding optimal state sequence efficiently performed dynamic programming technique known viterbi alignment 
viterbi algorithm described 

hidden markov models limitations generative models despite widespread hmms generative models appropriate sort model task labelling sequential data 
generative models define joint probability distribution observation label sequences 
useful trained model generate data distribution interest labelling data conditional distribution yjx label sequences observation sequence question 
defining joint distribution label observation sequences means possible observation sequences enumerated task hard observations elements assumed long distance dependencies 
generative models strict independence assumptions order inference tractable 
case hmm observation time assumed depend state time ensuring observation element treated isolated unit independent elements sequence 
fact sequential data accurately represented set isolated elements 
data contain long distance dependencies observation elements benefit represented model allows dependencies enables observation sequences represented non independent overlapping features 
example assigning pos tags words performance improved significantly assigning tags basis complex feature sets utilise information identity current word identity surrounding words previous pos tags word starts number upper case letter word contains hyphen suffix word 
features independent example suffix current word entirely dependent identity word contain dependencies current previous tags current word current tag 
fortunately conditional models labelling data sequences provides convenient method overcoming strong independence assump chapter 
directed graphical models tions required practical generative models 
modelling joint probability distribution observations states conditional models define conditional distribution sjx state sequences particular observation sequence 
means identifying state sequence observation sequence conditional distribution may directly argmax sjx argmax requires enumeration possible observation sequences marginal probability calculated 
maximum entropy markov models maximum entropy markov models form conditional model labelling sequential data designed address problems arise generative nature strong independence assumptions hidden markov models 
memms applied number labelling segmentation tasks including pos tagging segmentation text documents 
hmms memms concept probabilistic finite state model generating observations model probabilistic finite state acceptor outputs label sequences observation sequence 
memms consider observation sequences events conditioned generated 
defining types distribution transition distribution js representing probability moving state state observation distribution xjs representing probability emitting observation state memm single set jsj separately trained distributions form jx js 
maximum entropy markov models represent probability moving state observation fact functions specific state means choice possible states instant time depends state model time state observation transition functions conditioned observations means dependency graph memm takes form shown 
note observation graphical structure order memms sequences 
variables corresponding unshaded nodes generated model 
sequence conditioned generated distribution associated graph joint distribution random variables representing state memm time assuming state corresponds particular label chain rule probability conditional independences embodied memm dependency graph structure may joint distribution label sequences observation sequence yx jx jy treating observations events conditioned generated means probability transition may depend non independent interacting features observation sequence 
mccallum making maximum entropy framework discussed detail chapter defining state observation transition function chapter 
directed graphical models log linear model jx exp normalisation factor parameters estimated feature function takes arguments current observation potential state 
free parameters log linear model estimated generalised iterative scaling 
iterative scaling covered chapter 
feature function binary feature observation expresses characteristic empirical training distribution hold trained model distribution 
example feature observation word feature function indicates particular boolean feature true observation possible state takes particular value true labelling sequential data hmms memms label novel data identifying state sequence best describes observation sequence labelled 
state label associated probable label sequence observation sequence may trivially identified state sequence calculated 
find probable state sequence argmax sjx desirable form dynamic programming algorithm 
mccallum brief overview variant viterbi alignment enables state sequence efficiently identified 

maximum entropy markov models label bias problem maximum entropy markov models discriminative finite state models define set separately trained state probability distributions exhibit undesirable behaviour certain situations termed label bias lafferty 
label bias problem best described example 
consider memm 
finite state acceptor np np vp np np vp adjp pp robot wheels fred round wheels round finite state acceptor shallow parsing sentences 
designed shallow parse sentences robot wheels fred round robot wheels round segmenting non overlapping chunks phrases syntactically related words grouped 
suppose wish determine chunk sequence observation sentence 
done identifying state sequence best accounts observation sequence reading chunk labels states sequence chosen 
recalling joint probability state sequence observation sequence may decomposed sjx js chapter 
directed graphical models calculate js state sequences accounts observation sequence 
having done establishing state sequence results highest conditional probability sjx trivial 
second time steps observation words robot match transitions state state state state respectively 
far joint probability possible state sequence observations far robot robot moving observation element wheels matches transitions state probability mass accumulated far distributed states 
conservation probability mass arises fact probability transitions leaving state sum 
possible state sequences observation sequence far 
determine sequences best accounts observation sequence observe sequences extended account rest observation sequence compare probabilities state sequences result 
states single outgoing transition pass probability mass accumulated far successor ensure distribution possible state sequences observation sequence sums 
state normalisation requirement arises fact joint distribution state sequences decomposed product conditional probabilities states current states observation 
assume fred fred closer inspection reveals assumption entirely unwarranted data train memm training data contained transition state state observation word fred 
ideally 
maximum entropy markov models low probability event occurred data train model state normalisation requirement means choice ignore observation respect fact states reachable sj fred essentially states single outgoing transition forced ignore observations 
may generalised tendency states low entropy state distributions take little notice observations 
continuing identify state sequences account observation sequence question possible state sequences corresponding chunk labels np np vp np pp np np np vp adjp respectively 
assuming probabilities transitions state approximately equal label bias problem means probability chunk sequences observation sequence roughly equal irrespective observation sequence related note transitions state occurred frequently training data set probability transition greater causing state pass probability mass successor state associated transition 
situation result sequence chunk tags associated path preferred irrespective observation sentence 
consequence label bias problem significant problem natural language data example consider situation word associated particular partof speech tag 
labelling sentence rarer interpretation word correct word assigned common pos tag irrespective context contained rest sentence 
generative models hmms suffer label bias problem 
viterbi algorithm identify state chapter 
directed graphical models sequence observation sequence able weight possible branch state sequence basis observations appear branch point 
performance hmms memms compare performance memms hmms mccallum performed number experiments involving segmentation usenet multipart faqs 
hmm variants see details simple memm trained trained single document group faqs tested remaining documents 
features memm transition functions formatting features indentation numbered questions styles paragraph breaks 
features necessarily independent 
results experiments summarised table 
table clearly indicates memms outperform hmms non independent salient features appropriate representation observation data 
model precision recall memm table agreement probability segmentation precision segmentation recall hmm variants memm text segmentation task memms exhibited performance text segmentation task possible perform worse hmms labelling data results label bias problem 
investigate lafferty performed number experiments comparing hmms suffer label bias memms pos tagging labelling synthetic data specifically generated verify label bias problem 
synthetic natural language data memms performed worse 
chapter summary simple hmms direct result label bias problem 
results pos tagging task shown table 
model error oov error hmm memm table word error rate vocabulary word error rate pos tagging hmms memms 
chapter summary chapter introduced concept directed graphical models labelling sequential data 
overview hidden markov models provided limitations generative models discussed 
maximum entropy markov models introduced potential method overcoming strong independence assumptions required practical generative models 
label bias problem outlined motivating need conditional model suffer problem 
chapter conditional random fields conditional random fields crfs introduced form conditional model allow strong independence assumptions hmms relaxed overcoming label bias problem exhibited memms non generative directed graphical models discriminative markov models 
memms crfs conditional probabilistic sequence models directed graphical models crfs undirected graphical models 
allows specification single joint probability distribution entire label sequence observation sequence defining state distributions states current state 
conditional nature distribution label sequences allows crfs model real world data conditional probability label sequence depend non independent interacting features observation sequence 
addition exponential nature distribution chosen lafferty enables features different states traded weighting states sequence important 
chapter undirected graphical models followed explanation conditional random fields form undirected graphical model 
maximum entropy principle heavily influences chapter 
conditional random fields lafferty choice crf potential functions described leading explanation crf functional form 
overview analysis parameter estimation techniques currently crf training highlighting theoretical reasons desiring alternative method training crfs 
undirected graphical models markov random field undirected graphical model acyclic graph set nodes set undirected edges nodes 
nodes represent set continuous discrete random variables mapping nodes variables 
graphical model associated class joint probability distributions random variables represented nodes graph 
parameterisation probability distributions depend conditional independence relations random variables graph 
section saw joint probability distribution associated directed graphical model factorised product conditional probabilities conditional independence relationships topologically ordered set nodes identified 
specifically joint probability random variables represented written jv set parent nodes belonging parameterisation markov random field different directed graphical model 
possible assign node conditional probability neighbours undirected nature markov random fields means difficult ensure conditional probability node neighbours consistent conditional probabilities nodes 
undirected graphical models graph 
potential inconsistency means ensure conditional probabilities assigned nodes yield single joint distribution random variables graph 
reason joint distribution markov random field parameterised terms conditional probabilities defined product set local functions derived set conditional independence axioms 
step parameterising undirected graphical model identify sets nodes local function operate 
notion conditional independence 
letting represent disjoint index subsets random variables represented nodes conditionally independent represented nodes represented set nodes separates undirected graphical model utilise nave graph theoretic notion separation say conditionally independent path ag jj cg pass bg identify groups nodes operated set local functions note conditional independence properties undirected graphical model absence edge nodes implies nodes conditionally independent nodes graph 
choosing local functions ensure possible factories joint probability appear local function 
easiest way fulfil factorisation requirement assert local function may operate set nodes forms fully connected subset nodes clique clearly ensures local function refers pair nodes directly connected nodes appear clique dependence explicit defining local function clique appear 
refining concept local function observe define local function operate maximal clique clique extended include additional nodes simultaneously remain fully connected gain defining chapter 
conditional random fields potential functions cliques form subsets maximal clique 
simplest set local functions equivalently correspond conditional independence properties associated graph set functions function defined possible realisations maximal clique local functions known potential functions may strictly positive real valued functional form 
unfortunately product set positive real valued functions guaranteed satisfy axioms probability 
satisfy axioms ensure product joint probability distribution random variables represented nodes define normalisation factor wherec set maximal cliques joint distribution equivalence parameterisation joint distribution conditional independence characterisations undirected graph proved hammersley clifford theorem 
joint distribution random variables undirected graphical model written product potential functions important note isolated potential function direct probabilistic interpretation represent constraints configurations random variables function defined 
turn affects probability global configurations global configuration high probability satisfied constraints global configuration low probability 

crf graph structure crf graph structure conditional random field form undirected graphical model define joint probability distribution label sequences set observation sequences labelled 
letting jointly distributed random variables respectively ranging observation sequences labelled corresponding label sequences conditional random field undirected graphical model globally conditioned observation sequence 
formally define undirected graph fy vg 
words node set corresponding random variables representing component label sequence 
entire graph class distributions associated considered conditioned class joint distributions associated form jx particular realisations label observation sequences respectively 
random variable obeys markov property respect probability random variable random variables fu vg fu vg equal probability random variables corresponding nodes neighbouring conditional random field 
theory structure graph may arbitrary provided represents conditional independencies label sequences modelled 
modelling sequences simplest common graph structure encountered nodes corresponding elements form simple order chain structure illustrated 
note chapter 
conditional random fields random variables representing elements part graph wish define probability distribution form yjx 
additionally absence graphical structure elements highlights fact merely conditioning observation sequences independence assumptions graphical structure chain structured case crfs sequences 
variables corresponding unshaded nodes generated model 
maximum entropy principle lafferty choice potential functions crfs heavily principle maximum entropy 
maximum entropy framework estimating probability distributions set training data specifies assumptions constructing distribution warranted data :10.1.1.131.3506
entropy measure uniformity probability distribution uncertainty 
conditional entropy yjx model distribution label sequences observation sequences yjx yjx logq yjx empirical distribution training data 
maximised distribution label sequences yjx uniform possible 
principle maximum entropy maxent asserts probability distribution justifiably constructed incomplete information training data consisting set constraints 
maximum entropy principle maximum entropy subject constraints representing known 
distribution involve assumptions regarding unknown information entirely unwarranted 
order construct model accurately encodes know training data need method representing partial information 
appropriate method doing encode information positive valued feature functions 
example suppose training data contains sentence robot wheels fred round 
corresponding sequence chunk labels np np vp np pp 
express information second pos tag np second word robot pos tag bp define feature np robot np 
set features sort summarise important information contained training data 
ensure model agrees information encapsulated training data model distribution constrained expectation feature respect training data equal expected value feature respect model distribution yjx maximum entropy framework dictates choose distribution satisfies constraints feature values remaining uniform possible 
interestingly distribution chapter 
conditional random fields maximum likelihood gibbs distribution distribution minimises kullback leibler divergence empirical distribution model distribution 
identifying maximum entropy distribution satisfies feature constraints imposed training data constrained optimisation problem 
berger della pietra ratnaparkhi show parametric form maximum entropy constrained distribution yjx exp normalisation factor lagrangian multiplier associated feature intuitively parameter considered weighting indicating informativeness feature potential functions crfs maximum entropy framework provides significant justification choosing potential functions conditional random field joint distribution label sequences observation sequences yjx parametric form similar equation desideratum may satisfied defining potential function exp yj set indices nodes belonging maximal clique graph set maximal cliques strictly positive real valued potential function set possible realisations fact may possible choose potential functions crf distribution label sequences forms maximum entropy minimum divergence model general distribution maximum entropy modelling incorporates distribution :10.1.1.43.7345
distribution uniform model identical maximum entropy model described section 

potential functions crfs maximal clique addition satisfying requirement potential functions positive real valued functions choice potential function results joint distribution label sequence form yjx exp normalisation factor exp case commonly graph structure modelling sequential data order chain maximal cliques edges edge general form equation expanded exp feature entire observation sequence labels positions corresponding label sequence feature label position observation sequence 
expansion enables joint probability label sequence observation sequence written yjx exp specific instance general situation possible create crf takes hmm properties defining single feature state state pair state observation pair data set train crf chapter 
conditional random fields situation parameters corresponding features equivalent logarithm hmm transition emission probabilities jy xjy 
crfs solution label bias problem global nature distribution defined conditional random field means crfs suffer label bias problem described section 
understand consider crf probabilistic automata unnormalised weights associated state transition 
unnormalised nature weights means transitions necessarily assigned equal importance 
state may amplify dampen probability mass passed successor states caveat final weighting state sequence properly defined probability due global normalisation factor 
parameter estimation crfs frameworks inferring structure model including parameters set training data frequentist bayesian 
approaches give rise parameter estimation technique maximum likelihood estimation mle maximum priori estimation map respectively 
techniques estimate parameters crf current literature crfs address mle parameter estimation 

parameter estimation crfs maximum likelihood parameter estimation frequentist approach parameter estimation estimator function observed training data yield single estimate parameter values model question 
commonly estimator frequentist community maximum likelihood estimator 
bayesians assume unknown quantities random variables allowing probability training data fixed theta viewed conditional probability distribution 
frequentist framework view random variable 
probability training data fixed considered family distributions training data indexed written abusing notation 
viewpoint permits interpretation function fixed data values known likelihood assume training data consists set data points generated independently identically joint empirical distribution likelihood training data conditional model yjx log yjx properties likelihood function enable measure quality model yjx yjx 
maximum likelihood estimation uses likelihood function rank possible values specifically mle principle states value chosen maximise likelihood function qml argmax chapter 
conditional random fields ensuring data values observed training data assigned high probability 
words parameters maximise likelihood function result model close empirical distribution possible model framework 
product may prove difficult deal general value chosen maximise logarithm likelihood function involves manageable sum product log yjx likelihood 
alter value chosen logarithm monotonic function 
maximum likelihood estimation crfs maximum likelihood parameter estimation problem crf defines probability distribution yjx exp task estimating parameters set training data points independently identically generated empirical distribution log likelihood training data maximised 
substituting value yjx conditional random field equation definition log likelihood equation gives 

logz parameter vectors respectively feature vector feature vector 

parameter estimation crfs numerical optimisation point view log likelihood function crf behaved smooth concave entire parameter space 
concave nature log likelihood function means chosen value global maximum obtained gradient vector partial derivatives respect parameter zero 
differentiating log likelihood function respect parameter gives yjx yjx note setting zero gives maximum entropy model constraint expectation feature respect distribution equal expected value respect empirical distribution 
unfortunately generally possible find value maximises log likelihood analytically setting gradient log likelihood function zero solving yield closed form solution 
parameters maximise log likelihood chosen form iterative technique 
currently literature crfs covers algorithms parameter estimation improved iterative scaling iis generalised iterative scaling gis 
iterative scaling iterative scaling method iteratively refining joint conditional model distribution updating parameters model update rule dl update dl chosen new value closer maximum likelihood solution previous value 
lafferty pro chapter 
conditional random fields pose iterative scaling algorithms estimating maximum likelihood parameters conditional random field generalised iterative scaling gis improved iterative scaling iis 
section overview algorithms theoretical problems algorithm highlighted 
basic iterative scaling framework assumes model yjx parameterised 
aim find new set parameters dl dl result model higher log likelihood 
aim iterative scaling identify growth transformation updates parameters model increase log likelihood possible 
growth transformation applied iteratively convergence reached 
crf change loglikelihood bounded auxiliary function defined yjx exp dl exp finding maximises maximise change log likelihood 
gives rise iterative procedure calculating maximum likelihood parameter set qml initialise converged solve dl parameter update parameter dl setting partial derivative respect parameter 
parameter estimation crfs zero yields equation yjx exp dl update parameters dl may calculated 
gis iis variants basic principle employ slightly different techniques identifying update parameters equation 
generalised iterative scaling crfs generalised iterative scaling gis form iterative scaling follows basic principle outlined previous section update parameters model model converges expected value feature respect model equal expectation feature respect empirical distribution training data model ensure updates result convergence parameter values global optimum gis constrains feature set event training data constant ensure update vector calculated analytically defined sum active feature values observation label sequence pair satisfaction constraint requires definition global correction feature feature specific particular edge vertex 
chapter 
conditional random fields maximum value training data 
note features binary valued simply number active features event simply maximum possible number active features 
addition feature feature set ensures desired 
general adding new features feature set alter model distribution 
case new feature entirely dependent existing features feature set adds additional information constraints model 
trained model unchanged definition correction feature 
assuming features chosen crf sum constant events lafferty assert equation solved analytically follows 
logs sides equation loge log yjx exp dl loge yjx yields update log yjx rate convergence gis algorithm governed step size updates turn dictated magnitude constant large values give rise small step size slow rate convergence smaller values yield larger step size faster rate convergence 
fact careful analysis reveals algorithm intractable 
specifically gis algorithm dependent addition global correction feature ensure active features values pair sum constant 
added feature set correction feature treated identically features parameters estimated update equation 
feature calculating update requires computation 
parameter estimation crfs expectation respect product model distribution marginal distribution observation sequences yjx yjx exp dl general case intractable requires summation possible label sequences task exponential number possible labels 
lafferty note possible get round intractability edge vertex features dynamic programming technique described section 
neglect mention dynamic programming technique possible global features calculating expectation correction feature respect model distribution identifying parameter update correction feature intractable 
gis algorithm applied correctly correction feature treated feature model 
intractability outlined means possible lafferty gis algorithm estimating parameters crf 
improved iterative scaling crfs improved iterative scaling variant gis eliminates need correction feature allows faster convergence basic gis algorithm 
attempting solve equation parameter analytically iis observation equation polynomial exp dl solved dl simple technique newton raphson method 
represent equation polynomial exp dl may tractably solved lafferty mean field approximation max words approximate sum active feature values observation label sequence pair maximum pos chapter 
conditional random fields sible sum observation features observation sequence enables equation rewritten yjx exp dl equation may expressed polynomial exp dl observing set pairs partitioned max non overlapping subsets max maxt values 
rewriting equation sum values split gives max fx mg yjx exp dl assuming standard interpretation delta function 
define expectation yjx enables equation expressed polynomial exp dl max exp dl polynomial may solved newton raphson method 
efficiency iis crfs mean field approximation required iis variant tractable merely serves modify lower bound change log likelihood possible changed lower bound may result slow convergence 
sure lafferty experimental results involving crfs pos tagging indicate convergence iis variant slow 
estimating parameters exponential distribution form equation usual initialise parameters training commences model uniform distribution 
lafferty 
chapter summary unable train crf initialised uniform distribution convergence iterations 
able efficiently train crfs initialising crf parameter vector parameters memm trained convergence iterations 
set initial parameters iis algorithm crfs converged iterations 
trained memm parameters enabled lafferty train crfs reasonable time ideal solution depends availability trained memms 
addition osborne indicates iis yield multiple globally optimal models yield wildly differing performance levels depending initial parameter vector 
observation may mean decision start crf training trained parameters memm fact biasing performance crfs reported current literature 
chapter summary chapter introduced crfs undirected model labelling segmenting data 
theoretical basis functional form distribution defined crf outlined overview crfs solution label bias problem 
parameter estimation algorithms covered current crf literature described theoretical practical limitations discussed 
chapter numerical optimisation crf parameter estimation primary justification iterative scaling algorithms considerable ease implementation fact optimisation techniques gradient function optimised case loglikelihood function need calculated 
computations required required necessary evaluate expectation feature value respect new model distribution yjx 
highly advantageous model distributions calculation gradient vector computationally expensive case conditional random fields maximum entropy models element gradient vector yjx little computational advantage iterative scaling techniques utilise gradient directly 
investigate computational advantage iterative scaling algorithms training conditional maximum entropy models malouf compared performance number algorithms chapter 
numerical optimisation crf parameter estimation ing parameters conditional maximum entropy models range nlp problems 
interestingly malouf observed iterative scaling algorithms performed poorly comparison second order optimisation methods nlp problems considered limited memory variable metric algorithm performed substantially better algorithms 
malouf findings conditional maximum entropy models lend significant weight hypothesis second order numerical optimisation techniques result performance crf parameter estimation 
lafferty choice potential functions gives yjx exp conditional distribution label sequences observation sequence defined crf 
functional form equation similar non sequential conditional maximum entropy model yjx exp functional similarity malouf observations numerical techniques perform better iterative scaling parameter estimation conditional maximum entropy models hypothesise numerical techniques improve parameter estimation performance conditional random fields 
experimentally investigate hypothesis numerical optimisation techniques malouf perform estimating parameters conditional maximum entropy models compared lafferty iis variant crf parameter estimation 
chapter second order numerical optimisation techniques applied crf parameter estimation thesis described implementation details discussed 
performance numerical techniques implemented compared lafferty iis algorithm toy problem nlp task shallow parsing 

order numerical optimisation techniques order numerical optimisation techniques numerical optimisation techniques applied crf parameter estimation described thesis fall categories secondorder techniques 
order techniques information contained gradient vector function optimised repeatedly shift estimates parameters point gradient zero function optimum 
order methods applied task crf parameter estimation variants nonlinear conjugate gradient algorithm 
non linear conjugate gradient steepest ascent methods consider search direction times maximising function conjugate direction methods generate set non zero vectors known conjugate set successively maximise function directions 
non linear conjugate gradient methods particular form conjugate direction technique conjugate vector search direction generated previous search direction previous elements conjugate set 
specifically successive search direction selected linear combination steepest ascent direction gradient function maximised previous search direction iteration conjugate gradient update algorithm shifts parameters function maximised direction current conjugate vector update rule optimal step size selected approximate line search 
conjugate gradient methods appropriate maximising general convex function log likelihood conditional model form equation 
consider chapter 
numerical optimisation crf parameter estimation fletcher reeves polak positive algorithms 
algorithms theoretically equivalent may exhibit different numerical properties due different methods choosing search direction step size 
detailed discussion algorithms may 
second order numerical optimisation techniques second order optimisation techniques newton method improve order techniques conjugate gradient augmenting gradient values calculating parameter updates information regarding curvature second order derivatives function optimised 
general second order update rule calculated second order taylor series approximation matrix second partial derivatives respect log likelihood function hessian matrix 
setting derivative approximation zero results update rule update rule results fast convergence computation inverse hessian matrix may prohibitively expensive large scale problems encountered nlp 
second order methods direct hessian estimating parameters large models highly impractical 
variable metric quasi newton methods form second order technique similar newton method explicitly calculating inverse hessian rely entirely information contained gradient objective function 
iteration variable metric methods avoid 
second order numerical optimisation techniques second derivatives build model hessian measuring change gradient 
local approximation hessian empirically sufficiently variable metric methods exhibit superlinear convergence 
basic principle variable metric methods replace hessian matrix second order taylor approximation symmetric positive definite matrix approximates hessian 
results revised update rule iteration updated reflect parameter changes result previous iteration 
calculating afresh simply updated account curvature measured previous iteration task relies current gradient gradient previous step approximating hessian matrix enables variable metric methods exhibit improved convergence traditional newton method 
limited memory variable metric methods despite computational improvements obtained approximating hessian approximate hessian inverse prove sufficiently dense storage infeasible large scale problems 
case nlp tasks number parameters estimated may millions storage dense matrix tasks currently impossible 
possible modify variable metric methods implicit representations hessian approximations require storage small number vectors length number parameters estimated 
methods called limited memory variable metric methods 
chapter 
numerical optimisation crf parameter estimation limited memory variable metric methods fact iteration calculation product may performed sequence inner products vector summations involving set pairs fd ji iteration oldest pair set fd deleted replaced pair obtained current step ensuring pairs stored point time 
practice values sufficient obtain performance reduction storage space variable metric methods significant 
large scale nature problems crfs may prove useful means standard variable metric methods parameter estimation infeasible 
space reduction exhibited limited memory variable metric methods results storage requirements practical large scale tasks nlp 
reason attempt apply standard variable metric methods task crf parameter estimation consider limited memory variable metric methods 
implementation enable efficient performance petsc portable extensible toolkit scientific computation implementation basis 
petsc software library assists development scientific applications modelled partial differential equations providing variety data structures routines storing manipulating visualising large sparse matrices 
operations required training crfs may expressed terms matrix calculations 
framing parameter estimation way enables take 
implementation advantage utilities offered petsc framework improve efficiency 
representation training data stated section potential function chain structured crf function entire observation sequence labelled labels adjacent elements corresponding label sequence exp observation combined nature dynamic programming techniques lafferty enable efficient calculation feature expectations outlined section means appropriate method representing data training chain structured crf define sparse matrix rows corresponding particular tuples columns features 
model probability matrix calculations lafferty observe chain structured crf label sequence augmented start state conditional probability yjx label sequence observation sequence may expressed terms matrices 
specifically alphabet labels drawn define jy matrix random variable element yjx yjx exp recalling view crf finite state model unnormalised probabilities mentioned section matrix considered represent chapter 
numerical optimisation crf parameter estimation weights arc model time step matrix formulation means unnormalised conditional probability yjx label sequence expressed product appropriate entries drawn matrices sequence yjx jx normalisation factor depends observation sequence may calculated set matrices closed semirings algebraic structure results general framework solving path problems graphs 
specifically start entry product matrices start dynamic programming feature expectations iis training algorithm crfs numerical estimation techniques described sections involve calculation feature expectations respect empirical distribution edge feature feature expectations respect product model distribution yjx marginal yjx yjx edge feature 
calculating expectation feature respect crf model distribution nave fashion intractable due required sum possible label sequences 
instance observation sequence length 
implementation jy possible label sequences observation sequence 
clearly evaluating yjx possible label sequences infeasible 
fortunately lafferty propose dynamic programming method avoiding intractability calculating yjx 
lafferty dynamic programming technique fact model probability yjx may expressed terms matrices 
starting definition expectation edge feature note equation may rewritten yjx jx yjx yjx forward backward vectors defined respectively base cases yjx start yjx recurrence relations similarly lafferty state expectation vertex feature respect model distribution may expressed yjx yjx yjx chapter 
numerical optimisation crf parameter estimation representation training data project row training data matrix corresponds particular tuple means fact efficient represent expectation edge feature yjx jx yjx yjx expressions feature expectations means calculating entire model distribution jx need calculate possible jx value dynamic programming technique described 
addition observe rewriting definitions yjx similar fashion means information need empirical distribution may stored vectors jx possible values marginal distribution observation sequences training data 
information required estimating parameters iis algorithm numerical techniques contained distribution vectors jx matrices forward backwards vectors training data matrix 
data structures computations required parameter estimation may performed matrix calculations may efficiently performed petsc library 
optimisation techniques iis parameter estimation algorithm implemented data structures routines provided petsc 
estimation algorithms implemented tao toolkit advanced optimisation implementation numerical optimisation methods 
tao library built top petsc designed assist 
experiments implementation tasks involve non linear optimisation problems 
tao provides routines line searches convergence tests implementations standard optimisation algorithms conjugate gradient variable metric methods 
stopping criterion ensure fair comparison stopping criterion parameter estimation algorithms 
malouf judging convergence reached relative change log likelihood iterations fell predetermined threshold provided basis comparing parameter estimation techniques conditional maximum entropy models 
criterion malouf successfully compare parameter estimation algorithms conditional maximum entropy models appropriate choice crfs similar functional form models investigated malouf 
experiments perform comparison numerical optimisation techniques described sections lafferty iis algorithm implementation described previous section applied known sequential data labelling task statistical natural language processing text chunking shallow parsing 
task chosen representative sorts sequential data nlp problems previously studied different methods ranging adaptations non sequential techniques maximum entropy hmm methods 
chapter 
numerical optimisation crf parameter estimation shallow parsing shallow parsing task diving text non overlapping chunks phrases syntactically related words grouped 
example consider labelled sentence np vp np current account deficit vp narrow pp np pp np september 
text chunking nature useful resultant chunks assist process full parsing originally proposed abney 
conll shared task supervised learning problem involving annotating part speech pos tagged sentences non overlapping phrases 
overview task detailed description chunk types may tjong kim sang buchholz 
word sentence labelled chunk label forms label indicating word part phrase label indicating word particular phrase label indicating word inside phrase 
labels subdivided indicate type minor phrasal category word word part giving rise total chunks labels see table 
corpus sentences drawn consists material penn treebank ii wsj sections 
material split sentences sequences sentence split tokens words punctuation 
tokens additionally annotated part speech tag generated brill tagger 
features previous involving log linear models text chunking resulted identification set informative features capture salient aspects tagging task 
specifically performing text chunking 
experiments count chunk type np noun phrase vp phrase pp prepositional phrase advp adverb phrase sbar subordinated clause adjp adjective phrase prt particles conjp conjunction phrase intj interjection lst list marker ucp coordinated phrase table number chunks phrase type training data tokens 
tradeoff context accuracy 
context words pos tags greater accuracy model context vastly increases number sparsity features 
osborne results achieved considering pos tag current word pos tags words letters current word encode context nature crf features set features defined previous current state pair pair 
state state feature form label label state observation feature defined label chapter 
numerical optimisation crf parameter estimation boolean predicate regarding nature th word context example boolean predicate pos tag current word nn unfortunately implementation problems outlined meant possible context described crf parameter estimation techniques 
greatly reduced context consisting current pos tag letters current word 
method constructing crf features limited context identical process described 
performance parameter estimation algorithms original intention compare performance lafferty iis variant parameter estimation algorithms described sections training data conll shared task train crf models 
problems memory leaks crf implementation appeared occurring petsc library routines matrix creation manipulation destruction meant training process increasingly large amounts memory ran additional memory needed allocated 
memory leaks severely limited size data set amount context feasibly training crf models 
chose restrict training data toy problem consisting sentences drawn conll shared task corpus see table 
sentences special way accuracy model trained sentences indicative accuracy achieved models trained larger data set 
sentences provide common albeit tiny data set may compare relative performance parameter estimation algorithms question 
provide detailed thorough comparison 
experiments parameter estimation algorithms experiments compare training methods repeated full conll training data set memory leaks mentioned eliminated labels contexts features non zeros events table characteristics toy dataset compare algorithm performance 
results applying parameter estimation algorithms sentences drawn conll training data set summarised table 
training algorithm table indicates number iterations required train model convergence number log likelihood gradient evaluations required optimisation techniques require multiple evaluations iteration total elapsed time seconds 
despite method iterations ll evaluations time iis conjugate gradient fr conjugate gradient prp limited memory variable metric table results comparison 
highly trivial nature data set results clearly highlight performance differences algorithms 
lafferty iis algorithm slowest techniques iterations seconds reach relative change log likelihood iterations 
conjugate gradient methods faster iis variant requiring fewer iterations log likelihood calculations 
updated results www cogsci ed ac uk osborne msc projects wallach ps gz 
experiments performed single cpu dual processor intel xeon tm cpu mhz machine gb ram 
chapter 
numerical optimisation crf parameter estimation conjugate gradient methods fletcher reeves exhibits faster convergence 
fastest technique methods investigated limited memory variable metric algorithm 
technique trained crf convergence seconds iterations performed loglikelihood gradient calculations 
results obtained experiments highly encouraging fully confirm results comparison techniques need rerun larger nontrivial dataset issues regarding memory leaks crf implementation sorted 
importantly results echo malouf findings conditional maximum entropy models data set way representative kind dataset encountered nlp classification tasks 
re confirmation malouf experimental observations different theoretical framework significant implications training crfs training maximum entropy minimum divergence models 
particular malouf findings combined thesis show independent experimental scenarios involving different log linear models general gradient numerical optimisation techniques outperform iterative scaling considerable margin terms log likelihood evaluations total elapsed time 
additionally malouf experiments outlined thesis limited memory variable metric method takes account curvature log likelihood function calculating updates results significantly faster convergence order techniques considered 
chapter summary chapter number second order numerical optimisation techniques applied crf parameter estimation described implementation details program compare crf training algorithms 

chapter summary performance numerical techniques implemented compared lafferty iis algorithm toy problem nlp task shallow parsing 
experimental results indicated numerical optimisation techniques faster training crfs iterative scaling 
chapter aim described thesis compare performance general numerical optimisation techniques iterative scaling algorithms task estimating parameters conditional random fields 
initial step theoretical analysis crfs iterative scaling algorithms current literature carried 
analysis see chapter details revealed gis algorithm crfs fact intractable due sequential nature data modelled 
additionally lafferty iis algorithm requires mean field approximation may slow convergence 
addition theoretical disadvantages iterative scaling crfs experimental malouf suggested iterative scaling may practical method training crfs 
malouf demonstrated second order general numerical optimisation techniques significantly reduced time taken train non sequential conditional maximum entropy models type log linear model similar functional form distribution defined conditional random field 
investigate numerical optimisation techniques enable efficient training crfs crf implementation developed matrix vector primitives provided petsc software library 
iis chapter 
parameter estimation algorithm framework numerical optimisation algorithms implemented data structures routines provided petsc 
actual numerical optimisation routines conjugate gradient algorithms fletcher reeves polak limited memory variable metric algorithm benson 
implementations algorithms provided tao toolkit advanced optimisation framework implementing software involving optimisation routines 
original intention compare performance iterative scaling numerical optimisation algorithms text chunking data conll shared task train crf models 
implementation issues meant dataset size possible time frame project 
algorithms compared tiny subset data despite trivial nature clearly highlighted performance differences parameter estimation techniques compared 
results obtained crf parameter estimation nave dataset echo malouf findings non sequential conditional maximum entropy models confirming findings similar non identical theoretical framework 
striking feature results thesis numerical optimisation techniques outperformed lafferty iis method 
additionally benson limited memory variable metric algorithm performed better order conjugate gradient algorithms investigated 
described thesis indicates iterative scaling techniques current literature crf parameter estimation suffer theoretical practical disadvantages making crfs impractical choice labelling real world sequential data 
experiments performed need re run larger data sets experimental results reveal possible train crfs general second order numerical optimisation techniques bet ter convergence rate exhibited iterative scaling 
highly promising finding provides means crfs trained relying models provide initial parameter values 
bibliography steven abney 
principle parsing computation psycholinguistics chapter parsing chunks pages 
kluwer academic publishers boston 
satish william gropp lois barry smith 
petsc web page 

satish william gropp lois barry smith 
petsc users manual 
technical report anl revision argonne national laboratory 
iain miles osborne 
improved iterative scaling yield multiple globally optimal models radically differing performance levels 
coling taipei taiwan 
steve benson lois jorge 
toolkit advanced optimization tao web page 
steve benson lois jorge jason 
tao users manual 
technical report anl mcs tm revision argonne national laboratory 
steven benson jorge 
limited memory variable metric method bound constrained optimisation 
technical report anl acs argonne national laboratory 
berger miller 
just time language modelling 
proceedings icassp seattle wa 
adam berger 
improved iterative scaling algorithm gentle 
adam berger stephen della pietra vincent della pietra 
maximum entropy approach natural language processing 
computational linguistics 
bibliography jeff bilmes 
gentle tutorial em algorithm application parameter estimation gaussian mixture hidden markov models 
technical report icsi tr international computer science institute berkeley 
leon bottou 
une de apprentissage applications la reconnaissance de la parole 
phd thesis universite de paris xi paris 
brown della pietra della pietra mercer nadas roukos 
translation models learned features generalized csiszar algorithm 
ibm technical report 
stanley chen ronald rosenberg 
gaussian prior smoothing maximum entropy models 
technical report cmu cs school computer science carnegie mellon university february 
peter clifford 
markov random fields statistics 
geoffrey grimmett dominic welsh editors disorder physical systems honour john hammersley pages 
oxford university press 
thomas cormen charles leiserson ronald rivest 
algorithms 
mit press mcgraw hill 
darroch ratcliff 
generalized iterative scaling log linear models 
annals mathematical statistics 
durbin eddy krogh mitchison 
biological sequence analysis probabilistic models proteins nucleic acids 
cambridge university press 
shai fine yoram singer naftali tishby 
hierarchical hidden markov model analysis applications 
machine learning 
zoubin ghahramani michael jordan 
factorial hidden markov models 
david touretzky michael mozer michael hasselmo editors proc 
conf 
advances neural information processing systems nips volume pages 
hammersley clifford 
markov fields finite graphs lattices 
unpublished manuscript 
john hopcroft rajeev motwani jeffrey ullman 
automata theory languages computation 
addison wesley 
jaynes 
information theory statistical mechanics 
physical review may 
bibliography jaynes 
information theory statistical mechanics ii 
physical review october 
frederick jelinek 
statistical methods speech recognition 
mit press 
jordan bishop 
graphical models 
yee teh sam roweis 
alternative objective function markovian fields 
proceedings nineteenth international conference machine learning 
kim sang buchholz 
conll shared task chunking 
proceedings conll lisbon portugal 
rob 
chunking maximum entropy models 
proceedings conll lisbon portugal 
kupiec 
robust part speech tagging hidden markov model 
computer speech language 
john lafferty andrew mccallum fernando pereira 
conditional random fields probabilistic models segmenting labeling sequence data 
proc 
icml 
robert malouf 
comparison algorithms maximum entropy parameter estimation 
proceedings sixth conference natural language learning conll 
andrew mccallum dayne freitag fernando pereira 
maximum entropy markov models information extraction segmentation 
proc 
icml 
antonio molina pla 
shallow parsing specialized hmms 
journal machine learning research march 
murphy 
linear time inference hmms 
neural information processing systems december 
jorge nocedal 
large scale unconstrained optimization 
watson duff editors state art numerical analysis pages 
oxford university press 
jorge nocedal steve wright 
numerical optimization 
springer new york 
miles osborne 
shallow parsing part speech tagging 
proceedings conll lisbon portugal 
bibliography miles osborne 
shallow parsing noisy non stationary training material 
journal machine learning research 
paz 
probabilistic automata 
academic press 
della pietra della pietra lafferty :10.1.1.43.7345
inducing features random fields 
technical report cmu cs carnegie mellon university 
della pietra della pietra lafferty 
inducing features random fields 
ieee transactions pattern analysis machine intelligence 
pla antonio molina prieto 
improving text chunking means lexical contextual information statistical language models 
proceedings conll lisbon portugal 
rabiner 
juang 
fundamentals speech recognition 
prentice hall signal processing series 
prentice hall 
rabiner 
tutorial hidden markov models selected applications speech recognition 
proceedings ieee 
adwait ratnaparkhi 
maximum entropy part speech tagger 
proceedings empirical methods natural language processing conference 
university pennsylvania may 
adwait ratnaparkhi 
simple maximum entropy models natural language processing 
technical report institute research cognitive science university pennsylvania 
ronald rosenfeld 
adaptive statistical language modelling maximum entropy approach 
phd thesis carnegie mellon university april 
shannon 
mathematical theory communication 
bell system tech 
journal 
jun wu sanjeev khudanpur 
efficient training methods maximum entropy language modelling 
proceedings icslp volume pages beijing 
zhou jian su 
hybrid text chunking 
proceedings conll lisbon portugal 
