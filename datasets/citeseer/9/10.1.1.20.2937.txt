adaptive dimension reduction clustering high dimensional data chris ding zha horst simon nersc division lawrence berkeley national laboratory university california berkeley ca department computer science engineering pennsylvania state university university park pa lbl gov zha cse psu edu known high dimensional data clustering standard algorithms em means trapped local minimum 
initialization methods proposed tackle problem limited success 
propose new approach resolve problem repeated dimension reductions means em performed low dimensions 
cluster membership utilized bridge reduced dimensional subspace original space providing flexibility ease implementation 
clustering analysis performed highly overlapped gaussians dna gene expression profiles internet newsgroups demonstrate ectiveness proposed algorithm 
application areas information retrieval image processing computational biology global climate research analysis high dimensional datasets frequently encountered 
example text processing typical dimension word vector size vocabulary document collection tens thousands words phrases routinely molecular biology human gene expression profile analysis typically involves thousands genes image processing typical dim image pixels dimensions 
developing ective clustering methods handle high dimensional dataset challenging problem 
popular clustering methods means em methods su er known local minima problem iterations proceed trapped local minima configuration space due greedy nature algorithms 
high dimensional space equi potential cost function surface rugged 
iterations get trapped close initial starting configuration 
words di cult sample large configuration parameter space 
conventional approach large number runs random initial starts pick best result 
random starts number initialization methods concentrate intelligently choose starting configurations centers order close global minima possible 
approaches limited inherent di culty finding global minima high dimensional space place 
monte carlo methods 
propose new approach solve problem 
approach utilizes idea dimension reduction 
dimension reduction clustering classification machine learning data mining applications 
usually retains important dimensions attributes removes noisy dimensions irrelevant attributes reduces computational cost 
applications dimension reduction carried preprocessing step 
selection dimensions principal component analysis pca singular value decomposition svd popular approach numerical attributes 
information retrieval latent semantic indexing uses svd project textual documents represented document vectors svd shown optimal solution probablistic model document word occurrence 
random projections subspaces 
applications dimensions selected stay fixed entire clustering process 
dimension reduction process de coupled clustering process 
data distribution far gaussian example dimensions selected pca may deviate substantially optimal 
approach dimension reduction dynamic process adaptively adjusted integrated clustering process ii effective cluster membership bridge connecting clusters defined reduced dimensional space subspace defined full dimensional space iii connection clusters discovered low dimensional subspace avoid curse dimensionality adaptively re adjusted full dimension space global optimality 
process repeated convergence 
focus means em algorithms mixture model spherical gaussian components 
marginal distributions gaussian mixtures retain identical model parameters reduced low dimensional subspace original high dimensional space providing theoretical justification dimension reduction 
objective function means property 
centroid classification text studied dimension reduction centroids define subspace projection 
dimension reduction text processing extensively studied 
studies dimension reduction preprocessing approach dimension reduction performed adaptively 
consider projection methods new projected dimensions linear combination old dimensions 
optimal selection subset existing dimensions attributes substantially different approach 
selection subset attributes context clustering studied 
context classification subset selection extensively studied 
ective dimension clustering approach perform clustering low dimensional subspaces 
em essence fitting density functional form sensitive local density variations 
reduced dimension subspaces smoother density reducing chances trapped local minima 
may interpret low dimensional subspace containing relevant attributes linear combinations coordinates 
dimensionality reduced dimension subspace clustering 
argue linear discriminant analysis gaussian distributions means pooled covariance matrix dim space point classified belong class depending threshold spherical gaussian identity matrix subspace perpendicular direction enter consideration 
ective dimension clusters 
may consider pairs class classifications dimensions perpendicular directions irrelevant 
ective clustering dimensions spherical gaussians spanned centers 
call relevant dimensions passing centers dim subspace 
essentially geometric point view 
euclidean distances main factors clustering dimensions perpendicular relevant subspace clearly irrelevant 
ective dimensionality relevant subspace 
happens cluster centers lie subspace dimension 
example clusters centroids lie dim plane 
summary ective dimension clustering 
em relevant subspace algorithm easily naturally incorporated expectation maximization em algorithm applied spherical gaussian mixtures 
idea irrelevant dimensions integrated resulting marginal distribution follows gaussian mixture functional form 
freely move reduced dimension subspace original space 
approach cluster membership information posterior probabilities indicator variables plays critical role 
knowing subspace directly infer centers original space 
assume mixture model component spherical gaussian distribution exp vectors dim space 
denote 
note spherical gaussian function invariant properties important invariant orthogonal coordinate rotation operation rx rx orthonormal matrix model parameters 
ii invariant coordinate translation shift operation lx gaussian mixture model dimension reduction properly studied probabilistic framework marginal distributions 
reason need split space dim space contains relevant dimensions attributes dim space contains irrelevant dimensions noises 
split coordinates explicitly dim relevant space dim subspace noise orthogonal dim relevant space 
coordinate rotation coordinate transformation pca clearly separate relevant noisy dimensions 
marginal distribution defined det det jacobian related coordinate transformation 
orthonormal rotations pca det 
splitting coordinates 
marginal distribution dy exactly standard spherical gaussian dim subspace 
reason simply dim subspace 
conclude theorem 
em clustering spherical gaussian mixture models dimensions integrating irrelevant dimensions marginal probability exactly type gaussian distribution dim space 
relevant attributes clustering retained dim subspace 
adaptive dimension reduction em real world clustering problems clusters separated dim subspace initially obtained pca necessarily coincide subspace spanned cluster centers 
centers cluster memberships usual dimension reduction clustering necessarily correct accurate results 
correct adaptively modifying dim subspace current clustering results round clustering modified subspace 
repeat process times improved results 
point cluster centroid dim subspace mapping back original dim space unique 
fact infinite number points dim space project point dim subspace points vertical line project single point plane 
centers centroids means obtained clustering dim subspace uniquely traced back original dim space cluster membership data point 
observation basis adr em clustering 
cluster membership information contained posterior probability pr probability point belongs cluster current model parameters evidence value 
em algorithm initialize model parameters ii compute iii update compute number points belonging cluster update priors update centers update covariances rn steps ii iii repeated convergence 
em converges final cluster information contained 
information centers original dim space computed locations cluster centers original dim space known expressed matrix easily find new dimensional subspace spanned centers 
new subspace defined set orthonormal vectors 
note orthonormal basis unique rotation equally basis 
methods compute basis 
computational complexity number data vectors dataset 
svd basis compute singular value decomposition svd data centered linearly dependent 
rank 
matrix orthogonal basis new subspace 
svd basis useful property automatically orders dimensions importance 
example dimension important sub dimension principal component analysis 
project original data new subspace round em clustering starting cluster centers projections information priors 
qr basis way build orthogonal basis qr gram schmidt centroids 
loss generality smallest magnitude form matrix 
note vectors necessarily orthogonal 
gram schmidt procedure equivalently qr factorization linear algebra matrix orthonormal basis subspace 
upper triangle matrix containing projections components basis 
qr basis property close centers reasonably orthogonal 
project original data new subspace note construction centers coincide svd qr basis 
complete adr em algorithm complete adaptive dimension reduction expectation maximization adr em algorithm described follows 

preprocessing data fit better spherical gaussian model 
center data 
rescale data variance dimension 
choose appropriate input parameter 
choose dimensionality reduced dimension subspace 
general recommend 
appropriate 

dimension reduction pca methods including random starts 

run em dim subspace obtain clusters 
cluster membership construct cluster centroids original space 
check convergence 
go step 
compute new dim subspace spanned centroids svd qr basis 
project data new subspace 
go step 
output results converting posterior probabilities discrete indicators 
relevant attributes coordinates identified 
accurate results necessary may run final round em original data space starting existing parameters see section 
key feature adr matter data projected transformed shifted rotated subspaces cluster memberships subspace computed construct clusters original space need bookkeeping details data transformations reductions 
easily design hybrid schemes di erent data projections obtained cluster membership bridge form integrated clustering method 
relevant dimensions general optimal choice 
choice cases qr basis qr applied cf 
eq cf 
eq obtain basis vectors ii svd basis qr basis add additional basis vectors orthogonal existing basis 
additional basis vectors chosen particular emphasis chosen randomly 
randomly choosing additional basis vector help search broader configuration space making sure stuck local minimum 
choose 
centers define dim subspace locate near dim subspace 
example points dim space lie dim plane dim line 
cases rank deficient rank singular values svd basis drop near zero choose appropriate 
rank deficient may set computational ciency ectiveness 
especially important dealing large complex dataset believe example clusters 
due curse dimensionality dim space may high may set find clusters dim space em means typically ective 
dim space computation efficient dim results inspected dim graphics visualization tools 
case best clusters discovered may refine results setting re run algorithm cluster membership bridge 
test examples tested reduced method results generally method 
notice slower convergence em method 
adaptive dimension reduction means adr method applied means clustering 
set data vectors 
xn means clusters seeks find set centers 
minimize ck 
cluster represented center consists data vectors closest euclidean distance center cluster centroid data vectors 
means clustering viewed special case em simplifications ii iii 
key find relevant dim reduced space specified projection matrix 
theorem 
suppose know correct dim relevant subspace defined 
xn 
centroids dim subspace 
solve means problem dim subspace min 
cluster membership obtained reconstruct centers 
full dimensional space 
exact optimal solution full dimension means problem 
proof 
assume centroid matrix minimum cluster means 
construct projection matrix spans subspace gram schmit procedure construct orthonormal matrix 
spans subspace 
fact eq indicates centers closest sim space closest dim space independent write const 
know final solution easily construct rr orthonormal matrix spans correct subspace 
practice know problem solved 
theorem need find relevant subspace 
large flexibility defining finding relevant subspace easier finding directly 
usefulness theorem 
adaptive dimension reduction means theorem 
complete adr kmeans algorithm identical adr em algorithm 
cov cov gaussian clusters dim space 
covariances 
covariances 
data points shown pca components 
variance increases st cluster red squares nd cluster blue circles rd cluster black triangles 
applications highly overlapping gaussian mixtures example point synthetic dataset gaussians dim centers listed covariances prior distributions 
gaussians highly overlapped see fig results adr em shown quite close correct results 
repeated runs show method quite robust 
run em directly dim space em di culty finding correct clusters 
results change di erent run 
runs best results shown dimension reduction essential highly overlapped situation 
dna microarray gene expression profiling example molecular biology 
high density dna microarray technology simultaneously monitor expression level thousands genes determines di erent pathological states tissue drawn di erent patients 
study gene expression profiles non hodgkin lymphoma cancer data 
samples phenotypes classes pick largest classes total samples see fig samples di large cell lymphoma samples activated blood cell cell samples chronic leukemia samples lymphoma 
sample contains expression levels genes variables 
question ask discover phenotypes data directly human expertise test statistic criteria select top genes 
clustering problem focused samples dim space 
high dimensional problem 
adm em algorithm dataset setting 
clustering result shown contingency table ij ij number data points observed cluster computed clustering method belong cluster accuracy accuracy defined kk 
perform clustering directly dim space runs trapped local minimum 
usefulness pca gene expression analysis noted 
gene expression profiles lymphoma cancer dataset 
shown data pca components 
internet newsgroups clustering internet newsgroups dataset illustrate process adaptive dimension reduction 
news groups ng ng ng ng ng news articles group see details 
ng comp graphics ng rec motorcycles ng rec sport baseball ng sci space ng talk politics mideast 
words document frequency removed total distinct words retained 
document represented vector dimensional space 
set relevant dimension dim subspace 
start random initial dim subspace 
table list accuracy adaptive iteration 
repeated adaptive dimension reduction gradually converges correct subspace 
comparison run means algorithm original space initial clusters obtain accuracy 
indicates ectiveness adaptive method 
newsgroups dataset bow toolkit processing downloaded www cs cmu edu afs cs project theo www naive bayes html 
iteration accuracy min table clustering results adm kmeans algorithm 
discussions introduced new method clustering high dimensional data adaptive dimension reductions 
key ectiveness method lies theorems working subspace containing true cluster centers su cient find cluster centers 
subspace containing cluster centers dimension far smaller original dimension applications 
adaptive dimension reduction ective way converge subspace 
note finding subspace easier finding cluster centers directly due flexibility defining subspace 
concentrate em means algorithm adaptive dimension reduction approach extended clustering methods 
cluster membership bridge connect subspaces di erent dimensions extensions easy implement 
example may construct number subspaces di erent feature selection methods apply di erent clustering methods subset features move combine satisfy optimal conditions 
interesting subtle point functional form dim space cf 
eq dim subspace cf 
eq final parameters priors di er spaces case highly overlapped clusters di erent covariances 
reason probability separated product relevant coordinates irrelevant coordinates mixture component separable 
standard practice reporting results directly obtained reduced dimension subspace accurate 
reason suggest em dim space run parameters obtained dim subspace get accurate final parameters 
acknowledgments 
supported department energy ce science ce ad scientific research mics division ce laboratory policy infrastructure contract de ac sf 
aggarwal procopiuc wolf yu park 
fast algorithms projected clustering 
pages 
agrawal gehrke gunopulos raghavan 
automatic subspace clustering high dimensional data data mining applications 
pages 
alizadeh eisen distinct types di large cell lymphoma identified gene expression profiling 
nature 
berry dumais gavin brien 
linear algebra intelligent information retrieval 
siam review 
bradley fayyad 
refining initial points means clustering 
proc 
th international conf 
machine learning 
dasgupta 
experiments random projection 
proc 
th conf 
uncertainty artificial intelligence uai 
deerwester dumais landauer furnas harshman 
indexing latent semantic analysis 
amer 
soc 
info 
sci 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
royal stat 
soc 
pages 
dhillon modha 
concept decomposition large sparse text data clustering 
machine learning 
ding 
analysis gene expression profiles class discovery leaf ordering 
proc 
th int conf 
research comp 
mol 
bio recomb pages april 
ding 
multi class protein fold recognition support vector machines neural networks 
bioinformatics 
ding 
similarity probability model latent semantic indexing 
proc 
nd acm sigir conference pages aug 
drineas frieze kannan vempala vinay 
clustering large graphs matrices 
proc 
th acm siam symposium discrete algorithms 
duda hart stork 
pattern classification nd ed 
wiley 
golub van loan 
matrix computations rd edition 
johns hopkins baltimore 
golub slonim tamayo molecular classification cancer class discovery class prediction gene expression monitoring 
science 
grim ferri 
initialization normal mixtures densities 
proc 
int conf 
pattern recognition icpr dec 
park rosen 
dimension reduction centroids squares cient processing text data 
proc 
siam conf 
data mining 
john kohavi pfleger 
irrelevant features subset selection problem 
int conf 
machine learning pages 
principal component analysis 
springer verlag 
karypis 
han 
concept indexing fast dimensionality reduction algorithm applications document retrieval categorization 
proc 
th int conf 
information knowledge management cikm 
pitts 
finding number clusters 
pattern recognition letters 
mclachlan krishnan 
em algorithm extensions 
john wiley 
meila heckerman 
experimental comparison clustering initialization methods 
proc 
th conf 
uncertainty artificial intelligence uai pages 
moore 
fast em mixture model clustering multiresolution kd trees 
proc 
neural info 
processing systems nips dec 
pena lozano 
empirical comparison initialization methods means algorithm 
pattern recognition letters 
scott 
multivariate density estimation theory practice visualization 
john wiley 
williams 
mcmc approach hierarchical mixture modeling 
proc 
neural info 
processing systems nips dec 
zha ding gu simon 
spectral relaxation means clustering 
proc 
neural info 
processing systems nips dec 

