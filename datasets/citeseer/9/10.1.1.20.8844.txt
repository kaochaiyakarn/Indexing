lp adaptive algorithm information extraction web related texts fabio ciravegna department computer science university sheffield regent court portobello street dp sheffield uk lp algorithm adaptive information extraction web related text induces symbolic rules learning corpus tagged sgml tags 
induction performed bottom generalisation examples training corpus 
training performed steps initially set tagging rules learned additional rules induced correct mistakes imprecision tagging 
shallow nlp generalise rules flat word structure 
generalization allows better coverage unseen texts limits data sparseness overfitting training phase 
experiments publicly available corpora algorithm outperforms algorithm literature tested corpora 
experiments show significant gain nlp terms effectiveness reduction training time training corpus size 
machine learning algorithm rule induction 
particular focus nlp generalisation strategy pruning search space final rule set 

general agreement main barriers wide commercialization information extraction text difficulties adapting systems new applications 
classical systems rely approaches natural language processing nlp parsing humphreys grishman 
current systems require involvement experts new applications development ciravegna 
serious limitation wider acceptance especially internet realm small medium enterprises backbone new economy afford hire specialists 
reason increasing interest applying machine learning ml order build adaptive systems 
ml approached mainly ciravegna dcs shef ac uk nlp oriented perspective order reduce amount done nlp experts porting systems free text scenarios cardie miller yangarber 
years information extraction texts focusing progressively web away newspaper text analysed muc conferences 
due reduction strategic funds available research increase potential applications web realm 
web emphasises central role texts emails usenet posts web pages 
context structures html tags document formatting stereotypical language elements convey information 
linguistically intensive approaches classical systems grishman humphreys difficult unnecessary ineffective cases 
reason new research stream adaptive arisen convergence nlp information integration machine learning 
goal produce algorithms systems adaptable new internet related applications scenarios analyst knowledge knowledge domain scenario kushmerick califf muslea freitag mccallum soderland freitag kushmerick 
result successful commercial products created increasing interest internet market 
currently available technology effective applied highly structured html pages effective unstructured texts free texts 
opinion successful algorithms tend avoid generalisation flat word sequence 
applied unstructured texts data sparseness problem 
data sparseness relevant size training data sparse data examples needed training quality results sparse data cause generated rules applicable limited number cases overfitting training examples affecting effectiveness unseen cases 
presents lp learning pattern language processing adaptive algorithm designed new stream research shallow nlp order overcome data sparseness confronted nl texts keeping effectiveness highly structured texts 
experimentally algorithm outperforms algorithm literature number testbeds 
particular focus machine learning algorithm rule induction generalisation strategy pruning search space final rule set 
types induced rules lp learns rules generalising set examples marked sgml tags training corpus 
induces types symbolic rules tagging rules correction rules 
section presents types rules algorithm induces section focuses rule generalisation 
tagging rules tagging rule composed left hand side containing pattern conditions connected sequence words right hand side action inserting sgml tag texts 
rule inserts single sgml tag speaker 
lp different adaptive algorithms rules recognize slot fillers insert speaker speaker califf freitag multi slots soderland 
positive examples tagging rule induction algorithm uses sgml tags inserted user training corpus 
rest corpus considered pool negative examples 
positive example algorithm builds initial rule generalizes rule keeps best generalizations initial rule 
particular lp main loop starts selecting tag training corpus extracting text window words left words right 
information stored word window transformed condition initial rule pattern third word seminar condition action word 
insert tag seminar stime pm starting rule associated nlp knowledge inserting stime sentence seminar stime pm condition word seminar created 
initial rule generalised see sections best generalisations kept retained rules part best rules pool 
rule enters pool instances covered rule removed positive examples pool longer rule induction lp sequential covering algorithm 
rule induction continues selecting new instances learning rules pool positive examples void 
contextual rules applied test corpus best rules pool provides results terms precision limited effectiveness terms recall 
means rules insert tags low recall tags generally correct high precision 
just limited number rules able match absolute reliability condition required selection 
order reach acceptable effectiveness necessary identify additional rules able raise recall affecting precision 
lp recovers non best rules tries constrain application reliable 
interdependencies tags exploited constrain application rules 
mentioned lp learns rules inserting tags speaker independently tags speaker 
tags independent tags represent slots requires slots concatenated linguistic patterns presence slot indicator presence speaker anchor tag inserting stime 
summarized indicator presence 
considering single tag rules able model dependencies context reintroduced lp external constraint improve reliability rules 
particular lp low precision non best rules application context tags inserted best rules 
example rules close slots best rules able open close 
selected rules called contextual rules 
example consider rule inserting speaker tag capitalized word lowercase word 
best rule reports high recall low precision corpus reliable close open speaker 
applied best rules recognized open examples related case intuitive 
condition action word wrong tag move tag stime pm stime correction rule 
action shown shifts tag wrong correct position 
speaker corresponding speaker 
anchor tags contexts right rule space application anchor tag speaker left opposite case anchor tag speaker 
reliability contextual rules computed error rate best rules 
tagging rule set composed best rule pool contextual rules 
correction rules tagging rules applied test corpus report imprecision slot filler boundary detection 
typical mistake example time time pm pm part time expression 
reason lp induces rules shifting wrongly positioned tags correct position 
learns mistakes tagging training corpus 
correction rules identical tagging rules patterns match tags inserted tagging rules actions shift misplaced tags adding new ones 
example initial correction rule shifting stime stime stime pm shown 
induction algorithm best tagging rules shift rules initial instance identification generalisation test selection 
positive correct shifts negative wrong shifts correctly assigned tags counted 
shift rules accepted report acceptable error rate 

rule application testing phase information extracted word condition additional knowledge action index word lemma case tag art low seminar seminar noun low prep low stime digit low pm pm low verb low rule associated nlp knowledge 
test corpus steps initial tagging contextual tagging correction validation 
best rule pool initially tag texts 
contextual rules applied context introduced tags 
applied new tags inserted contextual rules match tags inserted contextual rules 
correction rules correct imprecision 
tag inserted algorithm validated 
meaning producing start tag speaker corresponding closing tag speaker vice versa uncoupled tags removed validation phase 

rule induction algorithm types rule mentioned induced algorithm 
mentioned initial rule pattern matches conditions word strings window instance 
rule generalised 
generalisation important analysing natural language input data sparseness due high flexibility natural language forms 
avoiding generalisation means producing big rule set composed rules covering limited number cases 
rule set produce results training corpus limited accuracy test corpus 
known problem overfitting training corpus hand system learns number rules covering unrelated cases cases rule apply testing time leading low recall 
hand rule set sensible errors training data errors prevent rules derived correct examples accepted report errors low recall test produce spurious results testing time low precision 
important hand generalise plain word surface training example order produce rules able overcome data sparseness 
hand necessary prune resulting rule set order reduce overfitting explained remainder section 
rule generalisation ways algorithm word condition action index word lemma case tag stime digit generalisation rule 
generalises initially induced rules hand constraints initial pattern dropped patterns reduced length forms wildcards allows model cases slightly differ pm pm modelled rule word word word pm 
hand conditions single elements relaxed nlp information 
shallow natural language processing associate additional knowledge word initial pattern morphological analyser providing lemma case information pos tagger lexical category noun userdefined dictionary gazetteer available 
conditions element rule pattern relaxed substituting constraints words constraints parts additional knowledge 
example mentioned pm pm rule word digit word pm able better generalise cases rule wildcard 
implemented different strategies rule generalisation 
na version algorithm word seminar word action stime matches word action stime matches word seminar word action stime word matches action stime matches corpus word action stime word matches ciravegna ciravegna generates possible rules parallel :10.1.1.33.1770
generalisation tested separately training corpus error score wrong matched calculated 
initial rule best generalisations kept word seminar word action stime matches word seminar word action stime word word pm matches action stime word matches instance ruleset instance loop notempty ruleset loop rule ruleset rule ruleset ruleset rule rule add ruleset return instance tag instance tag loop distance word instance pattern distance position distance current word tag inserted collect word tag position basic algorithm non nlp generalisation word action stime word word pm matches action stime word word pm matches action stime word word pm matches pattern generalisation seminar stime pm window conditions words 
rule action inserted part pattern tag inserted 
example matches inserts tag 
matches fields contain corpus rules apply 
report better accuracy cover positive examples cover different parts input 
na generalisation quite expensive computational terms 
describe efficient version algorithm uses general specific beam search best rules way similar aq michalski 
starts modelling specific instance general rule empty rule matching instance specialises greedily adding constraints 
constraints added incrementing length rule pattern adding conditions terms 
shows search space example type generalisation case conditions set words 
induction algorithm shown 
algorithm effective na efficient allows efficient rule testing 
matter fact matches specialised rule computed intersection matches general rules making testing order magnitude efficient see 
actual algorithm complex shown relaxes conditions words rule patterns conditions parts nlp additional knowledge associated word 
order introduce type generalisation algorithm modified ruleset longer contain rules sets rules derived instance ruleset instance loop notempty ruleset loop ruleset ruleset ruleset ruleset ruleset ruleset ruleset ruleset add ruleset add ruleset ruleset instance tag instance tag loop distance word instance pattern distance position distance current word tag inserted word tag position concurrent generalisation lexical items 
final algorithm shown 
rule set pruning final algorithm rule generalisation starting initial rule 
pruning performed efficiency reasons reduce search space generalisation reduce overfitting 
pruning removes rules unreliable coverage overlaps rules 
types unreliable rules high error rate performing poorly training corpus reporting limited number matches training corpus easy foresee behaviour test time 
order prune final rule set elements rule accuracy tested maximum error threshold set user running algorithm 
rules pass test discarded soon generated tested 
longer considered considered specialisation reliable addition constraints 
training algorithm tries optimise error threshold restricting rule selection condition excluding rules 
tests results reduced rule set training corpus stops pruning moment reduction accuracy seen mean precision recall 
word tag position loop condit word collect condit tag position ruleset rule ruleset rule ruleset collect rule rule ruleset loop ru ruleset ru rule ru score rule score includes ru coverage rule coverage return true return false rules pruned induction time cover cases training corpus safely test time amount training data allow guarantee safe behaviour run time 
rules detected generation time specialisation depending stopped 
way search space rule reduced algorithm efficient 
example threshold minimum coverage set cases rules generated example 
user set priori minimum coverage threshold threshold optimised training way similar error threshold optimisation 
level pruning overlapping rule coverage rules coverage subsumed general ones removed selected rule set 
goal determine minimum subset rules maximises accuracy training corpus 
rules coverage subsumed rules safely removed contribution final results irrelevant 
type pruning requires comparing coverage reliability involved rules algorithm produce rule empty initial rule 
general rules subsumed rules minor error rate safely removed rule induction 
rules subsumed ones worse error rate pruned final error covering threshold determined necessary see subsuming rule survive rule pruning process 
problem arises pruning level rules cover set examples error rate 
case heuristic 
number covered cases limited specific conditions chosen condition words 
training corpus provide evidence rule reliable rule requiring sequence words produce spurious lp bwi hmm srv rapier whisk speaker location stime etime slots results measure obtained cmu seminars experiments corpus training 
see ciravegna details experiments :10.1.1.33.1770
slot lp rapier bwi slot lp rapier id platform title application area salary req years des years state req degree city des degree country post date language slots measure misc jobs offered corpus training 
whisk obtained lower accuracy rapier califf 
compare lp bwi tested limited subset slots results test time requiring sequences conditions additional knowledge lexical categories 
rule generic conditions selected testing lexical categories provide coverage test corpus 

experimental results lp tested number tasks generalisation generalisation effect generalisation reducing data sparseness number rules covering number cases show cases languages english italian 
report results standard tasks adaptive cmu seminar announcements austin job announcements task consists uniquely identifying speaker name starting time time location seminar announcements freitag 
shows accuracy obtained lp compares obtained state art algorithms 
lp definitely outperforms nlp approaches wrt rapier califf wrt whisk soderland outperforms non nlp approaches wrt bwi freitag kushmerick wrt hmm freitag mccallum 
lp algorithm results go slots 
second task concerned job announcements taken misc jobs offered califf 
lp outperforms rapier whisk 

discussion lp main features contribute excellence experiments single tag rules correction phase rule generalisation shallow nlp processing 
points discussed ciravegna :10.1.1.33.1770
focus effect nlp generalisation 
lp induces rules instance generalisation shallow nlp processing 
generalisation examples training corpus allows reducing data sparseness capturing general aspects simple flat word structure 
morphology allows overcoming data sparseness due number gender word realisations aspect relevant morphologically rich slot lp lp ng speaker location stime etime slots lp lp ng average rule coverage selected rules rules covering case rules covering cases rules covering cases comparison nlp generalisation version lp corpora www isi edu muslea rise index html 
version generalisation lp ng languages italian pos tagging information allows generalisation lexical categories 
principle types generalisation produce rules better quality matching flat word sequence rules tend effective unseen cases 
morphology pos tagging generic nlp processes performing equally unseen cases rules relying results apply successful unseen cases 
intuition confirmed experimentally lp generalisation lp definitely outperforms version nlp generalisation pattern length generalisation lp ng test corpus having speaker stime etime location slots effect different window sizes learning tagging rules fields affected location speaker sensible window size 
speaker gen speaker etime gen etime effect training data quantity cmu seminar task generalisation version converges immediately optimal value etime shows positive trend speaker 
non nlp version shows overfitting speaker slowly converges etime comparable results training corpus 
lp produces general rules rules covering cases figures 
nlp reduces risk overfitting training examples 
shows experimentally lp avoids overfitting speaker field constant rise measure training corpus increased size lp ng clear problem overfitting effectiveness examples provided 
producing general rules implies covering algorithm converges rapidly rules tend cover cases 
means lp needs examples order trained rule generalisation allows reducing training corpus size 
shows etime lp able converge optimal values examples accuracy lp ng slowly increases training material able reach accuracy lp half corpus training 
surprisingly role shallow nlp reduction data sparseness relevant semi structured free texts cmu seminars highly structured documents html pages 
note rule selection phase lp able adopt right level nlp information task hand experiment texts written mixed italian english english pos tagger completely unreliable italian part input 
lp reached effectiveness analogous lp ng rules unreliable nlp information automatically discarded 
interesting note way rule pattern length affects accuracy 
slots location definitely need longer pattern 
important information monitor window size strongly influences training time 

lp successful algorithm 
hand outperforms state art algorithms scientific experiments popular tasks 
particular lp definitely outperforms algorithms making nlp information rapier whisk terms accuracy respectively cmu experiment 
mainly due tagging correction learning algorithm algorithms able exploit nlp information 
rapier uses tagging approach equivalent best tagging step lp able induce expressive rules making powerful wild cards uses randomly compression rule mechanism generalisation tends produce rules providing spurious results ciravegna 
whisk able sophisticated nlp information able output parser 
unfortunately uses multi slot rules increase data sparseness training phase leading low recall ciravegna :10.1.1.33.1770
concerning non nlp algorithms tagging approach generally equivalent best rule tagging phase lp tend sophisticated machine learning approaches 
bwi example learns single tag rules largely equivalent best rules lp boosting schapire emphasize examples learner doing poorly order derive additional rules 
rule formalism includes number wildcards contribute radically algorithm experimental results effectiveness 
bwi type nlp preprocessing 
lp slightly outperforms bwi experiments 
opinion due combined effect contextual tagging phase correction phase nlp generalization believe relevant 
complete comparison state art algorithms ciravegna 
lp successful basis building learningpinocchio tool adaptive applications having considerable commercial success 
number applications developed commercial companies number licenses released industrial companies application development ciravegna :10.1.1.33.1770
lp involve improvement rule formalism expressiveness wild cards shallow nlp generalisation 
concerning hand lp cascade named entity recogniser implemented lp 
allow generalisation named entity classes speaker person possible generalise class rules 
hand lp compatible forms shallow parsing chunking 
possible preprocess texts chunker insert tags chunk borders 
improve precision border identification reducing need correction 
acknowledgments developed naive version lp learningpinocchio itc irst centro la ricerca scientifica tecnologica trento italy 
learningpinocchio property itc irst see itc learning home ht ml 
parts previously published ciravegna :10.1.1.33.1770
common parts copyright association computational linguistics 
califf 
relational learning techniques natural language information extraction ph thesis univ texas austin www cs utexas edu users claire cardie empirical methods information extraction ai journal 
ciravegna satta 
bringing information extraction labs environment ecai proceeding th european conference artificial intelligence horn ed ios press 
ciravegna learning tag information extraction text ciravegna basili gaizauskas eds 
ecai workshop machine learning berlin www dcs shef ac uk fabio ecai workshop html ciravegna adaptive information extraction text rule induction generalisation proceedings th international joint conference artificial intelligence ijcai seattle august 
ciravegna shallow nlp adaptive information extraction web related texts proceedings conference empirical methods natural language processing emnlp pittsburgh june 
freitag 
information extraction html application general learning approach proceedings th national conference artificial intelligence aaai 
freitag mccallum 
information extraction hmms shrinkage aaai workshop machine learning orlando www isi edu muslea rise ml freitag kushmerick 
boosted wrapper induction ciravegna basili gaizauskas eds 
ecai workshop machine learning berlin www dcs shef ac uk fabio ecai workshop html grishman 
information extraction techniques challenges 
information extraction multidisciplinary approach emerging information technology pazienza ed springer verlag 
humphreys gaizauskas mitchell cunningham wilks 
description university sheffield lasie ii system muc 
proc 
th message understanding conference 
kushmerick weld doorenbos wrapper induction information extraction proc 
th international conference artificial intelligence ijcai 
hong 
multi purpose incremental learning system aq testing application medical domains proceedings th national conference artificial intelligence philadelphia morgan kaufmann publisher 
miller crystal fox ramshaw schwartz stone weischedel bbn description sift system muc proc 
th message understanding conference www muc saic com 
muslea minton knoblock wrapper induction semi structured web information sources proc 
conference autonomous learning discovery 
schapire singer improved boosting algorithms confidence rated predictions proc 
eleventh annual conference computational learning theory 
soderland learning information extraction rules semi structured free text machine learning 
yangarber grishman tapanainen automatic acquisition domain knowledge information extraction proc 
coling th international conference computational linguistics saarbr cken 
