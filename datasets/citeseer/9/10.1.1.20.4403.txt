research reports mathematical computing sciences series operations research department mathematical computing sciences tokyo institute technology oh ku tokyo japan sdpara semidefinite programming algorithm parallel version makoto yamashita makoto yamashita titech ac jp fujisawa fujisawa ac jp kojima kojima titech ac jp october semidefinite programming algorithm known cient computer software primal dual interior point method solving sdps semidefinite programs 
applications sdps larger larger large solve single processor 
execution applied large scale sdps computation called schur complement matrix cholesky factorization consume computational time 
sdpara semidefinite programming algorithm parallel version parallel version multiple processors distributed memory replaces parts parallel implementation mpi 
numerical results show sdpara pc cluster consisting processors attains high scalability large scale sdps losing stability 
key words 
semidefinite program primal dual interior point method optimization software parallel computation pc cluster department mathematical computing sciences tokyo institute technology oh ku tokyo japan 
department mathematical sciences tokyo university japan 
semidefinite programming sdp attractive problems mathematical programming years 
roughly speaking extension linear programming lp having linear objective function linear constraints nonnegative vector variables optimization problem having linear objective function linear constraints symmetric positive semidefinite matrix variables 
extension brought lot applications fields 
examples linear matrix inequalities control theory formulated sdps approximate values maximum cut problems theta functions graph theory computed ciently sdp relaxations robust quadratic optimization lp ellipsoidal uncertainty reduced sdp 
see applications 
primal dual interior point method known powerful numerical methods general sdps 
excellent features polynomial time convergence theory solves various sdps ciently practice 
see :10.1.1.140.5474
semidefinite programming algorithm computer software general sdps 
yamashita fujisawa kojima reported high performance various problems including benchmark problems 
high performance mainly due exploiting sparsity input data computation step lengths method 
sdps real world variational calculations arising quantum chemistry get larger larger 
solving large scale sdps single day computer di cult require enormous time memory 
hand rapid continuing growth field high performance computing 
particular cluster computing possible run parallel program easily multiple processors distributed memory computation distributed memory enables solve larger problems high speed attained far 
describe sdpara semidefinite programming algorithm parallel version parallel implementation numerical results pc clusters 
sdpara maintains numerical stability attains high scalability 
organized follow 
section introduce primal dual pair sdps 
outline algorithmic framework single processor point bottleneck lies constructing solving called schur complement equation compute search direction iteration 
section describe technical details sdpara resolves bottleneck stemmed schur complement equation applying parallel computation mpi multiple processors 
section presents numerical results sdpara pc cluster shows high scalability 
section compare sdpara existing software solving general sdps parallel numerical results 
give concluding remarks section 
algorithmic framework bottlenecks notation set symmetric matrices set symmetric positive semidefinite matrices respectively 
notation positive semidefinite positive definite respectively 


input data 
concerned standard form sdp dual sdp minimize subject maximize subject 

form denotes primal variable dual variable 
call number equality constraints size matrix variables 
say feasible solutions satisfy constraints respectively optimal solutions attain minimum maximum objective function values addition feasibility respectively 
interior feasible solutions feasible solutions respectively 
convenience say feasible solution optimal solution interior feasible solution sdp feasible solutions optimal solutions interior feasible solutions common basic idea described numerical tracing central path defined solution system equations 
xy denotes identity matrix 
known system unique solution assumptions 
linearly independent exists interior feasible solution sdp 
assumptions known converges optimal solution sdp 
specifically interior feasible solution sdp duality gap identity verified directly definition 
starts point noted initial point necessarily feasible solution sdp 
suppose satisfying initial point kth iterate compute iterate 
current point lies central path holds may regard point nearest point lie assuming choose target point 
generally take depending estimate close current point central path boundary interior region iterate restricted move 
principle take larger close move center region current point near boundary region smaller close decrease duality gap current point near central path 
compute search direction dx dx dy current point target point solving system modified newton equation dx dx dy 
dxy dy dy dy dy 
xy note symmetrization matrix dy needed dy symmetric matrix 
search direction called ksh direction :10.1.1.140.5474
proposed various search directions systems di erent modified newton equations 
ksh direction employed csdp 
update current point dx dx dy new iterate dx dx dy step size keep new iterate interior region repeat procedure feasibility errors 
get su ciently small denotes matrix norm 
procedure described solving system modified newton equation search direction dx dx dy time consuming part 
reduce dx dx dy dx dy dy dy ij 


py 

call equation schur complement equation matrix schur complement matrix respectively 
known symmetric positive definite matrix positive definite linear independence assumption input data matrices 
satisfied determines unique search direction dx dx dy central path exist 
iteration compute elements schur complement matrix applies cholesky factorization solve schur complement equation 
want emphasize strongly general computational time iteration occupied computation elements schur complement matrix cholesky factorization 
computation heavily depends sparse input data matrices 
common occupies largest portion computational time 
addition schur complement matrix usually fully dense input data matrices sparse 
cholesky factorization needs multiplication takes longer computational time computation elements especially input data matrices sparse number inequality constraints larger size matrix variables 
order confirm fact numerically pick characteristic problems control theta 
sdp arising control theory lovasz theta problem arising graph theory respectively 
table shows proportion total computational time occupied computation elements schur complement matrix denoted elements cholesky factorization denoted cholesky comparison parts denoted 
total denotes total computational time second 
executed single pentium iv ghz cpu gb memory linux 
control theta cpu time sec ratio cpu time sec ratio elements cholesky total table performance control theta single processor table explicitly indicates computational time spent elements control cholesky theta respectively 
cases computational time spent portions 
common elements cholesky occupy computational time solving sdps 
applying parallel computation parts elements cholesky quite reasonable shorten total computational time 
parallel implementation accelerate elements computation elements schur complement matrix cholesky cholesky factorization adopt parallel computation mpi mes sage passing interface communications multiple processors scalable linear algebra packages parallel cholesky factorization routine multiple processors 
call parallel version sdpara semidefinite programming algorithm parallel version 
number available processors attach numbers processor 
sdpara starts execution reading input data 
processor 
processor distributes data processors 
processors participate portion 
describe compute elements parallel 
element form ij elements ij 
ith row share common matrix 
di erent processors shared computation elements need compute entire matrix duplicate need transfer partial elements matrix 
avoid duplicate computation communication time di erent processors reasonable require single processor compute entire matrix elements ij 
ith row 
hand compute matrix elements kj 
independently 
assign computation elements row single processor 
precisely ith row computed processor denotes remainder divided nonzero zero 
illustrate case matrix number processors 
example computed processor computed processor 
mention upper triangular part necessary compute symmetric consistently combine technique employed exploiting sparsity input data matrices 
parallel computation technique adapted row wisely 
parallel implementation elements simple cient see numerical results section 
computation schur complement matrix processor processor processor processor processor processor processor processor processor dimensional block cyclic distribution schur complement matrix computation elements schur complement matrix apply parallel cholesky factorization provided assumes elements positive definite matrix factorized distributed dimensional block cyclic distribution distributed memory sdpara needs calling parallel cholesky factorization routine redistribute elements assumes 
illustrates example dimensional block cyclic distribution case elements matrix distributed processors block size 
example stored processor stored processor respectively 
parts sdpara elements cholesky explained essentially developed single processor 
numerical results numerical experiment executed presto iii pc cluster matsuoka lab tokyo institute technology 
node cluster athlon ghz cpu mb memory 
communication nodes done myrinet network interface card higher transmission capability gigabit ethernet 
capacity myrinet ects significantly performance parallel cholesky factorization provided 
sdps tested divided types 
sdps type selected benchmark problems type sdps arising quantum chemistry listed table 
selected control theta thetag 
sizes shown table 
control control control theory largest problems type theta theta thetag lovasz theta problems 
particular thetag spends maximal computational time problems 
table shows numerical results sdpara applied problems 
control control computational time spent elements computation elements schur complement matrix 
observe excellent scalability ratio real time solve problem respect number processor especially elements 
example sdpara processors solved control times faster sdpara single processor case processors solved problem times faster case single processor respectively 
scalability exceeds ratio number processors 
unusual phenomenon happened probably increased processors memory space processor access decreased access speed memory faster 
theta theta thetag computational time spent cholesky cholesky factorization schur complement matrix 
observe high scalability numerical results problems 
example sdpara processors solved theta times faster sdpara single processor case processors solved problem times faster case single processor respectively 
sdps table quantum chemistry 
characteristic type sdps number equality constraints large 
largest problem need store matrix schur complement matrix distributed memory 
matrix requires gb memory store need processors solve problem 
table shows numerical results sdpara applied problems listed table 
clear processors faster solved problem 
emphasized size problems larger sdpara attains higher scalability number processors increased reduction real time solve smallest problem bh attained reduction attained larger problem case lif 
name control control theta theta thetag table sdps picked number equality constraints define structure example mean data matrix symmetric block diagonal matrix blocks 
comparison compare performance sdpara numerical results 
parallel version dsdp sdp solver developed benson ye 
best number processors control elements cholesky total control elements cholesky total theta elements cholesky total theta elements cholesky total thetag elements cholesky total table performance sdpara multiple processors indicates lack memory system status basis bh sto hf sto nh sto lif 
sto ch sto table sdps arising quantum chemistry number processors bh sto elements cholesky total hf sto elements cholesky total nh sto elements cholesky total lif 
sto elements cholesky total ch sto elements cholesky total table performance sdpara multiple processors sdps arising quantum chemistry indicates lack memory knowledge parallel solver general sdps implemented sdpara 
major di erences sdpara 
di erence lies algorithmic frameworks sdpara ksh search direction dual scaling algorithm 
general attains higher accuracy dual scaling algorithm exploit sparsity input data 
ectively 
di erence lies means solve schur complement equation sdpara adopts cholesky factorization mentioned section employs cg conjugate gradient method known typical iterative method solving positive definite system linear equations 
general cg method works ciently conditioned positive definite system 
current point approaches optimal solution condition number schur complement matrix gets worse rapidly cg method requires computational time solve schur complement equation 
may say cg method ective need highly accurate solutions lots orts resolve di culty 
applied sdpara changing number processors sdp problems control control theta theta selected 
total time required solve problems shown table 
cases sdpara shows higher performance 
control problems sdpara achieves faster total time higher scalability 
processors solve control sdpara times faster 
overwhelming di erence mainly due fact sdpara computes elements schur complement matrix parallel communication processors 
enables sdpara obtain high scalability control problems spend computational time computation schur complement matrix 
see table 
problems theta theta arising graph theory sdpara faster 
observe di erence smaller number processors increases specifically ratio total time sdpara decreases roughly 
cholesky factorization occupies major computational time case see table cg method works ciently type problem shown 
number process control sdpara control sdpara theta sdpara theta sdpara table comparison sdpara multiple processors important point emphasize quality approximate solutions computed software packages 
table shows maximum relative gap obtained problem software package number processors 
relative gap defined max denote objective function values respectively theoretically attain optimal solution 
cases sdpara gained digits relative gap 
pointed paragraph section di erence mainly due di erence cholesky factorization cg method solving schur complement equation employed sdpara 
applied problem thetag problems quantum chemistry listed tables 
solve problems correctly giving numerical error 
sdpara control control theta theta table comparison relative gap sdpara outlined pointed computation elements schur complement matrix cholesky factorization bottlenecks single processor 
overcome bottlenecks illustrated implement sdpara parallel version multiple processors 
shown sdpara successfully solves large scale sdps quantum chemistry presto iii pc cluster 
attains high scalability particular larger scale sdps 
addition distributed memory pc cluster enables sdpara store fully dense large schur complement matrix solve large scale sdps having linear equality constraints 
numerical results comparison sdpara existing software package solving sdps parallel observed cases sdpara generated approximate optimal solutions higher accuracy shorter time 
acknowledgment 
authors grateful professor satoshi matsuoka tokyo institute technology ered high performance pc cluster laboratory numerical experiment reported sections 
nakata graduate school engineering kyoto university ered large scale sdps arising quantum chemistry numerical experiment 
benson ye dsdp home page 
www mcs anl gov benson dsdp 
benson ye parallel computing semidefinite programs preprint anl mcs www mcs anl gov benson dsdp ps 
ben tal nemirovskii lectures modern convex optimization analysis algorithms engineering applications siam philadelphia 
choi cleary azevedo demmel dhillon dongarra hammarling henry stanley walker whaley users guide society industrial applied mathematics philadelphia pa isbn paperback 
boyd linear matrix inequalities system control theory society industrial applied mathematics philadelphia pa isbn borchers csdp library semidefinite programming optimization methods software 
borchers library semidefinite programming test problems optimization methods software 
fukuda kojima lagrangian dual interior point methods semidefinite programs march 
revised october 
appear siam journal optimization 
fujisawa kojima nakata exploiting sparsity primal dual interior point methods semidefinite programming mathematical programming 
goemans williamson improved approximation algorithms maximum cut satisfiability problems semidefinite programming journal association computing machinery 
rendl vanderbei wolkowicz interior point method semidefinite programming siam journal optimization 
kojima hara interior point methods monotone semidefinite linear complementarity problems siam journal optimization 
monteiro primal dual path algorithms semidefinite programming siam journal optimization 
nakata fujisawa kojima conjugate gradient method interior point methods semidefinite programs japanese april proceedings institute statistical mathematics vol december 
nakata fukuda nakata fujisawa variational calculations second order deduced density matrices semidefinite programming algorithm journal chemical physics 
nakata density matrix variational theory application potential energy surfaces strongly correlated systems journal chemical physics 
yu 
nesterov todd primal dual interior point methods self scaled cones siam journal optimization 
todd toh matlab software package semidefinite programming version optimization methods software 
toh note calculation step lengths interior point methods semidefinite programming computational optimization applications 
toh kojima solving large scale semidefinite programs conjugate residual method siam journal optimization 
vandenberghe boyd positive definite programming mathematical programming state art birge murty ed michigan 
wolkowicz vandenberghe handbook semidefinite programming theory algorithms applications kluwer academic publishers massachusetts 
yamashita fujisawa kojima implementation evaluation semidefinite programming algorithm research report dept mathematical computing sciences tokyo institute technology oh tokyo japan september 

