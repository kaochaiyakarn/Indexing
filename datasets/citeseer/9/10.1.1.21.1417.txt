appear proceedings ieee international conference neural networks san francisco ca march april direct adaptive method faster backpropagation learning rprop algorithm martin riedmiller heinrich braun institut fur logik und university karlsruhe karlsruhe frg ira uka de new learning algorithm multilayer feedforward networks rprop proposed 
overcome inherent disadvantages pure gradient descent rprop performs local adaptation weight updates behaviour 
substantial difference adaptive techniques effect rprop adaptation process blurred influence size derivative dependent temporal behaviour sign 
leads efficient transparent adaptation process 
promising capabilities rprop shown comparison wellknown adaptive techniques 
backpropagation learning backpropagation widely algorithm supervised learning multi layered feed forward networks 
basic idea backpropagation learning algorithm repeated application chain rule compute influence weight network respect arbitrary ij net net ij ij weight neuron neuron output net weighted sum inputs neuron partial derivative weight known aim minimizing achieved performing simple gradient descent ij ij gamma ffl ij obviously choice learning rate ffl scales derivative important effect time needed convergence reached 
set small steps needed reach acceptable solution contrary large learning rate possibly lead oscillation preventing error fall certain value 
early way get rid problem introduce momentum term ij gammaffl ij ij gamma momentum parameter scales influence previous step current 
momentum term believed render learning procedure stable accelerate convergence shallow regions 
practical experience shown true 
turns fact optimal value momentum parameter equally problem dependent learning rate ffl general improvement accomplished 
adaptive learning algorithms algorithms proposed far deal problem appropriate weight update doing sort parameter adaptation learning 
roughly separated categories global local strategies 
global adaptation techniques knowledge state entire network direction previous weight step modify global parameters local strategies weight specific information partial derivative adapt parameters 
fact local adaptation strategies closely related concept neural learning better suited parallel implementations superiority global learning algorithms impressively demonstrated published technical report 
majority global local adaptive algorithms performs modification probably learning rate observed behaviour 
examples algorithms delta bar delta technique algorithm 
adapted learning rate eventually calculate weight step 
disregarded size taken weight step ij adapted learning rate partial derivative ij effect carefully adapted learning rate drastically disturbed behaviour derivative 
reasons lead development rprop avoid problem blurred adaptivity rprop changes size weight update ij directly considering size partial derivative 
acceleration techniques great variety modifications backpropagation procedure proposed modified sophisticated weight initialization techniques promise accelerate speed convergence considerably 
experience worked slightly better problems problems improve convergence gave worse results 
modification useful simply clip logistic activation function value reasonably distinguished boundary value 
results non zero derivative preventing unit getting stuck 
especially difficult problems technique worked far stable adding small constant value derivation activation function proposed 
ii 
rprop description rprop stands resilient propagation efficient new learning scheme performs direct adaptation weight step local gradient information 
crucial difference previously developped adaptation techniques effort adaptation blurred gradient behaviour whatsoever 
achieve introduce weight individual update value ij solely determines size weight update 
adaptive update value evolves learning process local sight learning rule ij gamma ij ij gamma ij gamma gamma ij ij gamma ij gamma ij gamma verbalized adaptation rule works follows time partial derivative corresponding weight ij changes sign indicates update big algorithm jumped local minimum update value ij decreased factor gamma derivative retains sign update value slightly increased order accelerate convergence shallow regions 
update value weight adapted weight update follows simple rule derivative positive increasing error weight decreased update value derivative negative update value added ij gamma ij ij ij ij ij ij ij exception partial derivative changes sign previous step large minimum missed previous weight update reverted ij gamma gamma ij ij gamma ij due backtracking weight step derivative supposed change sign step 
order avoid double punishment adaptation update value succeeding step 
practice done setting ij gamma ij adaptation rule 
update values weights changed time pattern set network learning epoch 
algorithm pseudo code fragment shows kernel rprop adaptation learning process 
minimum maximum operator supposed deliver minimum maximum numbers sign operator returns argument gamma argument negative 
weights ij gamma ij ij minimum ij gamma max ij gamma sign ij ij ij ij ij ij gamma ij ij maximum ij gamma gamma min ij ij gamma ij gamma ij ij gamma ij ij gamma sign ij ij ij ij ij parameters update values ij set initial value directly determines size weight step preferably chosen reasonably proportion size initial weights 
choice may 
results section show choice parameter critical 
larger smaller values fast convergence reached 
exception spiral learning task range update values restricted upper limit max lower limit min gamma avoid overflow underflow problems floating point variables 
experiments observed setting maximum update value considerably smaller value max reach behaviour decrease error 
choice decrease factor gamma increase factor lead considerations jump minimum occured previous update value large 
known gradient information minimum missed average guess halve update value gamma 
increase factor large allow fast growth update value shallow regions side learning process considerably disturbed large increase factor leads persistent changes direction 
experiments choice gave results examined problem 
slight variations value improve deteriorate convergence time 
order get parameter choice simple decided constantly fix increase decrease parameters gamma 
main advantages rprop lies fact problems choice parameters needed obtain optimal nearly optimal convergence times 
discussion main reason success new algorithm roots concept direct adaptation size weight update 
contrast algorithms sign partial derivative perform learning adaptation 
leads transparent powerful adaptation process straight forward efficiently computed respect time storage consumption 
discussed aspect common gradient descent size derivative decreases exponentially distance weight output layer due limiting influence slope sigmoid activation function 
consequently weights far away output layer modified learn slower 
rprop size weight step sequence signs magnitude derivative 
reason learning spread equally entire network weights near input layer equal chance grow learn weights near output layer 
iii 
results testing methodology study implemented learning procedures ordinary gradient descent backpropagation bp quickprop qp rprop 
allow fair comparison learning procedures wide variety parameter values tested algorithm 
learning time reported average number epochs required different runs respective standard deviation oe 
algorithm parameter setting gave best result 
binary tasks described learning complete binary criterion reached 
epoch defined period pattern training set 
activation unit output layer smaller target value bigger target value 
having large pattern sets possible practice test different parameter settings solution 
simplicity parameter choice important criterion learning procedure considered 
order get rough measure simplicity find optimal parameter set define region wr parameter ffl respectively rprop follows value ffl respectively lies wr task shall learned average times learning time algorithm achieved optimal parameter setting 
task learned minimum epochs values region wr convergence reached epochs 
noted clearly rough measure robustness algorithm reasons firstly algorithm converges fast times minimum epoch number smaller region slow converging algorithm 
secondly algorithms parameter 
region describes behaviour algorithm parameter dimension lot expense neglected arise searching parameter values 
measure disadvantageous rprop algorithm converges fast parameter adjust results regions worth noting 
experiments ffl denotes initial learning rate bp qp denotes initial update value rprop momentum bp denotes maximal growth factor qp 
encoder problem problem described encoder task discussed largely 
task learn autoassociation binary input output patterns 
network consists neurons input output layer hidden layer neurons 
table shows average learning times learning procedures encoder algo 
ffl epochs oe wr ffl bp qp rprop seen adaptive procedures rprop quickprop better original backpropagation algorithm respect convergence time robustness choice parameter values indicated width wr 
quickprop algorithm outperforms factor times fast original backpropagation 
best result achieved rprop learned task average time epochs 
shown width wr choice initial update value critical 
encoder problem encoder task learn autoassociation input output patterns 
network consists neurons input output layer hidden layer neurons tight encoder 
difficulty task find sensitive encoding decoding bit input output vector hidden neurons 
family tight encoder tasks demonstrates capability learning algorithm find difficult solution weight space 
table shows results learning algorithms respective best parameter setting tight encoder algo 
ffl oe wr ffl bp div 
div 
qp rprop wide variety parameter values tested original backpropagation able find solution task epochs 
job varying initial momentum parameter considerably acceptable learning times achieved 
despite adaptivity experiments needed find optimal parameter set 
quickprop converges fast number trials needed find value parameters 
best result obtained rprop 
side algorithm converges considerably faster side parameter choice easily broad wr 
demonstrates averaged behaviour adaptive algorithms encoder task respect variation initial learning parameter 
initial learning parameter average learning time qp rprop behaviour average learning time encoder task varying parameter ffl rsp 
shows dependency adaptive learning algorithms estimate initial parameter values 
men morris show performance learning procedures realistic problems characterized bigger networks larger pattern sets network trained play endgame men morris 
entire network built identical networks linked comparator neuron 
alternative moves respective partial network network decide move better 
partial network input layer units hidden layers neurons respectively single output unit 
pattern set consists patterns encoding alternative moves desired output comparator neuron 
results men morris problem listed see fig 
men morris algo 
ffl epochs oe wr ffl bp qp rprop trials find parameter values able learn task approximately time original backpropagation algorithm 
quickprop took average epochs learn task choice parameters easier compared 
epochs men morris bp qp rprop decrease error learning time men morris task improvement achieved rprop took epochs learn 
choice initial update value fairly 
spirals problem difficulty spirals problem demonstrated attempts solve problem backpropagation elaborated modifications 
pattern set consists patterns describing points distinct spirals plane 
network built input units hidden layers units output unit symmetric activation functions 
unit connected unit earlier layers short cut connections 
results reported far average epochs 
different runs backpropagation increasing rate average epochs nonlinear cross entropy error function 
quickprop average epochs different runs reported noted non standard arctan non standard weight decay needed obtain result 
order get statistically relevant results tested rprop different runs 
run considered successful solution epochs 
weights initialized randomly gamma 
maximal update value max chosen considerably small max avoid early occurence stuck units 
parameters gamma set standard values 
result listed spirals algorithm ep 
min 
average rprop comparison reported results rprop converges times faster backpropagation times faster quickprop backpropagation quickprop needed non standard extension converge 
fact pure rprop algorithm modifications applied demonstrates advantage new algorithm respect simplicity 
summary best results learning algorithms listed overview 
figures show average number epochs learning tasks described plus results additional recognition task 
second row shows results rprop algorithm standard parameter choice max 
seen annoying parameter tuning results achieved 
average number required epochs problem men morris rec 
bp best best qp best rprop std rprop best iv 
proposed new learning algorithm rprop easy implement easy compute local learning scheme modifies update values weight behaviour sequence signs partial derivatives dimension weight space 
number learning steps significantly reduced comparison original gradient descent procedure adaptive procedures expense computation rprop adaptation process held considerably small 
important feature especially relevant practical application robustness new algorithm choice initial parameter 
rprop currently tested learning tasks example pattern sets containing continuous target values 
results obtained far promising confirm quality new algorithm respect convergence time robustness 
rumelhart mcclelland 
parallel distributed processing 

joost werner 
optimization backpropagation algorithm training multilayer perceptrons 
technical report university koblenz institute physics 
jacobs 
increased rates convergence learning rate adaptation 
neural networks 

fast adaptive backpropagation scaling properties 
neural networks 
fahlman 
empirical study learning speed back propagation networks 
technical report 
braun 
learning strategies solving problem planning backpropagation 
proceedings neuro 
lang witbrock 
learning tell spirals apart 
proceedings connectionist models summer school 
morgan kaufmann 
