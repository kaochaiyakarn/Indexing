text chunking generalization winnow tong zhang ibm watson research center yorktown heights ny watson ibm com fred damerau ibm watson research center yorktown heights ny damerau watson ibm com david johnson ibm watson research center yorktown heights ny ibm com august describes text chunking system generalization winnow algorithm 
propose general statistical model text chunking convert classi cation problem 
argue winnow family algorithms particularly suitable solving classi cation problems arising nlp applications due robustness irrelevant features 
theory winnow may converge non separable data 
remedy problem employ generalization original winnow method 
additional advantage new algorithm provides reliable con dence estimates classi cation predictions 
property required statistical modeling approach 
show system achieves state art performance text chunking computational cost previous systems 
eld natural language processing important research area extraction formatting information unstructured text 
think goal information extraction terms lling templates codifying extracted information 
eld text information extraction long history arti cial intelligence computational linguistics areas 
accurate systems involve language processing modules substantial progress applying supervised machine learning techniques number processes necessary extracting information text 
task naturally decomposes sequence processing steps typically including tokenization sentence segmentation partof speech assignment named entity identi cation phrasal parsing sentential parsing semantic interpretation discourse interpretation template lling merging 
typically information extraction systems partially parse text chunk text typed phrases extent required instantiate attributes template 
key idea assumption wide range constrained applications prede ned attributes templates semantic types attribute values permit system reasonably accurate determination roles various typed chunks identi ed linguistic processors named entity identi ers noun phrase verbal group attempting complete sentence parse 
systems typically organized cascade linguistic processors 
application machine learning techniques information extraction motivated fact hand built systems time consuming develop require special expertise linguistics arti cial intelligence computational linguistics 
coupled fact information required build accurate system motivates research trainable information extraction systems 
important component information extraction natural language processing system development accurate text cascaded carry partial parsing required template lling 
order facilitate measurable progress area partial parsing natural language processing community de ned set common text chunking tasks conll described detail section 
years machine learning techniques applied nlp problems 
method quite successful applications snow architecture 
architecture winnow algorithm theory suitable problems irrelevant attributes :10.1.1.130.9013
natural language processing encounters high dimensional feature space features irrelevant 
robustness winnow high dimensional feature space considered important reason suitable nlp tasks 
convergence winnow algorithm guaranteed linearly separable data 
practical nlp applications data may linearly separable 
consequently direct application winnow lead numerical instability 
second problem original winnow method produce reliable estimate con dence prediction 
information useful statistical modeling 
fact kind con dence estimate required approach text chunking 
due mentioned problems modi cation winnow handle linearly non separable case produce reliable probability estimate 
version new method originally proposed earlier version :10.1.1.119.6259
basic idea modify original winnow algorithm solves regularized optimization problem 
resulting method converges linearly separable case linearly nonseparable case 
numerical stability implies new method suitable practical nlp problems may linearly separable 
apply regularized winnow text chunking tasks :10.1.1.11.8199
order rigorously compare system conll shared task dataset publicly available lcg www uia ac conll chunking 
advantage dataset large number state art statistical natural language processing methods applied data 
readily compare results reported results 
show state art performance achieved newly proposed regularized winnow method 
furthermore achieve result computation earlier systems comparable performance 
organized follows 
section describes text chunking problem conll shared task 
section general statistical model text chunking sequential prediction problem 
convert classi cation problem 
section describe winnow algorithm regularized winnow method 
section gives detailed description system employs regularized winnow algorithm text chunking 
section contains experimental results system conll shared task 
nal remarks section 
text chunking conll chunking text chunking problem divide text syntactically related non overlapping groups words chunks :10.1.1.11.8199:10.1.1.11.8199
example sentence interests real estate said position newly created divided follows np np vp np interests pp np real estate vp said np position vp newly created 
example np denotes non phrase vp denotes verb phrase pp denotes prepositional phrase 
text chunking important applications natural language processing 
addition shallow parsing information extraction tasks formulated grouping unstructured text syntactically semantically related chunks 
means method text chunking potentially useful similar nlp tasks 
past decade signi cant interest machine learning approaches text chunking problem 
facilitate comparison di erent methods conll shared task introduced 
attempt set standard dataset researchers compare di erent statistical chunking methods 
data extracted sections penn treebank 
training set consists wsj sections penn treebank tokens test set consists wsj sections tokens 
additionally part speech pos tag assigned token standard pos tagger trained penn treebank 
pos tags features machine learning chunking algorithm 
see section detail 
example previous example sentence associated pos tags parenthesis token nnp wdt vbz interests nns real jj estate nn said vbd dt position nn vbz newly rb created vbn 
conll data contains eleven di erent chunk types 
frequent types np vp pp remaining chunks accounts occurrences 
eleven chunk types adjp advp conjp intj lst np pp prt sbar vp ucp 
represented types tags associated word punctuation sentence rst word chunk type non initial word chunk type word outside chunk 
standard software program available lcg www uia ac conll chunking provided compute performance algorithm 
chunk gures merit computed precision percentage detected phrases correct recall percentage phrases data metric harmonic mean precision ucp chunk type appear test set 
counted evaluation program chunker evaluated produce ucp chunk type 
recall 
precision recall metric chunks computed 
metric gives single number compare di erent algorithms 
text chunking sequential prediction problem view text chunking sequential prediction problem predict chunk tag associated token sequence tokens 
case token word punctuation text 
denote fw sequence tokenized text 
goal nd chunk tag associated assume known probability model interested conditional probability sequence chunk tag ft fw simplify problem consider decomposition conditional probability ft fw jx ft fw called feature vector gives sucient statistics conditional probability ft fw stage detailed representation feature vector important 
practice may consider markov model determined text neighborhood may contains additional information part speech 
choices feature representation text chunking section 
probability model form goal estimate chunk tag sequence minimize error rate 
method practical applications nd sequence highest probability 
corresponds maximization log probability ln jx minimizing error 
principle expected number correct predictions sequence chunk estimates ft jx ft fw ft denotes sum sequences ft expression estimator minimizes expected error rate arg max ft jx ft fw ft denotes sum sequences ft th token left 
right hand side expression marginal conditional density 
clearly practical compute summation right hand size precisely 
necessary approximation 
employ idea related mean eld approximation method widely statistical physics graphical model inference replace ft fw fw 
right hand side jx fw 
summing obtain approximate solution arg max ft jx short hand notation ft fw 
right hand side approximation expected number correct predictions 
details concerning apply approximation text chunking section 
observed remaining task estimate conditional probability jx possible chunk tag regarded binary classi cation problem possible chunk tag feature vector probability associated chunk tag see section large number features text chunking problem 
consequently need classi cation algorithm robust large feature size 
regard winnow family algorithm suitable 
furthermore require algorithm provide probability information robust noise data 
necessary modify winnow algorithm describe 
winnow generalized winnow binary classi cation review winnow algorithm derive modi cation handle linearly non separable data provide con dence information 
consider binary classi cation problem determine label associated input vector useful method solving problem linear discriminant functions consist linear combinations components input vector 
speci cally seek weight vector threshold label label 
simplicity shall assume 
restriction cause problems practice append constant feature input data sets ect 
set labeled data number approaches nding linear discriminant functions advanced years 
especially interested winnow multiplicative update algorithm :10.1.1.130.9013
algorithm updates weight vector going training data repeatedly 
mistake driven sense weight vector updated algorithm able correctly classify example 
winnow algorithm positive weight employs multiplicative update linear discriminant function misclassi es input training vector true label update component weight vector exp parameter called learning rate 
initial weight vector taken prior typically chosen uniform 
variants winnow algorithm 
called balanced winnow equivalent embedding input space higher dimensional space 
modi cation allows positive weight winnow algorithm augmented input ect positive negative weights original input problem winnow online update algorithm may converge data linearly separable 
may partially remedy problem decreasing learning rate parameter updates 
ad hoc unclear best way 
practice quite dicult implement idea properly 
order obtain systematic solution problem shall rst examine derivation winnow algorithm motivates general solution 
consider loss function max called hinge loss 
general line learning framework data point consider online update rule weight seeing th example solution min ln ew max setting gradient formula zero obtain ln equation denotes gradient rigorously subgradient max takes value value value 
approximately solve replaced leads exp :10.1.1.11.8199:10.1.1.11.8199
practice simply set leads winnow update formula 
derivation solve non convergence problem original winnow method data linearly separable provide valuable insights lead systematic solution problem 
basic idea original winnow algorithm converted numerical optimization problem handle linearly non separable data :10.1.1.119.6259
resulting formulation closely related 
looking example time online formulation incorporate examples time 
formulation replaced hinge loss function reason choosing loss function clear 
speci cally seek linear weight solves arg min ln parameter called regularization parameter :10.1.1.11.8199
optimal solution optimization problem derived solution dual optimization problem see appendix derivation max exp th component exp winnow update rule derived dual regularized winnow formulation :10.1.1.11.8199
data point update approximately maximize dual objective functional gradient ascent max min exp 
dual variable initialized zero corresponds 
update repeatedly going data sequentially similar online algorithm original winnow method 
memorize revisit point iterations memorized plays role update formula 
speci choice loss function analyzed appendix show approximately minimizes expected loss obtain weight close conditional probability jx 
denote truncation :10.1.1.11.8199
note general formulation outlined section requires classi er provide conditional probability estimate 
results appendix imply regularized winnow suitable statistical approach text chunking problem 
regularized winnow attractive generalization properties 
shown original winnow method robust irrelevant features number mistakes obtain classi er separable case depends logarithmically dimensionality feature space :10.1.1.130.9013
generalization bounds regularized winnow similar mistake bound original winnow sense logarithmic dependent dimensionality :10.1.1.119.6259
results imply new method properly handle non separable data shares similar theoretical advantages winnow robust irrelevant features 
theoretical insight implies algorithm suitable nlp tasks large feature spaces 
system description encoding basic features advantage regularized winnow robustness irrelevant features 
include features possible algorithm nd relevant ones 
strategy ensures features important 
features requires memory slows algorithm 
practice necessary limit number features 
tok tok tok tok tok string tokenized text token word punctuation 
want predict chunk type current token tok word tok pos denote associated pos tag assumed conll shared task 
example consider sentence section nnp wdt vbz interests nns real jj estate nn said vbd dt position nn vbz newly rb created vbn 
assume current token looking tok 
tok pos vbz pos wdt 
list features input regularized winnow choose 
rst order features tok pos 
second order features pos pos pos tok 
addition sequential process predicted chunk tags tok available include extra chunk type features rst order chunk type features 
second order chunk type features pos chunk interactions pos 
data point corresponding current token tok associated features encoded binary vector input winnow 
component corresponds possible feature value feature feature lists 
value component corresponds test value corresponding feature achieves value value zero corresponding feature achieves feature value 
example pos feature list possible pos value pos corresponds component component value pos feature value represented component active value zero 
similarly second order feature feature list pos pos possible value set fpos pos represented component component value pos pos feature value represented component active value zero 
encoding applied rst order second order features possible test feature feature value corresponds unique component clearly representation high order features conjunction features active components active 
principle may consider disjunction features active components active 
features considered 
note representation leads sparse large dimensional vector 
explains include possible second order features consume memory 
list features necessarily best available 
included straight forward features pair wise feature interactions 
try higher order features obtain better results 
winnow robust irrelevant features usually helpful provide algorithm features possible 
main drawback features memory consumption mainly ects training 
time complexity winnow algorithm depend number features average number non zero features data point usually quite small 
way alleviate memory problem hash table 
approach works despite high dimensionality feature space dimensions empty 
earlier version current order save memory hash table limited number token feature values words punctuation removing frequent tokens 
implemented hash table 
token features results reported 
slightly improves accuracy accelerates computation due better memory locality 
consequently timing reported better 
consistency set features 
slight improvement obtained due fact limit number tokens 
enhanced linguistic features interested determining additional features linguistic content lead better performance 
esg english slot grammar system directly comparable phrase structure grammar implicit wsj treebank 
esg dependency grammar phrase head dependent elements marked syntactic role 
esg normally produces multiple parses sentence capability output highest ranked parse rank determined system de ned measure 
number incompatibilities treebank esg tokenization compensated order transfer syntactic role features tokens standard training test sets 
transferred esg part speech codes di erent wsj corpus attempt attach pp np np tags inferred esg dependency structure 
tags prove useful 
odd parser output input machine learning system nd syntactic chunks 
noted esg parser normally produces analyses kind applications chunking information extraction solution normally desired 
secondly esg fast parsing sentences ibm rs minutes clock time 
denote syntactic role tag associated token tok tag takes possible values 
features added system 
rst order features 
second order features self interactions iterations pos tags pos 
encoding scheme basic features described earlier 
possible value additional features represented component data vector input winnow 
component value feature value represented component active value zero 
clear particular encoding scheme quite versatile 
include additional features feature interactions paying attention semantics compatibility di erent features 
dynamic programming input vectors consisting features constructed apply regularized winnow algorithm train linear weight vectors 
winnow algorithm produces positive weights employ balanced version winnow transformed :10.1.1.11.8199
explained earlier constant term set ect threshold 
weight vector obtained 
prediction incoming feature vector 
chunk type regularized winnow compute weight appendix know regarded estimate jx jx 
equation solved dynamic programming 
dynamic programming sequential prediction problems new 
speci advantage dynamic programming approach constraints required valid prediction sequence handled principled way example see 
case text chunking produce sequence chunk tags consistent respect encoding scheme heuristic post processing done 
valid sequence chunk tags satisfy constraint current chunk tag previous chunk tag 
denote set valid chunk sequences sequence satis es chunk type constraint 
tok tok sequence tokenized text nd associated chunk types 
xm associated feature vectors text sequence 
sequence potential chunk types valid ft system nd valid sequence chunk types maximizes arg max ft mg min max optimization problem max ft mg don explicitly keep vector component inversely proportional correspondent component solved dynamic programming 
build table chunk types token tok xed chunk type de ne value max ft easy verify recursion max ft assume initial condition recursion iterate compute potential chunk type observe depends previous chunk types 
implementation chunk types create current feature vector determined follows 
arg max arg max ft computation determine best sequence follows 
assign chunk type largest value 
chunk type determined recursion arg max ft 
experimental results section study issues empirically uence parameters regularized winnow algorithm fold cross validation training set testset results default parameters comparison methods ect di erent feature space sizes 
mention ucp chunk type appear standard test set systems learn chunk 
approach truly regarded cheating may consider going discarding chunks type appear test set 
avoid problem opinion best learn test chunk types knowledge ucp chunk appear test set 
results reported obtained learning eleven chunk types leading class classi cation problem 
timing better learn ucp chunk type 
uence parameters regularized winnow choose default parameters uniform prior 
learning rate ran regularized winnow update formula repeatedly times training data 
experience default parameters suitable general problems including natural language processing problems applied regularized winnow algorithm 
experiments section performed basic features 
goal section study ect varying parameters 
experiments done fold cross validation standard conll training set time continuous block training set validation data rest training set estimate chunker 
rst consider varying regularization parameter 
table contains precision recall measures di erent values fold crossvalidation 
seen text chunking obtain results long choose relative small 
closely related fact binary classi cation problem nearly separable 
regularization parameter plays role balancing bias variance trade typically observed statistical estimation problem 
small bias small results appendix imply directly estimate conditional class probability function 
small variance relatively large algorithm stable 
natural choose regularization parameter gives small bias stability property 
text chunking problem table reasonable pick default value 
precision recall table ect regularization parameter table investigates ect di erent values prior table imply algorithm sensitive choice prior choice important reason contributes phenomenon xed number iterations 
numerical point view number iterations small ect similar small learning rate 
explain small lead slower convergence large lead instability 
undesirable results corresponding small priors table partially explained fact algorithm achieved convergence iterations 
results see default value shows balance quick convergence stability 
precision recall table ect prior value table shows ect di erent values learning rate 
general safer smaller learning rate corresponding convergence rate slower 
practically desirable learning rate small leads stable algorithm 
bad performance shows algorithm numerically unstable 
performance comparison learning rates default iterations indicates algorithm converge fast sucient dicult problems nearly separable usually observe phenomenon increases classi cation accuracy rst increases optimal point decreases 
number iterations converges correct solution 
learning rate issue thoroughly investigated 
fact results suggest default choice value general problems 
precision recall table ect learning rate table study performance regularized winnow various numbers iterations 
results indicate algorithm approximately achieved convergence iterations performance stabilized 
precision recall table ect iteration number training testset results comparison methods experimental results reported section obtained default values 
iterations may improve results little see table performance stabilized iterations 
results reported section obtained training standard conll training set test standard test set 
table gives results obtained basic features 
representation gives total number non zero hash table entries binary features counts 
number non zero features datum determines time complexity system 
training time mhz pentium machine running linux twelve minutes corresponds seconds category 
time dynamic programming produce chunk predictions excluding tokenization seconds 
non zero linear weight components chunk type corresponds sparsity 
features irrelevant 
previous systems achieving similar performance signi cantly complex 
example previously best performance reported literature achieved combination kernel support vector machines value 
kernel support vector machine computationally signi cantly expensive corresponding winnow classi er order magnitude classi ers 
implies system orders magnitudes expensive 
point veri ed training time day mhz linux machine 
mention result improved voting support vector machine outputs di erent chunk tag sequence representations 
fair comparison system reasonable believe achieve appreciable improvement similar voting approach 
excluding results reported early version current previously second best system combination di erent wpdv models value 
system complex regularized winnow approach propose best single classi er performance 
third best performance achieved combinations memory models value 
rest eleven reported systems employed variety statistical techniques maximum entropy hidden markov models transformation rule learners 
interested readers referred summary contains systems tested 
comparison implies regularized winnow approach achieves state art performance signi cant computation previous systems 
success method relies regularized winnow ability tolerate irrelevant features fast online style dual update algorithm solves associated optimization problem 
allows large feature space algorithm pick relevant ones 
addition algorithm simple 
approaches little ad hoc engineering tuning involved system 
simplicity allows researchers reproduce results easily 
table report results system basic features enhanced esg syntactic roles showing linguistic features enhance performance system 
addition regularized winnow able pick relevant features automatically easily integrate di erent features system systematic way concerning semantics features 
resulting value better previous system 
complexity system quite reasonable 
total number features nonzero features data point 
training time minutes number non zero weight components chunk type precision recall adjp advp conjp intj lst np pp prt sbar vp table chunk prediction results basic features interesting compare regularized winnow results original winnow method 
report results basic linguistic features table 
experiment setup regularized winnow approach 
start precision recall adjp advp conjp intj lst np pp prt sbar vp table chunk prediction results enhanced features uniform prior learning rate 
winnow update performed times repeatedly data 
training time approximately regularized winnow method 
clearly regularized winnow method enhanced performance original winnow method 
improvement consistent chunk types 
seen improvement appreciable dramatic case 
surprising data close linearly separable 
testset multi class classi cation accuracy 
average binary classi cation accuracy training set note train binary classi er chunk type close 
means training data close linearly separable 
bene regularized winnow signi cant noisy data improvement case dramatic 
mention noisy problems tested improvement regularized winnow method original winnow method signi cant 
precision recall adjp advp conjp intj lst np pp prt sbar ucp vp table chunk prediction results original winnow basic features impact features experiment section illustrates necessity features 
parameter values section 
rst order basic features 
results obtained regularized winnow full training set tested standard testset 
number non zero features datum 
total number binary features non zero linear weight components chunk type 
training time signi cantly faster minutes performance worse 
shows large number features improves prediction accuracy 
precision recall adjp advp conjp intj lst np pp prt sbar vp table chunk prediction results rst order basic features mention rst order basic features original winnow algorithm achieves precision recall 
clear di erence original winnow modi cation signi cant 
data separable case result explained fact original winnow handle noisy data 
described text chunking system generalization winnow call regularized winnow 
advantage new method compared original winnow ability handle non separable data ability provide reliable con dence estimates 
con dence estimates required statistical sequential modeling approach text chunking problem 
winnow family algorithms robust irrelevant features construct high dimensional feature space algorithm pick important ones 
shown state art performance achieved approach 
furthermore method propose computationally ecient systems reported literature achieved performance close 
success regularized winnow text chunking suggests method applicable nlp problems necessary large feature spaces achieve performance 
dual formulation regularized winnow note objective value goes 
know nite 
strictly convex objective functions unique solutions 
purpose section show theorem justi es validity dual formulation section 
general treatment type duality 
theorem solution gives solution 
proof 
di erentiating respect obtain rst order condition satis ed optimal solution ln exp cf max min need show solution 
de nition :10.1.1.11.8199
need show satis es kkt condition 
obtain 
obtain exp easy verify equations set kkt conditions 
follows solves 
regularized winnow conditional probability estimator consider binary classi cation problem section 
assume class conditional probability jx 
show xy small small denotes expectation truncation :10.1.1.11.8199
implies regarded approximation conditional class probability try minimize expected value xy 
result justi es choice loss function lemma min max xy jjw proof 
number :10.1.1.11.8199:10.1.1.11.8199
consider situations easy verify qf easy verify qf easy verify qf summarize inequality qf jjp lemma follows equality observation xy theorem assume exists :10.1.1.11.8199:10.1.1.11.8199
assume nd approximately minimizes xy inf xy proof 
note schwartz inequality obtain jj lemma xy lemma assumptions theorem obtain xy xy implies approximately minimizing expected value xy effectively try obtain weight 
regarded approximation conditional class probability 
particular theorem implies approximately minimizing expected value xy obtain arbitrarily close 
abney :10.1.1.11.8199
parsing chunks 
berwick abney editors parsing computation psycholinguistics pages 
kluwer dordrecht 
eric brill 
advances rule part speech tagging 
proc 
aaai pages 
dagan karov roth 
mistake driven learning text categorization 
proceedings second conference empirical methods nlp 
gentile warmuth 
linear hinge loss average margin 
proc 
nips 
grove roth 
linear concepts hidden variables 
machine learning 
khardon roth valiant 
relational learning nlp linear threshold elements 
proceedings ijcai 
kivinen warmuth 
additive versus exponentiated gradient updates linear prediction 
journal information computation 
taku kudo yuji matsumoto 
chunking support vector machines 
naacl 
taku yuji matsumoto 
support vector learning chunk identi cation 
proc 
conll lll pages 
littlestone :10.1.1.130.9013
learning quickly irrelevant attributes abound new linear threshold algorithm 
machine learning 
michael mccord 
slot grammar system simple construction practical natural language grammars 
natural language logic pages 
ion muslea 
extraction patterns information extraction tasks survey 
aaai workshop machine learning information extraction pages 
punyakanok dan roth 
classi ers sequential inference 
todd leen thomas dietterich volker tresp editors advances neural information processing systems pages 
mit press 
erik tjong kim sang sabine buchholz 
conll shared tasks chunking 
proc 
conll lll pages 
hans van halteren 
chunking wpdv models 
proc 
conll lll pages 
tong zhang :10.1.1.119.6259
regularized winnow methods 
advances neural information processing systems pages 
tong zhang 
dual formulation regularized linear systems 
machine learning 
tong zhang fred damerau david johnson 
text chunking regularized winnow 
acl pages 

