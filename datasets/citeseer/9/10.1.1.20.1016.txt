diss 
eth entropy measures unconditional security cryptography dissertation submitted swiss federal institute technology urich degree doctor technical sciences christian cachin dipl 
informatik ing 
eth born february citizen vd zurich accepted recommendation prof dr maurer referee prof dr massey referee acknowledgments prof maurer support advice encouragement collaboration led thesis 
am grateful prof jim massey interest careful reading text acceptance referee 
pleasure jan camenisch discussions cryptography computer problems lots things 
go members cryptography information security group daniel bleichenbacher markus stadler stefan wolf martin hirt ronald cramer 
department members theory floor sharing time am grateful creating open stimulating relaxed atmosphere 
am indebted parents supporting education possible 
irene 
important properties cryptographic system proof security 
information theoretic methods proving security unconditionally secure cryptosystems 
security systems depend unproven intractability assumptions 
survey entropy measures applications cryptography 
new information measure smooth entropy introduced quantify number uniform random bits extracted source probabilistic algorithms 
smooth entropy unifies previous privacy amplification cryptography entropy smoothing theoretical computer science 
enables systematic investigation spoiling knowledge proof technique obtain lower bounds smooth entropy 
enyi entropy order random variable lower bound smooth entropy assumption enyi entropy order equivalent shannon entropy weak guarantee non trivial amount smooth entropy 
gap enyi entropy order closed proving enyi entropy order ff lower bound smooth entropy small parameter depending ff alphabet size failure probability 
operation unconditionally secure cryptosystems divided phases advantage distillation information reconciliation privacy amplification 
relation privacy amplification information reconciliation investigated particular effect side information obtained adversary initial reconciliation step size secret key distilled safely subsequent privacy amplification 
shown bit side information reduces size key generated iv bit negligible probability 
private key cryptosystem protocol key agreement public discussion proposed unconditionally secure sole assumption adversary memory capacity limited 
systems random bit string length slightly larger adversary memory capacity received parties 
eine der systems ist ein fur 
dieser arbeit werden methoden um die von zu die nicht auf 
eine uber und ihre anwendungen der wird 
es wird ein die smooth das die uniform bits die aus einer mit von algorithmen werden 
das der smooth arbeiten zu privacy amplification der und entropy smoothing der informatik 
es eine der zur von fur die smooth 
die enyi der ordnung einer ist eine fur ihre smooth 
eine uber die enyi der ordnung die zur shannon ist nicht aus um eine grosse der smooth zu 
die zwischen enyi der ordnung und wird dem wird dass der ordnung ff zwischen und eine fur die smooth ist bis auf einen der von ff der grosse des und der 
viele systeme mit den advantage distillation information reconciliation und privacy amplification 
die zwischen information reconciliation und privacy amplification wird untersucht der von die der vi information reconciliation auf die grosse des der mit privacy amplification werden kann 
es wird dass bit die um ein bit mit 
werden ein und ein zur eines durch die sind unter der dass ein hat ohne 
diese systeme auf einem bitstring nur als der des der von einem sender alle wird 
contents preliminaries discrete probability theory 
inequalities 
jensen inequality 
moment markov inequalities 
chernoff hoeffding bounds 
entropy information theory 
enyi entropy 
asymptotic equipartition property 
universal hashing privacy amplification 
information measures cryptography 
scenarios information measures 
perfect secrecy shannon entropy 
authentication relative entropy 
privacy amplification enyi entropy 
guessing keys min entropy guessing entropy 
hash functions collision probability 
probabilistic bounds variational distance 
relations information measures 
shannon entropy uniform distributions 
viii contents smooth entropy 
general formulation 
previous related concepts 
privacy amplification cryptography 
entropy smoothing pseudorandom generation relation entropy 
relation intrinsic randomness 
application learning theory 
extractors weak random sources derandomization 
spoiling knowledge 

spoiling knowledge increasing smooth entropy probability 
spoiling knowledge increasing smooth entropy probabilistic bounds 
bounds spoiling knowledge 
bound enyi entropy order ff tighter bound profile distribution 
smoothing unknown distribution 

unconditional security cryptography 
unconditionally secure key agreement protocols 
linking reconciliation privacy amplification 

effect side information enyi entropy uniform distributions 
independent repetition random experiment 
memory bounded adversaries 

pairwise independence entropy smoothing extracting secret key randomly selected subset 
private key system 
key agreement public discussion 
contents ix discussion 
concluding remarks bibliography index chapter secure information transmission storage paramount requirement emerging information economy 
techniques realizing secure information handling provided cryptography defined study communication adversarial environment riv 
classical goals cryptography secrecy authentication protect information unauthorized disclosure unauthorized modification 
years research cryptography addressed broad range advanced questions ranging authorization user access computer systems secure electronic voting schemes realization untraceable electronic cash 
surveys contemporary cryptography available number works sti gol sim riv 
roots cryptography traced back invention writing 
roman emperor caesar devised simple encryption scheme bears name today kah 
cryptography science originates seminal shannon laid foundations information theory sha treats cryptography applications sha 
cryptographic systems ciphers designed secret key known communicating partners encrypt decrypt information 
methods called symmetric secret key private key systems contrast methods public key cryptography appeared diffie hellman dh 
revolutionary idea separate keys encryption decryption public key anybody secret key known owner 
asymmetry fundamental realizing basic primitives modern cryptography public key cryptosystems digital signature schemes 
public key cryptosystem allows partners generate secret key exchange information secretly communicating public channels adversary 
participant public key setup public key party encrypt message 
corresponding secret key known recipient needed decrypt message encrypted public key 
public key system provides secret transmission information 
dual digital signature scheme provides authenticity uses public key secret key user 
user digitally sign message secret key protect integrity message 
participant verify signature public key signer 
rsa cryptosystem proposed rivest shamir adleman rsa widely known public key systems today employed public key cryptosystem digital signature scheme 
security currently cryptosystems difficulty underlying computational problem factoring large numbers computing discrete logarithms public key systems 
security proofs systems show ability adversary defeat cryptosystem significant probability contradicts assumed difficulty problem 
notion security conditional unproven assumption 
hardness problems moment dangerous base security global information economy small number mathematical problems 
contrast stronger notion information theoretic unconditional security assumes limits adversary computational power base security intractability assumptions 
shannon information theoretic definition perfect secrecy led immediately famous pessimistic theorem sha states roughly shared secret key perfectly secure cryptosystem long plaintext encrypted 
developments show practical provably secure cryptosystems possible small modifications shannon model 
thesis contributes research unconditionally secure cryptographic systems number ways 
chapter basic concepts information theory introduced main tools reasoning unconditional security 
describe privacy amplification building block unconditionally secure cryptosystems introduce enyi entropy plays important role connection entropy smoothing privacy amplification 
chapter contains survey information measures cryptographic applications 
comparison demonstrates information measures standard shannon entropy answer natural quantitative questions various cryptographic scenarios 
collection bounds link information measures discussion role shannon entropy cryptographic applications completes chapter 
chapter introduce notion smooth entropy allows unifying formulation privacy amplification entropy smoothing 
smooth entropy measure number uniform random bits extracted random source probabilistic algorithms 
examine spoiling knowledge proof technique obtain lower bounds smooth entropy give characterization kinds spoiling knowledge lead better lower bounds 
addition establish new connection smooth entropy enyi entropy proving lower bound smooth entropy terms enyi entropy order ff ff 
previously known enyi entropy order random variable lower bound smooth entropy statement shannon entropy enyi entropy order 
chapter focus realization unconditionally secure cryptosystems 
systems usually divided phases advantage distillation information reconciliation privacy amplification 
part chapter investigate effect side information adversary obtains information reconciliation privacy amplification 
show high probability bit side information reduces size key safely distilled bit 
second part chapter propose private key cryptosystem protocol key agreement public discussion unconditionally secure sole assumption adversary memory capacity limited 
system random bit string length slightly larger adversary memory capacity received parties 
chapter preliminaries chapter reviews probability theory information theory needed chapters 
purpose introduce notation give self contained treatments probability theory information theory 
topics books feller fel billingsley bil blahut bla cover thomas ct 
material chapter mau lub en 
section introduce notion privacy amplification cryptography fundamental remaining chapters 
logarithms base 
concatenation symbols denoted ffi concatenation random variables juxtaposition 
cardinality set denoted jsj 
vectors typeset boldface 
discrete probability theory discrete probability space consists finite countably infinite set omega gamma sample space probability measure elements sample space omega called elementary events subsets omega called events 
elementary event viewed possible outcome experiment 
probability distribution probability measure mapping set events real numbers satisfied 
event omega gamma 
omega gamma 
preliminaries 
events ae omega called probability event events probability union event gamma union bound simple consequence events called independent delta conditional probability ajb event event occurs defined ajb positive 
discrete random variable mapping sample space omega alphabet assigns value elementary event omega probability distribution function px 
px omega gamma 
random variables denoted capital letters written stated alphabet random variable denoted corresponding script letter 
sequence xn random variables alphabet denoted multiple random variables defined sample space 
joint distribution random variables defined distribution single vector valued random variable xy alphabet theta distributions determined uniquely pxy discrete probability theory conditional probability distribution random variable event positive probability defined xja conditioning event involves random variable defined sample space conditional probability distribution takes value pxy py positive 
random variables called independent pxy px delta py random variables real values particular importance 
expected value discrete random variable ae variance var gamma gamma notation ex delta state explicitly random experiment expectation taken random experiment underlying random variable collision probability random variable defined px denotes probability takes value twice independent experiments 
collision probability satisfies jx equality left px uniform distribution equality right px preliminaries distances probability distributions quantified related measures distance probability distributions px alphabet defined kpx gamma jp gamma variational distance px py kpx gamma py max fi fi fi px gamma py fi fi fi kpx gamma py variational distance distribution random variable uniform distribution pu interpreted way 
assume kpx gamma pu ffl 
refinement probability space underlying event exists probability gamma ffl gamma pu behaves uniformly distributed random variable probability gamma ffl 
inequalities jensen inequality function called convex convex interval gamma gamma called strictly convex inequality strict 
function called strictly convex concave gammag strictly convex 
jensen inequality states convex function random variable strictly convex equality implies probability 
descriptive terminology massey mas 
inequalities moment markov inequalities useful inequalities observation real valued random variable function interval real numbers constant characteristic function delta delta application th moment inequality integer jxj jxj follows jxj gammat noting delta 
special case known markov inequality positive valued random variable special case known inequality real valued random variable jx gamma var inequality yields tight bounds cases real valued random variable gammar follows gammar gamma noting delta 
chernoff hoeffding bounds sharp bounds attributed chernoff hoeffding exist sum independent identically distributed random variables 
independent sampling error probability approximation decreases exponentially number sample points 
preliminaries xn sequence real valued random variables distribution px expected value range interval 
hfi fi fi gamma fi fi fi gamma gammaa deltan similar bounds hold sums random variables limited independence sss xn wise independent random variables distribution px alphabet interval expectation fi fi fi gamma fi fi fi kn entropy information theory shannon entropy sha random variable probability distribution px alphabet defined gamma px log px convention log justified fact lim log 
conditional entropy conditioned random variable xjy xjy xjy denotes entropy computed conditional probability distribution definition entropy stated expected value ex gamma log px entropy random variable measure average uncertainty 
minimum number bits required average describe value random variable similarly xjy average number bits required describe known 
communication theory information theory originates entropy information theory entropy source gives ultimately achievable error free compression terms average codeword length source symbol fundamental result information theory 
proposition 
important immediate properties 
entropy positive equality px 
entropy bounded log jx equality uniformly distributed 
conditioning side information reduces entropy xjy equality independent 

chain rule xy jx 
probability distribution binary random variable completely characterized parameter px 
binary entropy function defined entropy gammap log gamma gamma log gamma relative entropy discrimination probability distributions px alphabet defined px px log px py conventions log log 
conditional relative entropy px conditioned random variable defined kp jz pz log jz similar entropy relative entropy non negative 
zero px py think relative entropy px kp increase information experiment probability distribution corresponding knowledge experiment changed px total uncertainty distribution assumed relative entropy px kp plus entropy preliminaries useful relation connects entropy relative entropy size alphabet pu uniform distribution px log jx words knows random variable exists alphabet uncertainty log jx learns distribution px information increases px uncertainty remains 
relative entropy forms basis definition mutual information random variables px theta denote product distribution px px theta px delta mutual information relative entropy joint distribution product distribution pxy kpx theta py equivalently mutual information defined reduction uncertainty learned gamma xjy follows symmetry 
similarly conditional mutual information jz random variable jz pxy jz theta jz information theory communication channels modeled systems output depends probabilistically input characterized transition probability matrix 
capacity communication channel input output defined max px second fundamental result information theory shows capacity maximum rate information sent channel recovered output vanishing probability error 
enyi entropy enyi entropy enyi entropy best introduced concepts generalized probability distributions generalized random variables extensions corresponding ordinary notions random experiments observed 
presentation follows enyi original en en 
consider discrete probability space omega omega omega omega 
omega define generalized discrete probability space differs probability space fact thatp omega possible 
random variable defined generalized discrete probability space called generalized discrete random variable 
omega complete ordinary random variable omega incomplete random variable 
interpreted quantity resulting random experiment observable observed omega 
probability distribution px generalized random variable called generalized probability distribution 
weight defined px follows equality ordinary random variable 
axiomatic characterizations information measures random experiments studied intensively mathematical community ad 
information measure sense associates random variable number corresponds information content 
enyi showed postulates information measure uniquely define shannon entropy en 
postulate reordering correspondence values probabilities px change 
postulate denotes singleton generalized random variable fxg px continuous function interval 
postulate binary random variable pb pb 
preliminaries postulate generalized random variables define theta generalized random variable alphabet theta distribution px thetay px delta theta postulate generalized random variables define generalized random variable alphabet px px px py proposition 
information measure generalized random variable satisfies postulates 
uniquely defined gamma px log px px postulate imposes arithmetic mean value information measure 
general form mean value numbers positive weights wn sum gamma monotonic continuous function arithmetic mean value postulate replaced generalized mean value gamma shown admissible functions context linear functions ax lead shannon entropy proposition exponential functions gammaff lead enyi entropy en en proposition 
enyi erroneously states ff gamma en easy verify definition enyi entropy requires gammaff enyi entropy postulate generalized random variables define generalized random variable alphabet px px px ff ff ff gammaff gamma ff ff ff ff ff enyi entropy order ff generalized random variable alphabet defined ff gamma ff log px ff px proposition en 
information measure generalized random variable satisfies postulates postulate 
uniquely defined equal enyi entropy ff rest section properties enyi entropy described complete random variables 
easy see lim ff ff 
explains interpreted enyi entropy order written 
similarly min entropy defined gamma log max px results lim ff ff 
interval admissible ff enyi entropy order defined logarithm alphabet size log jx convention 
important property enyi entropy shown proposition 
proposition 
enyi entropy ff ff positive decreasing function ff ff fi ff fi equality uniformly distributed ff uniformly distributed subset ff 
preliminaries proof 
ff fi ff fi ff gamma ff log px ff gamma log px ff gamma ff gamma gamma log px ff gamma fi gamma ff gamma fi gamma gamma log px ff gamma fi gamma ff gamma fi gamma gamma log px fi gamma fi gamma gamma fi log px fi fi observe convex convex 
inequality derivation follows jensen inequality cases fi ff fi gamma ff gamma convex fi gamma fi ff fi gamma ff gamma convex fi gamma fi ff fi gamma ff gamma convex fi gamma 
ff fi jensen inequality applied directly 
conditions equality follow directly jensen inequality 
define conditional enyi entropy ff xjy similar conditional entropy ff xjy ff xjy agreement standard definition conditional enyi entropy 
contrast shannon entropy chain rule property conditioning side information reduces entropy proposition hold definition conditional enyi entropy ff ff xjy ff xy ff ff xjy possible general 
definitions conditional enyi entropy examined csisz ar csi 
asymptotic equipartition property asymptotic equipartition property asymptotic equipartition property aep form law large numbers information theory 
aep states alphabet sequence xn independent identically distributed random variables distribution px divided sets typical set non typical set probability typical set goes 
furthermore typical sequences equally probable probability typical sequence close material section presentation cover thomas ct uses method types closely related method strongly typical sequences 
fix alphabet sequence symbols type empirical probability distribution mapping specifies relative proportion occurrences symbol na number times symbol occurs sequence type sequence interpreted empirical probability distribution 
set types denominator denoted qn particular type qn set sequences length type called type class denoted fx qg proposition summarizes basic properties types 
proposition ct ck 
xn sequence random variables distribution px alphabet qn set types 

number types denominator polynomial particularly jx 
probability sequence depends type px gamman kpx preliminaries 
qn size type class approximately nh precisely jx nh jt nh 
qn probability type class approximately qkp precisely jx qkp px qkp important property types polynomial number types exponential number sequences type 
probability type class depends exponentially relative entropy distance type distribution px type classes far true distribution px exponentially small probability 
type classes close px high probability form typical set 
ffl define strongly typical set ffl sequences length distribution px ffl fi fi fi gamma px fi fi fi ffl jx denotes number times symbol occurs sequence ready state aep strongly typical sequences proof csisz ar korner ck 
proposition aep 
xn sequence random variables distribution px ffl 
typical set probability theta ffl gamma jx delta gamma ln ffl jx 
number elements typical set close nh ffl log ffl jx gamma jx log log js ffl gamma ffl log ffl jx jx log asymptotic equipartition property 
probability typical sequence upper bounded expression close ffl px gamman ffl log ffl jxj proof 
type non typical satisfies kq gamma px ffl jx ln delta ffl jx lemma see page 
bound probability non typical sequences theta ffl qn kq gammap 
ffl jx px 
jx delta gamman ln ffl jxj prove second statement theorem note ffl satisfy kq gamma px ffl jh gamma gammaffl log ffl jx ffl lemma see page 
third property types bound size type class typical sequence jx nh ffl log ffl jxj jt nh gammaffl log ffl jx clearly ffl ffl establishes lower bound 
similarly ffl contained union type classes corresponding typical sequences 
upper bound extended jx types 
result equivalent 
follows non negativity relative entropy typical sequence ffl log ffl jx bound probability typical sequence follows immediately second property types 
preliminaries universal hashing privacy amplification privacy amplification universal hashing key component unconditionally secure cryptographic protocols recurring theme 
section presents idealized cryptographic scenario introduce privacy amplification 
applications motivating discussions provided chapters 
privacy amplification universal hash functions introduced carter wegman cw wc 
universal hash functions privacy amplification bennett brassard robert 
definition 
universal hash function set functions distinct jgj jyj gamma functions delta delta delta 
universal hash function universal term universal hash function usually refers universal hash function 
stronger notion universal hash functions closely related wise independent random variables see section lw 
definition 
strongly universal hash function set functions distinct necessarily distinct exactly jgj jyj functions take assume alice bob share random variable eavesdropper eve knows correlated random variable summarizes knowledge details distribution unknown alice bob know lower bound enyi entropy pw jv particular value eve knowledge example eve received symbols result function parity bits 
alice bob know eve knowledge fact satisfies lower bound enyi entropy order 
authenticated public channel susceptible eavesdropping immune tampering alice bob wish agree function eve knows nearly 
theorem bennett brassard cr maurer shows alice bob choose random universal hash universal hashing privacy amplification function suitable eve information negligible 
theorem privacy amplification theorem 
random variable alphabet probability distribution px enyi entropy random variable corresponding random choice uniform distribution member universal hash function 
jg jg log jyj gamma log jy gammah ln statement random variable quantity jg jg average choices function possible jg differs log jyj non negligible amount occur negligible probability log jyj 
entropy maximal distribution close uniform 
theorem applies conditional probability distributions pw jv discussed 
eve enyi entropy jv known alice bob choose bit string secret key jg gamma gammat ln key virtually secret jg arbitrarily close maximum precisely eve total information decreases exponentially excess compression gamma previous version theorem developed bennett brassard robert restricted deterministic eavesdropping functions delta eve get 
result generalized probabilistic eavesdropping strategies 
pointed theorem generalized enyi entropy conditioned random variable gamma gammah jv ln false general 
chapter information measures cryptography information measures main abstractions modeling cryptographic scenarios information theoretic methods 
chapter presents survey information measures significance cryptographic systems provide unconditional security 
fundamental concepts information theory definitions measures uncertainty outcome random experiment information measured reduction uncertainty 
entropy measure mapping probability distributions real numbers associates number probability distribution 
entropy measures play different roles connection unconditionally secure cryptographic systems 
hand positive results obtained form information theoretic security proofs systems 
hand lower bounds required key sizes scenarios negative results follow entropy arguments 
separate aspects entropy measure formal definition operational characterization 
entropy measure usually defined formally terms probability distribution numerical value computed immediately probability distribution 
justification definition operational information measures cryptography characterization entropy measure application scenario entropy measure gives answer important question arising context 
chapter focus operational characterizations cryptography number entropy measures defined formally terms probability distribution 
reverse process possible 
chapter instance give operational definition entropy measure called smooth entropy quantifies number uniform bits extracted random source probabilistic algorithms 
search bounds numerical value relation formally defined entropy measures 
example operationally defined information measure secrecy capacity key agreement common information public discussion introduced maurer mau mw 
formal definitions information measures discussed chapter summarized 
shannon entropy 
shannon entropy random variable gamma px log px relative entropy 
relative entropy discrimination probability distributions px alphabet px px log px enyi entropy order ff 
ff ff enyi entropy order ff random variable ff gamma ff log px ff min entropy 
min entropy random variable gamma log max px guessing entropy 
random variable values denote elements probability distribution px pn scenarios information measures delta delta delta pn average number guesses needed determine optimal strategy ip call quantity guessing entropy collision probability 
collision probability random variable px variational distance 
variational distance probability distributions px alphabet kpx gamma max fi fi fi px gamma py fi fi fi notable omission list quantum information forms basis quantum cryptography 
notion quantum entropy terms probability distribution 
refer surveys brassard cr bc spi treatments quantum cryptography foundations 
chapter organized follows 
cryptographic applications information measures section main part chapter 
overview bounds relating different information measures subject section section examines converting lower bounds shannon entropy random variable cryptographically relevant information measures 
scenarios information measures section contains selective survey cryptographic scenarios information measures mentioned play important role 
classical dual goals cryptography secrecy authentication covered relation fundamental concepts information theory entropy relative entropy information measures cryptography discrimination 
material section cited 
perfect secrecy shannon entropy notion perfect security perfect secrecy introduced shannon means adversary information secret typically secret plaintext transmitted cryptographic system 
equivalent saying random variable constituting secret random variable modeling adversary knowledge independent 
perfect security applications symmetric cryptosystems secret sharing conference key distribution 
cases lower bounds amount information participant transmit store obtained suggest perfectly secure systems impractical 
applications construction meet lower bound equality cases 
interestingly constructions remarkably similar 
secret key cryptosystems shannon model secret key cryptosystem consists sender alice receiver bob eavesdropper eve open channel alice bob eve sha 
secret key known alice bob 
alice encrypts plaintext encryption rule system resulting sent bob received eve 
bob recover knowledge cipher model called perfect plaintext independent random variables equivalently xjy 
bob able recover uniquely xjy 
follows cryptosystem perfect security xjy zjy xjy zjy shannon famous result entropy secret key large entropy plaintext encrypted 
scenarios information measures time pad prime example perfectly secure cryptosystem 
uses randomly uniformly chosen bit secret key encrypt decrypt bit plaintext simple xor operation 
secret sharing secret sharing important widely studied tool cryptography distributed computation sti 
perfect secret sharing scheme protocol dealer distributes secret set participants specific subsets defined access structure recover secret time non qualified subset obtain information secret 
access structure allows subset participants reconstruct secret set gamma participants secret sharing scheme called threshold scheme 
implemented shamir construction sha polynomial interpolation 
set participants fa dealer assumed access structure family subsets containing sets participants qualified recover secret 
natural require monotone secret share denote random variables describing shares participant group participants respectively 
secret sharing scheme called perfect set qualified participants uniquely determine sjx unqualified set obtain information sjx 
argument similar shannon lower bound perfect security follows share participant access structures proved shares perfect scheme considerably larger secret 
consider set fa dg access structure equal monotone closure fab bc cdg 
definition perfect secret sharing scheme derive lower bound bc implies 
share times length secret longer 
perfect secret sharing scheme example realized information measures cryptography follows 
bit string length denote half second half choose bit strings randomly uniform distribution phi phi phi shares phi denotes xor operation 
construction meets lower bound equality 
recursive application argument similar lower bound shows access structures participants infinitely size shares log times size secret csi 
known general techniques realizing perfect secret sharing schemes bl isn produce exponentially large shares open gap lower upper bounds size share general case 
key distribution dynamic conferences key distribution scheme dynamic conferences method initially dealer distributes private individual pieces information set users blo bsh sti 
qualified conference users contained key structure able compute common key interaction user needs private piece information identities conference members 
key conference perfectly secure coalitions users contained forbidden structure sense set malicious users pool pieces information key conference disjoint 
adopt notation denote users random variables representing pieces information interchangeably capital letters 
terminology key sx conference computed user interaction satisfies sx ja 
addition key sx set disjoint required sx jy sx 
consider important special case key distribution scheme set participants key structure consist subsets certain cardinality forbidden structure secure non interactive conference key distribution scheme gamma defined distribution scheme information pieces consists set cardinality contains sets scenarios information measures cardinality assuming common keys groups users entropy blundo bsh show secure non interactive conference gamma gamma user example consider particular case users gamma 
users share key secure coalition remaining gamma users 
size information user needs store gamma times size common key total information stored users gamma 
known problem establishing secret key pair set users 
general construction meets bound bsh 
denote set users ng gf finite field elements prime dealer chooses symmetric polynomial gf degree uniformly random 
symmetric polynomial satisfies oe oe permutations oe tg tg 
dealer sends user polynomial obtained evaluating delta users set fi want set conference key user evaluates gamma common key users equal 
interaction users allowed dealer finished distributing pieces information efficient schemes possible 
example secure interactive conference key distribution scheme constructed gamma secure non interactive conference bsh 
idea designated member group users chooses key random sends gamma group members encrypted time pad keys obtained gamma secure conference 
protocol secure coalitions users random key encrypted gamma times different keys gamma secure see bc 
information measures cryptography authentication relative entropy message authentication techniques focus authentication theory 
authentication provides assurance receiver message originates specified legitimate sender adversary may intercepted modified inserted message 
unconditionally secure authentication assumes unlimited computational power adversary secret information shared sender receiver wc mas 
section maurer mau shows problem deciding received message authentic seen hypothesis testing problem 
receiver decide hypotheses true message generated legitimate sender possession secret key opponent knowledge secret key 
central information measure hypothesis testing relative entropy discrimination 
hypothesis testing task deciding hypotheses true explanation observed measurement bla 
words possible probability distributions denoted pq pq space possible measurements 
true generated pq true generated pq decision rule binary partitioning assigns hypotheses possible measurement possible errors decision called type error accepting hypothesis true type ii error accepting true 
probability type error denoted ff probability type ii error fi 
method finding optimum decision rule neyman pearson theorem 
decision rule specified terms threshold parameter ff fi functions threshold maximal tolerable probability fi type ii error ff minimized assuming hypothesis observation log pq pq find optimal decision rule values examined general 
term left called log likelihood ratio 
expected value log likelihood ratio respect pq equal relative entropy pq pq pq scenarios information measures relative entropy important information measure distinguishing probability distributions hypothesis testing 
ff fi defined ff fi ff log ff gamma fi gamma ff log gamma ff fi fundamental result hypothesis testing states type type ii error probabilities ff fi satisfy ff fi pq implies ff fi gammad similar result holds generalized hypothesis testing scenario distributions pq pq depend additional random variable decision rule probability distributions error probabilities parameterized pq jv average type type ii errors ff pv ff fi pv fi 
bound ff fi pq jz jz consider scenario sender alice wants send sequence plaintext messages xn receiver bob 
alice bob share secret key authenticate message separately encoding way depending assume bob determine uniquely gamma bob decide authenticity th message gamma opponent eve reading writing access communications channel alice bob different strategies cheating 
impersonation attack time eve waits observed modified gamma sends forged message hopes accepted bob denote eve impersonation success probability particular observed sequence gamma gamma gamma impersonation success probability gamma computed assuming eve uses optimum strategy maximizes probability successful impersonation 
substitution attack time eve observes gamma intercepts replaces different value substitution success probability bob accepts valid decodes information measures cryptography denoted substitution success probability particular observed sequence denoted assuming eve uses optimal strategy 

consider impersonation attack eve particular sequence gamma gamma transmitted gamma steps 
bob sees message decide knowledge key correct message alice hypothesis fraudulent message inserted eve hypothesis 
bob rejects valid message alice type error results probability ff bob accepts message eve type ii error results probability fi 
hypothesis distribution zjy gamma gamma alice knows secret key eve priori knowledge distribution hypothesis jy gamma gamma theta zjy gamma gamma pa theta pb denotes product distribution pa pb section 
eve free choose distribution jy gamma gamma particular distribution jy gamma gamma case follows definition mutual information ff gamma zjy gamma gamma lower bound impersonation probability round previous messages gamma gamma bob constrained reject valid messages alice probability ff 
corresponding bound impersonation success probability follows ff zjy delta delta delta gamma reduces gammai zjy delta delta deltay gamma ff 
similar arguments derive lower bounds substitution success probability particular sequence average substitution attack probability ff ff mau gammah zjy scenarios information measures gammah zjy delta delta deltay combining bounds impersonation substitution success probabilities yields delta gammah zjy delta delta deltay gamma gammah zjy delta delta deltay gamma part secret key way message preventing impersonation substitution 
privacy amplification enyi entropy privacy amplification key component unconditionally secure cryptographic protocols see section chapter 
assume alice bob share random variable eavesdropper eve knows correlated random variable summarizes knowledge details distribution unknown alice bob eve choose eavesdropping strategy secretly 
communication public channel totally susceptible eavesdropping eve alice bob wish agree compression function eve knows nearly 
eve arbitrarily small information value cryptographic key unconditionally secure encryption authentication 
enyi entropy order ff important defining eve admissible eavesdropping strategies privacy amplification 
need privacy amplification shows typically protocol unconditional security highly secret key generated large body partially secret information 
information leaked different reasons occurs protocols 
quantum key distribution bbb bc example key bits encoded nonorthogonal states quantum system eavesdropper prevented extracting complete information uncertainty principle quantum mechanics 
obtain partial information specific measurements disturb quantum states information measures cryptography slightly noise generated sender receiver equipment detected 
leaking information unconditionally secure key agreement protocols proposed maurer mau mau 
protocols output transmitted alice bob eve partially independent noisy channels insert errors certain probabilities 
alice bob apply protocols order obtain values high probability 
error correction done communicating public channel information leaks eve 
consider possible kinds partial information eve bit string shared alice bob 
consist physical bits positions alice bob know 
alternatively eve may obtained bits output arbitrary function choice unknown alice bob applied kinds eve information bennett showed alice bob extract gamma virtually secret bits function randomly chosen special set functions called universal hash function 
universal hash function set functions distinct chosen uniformly probability jyj see definition subsequent bennett shows security achieved universal hash functions general case eve particular knowledge leaves enyi entropy order 
assume eve observed value enyi entropy jv random variable corresponding random choice uniform distribution member universal hash function 
gamma gammah jv ln implies eve arbitrarily small information compressed bit key gamma gamma knowledge satisfies gv gammas ln generalized restrictions eve knowledge privacy amplification works enyi entropy order ff scenarios information measures ff cac see section 
assume eve particular value leaves enyi entropy ff jv ff 
integer gammalog bit key computed universal hash function ff jv gamma log gamma ff gamma gamma gamma gamma event probability gamma gammar gamma gammaq gamma gammas ln implies security probability gammar gammaq ff term ff gamma dominating preventing alice bob extracting secret key 
assumption terms shannon entropy enyi entropy order guarantees trivial amount secrecy privacy amplification scenario see example guessing keys min entropy guessing entropy consider problem guessing value random variable asking questions form equal correct value answer 
study problem initiated massey mas 
situations occur example cryptanalysis computationally secure ciphers 
assuming cryptosystem secure way intended designers attack finding secret key trying possible keys sequence plaintext ciphertext pair 
key guessing attacks especially relevant secret key algorithms des idea specialized technique linear differential cryptanalysis reduce number keys tried 
cryptanalysis public key systems hand usually requires sophisticated mathematical methods public key systems intractable problems rich mathematical structure factoring integers direct key guessing attacks prohibitively inefficient 
focus cryptanalysis lies mathematical aspects problem brute force searching large space needed pom 
information measures cryptography probability correct value guessed trial directly linked min entropy equal gammah max px optimal strategy 
upper bound probability terms shannon entropy provided known fano inequality gives lower bound error probability guessing knowledge correlated random variable ct 
estimate function fano inequality states error probability satisfies log jx gamma xjy optimal strategy successive guessing value obviously try possible values order decreasing probability 
denote elements probability distribution px pn delta delta delta pn jx fixed optimal guessing strategy guessing function function denotes number guesses needed average number guesses needed determine ip called guessing entropy case guessing knowledge correlated random variable xjy guessing function xjy guessing function probability distribution fixed xjy xjy called conditional guessing entropy massey obtained lower bound terms shannon entropy gamma random variable 
mceliece yu showed guessing entropy provides weak lower bound shannon entropy log jx jx gamma gamma gamma delta scenarios information measures connection guessing entropy enyi entropy order ff ff ari 
ff xjy ff gamma ff log pxy ff ff ff xjy possible definition conditional enyi entropy order ff see discussion page 
observed ari ff xjy satisfies ff xjy ff ff 
result provides lower bound ae th moment guessing function ae xjy ae ln jx gammaae pxy ae ae ln jx gammaae ae ae xjy hash functions collision probability cryptographic hash functions play fundamental role modern cryptography particular ensuring data integrity message authentication 
hash functions take message arbitrary length input produce fixed length output called hash value 
number inputs exceeds number outputs collisions occur different inputs mapped output 
basic idea cryptographic hash functions hash value serves compact representation input uniquely identifiable longer message 
section discuss applications collision probability cryptographic hash functions way functions 
way functions similar cryptographic hash functions fixed input output sizes equal cases 
function called way takes argument efficiently produces value computationally infeasible find including terminology menezes cryptographic hash function function computed efficiently input maps input arbitrary information measures cryptography length output fixed length 
addition properties preimage resistance way described 
nd preimage resistance input computationally infeasible find second input 
collision resistance computationally infeasible find distinct inputs hash output 
way hash function satisfies conditions preimage resistance nd preimage resistance 
way hash functions connection password user authentication computer systems unix operating system 
user password file contains value way function applied password longer password 
way password file write protected read protected ordinary users passwords kept secret 
access user system computes way function entered password compares stored entry user 
assume password particular user stored way hash function adversary usually sample probability distribution assuming uniformly distributed lack knowledge assuming chosen dictionary words possibly certain preferences 
knowledge adversary randomly choose distribution px compare success leads impersonation user occurs probability equal collision probability adversary repeat choice succeed independently probability time 
crucial way hash functions produce collisions probability substantially larger jyj 
incidentally universal hash functions achieve probability pair distinct inputs random choice function necessarily way 
second application collision probability hash functions involves security collision resistant cryptographic hash functions 
secure called birthday attacks 
assume collision resistant cryptographic hash function uniformly distributed uniformly chosen scenarios information measures finding collision equivalent known birthday problem fel probability people date birth year ignoring year 
assumed uniformly distributed year 
solution applied independent repetitions random variable implies collision occurs probability gamma jyj 
jyj gamma 
jyj approximated fel gamma gamma gamma jyj jyj applications needed break hash function producing collision 
implies hash functions designed output size bits operations considered infeasible 
imperfect hash function may induce nonuniform distribution uniformly distributed inputs 
case probability collision independent repetitions increases approximately gamma jyj 
jyj gamma 
jyj 
jyj gamma jyj gamma gamma gamma jyj connection follows approximation birthday problem nonuniform distributions 
effect slightly nonuniform output weak 
probabilistic bounds variational distance section compare different probabilistic statements express knows outcome random experiment 
situation restricted cryptography statements uncertainty adversary secret 
variational distance distribution random variable uniform distribution range adequate information measures eliminating probabilities bounds 
distance px defined kpx gamma py jp gamma py information measures cryptography differs variational distance factor distributions px alphabet kpx gamma py kpx gamma py setting model experiment random variable alphabet investigate bounds terms shannon entropy variational distance 
strongest notion ignorance demand distribution uniform distribution pu coincides notion ignorance underlying perfect secrecy see section 
case entropy satisfies log jx distance px pu zero kpx gamma pu 
weaker bounds allow small priori knowledge form log jx gamma ffi kpx gamma pu ffi small parameter ffi 
statement kind converted second kind described section weaker bounds probabilistic sense event assumed exist occurs high probability uniformity bounds hold occurs 
small ffl assume gamma ffl xje log jx gamma ffi terms entropy gamma pu ffi terms variational distance 
statements particular importance results kind derived situations 
probabilistic bounds converted bounds hold probability variants behave quite differently 
event induced error indicator random variable alphabet corresponds 
bound terms entropy conclude xjz pz xjz pz xjz gamma ffl log jx gamma ffi consider example bit string error probability ffl uniformity ffi 
leaves open possibility bits known case 
clearly weaker statement ignorance specification example 
scenarios information measures variational distance gamma pu ffi gamma ffl imply kpx gamma pu ffl ffi probability explicitly proved lemma 
numbers example gamma pu probability follows kpx gamma pu 
contrast bounds terms entropy probabilistic bounds terms variational distances converted easily bounds involving probability security loss 
lemma 
independent random variables alphabet arbitrary event 
assume gamma ffl gamma je ffi kpx gamma py ffl ffi proof 
error indicator random variable defined 
kpx gamma py max fi fi fi px gamma py fi fi fi max fi fi fi gamma fi fi fi max fi fi fi gamma gamma fi fi fi max fi fi fi gamma fi fi fi max se fi fi fi se gamma se fi fi fi pz delta gamma jz pz delta gamma jz ffl ffi inequality follows triangle inequality second inequality assumption lemma fact variational distance distributions 
information measures cryptography 
statement lemma equivalent 
random variables alphabet assume random variable exists takes value gamma jz ffi probability gamma ffl 
kpx gamma ffl ffi relations information measures section collection inequalities relate entropy measures discussed preceding section 
statements section denote random variables alphabet pu denotes uniform distribution table page provides systematic overview relations entropy measures 
bounds terms distance measures px kp distance included table distances px pu results general hold distances arbitrary distributions px variational distance included equivalent half distance 
lemma 
enyi entropy ff ff shannon entropy enyi entropy fi fi fi min entropy satisfy ff fi log jx equality holds px equality holds px jx equality holds set px jx proof 
lemma follows propositions 
lemma ct 
px ln kpx gamma py lemma 
log jx gamma ln kpx gamma pu relations information measures proof 
combine lemma 
lemma ct 
kpx gamma jh gamma gamma py log kpx gamma jx lemma 
kpx gamma pu log jx kpx gamma pu log kpx gamma pu jx proof 
combine lemma 
distance px py defined kpx gamma py gamma px gamma py delta lemma 
gamma log jx kpx gamma pu proof 
delta px gamma jx gamma log px gamma log jx delta gamma log jx delta lemma follows noting delta kpx gamma pu information measures cryptography delta ff deltak delta ff table 
overview inequalities linking different entropy measures random variable 
entries distance measures delta deltak refer distance uniform distribution 
stands lemma comma alternatives dash applying bound 
shannon entropy uniform distributions shannon entropy uniform distributions shannon entropy central concept information theory expressing uncertainty random variable uncertainty measures appropriate specific cryptographic scenarios section demonstrates 
statements terms shannon entropy conventional way guarantee security unconditionally secure cryptosystems 
statements typically form eve entropy secret key communicated plaintext negligibly close maximal corresponding probability distribution close uniform 
purpose section justify convention investigating connections shannon entropy cryptographically relevant uncertainty measures distributions close uniform 
basically entropy measure random variable quantifies aspect guessing game determine property number questions asked value known 
distinguish entropy measures restrictions kind guessing questions allowed property determined definition success ffl shannon entropy quantifies minimum number binary questions required average determine value average taken independent repetitions ffl min entropy negative logarithm probability determined correctly guess form equal optimal strategy 
ffl guessing entropy quantifies average number questions form equal value determined average taken cryptographically relevant definitions success guessing game allow correct value determined non negligible probability 
guessing scenarios shannon entropy guessing entropy modified accordingly 
aware proposals formalizations corresponding entropy measures 
information measures cryptography examine difficulty guessing games mentioned distributions close uniform 
assume log jx gamma ffi small positive ffi interpretation shannon entropy log jx arbitrary binary questions needed average determine value pmax max px 
probability determined guess optimal strategy equal gammah pmax upper bounded fano inequality states pmax satisfy gamma pmax gamma pmax log jx gamma log jx gamma ffi verified easily pmax close jx small ffi pmax jx possible ffi 
expected number questions sense guessing entropy massey result implies log jx gamma gammaffi gammaffi jx factor jx maximum average number questions needed px uniform 
distribution random variable shannon entropy maximal close uniform distribution pu terms variational distance 
follows log jx gamma ffi lemma kpx gamma pu ln ffi summary preceding discussion shows statements form log jx gamma ffi small ffi sufficient guarantee security cryptographic key transformed tight lower bounds terms entropy measures relevant cryptographic scenarios 
chapter smooth entropy subject chapter entropy smoothing process converting arbitrary random source source smaller alphabet uniform distribution 
introduce formal definition smooth entropy quantify number uniform bits extracted random source probabilistic algorithms 
main question arbitrary random source uniformly random bits extracted 
allow arbitrarily small deviation output bits perfectly uniform random bits may include small correlation random bits smoothing 
inclusion randomized extraction functions main difference entropy smoothing pure random number generation information theory additional random sources available 
entropy smoothing consider auxiliary random bits resource extractors theoretical computer science 
entropy smoothing applications theoretical computer science cryptography 
fact different parties involved scenario different knowledge motivates distinction multiple random sources 
applications smoothing algorithm depend distribution source sources certain structural property 
smoothing algorithms depend probability distribution source 
outline chapter follows 
introductory considerations motivating examples general framework entropy smoothing definition smooth entropy introduced smooth entropy section 
relation previous concepts privacy amplification data compression random number generation extractors discussed section 
things observe smooth entropy lower bounded enyi entropy order average smooth entropy corresponds shannon entropy 
turn bounds smooth entropy spoiling knowledge proof technique investigated section 
section prove lower bounds smooth entropy terms enyi entropy profile distribution 
particular show smooth entropy lower bounded enyi entropy order ff ff 
section prove generalized version main theorem entropy smoothing extends case smoothing random variable unknown distribution 
chapter partially cm cac 
consider random variable alphabet distribution px want apply smoothing function uniformly distributed range size largest sufficiently uniform measure amount smooth entropy inherent extractable relative allowed deviation perfect uniformity 
alternative view process eliminates partial information available concentrates possible uncertainty reasons called extraction function concentration function 
proceed need way quantify remaining divergence perfectly uniform distribution 
fix particular information measure information measure discussed chapter long measure nonuniformity 
moment relative entropy uniform distribution pu px log jx gamma distance uniform distribution kpx gamma pu jp gamma pu gamma gamma gamma gamma gamma table 
table shows random variable values example converted uniform random variables sg values deterministic functions shown implicitly 
example 
suppose want produce uniform random variable values distribution px best possible concentration functions output sizes resulting outputs shown table 
output sizes optimal functions turn respect nonuniformity measures 
output deviations perfectly uniform distribution pu corresponding alphabet terms relative entropy distance gamma pu observe general smaller alphabet output uniform distribution 
note smooth entropy tion rule kp gamma pu kp gamma pu perfectly uniform bit random variable uniform bit random variable distance produced general expect output uniform decreasing size 
trade size output uniformity strict example shows 
example hints difficulty finding best smoothing function distribution 
fact easy see bin packing np complete pap reduced problem 
bin packing problem deciding positive integers items partitioned subsets bins sum subset far fixed length outputs considered 
function variable length output depends input achieve better uniformity 
example 
consider random variable distribution px possible generate perfectly random bit grouping hand produce uniformly random bits probability ignored mapped empty output mapped output 
manner uniformly random bits result average 
random bits extracted variable length case pursue 
reason intended applications exact source distributions generally known party chooses extraction function 
randomized smoothing algorithms allowed needed entropy smoothing 
additional randomness smoothing function independent taken account calculating uniformity output 
suitable randomized smoothing function modeled family deterministic functions selected random applied example 
consider random variable distributed example correspondence values probabilities known 
deterministic function outputs bits possible output values probability general formulation mapped 
resulting output 
consider functions map inputs pass rest output 
selected uniform distribution expected relative entropy distance uniform distribution drops 
count amount additional randomness choose function family model 
primary difference entropy smoothing extractors complexity theory see section 
example shows smoothing problem involved distribution available 
focus smoothing sources certain structural characteristics 
need universal smoothing algorithms member family random variables sharing property bound maximal probability value 
algorithm kind universal hashing 
originally introduced storing keys table carter wegman cw universal hash functions entropy smoothing cryptography theoretical computer science ill 
applications entropy smoothing shown diverse areas unconditionally secure cryptographic protocols mau quantum cryptography bbb pseudorandom generation hill lub derandomization algorithms lw computational learning theory kv computing weak random sources numerous areas complexity theory dealing probabilistic computations nis 
applications discussed section formalization smooth entropy 
general formulation main questions addressed formal notion smooth entropy 
random bits certain uniformity extracted high probability random variable 

structural characterization random variables certain amount smooth entropy extracted high probability 
smooth entropy formalization respect aspects mentioned previous section ffl different measures uniformity output ffl producing uniform outputs reducing output size ffl probabilistic smoothing functions ffl small probability failure 
measure uniformity output need way quantify distance random variable uniform distribution corresponding alphabet 
nonuniformity measure sense reflect intuitive notion distance px uniform distribution nonuniformity measure associates random variable positive number px uniform distribution strictly greater random variable strictly uniform sense px px jx px px jx holds nonuniformity measure define expected nonuniformity random variable xjy xjy xjy denotes nonuniformity random variable probability distribution nonuniformity measures sequel relative entropy distance uniform distribution 
choices maximum probability value expressed min entropy log jx gamma variational distance bar distance gns 
measures studied systematically 
smoothing algorithm able produce outputs achieve desired uniformity 
demonstrated example uniform outputs usually obtained reducing output size 
fixed input distribution possible produce arbitrarily uniform outputs 
introduce parameter control trade uniformity output amount entropy lost smoothing process 
general formulation formalize probabilistic concentration functions extending input additional random variable models random choices randomized algorithm converted form 
independent value taken account computing uniformity think catalyst random input enables extraction uniform entropy distribution 
tolerated uniformity bound extraction process fails error event occurs 
small probability denoted ffl may depend uniformity calculated case complementary event occurs 
explicitly ignore size public random input considerations assume additional randomness available abundance 
ready definition smooth entropy quantifies number uniform random bits specific random variable 
incorporates mentioned aspects universality smoothing process 
definition 
nonuniformity measure delta 
decreasing non negative function 
random variable alphabet smooth entropy psi delta terms probability gamma ffl psi maximum security parameter exist random variable function theta jyj gammas failure event probability ffl expected value nonuniformity delta 
formally psi max fi fi fi theta jyj gammas ffl jt delta definition illustrated 
remarks technical aspects definition due 
definition stated average choices allows jt exceed delta 
probability occur controlled increasing eliminated rate delta converges 
smooth entropy 
jt expected nonuniformity output alphabet gammas values 
definition smooth entropy requires jt bounded delta 
noted definition smooth entropy ignores size amount public randomness needed 
practical applications smoothing function polynomial time computable limits size jt restriction case smoothing universal hash functions see proof corollary 

notation psi explicit omits specification information measure smoothness function delta failure probability ffl 
parameters mentioned separately clear context usually case 
definition desired universality allowing smoothing function different distributions 
applications known random variable property shared 
smooth entropy family random variables defined requires smoothing function works random variables family 
definition 
nonuniformity measure delta 
decreasing non negative function 
family random variables previous related concepts alphabet smooth entropy psi delta terms probability gamma ffl psi maximum security parameter exist random variable function theta jyj gammas failure event probability ffl expected value nonuniformity delta 
formally psi max fi fi fi theta jyj gammas ffl jt delta singleton sets fxg psi psi fxg equivalently 
distance bound definition smooth entropy calculated condition occur probability gamma ffl 
probability eliminated general information measures required close output satisfies jt delta 
deviation integrated uniformity parameter delta certain nonuniformity measures distance variational distance 
easy see random variable smooth entropy psi delta terms variational distance probability gamma ffl smooth entropy psi delta ffl terms variational distance probability see lemma 
similar relation holds distance 
distinction nonuniformity measures quantify deviation central treatment 
mainly relative entropy distance 
lemmas statements smooth entropy terms relative entropy easily converted distance variational distance 
shows properties smooth entropy extent independent nonuniformity measure 
previous related concepts review history applications entropy smoothing cryptography theoretical computer science 
point relation concepts information theory data smooth entropy compression intrinsic randomness extractors context derandomization computing weak random sources 
privacy amplification entropy smoothing introduced independently known time 
techniques build fact uniform entropy extracted universal hash functions see section 
universal hash functions context bennett brassard robert impagliazzo levin luby ill recognized importance enyi entropy entropy smoothing 
overview applications subsections 
privacy amplification cryptography privacy amplification key component unconditionally secure cryptographic protocols introduced section 
overview unconditionally secure cryptographic systems chapter provided 
privacy amplification theorem theorem applications cryptography 
privacy amplification forms final step protocol generate secret key 
eliminates information leaked opponent protocol 
examples quantum key distribution bbb bc key agreement common information mau mau 
privacy amplification implement cryptographic primitive oblivious transfer unconditional security bc cr 
theorem implies enyi entropy order random variable lower bound smooth entropy 
crucial smoothing universal hashing require knowledge distribution may random variable sufficient enyi entropy 
smoothing algorithm applied family random variables produce output desired size uniformity 
random variable select member universal hash function uniform distribution 
stated corollary privacy amplification theorem 
corollary 
smooth entropy family random variables gammas ln terms relative entropy probability minimum enyi entropy order psi min previous related concepts proof 
set functions universal hash function 
choose prime jx exists universal hash function gamma jyj gamma consists functions ax mod mod jyj 
sufficiently primes chosen jx member universal family selected log jx random bits 
provided exists suitable universal hash function selected uniform distribution 
inserting log jyj gamma shows jt log jyj gamma gammas ln 
known universal hash function operates binary strings described section page 
addition set surjective functions universal sar role smooth entropy privacy amplification characterize probability distributions adversary eve may know 
argued assumed eve knowledge value known alice bob random variable smooth entropy psi alice bob public discussion extract psi bits guaranteed completely hidden eve 
entropy smoothing pseudorandom generation random bits ubiquitous valuable resource computer science lub 
pseudorandom generator deterministic polynomial time computable algorithm input short random seed produces longer string looks random polynomial time bounded observer 
pseudorandom generator effectively convert small amount true randomness larger amount pseudorandomness distinguished truly random string polynomial time adversary 
cryptographically secure pseudorandom generators concept way function important concepts smooth entropy modern cryptography 
function takes argument efficiently produces value computationally infeasible adversary find including hastad impagliazzo levin luby hill show construct pseudorandom generator way function construction uses iteration generates somewhat pseudorandom uniformly distributed bits 
bits converted uniform random bits universal hash function 
theorem similar theorem appeared ill guarantees output uniform hill 
theorem 
positive integer random variable alphabet positive integer parameter random variable corresponding random choice uniform distribution member universal hash function gamma uniformly distributed gamma gamma gammae obtain lower bound smooth entropy 
proof similar corollary 
corollary 
smooth entropy family random variables gammas terms distance probability minimum enyi entropy order psi min note corollary follows corollary factor smooth entropy gammas lemma gammas ln py ln gamma pu gammas gamma pu relation entropy information theory demonstrates fundamental measure randomness entropy 
random variable random uniformly distributed bit string length 
shown constructing optimal prefix free code achieves average length previous related concepts average taken independent repetitions code optimal prefix free code expected length expectation choice smooth entropy psi denotes number uniform random bits difference psi 
important distinction entropy denotes average length optimal prefix free code instance variable length code block code large number independent versions smooth entropy corresponds length fixed length code single random variable average values observed general random experiment repeated times independently effects law large numbers 
expects intuitively average smooth entropy corresponds entropy sense ffl entropy upper bound smooth entropy ffl entropy lower bound average smooth entropy 
bounds formally remainder section 
theorem similar lower bound pseudorandom generation lub shannon enyi theorem 
upper bound smooth entropy simple consequence information theoretic principle processing increase entropy 
theorem 
delta non negative function random variable smooth entropy psi delta terms relative entropy probability upper bounded entropy sense psi delta precisely logb psi delta proof 

exists function jy psi definition smooth entropy terms relative entropy log jy gamma jt jt kp delta smooth entropy jt jt gamma log jy gamma delta logb psi gamma delta inequality follows deterministic function formal statement lower bound asymptotic equipartition property aep information theory see section set values sequence independent identically distributed random variables distribution px divided sets typical set non typical set 
aep states observed sequence lies typical set high probability probability typical sequence close occurring sequences lie typical set sequences typical set equally probable 
enyi entropy equal entropy uniform distribution average smooth entropy repetition close entropy 
theorem 
xn sequence random variables distribution px alphabet ffl 
average smooth entropy random variable gammas ln terms relative entropy smaller value close entropy high probability 
precisely psi ffl log ffl jx probability gamma jx delta gamma ln ffl jx proof 
aep proposition states typical set ffl probability gamma jx delta gamma ln ffl jxj typical sequences satisfy px gamman ffl log ffl jx ffl log ffl jx previous related concepts proposition 
theorem follows corollary 
entropy lower bound smooth entropy 
observed bennett illustrated 
example 
suppose know random variable px px gamma log jx gamma px gamma jx gamma satisfies gamma log jx gamma occurs probability matter small extracted value predicted probability knowledge lower bound probability guessed reducible small part randomness converted uniform bits 
entropy random variable adequate measure smooth entropy 
words random variables arbitrarily large entropy smooth entropy 
relation intrinsic randomness intrinsic randomness introduced generating random bits arbitrary source vv 
short intrinsic randomness differs smooth entropy deterministic extraction functions considered smooth entropy allows probabilistic extraction functions 
goal extracting random bits small deviation uniform distribution terms distance measures 
addition investigate fixed length variable length outputs separately define intrinsic randomness asymptotically focusing achievable rate intrinsic randomness general random source fpx ng intrinsic randomness rate um source defined largest asymptotic rate independent equiprobable bits generated deterministic transformation source respect distance measure distance measures variational distance bar distance normalized relative entropy 
show intrinsic randomness rate measures equal inf entropy rate fixed length case equal variable length case see hv vv definitions 
smooth entropy stationary ergodic sources fixed length variablelength intrinsic randomness rates equal entropy rate source lim 
number deterministically extractable uniform bits corresponds entropy source corresponds number probabilistically extractable bits sequence independently repeated experiments see results smooth entropy previous section 
arbitrary sources fixed length intrinsic randomness rate equal inf entropy rate corresponds min entropy finite case 
intuitively means random source random uniform distribution values maximum probability bounded gammah intrinsic randomness 
precisely amount randomness extracted asymptotically deterministic functions 
example source inf entropy rate ffi called ffi source defined bit source max px compared smooth entropy allows probabilistic extraction functions fact deterministic functions required intrinsic randomness difference producing uniformly random bits 
bearing mind comparison asymptotic finite concepts completely different formalizations see corollary smooth entropy source enyi entropy order fixed length intrinsic randomness asymptotically equal min entropy 
follows results section smooth entropy lower bounded enyi entropy order ff ff application learning theory computational learning theory provides formalization learning valiant pac probably approximately correct model val kv 
concept learned simply subset dimensional example space containing positive examples concept 
goal learning algorithm identify target concept set concepts known algorithm 
learning examples sampled target distribution learning problem classification example positive negative 
target distribution depend learning algorithm outputs probability gamma ffi previous related concepts hypothesis error exceed ffl 
error error learning algorithm defined probability random sample target distribution mis classified classifications differ 
efficient pac learning algorithm takes polynomial time ffl ffi 
actual definition takes account question representing examples concepts ignore 
research computational learning theory focuses constructing efficient learning algorithms characterizing problems efficient learning algorithms exist 
important result direction shown states assumption factoring blum integers computationally hard boolean formulas efficiently pac learnable general target distributions 
precisely result holds target distribution enyi entropy order greater logn 
main step proof uses entropy smoothing concentrate hard instances problem sufficiently prevent efficient learning 
results section extended enyi entropy order ff ff extractors weak random sources derandomization extractor tool developed running randomized algorithms sources non perfect randomness uniform random bits 
sources called weak random sources cg 
focus research extractors complexity theory lies characterizing random sources needed polynomial time probabilistic computations 
principal difference extractors entropy smoothing framework extractors number auxiliary random bits counted resource small 
logarithmic number deterministic procedure try possibilities polynomial time original algorithm 
formally ffl extractor function theta random variable min entropy output extractor satisfies kp gamma pu ffl chosen uniformly random pu denotes uniform distribution ts 
log jx log jt log jyj best currently smooth entropy known constructions extractors produce omega gamma uniform bits log ffl truly random bits bit source min entropy omega gamma 
construction achieves poly log ffl extracting min entropy ts requiring larger auxiliary random input 
core extractor constructions known entropy smoothing variations universal hashing sense theorems 
requires uniformly random bits smooth bit source universal hashing steps taken expand truly random input 
recursive construction initialized adds part input randomness step smoothing parts steps 
refer survey nisan nis information extractors 
randomized algorithms widespread today speed space efficiency simplicity desirable properties 
order implement sources true random bits needed usually available replaced pseudorandom generators 
alternatively physical sources noise employed diodes 
problem physical sources output truly unbiased independent bits name weak random sources 
general model source ffi source bit source min entropy ffi 
extractor compute randomized algorithm ffi source simulating possible choices current extractor constructions allow rp simulated polynomial time sources constant 
bpp simulated polynomial time long omega gamma 
applications extractors include decreasing error probability randomized algorithms deterministic amplification oblivious sampling construction graphs random properties expanders 
information applications extractors zuckerman sz wz nz nisan nis 
spoiling knowledge spoiling knowledge turn characterizations smooth entropy lower bounds terms enyi entropy 
corollary shows enyi entropy order lower bound smooth entropy distribution 
mentioned section counter intuitive property conditional enyi entropy order ff increase average conditioned random variable provides side information 
suppose side information increases enyi entropy available conceptual oracle 
increase exploited prove lower bounds smooth entropy better enyi entropy order 
side information kind introduced bennett called spoiling knowledge leads information output smoothing process 
section distinguish kinds spoiling knowledge yield bounds hold probability include small failure probability 
characterize spoiling knowledge type section examine second type section 
results section stated smooth entropy random variable terms relative entropy 
hold similarly smooth entropy families random variables terms distance nonuniformity measures discussed section 
oracle giving spoiling knowledge thought experiment proofs smoothing random variable 
bounds provided method hold oracle exist actual scenario involving smoothing 
oracle knows distribution source prepare side information depending particular distribution 
proof technique auxiliary random variables spoiling knowledge introduced bennett privacy amplification scenario 
spoiling knowledge modeled random variable provided oracle 
side information increases enyi entropy xju exceeds certain probability 
arbitrary distribution marginal distribution coincides px maximizing choices smooth entropy theorem gives jg log jyj gamma pu delta min ae log jyj log jy gammah xju ln oe discussion suggests maximization expected conditional enyi entropy xju pu xju corresponds maximization right hand side 
case pu delta min ae log jyj log jy gammah xju ln oe pu delta min ae log jyj jyj ln oe jyj ln pu delta min ae ln log jyj jyj oe see right hand side equation maximal expression minimal choices consistent px pu delta min ae ln jyj jyj oe example 
consider random variable alphabet fx distribution px suppose hashed jyj universal hash function described page 
calculation uniformity output shows jg 
lower bound theorem uses enyi entropy order shows jg 
enyi entropy yields weak bound value high probability enyi entropy lies significantly entropy 
bound improved introducing auxiliary random variable reveals probability spoiling knowledge px joint distribution pu gamma px 
large 
discussion preceding paragraph chosen maximize expected conditional enyi entropy xju minimize goals achieved simultaneously goal xju jg maximize cond 
enyi entropy minimize expression side info 
thm 
row repeated comparison 
bound comes close true value jg 
accept desired uniformity achieved small probability state probably sharper conditional bound uniformity 
example 
probability xju jg follows 
setting holds probability xju jg 
example shows spoiling knowledge argument involving minimization equivalent maximizing right hand side yield better lower bounds smooth entropy enyi entropy order 
effect spoiling knowledge fact enyi entropy increase conditioning optimal side information simply maximizing conditional enyi entropy order 
characterization optimal spoiling knowledge sense minimizing section 
alternative way derive better lower bounds smooth entropy accept failure event small probability ffl example 
enables general side information exploited characterizing smooth entropy probability gamma ffl 
side information investigated section 
lower bounds smooth entropy kind spoiling knowledge argument obtained section 
smooth entropy spoiling knowledge increasing smooth entropy probability goal subsection investigate suitable choices auxiliary random variable probability increase lower bound smooth entropy restrict attention relative entropy distance nonuniformity measure smoothing universal hash functions 
recall definition smooth entropy probability requires existence hash function jyj gammas satisfies jt delta measure nonuniformity consider optimization problem maximize smooth entropy finding suitable side information log jyj maximized minimized jointly optimization involved looks glance 
explicit lower bound smooth entropy random variable obtained trivial bound jyj jx oracle knows prepare side information depending distribution result summarized theorem 
theorem 
smooth entropy psi gammas ln probability random variable lower bounded expression involving minimization arbitrary random variable joint distribution consistent px psi gamma log min pu pu delta min ln jx jx remainder section notation 
ng px delta delta delta pn ln jyj jyj constant determined size output alphabet satisfies ln jx jx consider binary auxiliary random variables denote joint distribution fl gamma fl spoiling knowledge lemma shows simple useful fact conditional enyi entropy 
lemma 
arbitrary random variable binary random variable 
pu delta gammah xju pu delta gammah xju gammah max nx px equality statements pu pu 
proof 
statement lemma equivalent pu pu px second statement follows immediately 
notation introduced equivalent fl fl gamma fl gamma fl equality trivial side information pu pu obvious 
show inequality strict non trivial side information 
subtracting multiplying fl gamma fl get ix fl gamma fl ix gamma fl fl gamma ix fl gamma fl fl gamma ix fl fl ix gamma fl fl gamma fl gamma ix fl ix smooth entropy fl gamma fl fl ix gamma fl fl fl gamma ix fl ix fl fl gamma fl fl fl gamma fl fl fl fl gamma fl ix fl underlined sums cancel 
isolating remaining terms get fl gamma fl fl ix fl fl gamma fl strictly positive 
investigate effect binary spoiling knowledge assume xju xju fl fl gamma fl gamma fl minimization theorem fl delta min ae fl fl oe gamma fl delta min ae gamma fl gamma fl oe constant ln jx jx depending selections minimum operators different cases considered minimization 
interesting result shows 
spoiling knowledge lemma 
ln jx jx binary side information increase lower bound smooth entropy psi probability sense theorem fl fl gamma fl gamma fl proof 
side information increase smooth entropy expression smaller possible cases minimum operators case fl fl reduces trivial bound entropy non negative 
second case mentioned lemma 
third case gamma fl gamma fl bound smooth entropy increased side information fl fl gamma fl gamma fl possible lemma 
increase lower bound possible third case 
binary spoiling knowledge useful observing values leaves small enyi entropy xju gammad determined size hashing output 
property extends general case auxiliary random variable arbitrary alphabet sharpen lower bound smooth entropy xju gammad assume mg xju xju theorem 
ln jx jx side information increase lower bound smooth entropy psi probability sense theorem xju gammad xju gammad proof 
proof lemma 
suppose auxiliary random variable minimizing satisfying xju gammad xju gammad smooth entropy consider random variable alphabet ng distribution px fk lg binary auxiliary random variable joint distribution px px follows lemma non trivial side information px ju px ju px fk lg contradicts minimality 
theorem shows optimal spoiling knowledge xju gammad optimal distribution minimization variables 
minimal value achieved values indicate value correspondence 
corollary 
optimal auxiliary random variable gives lower bound smooth entropy psi probability sense theorem solving optimization problem variables fl fl minimize fl fl gamma fl subject fl proof 
suppose fl fl solutions minimization 
fi indices values affected side information 
define random variable mg jointly distributed fl gamma fl 
spoiling knowledge satisfies xju gammad 
consequence theorem cases minimum operators relevant 
easy see auxiliary random variable achieving minimum distribution different pu yield better lower bound 
distribution optimal spoiling knowledge solving optimization problem numerically 
contains jx variables 
said solution 
term minimization corresponds probability large probabilities contribute undesirably large amount see example page 
theorem shows best strategies smooth side information assign smaller fl larger probabilities way larger values px prevented increasing 
theorem 
optimal auxiliary random variable gives lower bound smooth entropy psi probability sense theorem satisfies fl delta delta delta fl proof 
minimization corollary equivalent finding minimum theta fl fl gamma gamma fl delta fl assume theta minimal exist indices fl fl remember delta delta delta pn 
construct theta theta follows choose fl fl fl fl pa fl gamma fl fl fl fl observe fl fl theta gamma theta fl fl gamma gamma fl delta fl gamma fl fl gamma gamma fl delta fl smooth entropy ffi gamma fl reduces fl fl ffi fl fl ffi gammafl fl ffi gamma fl fl ffi ffi fl fl gamma fl gamma fl fl fl gamma fl gamma fl fl fl gamma fl fl gamma fl gamma fl gamma fl fl fl fl gamma fl fl quadratic polynomial fl results step expanding simplifying previous expression 
follows basic calculus quadratic polynomial lower bounded ac gamma fl fl gamma fl gamma fl fl gamma fl gamma gamma fl fl fl fl fl fl gamma fl fl gamma fl fl fl delta fl gamma fl positive assumptions fl fl results subsection characterize side information increases lower bounds smooth entropy probability 
turns suitable side information reduces large values probability distribution constant lower bound minimum operator theorem 
spoiling knowledge cuts probable values random variable increase lower bounds smooth entropy 
distribution optimal side information numerical optimization methods 
spoiling knowledge increasing smooth entropy probabilistic bounds results preceding section show auxiliary random variables provided conceptual oracle obtain better bounds spoiling knowledge smooth entropy 
side information investigated confined change probability resulting bound holds 
relax constraint allow failure event probability ffl broader range side information applicable 
refer example page illustration 
smooth entropy probability gamma ffl defined terms failure event ffl 
characterizing spoiling knowledge increase corresponding lower bounds statement theorem conditioned theorem shows smooth entropy probability spoiling knowledge increase bound theorem xju large adopted terminology theorem 
focus conditioning leads increase xju bound theorem 
theorem summarizes formally 
theorem 
random variable side information alphabet mg theorem 
lower bound smooth entropy psi gammas ln probability gamma ffl random variable maximizing conditional enyi entropy xju choice binary side information consistent gamma ffl psi gamma log min pu ps gammaffl pu gamma pu ln jx jx main result section shows optimal spoiling knowledge sense theorem provided random variable cuts values probability distribution level oe total probability mass oe ffl 
notation similar preceding section ng delta delta delta pn summations implicit restricted 
side information induces denoted random variable alphabet 
consider algorithm pseudocode takes eps ffl pn inputs returns ffl ffl show ffl ffl determined algorithm correspond distribution side information provides maximal increase conditional enyi entropy probability smooth entropy oe ffl 
probability distribution pn optimal spoiling knowledge theorem 
index largest probability equal oe 
gamma ffl 
spoil float eps int float int float eps min eps eps return theorem 
optimal side information sense theorem joint distribution gamma ffl ffl ffl ffl determined algorithm described 
furthermore ffl gamma npn ffl ffl nonnegative numbers spoiling knowledge ffl ffl gamma ffl oe ffl constant oe determined ffl pn ffl gamma npn ffl gamma pn uniform distribution induced xju log jx proof 
proof consists parts algorithm shown establish statement theorem 
show binary random variable provide better bound 
correctness algorithm typewriter font dynamic values algorithm 
invariant main loop line repetition eps ffl gamma gamma clearly holds initially 
note eps line loop terminate 
invariant guaranteed line ijk gamma gamma gamma line 
loop terminates eps case repetition changed second part invariant ffl gamma gamma holds line 
part eps implies ffl ffl 
gamma ffl gamma case theorem follows 
second case termination invariant holds gamma gamma ffl pn gamma ffl second case theorem 
second case results gamma npn gamma pn ffl ffl smooth entropy optimality binary side information case theorem proved 
second case optimal xju log jx maximal 
suppose random variable ju gamma ff ju ff ju ff ff ffl xju xju 
minimal ffl 
ffl oe see 
ffi gamma ff gamma oe gamma 
gamma gamma ff ffl oe ffi ik gamma ff gamma oe gamma ik oe ffi ik gamma ff ff gamma gamma oe gamma ik oe ffi gamma ik ff ffi ik ff oe ffi gamma oe ik ff ffi ik ff ffi ik ff inequality follows oe second ffi gamma oe gamma ff ffl gamma ff ffl gamma ff ik ff ik ff ff ffl 
inequality equivalent xju xju contradiction assumption 
bounds spoiling knowledge bounds spoiling knowledge spoiling knowledge gives best lower bounds smooth entropy characterized section 
unfortunately characterizations translate directly simple bounds smooth entropy 
bounds derived non optimal side information 
subject section 
bound shows smooth entropy lower bounded asymptotically enyi entropy order ff ff second bound links smooth entropy shannon entropy assumptions profile defined distribution 
construction lower bounds introduce special side information alphabet mg partitions values sets values approximately equal probability 
deterministic function px gammam gamma log px 
call side information type log partition spoiling knowledge partitions values sets approximately equal probability useful log jx values probability distributions differ factor min min px pmax max px lemmas show enyi entropy order shannon entropy differ arbitrarily probability distributions min pmax constant factor apart 
lemma 
random variable alphabet pmax delta min 
jx gamma min jx jx pmax jx gamma proof 
easy see maximum pmax gamma min reached px min maximal probability pmax delta min lemma follows directly 
smooth entropy minimum maximum probability distribution px differ constant factor enyi entropy order constant shannon entropy 
lemma 
random variable alphabet pmax delta min 
gamma log proof 
lemma second inequality derivation gamma log px log jx log gamma jx max delta log jx pmax log jx jx gamma log log jx jx gamma log bound enyi entropy order ff connection entropy smoothing enyi entropy established independently bennett impagliazzo ill 
results theorem theorem respectively show enyi entropy order lower bound smooth entropy 
random variable assuming lower bound approximately uniform random bits extracted deviation uniform distribution decreases exponentially fewer bits extracted 
applications stronger bound terms min entropy assumed equivalent bounding maximum probability value smoothing results theorems hold assumption ff ff ff ff proposition 
hand follows example page lower bound sufficient guarantee nontrivial amount smooth entropy 
smooth entropy arbitrarily small assumptions 
section bounds spoiling knowledge examine remaining range ff 
show high probability smooth entropy lower bounded ff logarithm alphabet size security parameters depending ff error probability 
approach uses spoiling knowledge argument 
side information distribution high probability takes value xju far ff 
simple weak bound holds follows lemma 
lemma 
random variable ff ff ff gamma ff proof 
ff ff ff gamma gamma ff log max px ff gamma ff log px ff ff lower bound follows 
conclude ff gamma ff ff ff 
bound multiplicative ff gamma limits usefulness ff 
tighter bound derived additive ff gamma gamma theorem provides connection enyi entropy order ff conditioned side information enyi entropy joint distribution 
theorem 
ff 
arbitrary random variables probability takes value ff xjy ff xy gamma log jyj gamma ff gamma gamma gamma gammar gamma gammat smooth entropy proof 
straightforward expand enyi entropy xy ff xy gamma ff log pxy ff gamma ff log delta ff gamma ff gamma ff log py ff gamma log py gammaff ff xjy introduce function fi ff xjy interpret ff xjy function consider random variables fi 
equation equivalent ey gammaff fi ff gamma log py gammaff ff xy ey gammaff fi ff gamma log py gamma gammaff ff xy gammar gammar inserting right hand side inequality yields py gamma ff fi ff gamma log py gamma gamma ff ff xy gammar see dividing gamma ff probability gamma gammar takes value ff xjy ff xy log gamma ff gamma thing missing bound term log 
large values jlog py occur small probability 
theta py gammat jyj py gammat jyj py gammat jyj terms summation 
probability gamma gammat takes value log py gammat gamma log jyj theorem follows union bound 
bounds spoiling knowledge applying bound log partition side information gives main result section shows smooth entropy lower bounded enyi entropy order ff ff 
theorem 
fix integer gamma log log jx security parameter smooth entropy 
ff smooth entropy random variable gammas ln terms relative entropy probability gamma gammar gamma gammat lower bounded enyi entropy order ff sense psi ff gamma log gamma ff gamma gamma gamma proof 
log partition spoiling knowledge alphabet mg defined 
deterministic function ff xu ff theorem shows takes value ff xju ff gamma log ju gamma ff gamma gamma probability gamma gammar gamma gammat log jx lemma applied follows xju xju gamma ff xju gamma combining results shows probability takes value xju ff gamma log gamma ff gamma gamma gamma gamma gammar gamma gammat recall proof theorem values probability gammat gammalog juj excluded 
chosen px gammam px jx delta gammam gammat gammalog juj occur 
choosing gamma log log jx achieves applying theorem completes proof 
smooth entropy corollary 
family random variables defined theorem 
ff smooth entropy gammas ln terms relative entropy probability gamma gammar gamma gammat satisfies psi min ff gamma log gamma ff gamma gamma gamma corollary follows fact oracle knows distribution random variable smoothed prepare side information accordingly 
especially large alphabets results yield better bounds smooth entropy enyi entropy order 
logarithmic term vanishes asymptotically alphabet size ff ratio smooth entropy logarithm alphabet size asymptotically lower bounded ratio enyi entropy order ff logarithm alphabet size 
example 
consider random variables fi alphabet distribution px fi gamman fi gamma gamman fi gamma fi 
fi example 
lower bound psi enyi entropy order weak fi 
fi close bits 
displays enyi entropy ff fi ff 
ff close equal fi enyi entropy order corollary shows psi gammas ln probability 
allowing failure bound probability gamma lower bound theorem psi probability gamma gamma gamma log gamma enyi entropy order ff simplifying choice log jx 
psi probability gamma gamma compared enyi entropy order conclude psi 
ff bound theorem reduced shannon entropy 
shown example section yields weak lower bound psi 
example shows transition ff 
bounds spoiling knowledge alpha 
enyi entropy ff fi function ff 
random variables fi fi defined example 
graph shows theorem enyi entropy order ff close yield better bounds smooth entropy enyi entropy order 
example 
random variable alphabet examine lower bounds psi ff assumed various ff see 
ff psi guaranteed corollary 
theorem shows psi probability gamma gamma close ff 
bound decreases sharply ff 
ff assumed random variable constructed example smooth entropy 
tighter bound profile distribution section shown smooth entropy lower bounded enyi entropy order ff ff 
bound smooth entropy psi alpha 
dependence lower bound psi order ff enyi entropy 
graph shows lower bound theorem smooth entropy psi gammas ln probability gamma deduced ff function ff 
note sharp decrease ff 
see example tight small alphabet sizes 
derive tighter bound section depends assumption profile probability distribution defined 
bound tighter theorem especially smaller alphabets 
log partition spoiling knowledge mg defined 
fixed value define profile random variable function fi fi fi phi gammau gamma px gammau psi fi fi fi fi fi fi phi px gammam psi fi fi fi expected difference logarithm profile conditional entropy xju bounds spoiling knowledge obtain lower bound smooth entropy 
examining structure probability distributions see logarithm profile close conditional entropy xju sense log xju log gamma gamma delta denotes binary entropy function 
note xju remaining 
eu log xju eu log gamma gamma delta ready state main result section 
theorem 
random variable ffl integer log jx log ffl positive integer 
log partition side information introduced max ae log gamma eu log gamma gamma delta eu log gamma log gamma gamma delta oe eu log 
eu theta ffl delta lower bound smooth entropy gammas ln terms relative entropy holds probability gamma ffl psi xju gamma gamma gamma log gamma gamma proof 
fl xju function denotes entropy consider random variable fl 
expectation equal xju gamma log 
applying th moment inequality see theta jc gamma theta jc gamma smooth entropy probability small xju xju gamma high probability 
bound probability fi fi gamma fi fi pu fi fi xju gamma xju fi fi pu xju xju xju pu xju gamma xju xju xju pu xju gamma xju pu xju xju xju pu log gamma eu theta log gamma xju xju pu eu theta log gamma log gamma gamma delta pu step follows form definition 
conclude assumption theorem xju xju gamma occurs probability gamma ffl 
follows lemma xju xju gamma gamma event small probability choice guarantees px gammam px jx delta gammam ffl union bound total probability fails ffl proof completed applying theorem 
smoothing unknown distribution example 
consider random variable example page 
desired total failure probability gamma bound theorem applied resort enyi entropy order shows psi gammas ln terms relative entropy 
applying theorem ffl gamma shows psi 
bit string extracted randomly chosen universal hash function jt gamma gamma ln 
example shows bound smooth entropy theorem tighter enyi entropy order tighter bound theorem 
comes cost stronger assumption terms profile distribution smoothed 
smoothing unknown distribution discussion chapter concentrated smooth entropy random variable distribution 
distribution known distributions family happens universal hash function applied distribution known 
possible quantify ignorance bits extracted random variable unknown probability distribution 
section answer question positively 
show ps assumed distribution random variable px true distribution amount uniformly looking randomness extracted corresponds gamma log interpreted independent random variables probability space 
uncertainty outcome random variable unknown distribution consists parts related uncertainty random variable lack knowledge correct distribution 
specifically assumed distribution random variable ignorance characterized px kps called inaccuracy ck 
inaccuracy consists additive parts px kps due ignorance correct distribution due uncertainty 
smooth entropy inaccuracy measures lack knowledge correct distribution similar entropy ct 
consider construction optimal binary prefix free code random variable distribution known optimal code average length justification entropy fundamental measure uncertainty 
expected number arbitrary binary questions needed describe outcome 
guessing entropy discussed chapter refers different guessing scenario questions form allowed 
px unknown code optimal distribution average length code lies px kp px kp 
expected number binary questions needed describe outcome px kp 
clear dlog jx je questions sufficient assuming uniform distribution corresponds 
extend theorem considering universal hashing random variable unknown distribution 
show ignorance output process depends probability value guessed correctly 
precisely denotes random variable distribution assume shorter value extracted universal hashing size largest ignorance close maximum log jyj characterized gamma log 
theorem 
independent random variables alphabet probability distributions px ps respectively random variable corresponding random choice uniform distribution member universal hash function 
jg jg kp zjg log jyj gamma jyj delta ln proof 
expanding definition conditional relative entropy obtain pg jg jg pg pg jg log pg jg pg jg pg pg jg log pg jg smoothing unknown distribution gamma pg jg log pg jg gammah jg gamma log pg jg jg gammah jg gamma log gammah jg gamma log delta jx gammah jg gamma log delta jyj gamma gammah jg gamma log jyj gamma gammah jg gamma log jyj gamma jyj delta inequality follows application jensen inequality negative sum 
step argument logarithm corresponds probability independent selected randomly pg second inequality consequence universality hash function chosen uniform probability 
logarithms base theorem follows expression inequality log ln 
theorem strict generalization theorem gammah px independent random variables distributions equal px px kps px ps extreme case known px size alphabet uniform distribution assumed applying hash function unnecessary 
extreme cases situations px unknown assumed distribution find value series binary questions code optimal theorem shows average number questions needed close maximum log jyj jyj gamma average number questions arbitrarily close maximum decreasing size smooth entropy example 
consider random variable alphabet distribution px 
assumed distribution px gamma 
probabilities assumed exactly reverse order 
note px kp 
enyi entropy order guessing probability satisfies gamma log 
universal hash function 
distribution known average number binary questions determine jg theorem 
distribution assumed jg pg jg jg needed average demonstrated theorem 
result shows smoothing universal hashing extends random variables unknown distributions 
learning theory example situation conceivable known distribution learning examples target distribution see section 
extending notion smooth entropy effect allowing unknown distributions vain 
applications entropy smoothing usually require satisfy certain condition characterizing information adversary 
satisfying condition assumed distinguishing unknown assumed distributions meaningless 
chapter concept smooth entropy introduced quantify number uniform bits extracted random source probabilistic algorithms 
smooth entropy unifies privacy amplification cryptography entropy smoothing complexity theory 
turns notion smooth entropy falls intrinsic randomness formalization uniform entropy extractable deterministic functions concept extractors differs respect takes account number auxiliary random bits 
formalization smooth entropy allows systematic investigation spoiling knowledge proof technique obtaining lower bounds extractable uniform randomness 
application special type spoiling knowledge able show smooth entropy lower bounded enyi entropy order ff ff 
closes gap values ff 
previously known enyi entropy order higher lower bound statement possible enyi entropy order 
results applied scenarios entropy smoothing particular applications privacy amplification cryptography 
analysis shows entropy smoothing universal hashing generally efficient known previous results enyi entropy order 
questions respect smooth entropy remain open 
existence different smoothing functions 
functions extract randomness universal hashing 
extractors general consist composed universal hash functions steps taken achieve better parameters universal hashing size auxiliary input 
smooth entropy investigated relative entropy distance nonuniformity measures 
know smooth entropy substantially different measures 
evidence case nonuniformity measures linked general bounds see section 
furthermore investigation intrinsic randomness shows intrinsic randomness rate different nonuniformity measures vv 
smooth entropy formalized asymptotically terms smooth entropy rate tradition information theory similar intrinsic randomness rate 
benefits formulation simpler statements results asymptotic effects 
explicitly chosen non asymptotic treatment focus applications 
chapter unconditional security cryptography important properties cryptographic system proof security reasonable general assumptions 
design involves trade level security important qualities cryptosystem efficiency practicality 
security models currently cryptography include computational security provable computational security unconditional security terminology menezes 
notion computational security amount computational required break system best currently known methods 
discovering relevant attacks requires thorough examination design guaranteed remain secure 
computational security equivalent historical difficulty problem decrease development new cryptanalytic techniques 
cryptosystem computationally secure amount needed break exceeds computational resources adversary substantial margin 
currently public key cryptosystems private key systems fall category 
public key schemes advanced protocols voting electronic payment systems difficulty intractable unconditional security cryptography computational problem factoring large numbers computing discrete logarithms security proved equivalent solving underlying problem 
contrast cryptosystems provable computational security security proofs showing ability adversary defeat cryptosystem significant probability contradicts supposed intractability underlying problem riv 
provable computational security conditional security assumption typically terms number theoretic problem factoring discrete logarithm 
providing proof assumption sufficiently general model computation cryptographically relevant notion solving problem continues difficult tasks complexity theory 
provable computational security attractive security assumption formalized concentrated specific location 
time concentration drawback cryptosystems rely problems suitable problems known 
hardness problems moment dangerous base security global information economy small number mathematical problems 
advances quantum computing show precisely problems factoring discrete logarithm solved efficiently quantum computers built sho 
alternative proofs computational security model offered stronger notion information theoretic unconditional security limits imposed adversary computational power 
addition security need intractability assumptions 
information theoretic definition perfect secrecy shannon sha led immediately famous theorem states roughly shared secret key perfectly secure cryptosystem long plaintext encrypted see section 
time pad prime example perfectly secure impractical system 
unconditional security considered expensive long time 
developments show shannon model modified mas practical provably secure cryptosystems possible 
modification relax requirement perfect security means complete independence plaintext adversary knowledge allow arbitrarily small correlation 
second crucial modification removes assumption adversary receives exactly information legitimate users 
primitives realistic mechanisms proposed far limiting information available adversary 
quantum channel quantum cryptography developed mainly bennett brassard bbb bc uses photons polarized light pulses low intensity transmitted fiber optical channel 
basic quantum key agreement protocol allows parties generate secret key communicating received values 
unconditional secrecy key guaranteed uncertainty relation quantum mechanics 
current implementations quantum key distribution span distances km 
time possible realize bit commitment security quantum channel results show case may cr 
noisy channel noisy channel cryptographic purposes introduced wyner channel scenario 
sender wants transmit data secretly noisy channel receiver 
adversary views channel output second noisy channel 
wyner result shows secret information transmission sender receiver possible unconditional security 
realistic model proposed maurer output random source transmitted participants partially independent noisy channels insert errors certain probabilities mau 
parties generate secret key received values public discussion 
secrecy key information differences channel outputs assumption channel completely error free 
system practical works realistic case adversary receives random source better channel legitimate users 
power noisy channel demonstrated cr kilian showed unconditionally secure bit commitment oblivious transfer primitive ck cr 
memory bound showed cm realize unconditionally secure cryptosystems assumption memory size adversary limited see section 
means unconditional security cryptography enemy unlimited computing power compute probabilistic function huge amount public data infeasible store 
long function output size exceed number available storage bits prove sole assumption proposed secret key system public key agreement protocol information theoretically secure 
chapter focus mechanisms realizing public key agreement secret key encryption 
start section overview unconditionally secure key agreement protocols involve phases called advantage distillation information reconciliation privacy amplification 
section investigates effect side information adversary obtains information reconciliation privacy amplification 
section propose unconditionally secure cryptosystems assumption adversary memory capacity bounded 
rest chapter partially taken cm cm 
unconditionally secure key agreement protocols generation shared secret key partners alice bob fundamental problems cryptography 
partners share secret information 
assume chapter linked authenticated communication channel insecure sense passive adversary eve read modify messages 
scenario key agreement computational security realized diffie hellman protocol dh rsa public key system rsa widely today 
key agreement provable computational security possible demonstrated goldwasser micali public key encryption scheme gm 
focus chapter realizing unconditional security 
discussed previous paragraphs practical unconditional security achieved basic mechanism initially way eve receive exactly information alice bob 
general initialization phase gives participants alice bob eve access correlated random variables respectively distributed joint distribution unconditionally secure key agreement protocols distribution pxy may partial control eve details may unknown alice bob eve 
case quantum cryptography bbb instance eve undertake partial measurements quantum channel alice sends polarized photons bob 
probability distribution values measured bob arbitrary way unknown limited laws quantum mechanics 
physical laws guarantee measurement detected bob gives eve information allowed protocol 
hand may case alice bob know distribution eve information common knowledge privacy amplification scenario see section 
goal key agreement protocol alice bob establish secret key exchanging series messages public channel 
alice bob able determine public communication respectively eve information protocol messages exchanged 
protocol requirements described detail 
provide secret communication alice bob directly key time pad encryption see page 
alternatively alice bob unconditionally secure message authentication see section 
assume public channel alice bob authenticated eve read access channel abundance 
difficult guarantee unconditional authenticity practical applications 
case assume alice bob initially share short secret key authentication 
public key agreement protocol method expanding key arbitrary length 
key agreement authentic channels investigated maurer mau pursued 
key agreement protocol generally consists phases 
phases may missing certain scenarios explained 
advantage distillation mau purpose phase create random variable alice bob information eve 
advantage distillation needed immediately available alice bob create exchanging messages public channel denote random variable terms unconditional security cryptography entropies goal advantage distillation establish conditions 
information reconciliation bbb bs agree common string alice bob exchange redundant error correction information public channel 
sequence parity checks systematic error correcting code bla 
alternatively alice bob carry interactive protocol cascade proposed brassard bs 
specialized protocols number advantages compared errorcorrecting codes particular communication public channel assumed error free 
information reconciliation alice bob able determine common string respectively 
eve incomplete information consists alice knows string reconciliation bob information eve alice bob choose string transmit missing information alice bob reconciliation 
informationtheoretic terms random variable jy 
case bob tries determine reconciliation string reconciliation serves establish jy cu eve substantial amount uncertainty ae 
privacy amplification final phase alice bob agree publicly compression function distill shorter string eve negligible amount information 
terms entropy large possible eve information arbitrarily close zero gamma alice bob compute 
subsequently secret key 
examine different scenarios unconditionally secure key agreement privacy amplification quantum cryptography key agreement noisy channels key agreement memory bounded adversaries illustrate concrete realizations phases scenarios 
privacy amplification described phase protocol described key agreement protocol unconditionally secure key agreement protocols phases absent 
alice bob know eve partial information corresponds scenario described section theorem replaced alice bob know lower bound ff jv ff eve enyi entropy order ff particular value privacy amplification theorem results section theorem alice bob generate ff jv bits secret key quantum cryptography bc advantage distillation necessary raw bits transmitted quantum channel measured accurately eve bob 
eavesdropping eve guaranteed detected alice bob uncertainty relation quantum mechanics 
quantum cryptographic protocols proceed directly information reconciliation applied raw bits correct errors caused dark counts defects non ideal devices 
unconditionally secure secret key agreement noisy channels mau mau takes place scenario result binary random string broadcast satellite received alice bob eve independent noisy channels binary symmetric channels ct bit error probabilities ffl ffl ffl secret key agreement possible eve channel reliable alice bob channels ffl ffl ffl ffl 
protocol described maurer gm operates pairs bits alice bob advantage distillation phase 
generates shorter bit string bob bit error rate respect alice smaller eve error rate respect alice 
information reconciliation protocol privacy amplification step follow extract secret key 
unconditionally secure key agreement memory bounded adversaries section random bit string length slightly larger adversary memory capacity received alice bob eve 
random bit string instance broadcast satellite optical network communicated insecure channel alice bob 
legitimate users alice bob select store independently randomly selected subset broadcast 
predetermined interval exchange indices selected positions public messages determine positions contained subsets 
eve stores com unconditional security cryptography plete broadcast string partial information randomly selected subsets 
privacy amplification applied part broadcast alice bob common 
participants receive random bit string errors information reconciliation necessary 
random selection subset interpreted advantage distillation phase 
linking information reconciliation privacy amplification described previous section information reconciliation allows parties knowing correlated random variables noisy version partner random bit string agree shared string 
privacy amplification allows parties sharing partially secret string opponent partial information distill shorter completely secret key communicating insecure channel long upper bound opponent knowledge string known 
relation techniques developed literature 
important understand effect side information obtained opponent initial reconciliation step size secret key distilled safely subsequent privacy amplification 
purpose section provide missing link techniques presenting bounds reduction enyi entropy random variable induced side information 
show high probability bit side information reduces size key safely distilled bit 
important special case side information raw key data generated independent repetitions random experiment bit side information reduces size secret key bit 
known enyi entropy order reconciliation distribution pw jv directly provides lower bound size secret key distilled safely privacy amplification 
applying theorem similar statement enyi entropy order ff ff 
results section stated terms enyi entropy order linking reconciliation privacy amplification provides direct connection privacy amplification 
zc summarize eve total knowledge reconciliation adhering notation previous section deriving lower bounds eve final information secret key consider particular value eve knows average possible values results particular considered stronger averaging results hold instance protocol execution 
eve information modeled probability distribution pw jv alice bob incomplete knowledge 
rest section organized follows 
section presents upper bounds reduction enyi entropy due side information arbitrary probability distributions 
non interactive reconciliation protocols uniform close uniform probability distributions investigated section 
results applied section analyze important class scenarios random experiment repeated times independently 
effect side information enyi entropy reconciliation step consists alice bob exchanging suitable error correction information public channel 
information decreases eve shannon entropy usually enyi entropy non interactive reconciliation alice chooses appropriate error correction function sends bob reconstruct high probability prior knowledge results section derived arbitrary random variable probability distribution px side information random variable jointly distributed just applied conditional distributions intended application key agreement scenario mentioned px replaced pw jv pw jv respectively 
general giving side information implies reduction entropy 
goal derive upper bounds size reduction 
giving side information fact takes particular value possible shannon enyi entropies entropy increases decreases 
size reduction arbitrarily large 
unconditional security cryptography expected reduction values shannon entropy giving bounded called mutual information 
gamma xju follows symmetry fact shannon entropy conditional positive 
example illustrates facts 
reduction enyi entropy order implied giving side information exceed reduction shannon entropy gamma xju gamma xju possible 
second shows natural generalization enyi entropy order gamma xju true general 
theorem demonstrates weaker inequality gamma xju satisfied 
example 
random variable alphabet fa distributed px px 

defined ae fa fa 
xju xju 
reduction enyi entropy order exceeds reduction shannon entropy gamma xju gamma xju 
deterministic gamma xju 
expected entropy reductions gamma xju gamma xju 
addition gamma xju greater 
maximal expected decrease shannon entropy upper bound expected decrease enyi entropy order theorem states 
linking reconciliation privacy amplification theorem 
random variables alphabets respectively 
expected reduction enyi entropy order exceed shannon entropy gamma xju equality defined uniquely pu uniform distribution subset proof 
collision probability written px pu pu xju inequality follows nonnegative equality holds inserting definition enyi order entropy gives gamma log gamma log pu xju gamma log pu theta pu xju gamma pu log theta pu xju gamma pu theta log pu log xju gamma pu log pu gamma pu log xju unconditional security cryptography pu xju xju second inequality follows jensen inequality holds equality pu uniform distribution subset contrast shannon entropy expected conditional enyi entropy increase side information revealed ff ff xju possible ff 
property related spoiling knowledge proof technique described section respect smooth entropy 
theorem markov inequality probability leaking information decreases enyi entropy order kh probability takes value gamma xju kh high probability partially exposing string unacceptable key agreement scenario 
theorem provides stronger result showing enyi entropy order decreases fact log ju negligible probability 
theorem page 
theorem 
random variables alphabets respectively arbitrary security parameter 
probability gamma gammas takes value gamma xju log ju 
earlier version cm weaker version theorem derived showing enyi entropy order decreases log ju negligible probability 
version improves factor optimal seen lemma 
proof 
apply theorem ff 
importance restate theorem key generation scenario replacing px pw jv side information consisting bits instance parity checks bit string 
linking reconciliation privacy amplification corollary 
random variable alphabet particular values correlated random variables alphabets respectively log ju security parameter 
probability gamma gammas takes value decrease enyi entropy order giving jv gamma jv 
uniform distributions shown giving side information form reduce enyi entropy order arbitrary amount probability happens bounded theorem 
section derive better bounds probabilistic reduction case non interactive reconciliation special probability distributions 
uniform distributions deterministic side information reduction enyi entropy depends size preimage 
lemma 
random variable alphabet arbitrary function values set defined set xu fx ug 
distributed uniformly gamma xju log jx particular jx knowledge reduces enyi entropy order log ju proof 
enyi entropy equals shannon entropy uniform distribution log jx xju log claim immediately follows 
prove second claim note case jx ju theorems state bounds reduction enyi entropy uniform distributions 
results applied section analysis important class scenarios random experiment repeated times independently 
unconditional security cryptography theorem 
ff fi random variable alphabet probability distribution px ff jx px fi jx define lemma 
gamma xju log jx jx log ff log fi particular jx gamma xju log ju log ff log fi 
proof 
bound px jx ff jx ff jx pu get similar upper bound xju xju xu pu xu px ff jx jx fi jx ff fi jx combining gives xju jx ff fi theorem follows logarithms sides 
theorem provides tighter bound distributions close uniform 
particular theorem strictly tighter theorem fl 
fl tighter 
theorem 
fl random variable alphabet probability distribution px gamma fl jx px fl jx define lemma 
gamma xju log jx jx log fl gamma fl linking reconciliation privacy amplification proof 
define ffi deviation px uniform distribution px jx ffi jffi fl jx xju expressed xju xu xu pu xu px pu xu px gamma xu px delta xu jx ffi xu jx xu ffi jx jx xu ffi xu ffi jx jx xu ffi gamma xu ffi delta jx jx jx fl jx jx fl jx jx gamma jx jx fl jx fl fl jx gamma third step fact deterministic function px 
jx get xju jx jx delta fl gamma fl theorem follows 
independent repetition random experiment practical scenarios certain random experiment repeated independently large number times 
example unconditional security cryptography result receiving independently generated bits memoryless channel satellite scenario mentioned section 
asymptotic equipartition property aep fundamental information theory states scenario occurring sequences divided typical set non typical set probability randomly selected sequence length lies typical set approaches sufficiently large see section 
furthermore sequences typical set equally probable 
allows bound decrease enyi entropy results section 
strongly typical sequences introduced section 
consider random variable alphabet xn sequence symbols define number occurrences symbol sequence sequence called ffl strongly typical satisfies fi fi fi gamma px fi fi fi ffl jx ffl set ffl strongly typical sequences length define xn sequence independent identically distributed random variables px px words px px 
function lim 
lemma asymptotic formulation aep proposition asserts sufficiently large probability ffl approaches cardinality ffl close nh lemma bla 
sequence random variables distributed px 
ffi ffl gamma ffi sufficiently large 
ffl px 
js ffl nh sequences ffl equally probable sufficiently large reduction enyi entropy similar case uniform probability distribution enyi entropy order behaves shannon entropy 
observation stated theorem 
linking reconciliation privacy amplification theorem 
sequence random variables distributed px arbitrary function values set define ffl fx ffl ug 
ffi sufficiently large holds probability gamma ffi lies ffl reduction enyi entropy giving upper bounded gamma ju nh gamma log js ffl particular jfx ju knowledge reduces enyi entropy 
proof 
lemma px ffl js ffl nh application theorem ff fi gives gamma ju log js ffl js ffl nh gamma log js ffl second part theorem applies particular linear functions parity checks linear error correcting codes 
due widespread linear error correcting codes employed reconciliation phase 
theorem replace spoiling knowledge argument maurer proof mau known results secret key rate mau hold stronger notion secrecy 
described link information reconciliation privacy amplification unconditionally secure key agreement summarized follows 
assume alice knows random variable bob eve partial knowledge characterized random variables respectively 
random variables instance result described satellite scenario functions xc respectively zc 
order state results strongest possible form consider particular value held eve average values unconditional security cryptography gives information jv jw lower bound enyi entropy order eve probability distribution known jv alice bob generate shared secret key follows 
alice bob exchange error correcting information consisting jw bits public channel bob reconstruct jw 
eve gains additional knowledge seeing corollary shows probability gamma gammas values security parameter chosen arbitrarily enyi entropy order bounded jv gamma gamma gamma 
privacy amplification alice bob generate bit secret key chosen smaller gamma gamma gamma eve total information exponentially small gamma gamma gamma gamma gamma gammak gamma gamma ln bits 
main advantage theorem applies distribution reconciliation protocol previously obtained results held particular distributions protocols 
sharper bounds probabilistic statement theorem hold probability obtained special distributions theorems show 
memory bounded adversaries section propose private key cryptosystem protocol key agreement public discussion unconditionally secure sole assumption adversary memory capacity limited 
assumption computing power 
scenario assumes random bit string length slightly larger adversary memory capacity received parties 
random bit string instance broadcast satellite optical network transmitted insecure channel communicating parties 
proposed schemes require high bandwidth practical 
public data output random source broadcast high rate 
legitimate users alice bob randomly select small subset broadcast store values 
memory bounded adversaries selection performed described 
random selection process average fraction information adversary eve selected subset fraction information complete broadcast 
applying privacy amplification alice bob eliminate eve partial knowledge selected subset 
describe different cryptographic tasks implemented mechanism depending alice bob select random subset 
share secret key initially system realizes private key encryption key select identical subsets 
alice bob share secret information perform key agreement protocol public discussion select store independently random subset broadcast 
predetermined interval exchange indices selected positions public determine positions contained subsets 
privacy amplification applied part broadcast common 
model realistic current communication highspeed networking technologies allow broadcasting rates multiple second 
storage systems hundreds terabytes large hand require major investment potential adversary 
reach government budgets example method attractive reasons security assumption adversary memory capacity 
second storage costs scale linearly estimated accurately 
third system offers proactive security sense increase storage capacity break secrecy messages encrypted earlier 
precursor system rip van cipher proposed massey mi mas 
private key system provably computationally secure totally impractical legitimate receiver wait longer receiving message takes adversary decrypt 
related maurer mau describes system large public read entirely feasible time 
maurer contains idea realizing provably secure encryption assumptions enemy available memory 
system key agreement described mitchell mit security proof 
analysis provides proof unconditional security achieved memory unconditional security cryptography bounded adversaries 
aumann rabin rab proved conjecture maurer mau effect 
borrow methods zuckerman called extractors uniform randomness weak random sources described section 
section organized follows 
section introduce building blocks system 
main result concerning eve information randomly selected subset section 
primitive sections describe realize private key encryption public key agreement respectively 
section concludes discussion underlying assumptions perspectives 
pairwise independence entropy smoothing section construction sequence wise independent random variables universal hash functions 
universal hash functions introduced carter wegman cw wc applications cryptography theoretical computer science lw see definitions section 
strongly universal hash function generate sequence wise independent random variables way select uniformly random apply fixed sequence distinct values easily verified wise independent uniformly distributed random variables advantage technique compared selecting independent samples requires log jyj log jyj random bits 
example strongly universal hash function gf gf set gamma gamma delta fi fi fi gf gamma denotes significant bits operations gf 
construction nice property generate sequence pairwise independent random variables values sequence distinct memory bounded adversaries 
assume pairwise independence construction refer resulting distribution uniform pairwise independent repeating values excluded 
strongly universal family universal set 
member universal family specified bits 
universal hash functions main technique concentrate randomness inherent probability distribution result known different contexts entropy smoothing theorem leftover hash lemma lub privacy amplification theorem theorem section 
extracting secret key randomly selected subset going show alice bob exploit fact adversary eve store complete output public random source broadcast participants 
security proof consists steps 
step fact eve storage capacity limited establish lower bound min entropy eve broadcast bits 
second step shows eve randomly selected subset broadcast bits large high probability 
third step apply privacy amplification selected subset obtain secret key 
suppose output uniformly distributed binary source broadcast error free channel received participants 
source independent participants operated legitimate users alice generate transmit authenticated public channel bob 
generally source trusted output random bits sufficient capacity 
channel high capacity realized example satellite technology digital tv broadcasting optical networks 
channel times succession broadcast bits denoted rn assume eve total storage bits available record complete broadcast leaving partial knowledge broadcast eve may compute arbitrary function unlimited computing power additional private random bits 
model output function stored unconditional security cryptography bits memory random variable alphabet subject log jzj uniformly distributed enyi entropy order ff shannon entropy satisfy ff lemma shows min entropy corresponds eve knowledge gamma negligible fraction values precisely lemma implies particular value takes satisfies jz gamma gamma probability gammar lemma 
random variable alphabet arbitrary random variable alphabet 
probability gamma gammar takes value xjz gamma log jzj gamma proof 
gammar jzj 
pz pz gammar follows pz xjz gamma log max gamma log max px zjx pz gamma log max px gamma gamma log jzj proves lemma 
rest section denote eve knowledge particular value observed random variable xn alphabet distribution arbitrary subject gamma gamma lemma 
strategy legitimate users alice bob select values positions ng randomly broadcast symbols vector valued random variable values ng list selected positions xs xs denoted selection performed uniform distribution pairwise independence memory bounded adversaries construction sequence values ng described section resulting distinct viewed set values 
addition determined efficiently log bits 
assume value known random variable 
private key system described eve assumed obtain oracle public random string broadcast 
eve know bits selected alice bob 
intuitively expect fraction bits known eve corresponds fraction bits eve knows bit understood binary digit information theoretic sense 
case observed zuckerman context weak random sources nis 
easy show fraction eve shannon information corresponds expected value see theorem 
privacy amplification applied lower bound enyi entropy order known follows stronger bound lemma 
lower bound shannon entropy sufficient privacy amplification demonstrated example theorem outlined shows shannon entropy corresponds high probability expected value 
theorem 
random variable alphabet correspond positions selected wise independently gamma gamma gamma bk probability choice proof 
jx delta delta delta gamma function random variable uniform distribution ng 
chain rule entropy 
fact conditioning reduces entropy obtain lower unconditional security cryptography bound jx delta delta delta gamma xs jx delta delta delta xs gamma chosen wise independently set ng apply chernoff hoeffding bound limited independence random variable get fi fi fi gamma fi fi fi bk substituting implies gamma bk proof theorem works shannon entropy property side information reduce average uncertainty 
case expected conditional enyi entropy order ff main obstacle extending proof enyi entropy 
stronger result zuckerman shows fraction eve min entropy selected positions high probability close corresponding fraction total min entropy 
min entropy random variable lower bound enyi entropy ff lemma sufficient applying privacy amplification selected subset 
lemma 
random variable alphabet min entropy ffi ffi chosen pairwise independently described section ae ae ae log ffi ffi ffl ael aen log ffi value exists random variable alphabet min entropy ael probability gamma ffl choice px ffl close variational distance theta kp gamma ffl gamma ffl memory bounded adversaries 
fixed large value ae resulting choice lemma increases monotonically ffi ffi smaller exists unique ae satisfying ae ae log ffi ffi verified easily 
proof 
statement lemma slightly different zuckerman asymptotic result lemma respect ae place ff ffl follows original proof 
describe differences lead formulation lemma 
straightforward verify gamma gamma delta gammai gamma delta gamma gamma delta gamma delta 
implies gamma gammaj delta gammaj gamma delta bound gammai follows immediately 
approximation gamma delta binary entropy function ct nh gamma delta nh implies delta nh ae second inequality follows ae choosing ae described statement lemma guarantees delta delta nh ae log ffi required proof lemma 
choice ffl value resulting proof lemma 
ready state main result section 
summarize scenario choice parameters 
random bit string uniform distribution broadcast alice bob want generate secret key adversary eve total bits memory available 
random variable denote eve knowledge arbitrary error probabilities delta parameter denotes amount information may leak eve 
parameters unconditional security cryptography 
ffi min phi gamma gamma gamma log delta psi 
ae ae ae log ffi ffi 
xi gamma ae gamma ae log ffi gamma delta gamma pi 
xi log delta ael gamma pi alice bob select randomly ng pairwise independence construction described section store bits select function uniformly random universal hash function bit strings bit strings compute secret key 
random experiment consists choices mentioned theorem proved weaker assumption known eve may case 
theorem 
described scenario exists security event probability gamma gamma eve information particular knowledge delta 
formally gamma gamma delta proof 
applying lemma error probability shows jz gamma gamma log leading value ffi lemma shows takes value distribution pr jz probability gamma 
privacy amplification applied ael choice guarantees gamma delta theorem gammah ln delta 
failure uniformity bound equivalent event consists union events 
bound lemma fail probability second may deviate random variable distribution pr jz probability third distance kpx gamma outside allowed range occurs probability memory bounded adversaries lemma 
applying union bound see gamma gamma gamma delta theorem follows definition mutual information noting realistic cryptographic application theorem choice parameters somewhat simplified typically large choosing reasonable safety margin implies ae case parameters ffi ae depends close storage required alice bob size resulting secret key inversely proportional square desired error probability 
private key system describe example practical private key encryption system offers virtually security time pad 
assume alice bob share secret key access broadcast public random source addition connected authenticated public channel eve read modify messages 
pairwise independent selection size log bit 
initial communication partners needed interval observe parameters fixed 
authenticated public channel needed exchange description hash function extract secret value straightforward implementation alice bob need log bit storage hold values broadcast high speed positions observe precomputed recalled increasing order 
legitimate users able synchronize broadcast channel read bit time time 
adversary needs equipment high bandwidth channel interface mass storage order store substantial part considerations demonstrate system verge practical 
broadcast channel realized satellite 
typically current communications satellites capacity gbit sei 
commercial satellite communications services offer unconditional security cryptography broadcast data rates gbit consumer electronics prices 
far capacity offered fiber optical networks chk 
test bed optical networking consortium example capacity demonstrated gbit kdd limited number sources available 
hand tape libraries capacities range bytes major investment iee 
example private key system consider gbit satellite channel day making theta size secret key bit 
assume adversary store theta 
delta gamma error probabilities gamma gamma see ffi ae theta theta mbyte virtually secret information extracted 
legitimate users need gbyte storage hold indices selected bits 
privacy amplification announce randomly chosen universal hash function takes mbyte 
adversary knows gamma bit probability gamma directly encryption time pad example 
memory requirements alice bob reduced fast computation enables implicit representation indices feasible simple operations needed pairwise independence selection method 
assuming example values computed minute positions observed minute stored 
figures preceding example reduces storage requirements mbyte current block indices plus total mbyte computation indices takes longer observation random broadcast halted indices available 
system repeatedly initial key small part secret key obtained round safely secret key subsequent round forth 
addition part employed relax authenticity requirement public channel unconditionally secure message authentication techniques wc 
key agreement public discussion methods establish secret key users sharing secret information access random memory bounded adversaries broadcast linked public channel 
communication public channel assumed authenticated adversary read modify messages 
system offers public key agreement virtually security time pad sole assumption adversary memory capacity limited 
public communication channel different public broadcast channel purpose distribute large number random bits 
agree secret key alice bob independently select store subset broadcast random bits predetermined amount time announce chosen set positions public channel 
secret key extracted values common positions privacy amplification 
keep communication storage requirements alice bob reasonable level crucial memory efficient description index set 
fortunately pairwise independent selection method achieves 
alice bob select sequence uniform pairwise independent indices respectively ng described section 
values parameters fixed known eve 
alice stores values indices denoted bob stores indices 
assume implicit representation index set described earlier private key system line recomputation indices necessary 
way alice bob need approximately log bits memory 
pairwise independent selection index sets determined log bits 
descriptions exchanged public channel short 
order apply theorem set fs ft fu common positions need lemma sure uniform pairwise independent distribution 
easy see expected number common indices lemma 
independent sequences uniform pairwise independent random variables respectively alphabet ng distribution described section sequence restricted values occurring index 
sequence restricted unconditional security cryptography positions different pairwise independent 
proof 
pairwise independence construction section values sequences repeated 
implies gamma qg ng sequence satisfies js 
qg ng 
considering positions sequence values different see qg ng delta gamma delta gamma gamma furthermore qg gamma gamma pair distinct occurs probability sequence probability equal gamma qg lemma follows 
illustrate concrete example system assume alice bob access gbit broadcast channel 
need network capacity public key agreement private key encryption achieve similar error probability 
channel theta seconds days theta eve allowed store theta bit 
delta gamma error probabilities gamma gamma parameters ffi ae theta theta order common indices memory bounded adversaries average alice bob store ln theta bit gbyte assuming index sequences represented implicitly 
public communication alice bob consists log bit direction selected indices plus mbyte direction privacy amplification 
probability gamma eve knows gamma bit kbyte secret key alice bob obtain 
order inverse squared error probability probabilities example relatively large keep storage requirements alice bob reasonable level 
generating shorter key help reduce storage space depends primarily interesting open question lemma improved order reduce influence error probability 
large size hash function communicated privacy amplification reduced universal hash functions wise independent random variables constructed efficiently 
functions described log jyj log jx bits replace universal hash functions privacy amplification gw sz 
discussion results show unconditional security assumptions adversary available memory 
essence system exploits capacity gap fast communication mass storage technology 
discuss implications fact 
generating random bits sufficiently high rate may expensive merely transmitting 
large investment random source amortized potentially high number participants source simultaneously 
drawback system security margin linear sense memory costs directly proportional offered storage capacity technological advances 
computationally secure encryption systems complexity brute force attack grows exponentially length keys 
system provably secure account current storage capacity adversary possible attack store broadcast data sent 
contrast computationally secure systems broken retroactively better algorithms discovered faster processing possible 
unconditional security cryptography broadcast channel error free black box communication primitive system legitimate users need full functionality need receive complete broadcast small part 
conceivable receiving device simpler expensive synchronize read small arbitrary part traffic 
receivers allow greater capacity channel 
described protocols offer resilience errors broadcast channel 
take account errors alice bob perform information reconciliation bs selected subset 
methods bounding effect additional information provided eve known cm 
system rests gap technologies fast communication mass storage 
impressive developments expected fields 
mention enormous potential optical networks hand developments holographic molecular storage hand 
chapter concluding remarks challenge development cryptographic algorithms find practical systems strong guaranteed security properties 
today unconditional security model offers attractive way realizing practical provably secure cryptosystems proofs computational security cipher far away 
goal formulated outset thesis reached 
formalization general theory unconditionally secure key agreement correlated information 
step direction formalization smooth entropy chapter 
measure allows comprehensive view important part general key agreement scenario shows entropy smoothing information theoretic concept applications areas theoretical computer science 
formalization paved way closing gap connection smooth entropy enyi entropy order ff ff 
general theory corresponding information measure characterize number secret key bits generated key agreement common information remain distant goal 
respect smooth entropy open question efficient smoothing algorithms universal hash functions extracting uniform random bits 
unclear enyi entropy order ff relevant extractors complexity theory 
current literature extractors formulated terms min entropy weak random source 
natural generalize concluding remarks results enyi entropy order ff 
considerably relax assumptions weak random sources needed polynomial time probabilistic computation 
main part extension generalization lemma min entropy enyi entropy order order ff 
show min entropy randomly selected subset enyi entropy high probability close expected fraction entropy longer string 
chapter shows unconditionally secure key agreement possible assumption adversary memory capacity 
methods provide third mechanism addition quantum channels noisy channels proofs unconditional security built 
conceivable mechanism realizing cryptographic primitives key agreement oblivious transfer bit commitment 
efficiency improvement key agreement protocol result described generalization lemma enyi entropy 
bibliography ad acz el dar measures information characterizations mathematics science engineering vol 
academic press new york 
noga alon oded goldreich johan hastad ren simple constructions wise independent random variables random structures algorithms preliminary version st focs 
ari information measures capacity order ff discrete memoryless channels topics information theory csiszar elias eds north holland pp 

ari inequality guessing application sequential decoding ieee transactions information theory 
bbb charles bennett francois gilles brassard louis john experimental quantum cryptography journal cryptology 
charles bennett gilles brassard claude cr maurer generalized privacy amplification ieee transactions information theory 
charles bennett gilles brassard jean marc robert reduce enemy information advances cryptology crypto hugh williams ed lecture bibliography notes computer science vol 
springer verlag pp 

charles bennett gilles brassard jean marc robert privacy amplification public discussion siam journal computing 
bc amos benny chor interaction key distribution schemes advances cryptology crypto douglas stinson ed lecture notes computer science vol 
springer verlag 
bc gilles brassard claude cr years quantum cryptography sigact news 
bc gilles brassard claude cr oblivious transfers privacy amplification advances cryptology eurocrypt walter ed lecture notes computer science vol 
springer verlag pp 

gilles brassard claude cr richard jozsa denis quantum bit commitment scheme provably parties proc 
th ieee symposium foundations computer science focs 
bil patrick billingsley probability measure third ed wiley new york 
bl josh benaloh jerry generalized secret sharing monotone functions advances cryptology crypto shafi goldwasser ed lecture notes computer science vol 
springer verlag pp 

bla richard blahut theory practice error control codes addison wesley reading 
bla richard blahut principles practice information theory addison wesley reading 
blo rolf blom optimal class symmetric key generation systems advances cryptology proceedings eurocrypt thomas beth norbert cot eds lecture notes computer science vol 
springer verlag pp 

bibliography bs gilles brassard louis secret key reconciliation public discussion advances cryptology eurocrypt tor ed lecture notes computer science vol 
springer verlag pp 

bsh carlo blundo alfredo de santis amir herzberg kutten uga vaccaro moti yung perfectly secure key distribution dynamic conferences advances cryptology crypto ernest ed lecture notes computer science vol 
springer verlag pp 

cac christian cachin smooth entropy enyi entropy advances cryptology eurocrypt walter ed lecture notes computer science vol 
springerverlag pp 

cg benny chor oded goldreich unbiased bits sources weak randomness probabilistic communication complexity proc 
th ieee symposium foundations computer science focs pp 

chk cruz hill kellner ramaswami sasaki eds special issue optical networks ieee journal selected areas communications 
ck csisz ar korner information theory coding theorems discrete memoryless systems academic press new york 
ck claude cr joe kilian achieving oblivious transfer weakened security assumptions proc 
th ieee symposium foundations computer science focs 
cm christian cachin maurer linking information reconciliation privacy amplification advances cryptology eurocrypt alfredo de santis ed lecture notes computer science vol 
springer pp 

bibliography cm christian cachin maurer linking information reconciliation privacy amplification journal cryptology 
cm christian cachin maurer smoothing probability distributions smooth entropy preprint proc 
ieee international symposium information theory ulm 
cm christian cachin maurer unconditional security memory bounded adversaries advances cryptology crypto burt kaliski ed lecture notes computer science springer verlag 
cr claude cr going quantum bit commitment proceedings czech technical university publishing house 
cr claude cr efficient cryptographic protocols noisy channels advances cryptology eurocrypt walter ed lecture notes computer science vol 
springer verlag pp 

de santis vaccaro size shares secret sharing schemes advances cryptology crypto joan feigenbaum ed lecture notes computer science vol 
springer verlag pp 

csi size share large advances cryptology eurocrypt alfredo de santis ed lecture notes computer science vol 
springerverlag pp 

csi csisz ar generalized cutoff rates enyi information measures ieee transactions information theory 
ct thomas cover joy thomas elements information theory wiley 
cw lawrence carter mark wegman universal classes hash functions journal computer system sciences 
bibliography dh diffie martin hellman new directions cryptography ieee transactions information theory 
fel william feller probability theory rd ed wiley 
gm shafi goldwasser silvio micali probabilistic encryption journal computer system sciences 
gm martin maurer secret key rate binary random variables proc 
ieee international symposium information theory 
gns robert gray david neuhoff paul shields generalization ornstein distance applications information theory annals probability 
gol oded goldreich foundations cryptography fragments book electronic colloquium computational complexity eccc www eccc uni trier de eccc 
gw oded goldreich avi wigderson tiny families functions random properties quality size trade hashing preprint available authors preliminary version th stoc january 
hill johan hastad russell impagliazzo leonid levin michael luby construction pseudo random generator way function tech 
report international computer science institute icsi berkeley 
hv te sun han sergio approximation theory output statistics ieee transactions information theory 
iee proc 
th ieee symposium mass storage systems ieee computer society press 
ill russell impagliazzo leonid levin michael luby pseudo random generation way functions proc 
bibliography st annual acm symposium theory computing stoc pp 

isn ito akira saito multiple assignment scheme secret sharing journal cryptology 
kah david kahn story secret writing macmillan new york 
kdd koch koren wideband optical network ieee journal selected areas communications 
cryptographic hardness learning proc 
th annual acm symposium theory computing stoc pp 

kv michael kearns umesh vazirani computational learning theory mit press 
lub michael luby pseudorandomness cryptographic applications princeton university press 
lw michael luby avi wigderson pairwise independence derandomization tech 
report international computer science institute icsi berkeley 
mas james massey contemporary cryptography contemporary cryptology science information integrity simmons ed ieee press pp 

mas james massey lecture notes applied digital information theory abteilung fur eth zurich 
mas james massey guessing entropy proc 
ieee international symposium information theory 
mau maurer conditionally perfect secrecy provably secure randomized cipher journal cryptology 
bibliography mau maurer secret key agreement public discussion common information ieee transactions information theory 
mau maurer strong secret key rate discrete random triples communications cryptography sides tapestry richard blahut eds kluwer 
mau maurer lecture notes theoretische informatik ii abteilung fur informatik eth zurich 
mau maurer unified generalized treatment authentication theory proc 
th annual symposium theoretical aspects computer science stacs reischuk claude puech ed lecture notes computer science vol 
springer verlag pp 

mau maurer information theoretically secure secret key agreement authenticated public discussion advances cryptology eurocrypt walter ed lecture notes computer science springer verlag 
may dominic trouble quantum bit commitment manuscript available los alamos reprint archive quant ph march 
mi james massey rip van cipher simple provably computationally secure cipher finite key proc 
ieee international symposium information theory 
mit mitchell storage complexity analogue maurer key establishment public channels cryptography coding th ima conference uk colin boyd ed lecture notes computer science vol 
springer pp 

rajeev motwani prabhakar raghavan randomized algorithms cambridge university press 
alfred menezes paul van oorschot scott vanstone handbook applied cryptography crc press boca raton fl 
bibliography mw maurer stefan wolf intrinsic conditional mutual information perfect secrecy tech 
report department computer science eth zurich 
robert mceliece zhong yu inequality entropy proc 
ieee international symposium information theory 
muller underwater quantum coding nature 
nis noam nisan extracting randomness survey proc 
th annual ieee conference computational complexity 
thomas birthday solution nonuniform birth frequencies american statistician 
nz noam nisan david zuckerman randomness linear space preprint available authors preliminary version th stoc 
pap christos papadimitriou computational complexity addison wesley reading 
pom carl factoring cryptology computational number theory carl ed proceedings symposia applied mathematics vol 
american mathematical society pp 

rab michael rabin personal communication 
en ed enyi measures entropy information proc 
th berkeley symposium mathematical statistics probability berkeley vol 
univ calif press pp 

en ed enyi foundations information theory rev inst 
internat 
stat 
reprinted tur 
en alfred enyi probability theory north holland amsterdam 
bibliography riv ronald rivest cryptography handbook theoretical computer science van leeuwen ed elsevier pp 

rsa ronald rivest adi shamir leonard adleman method obtaining digital signatures public key cryptosystems communications acm 
sar dilip note universal hash functions information processing letters 
sei lawrence satellites wideband access ieee communications magazine 
sha claude shannon mathematical theory communication bell system technical journal 
sha claude shannon communication theory secrecy systems bell system technical journal 
sha adi shamir share secret communications acm 
sho peter shor algorithms quantum computation discrete log factoring proc 
th ieee symposium foundations computer science focs pp 

sim simmons ed contemporary cryptology science information integrity ieee press 
spi timothy quantum information processing cryptography computation teleportation proceedings ieee 
sss schmidt alan siegel aravind srinivasan chernoff hoeffding bounds applications limited independence siam journal discrete mathematics 
sti stinson explication secret sharing schemes designs codes cryptography 
bibliography sti douglas stinson cryptography theory practice crc press 
sti douglas stinson methods unconditionally secure key distribution broadcast encryption preprint 
sz aravind srinivasan david zuckerman computing weak random sources preprint available authors preliminary version th focs 
ts amnon ta extracting randomness weak random sources proc 
th annual acm theory computing stoc pp 

tur paul turan ed ed enyi selected papers budapest volumes 
val valiant theory learnable communications acm 
vv sridhar sergio generating random bits arbitrary source fundamental limits ieee transactions information theory 
wc mark wegman lawrence carter new hash functions authentication set equality journal computer system sciences 
wyner wire tap channel bell system technical journal 
wz avi wigderson david zuckerman expanders beat eigenvalue bound explicit construction applications preprint available authors preliminary version th stoc 
david zuckerman simulating bpp general weak random source proc 
nd ieee symposium foundations computer science focs pp 

bibliography david zuckerman randomness optimal sampling extractors constructive leader election proc 
th annual acm symposium theory computing stoc pp 

david zuckerman simulating bpp general weak random source algorithmica preliminary version nd focs 
index kpx gamma variational distance kpx gamma distance px kp relative entropy expected value guessing entropy ff enyi entropy ff xjy conditional enyi entropy ff xjy conditional enyi entropy entropy xjy conditional entropy binary entropy min entropy information collision probability var variance psi smooth entropy psi smooth entropy advantage distillation aep alphabet authentication bin packing binary entropy function birthday attack capacity inequality chernoff hoeffding bound collision probability computational security concave concentration function conditional entropy conditional probability conditional relative entropy conditional enyi entropy convex cryptographic hash function ffi source derandomization digital signature scheme discrimination see relative entropy entropy enyi see enyi entropy binary chain rule conditional conditioning reduces index shannon see entropy smooth see smooth ent 
expected value extraction function extractor extractors fano inequality guessing entropy hash function cryptographic see cryptographic hash function universal see universal hash function hash value hoeffding see chernoff hoeffding hypothesis testing impersonation attack inaccuracy independence inequality chernoff hoeffding jensen markov moment inf entropy rate information reconciliation information reconciliation intrinsic randomness jensen inequality joint distribution wise independence key distribution scheme distance distance learning theory log partition spoiling knowledge markov inequality memory bound min entropy moment inequality mutual information neyman pearson theorem noisy channels nonuniformity measure time pad way function way hash function optimal code pac learning pairwise independence perfect security privacy amplification private key cryptosystem see secret key cryptosystem probability discrete distribution generalized measure index space profile provable computational security pseudorandom generator public key agreement public key cryptosystem quantum cryptography random variable generalized relative entropy enyi entropy sample space secret sharing secret key cryptosystem shannon entropy see entropy shannon theorem side information smooth entropy smoothing function spoiling knowledge substitution attack type type class typical sequences see aep typical set uncertainty unconditional security union bound universal hash function unknown distribution variance variational distance weak random source channel 
