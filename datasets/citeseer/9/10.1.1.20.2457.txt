support vector machines extended named entity recognition takeuchi nigel collier national institute informatics ku tokyo japan mail collier nii ac jp explores support vector machines svms extended named entity task 
investigate identification classification technical terms molecular biology domain contrast results obtained traditional ne recognition muc data set 
furthermore compare performance svm model standard hmm bigram model 
results show svm utilizing rich feature set context window pos features muc significant performance advantage muc molecular biology data sets 
named entity ne extraction firmly established core technology understanding low level semantics texts 
ne formalized darpa sponsored message understanding conference muc muc methodologies widely explored heuristics rules written human experts inspecting examples fukuda supervised bikel labelled training examples non supervised methods collins singer :10.1.1.50.11:10.1.1.114.3629
ne main role identify expressions names people places organizations date time expressions 
expressions hard analyze traditional natural language processing nlp belong open class expressions infinite variety new expressions constantly invented 
application ne non news domains requires consider extending ne capture types instances conceptual classes individuals 
distinguish traditional ne extended ne refer ne 
issues may mean ne challenging ne 
important number variants ne expressions due graphical morphological shallow syntactic discourse variations 
example head sharing combined embedded abbreviations apo 
expressions require syntactic analysis simple noun phrase chunking successfully captured 
ne expressions may require richer contextual evidence needed regular nes example knowledge head noun predicate 
ontology level complex issues related granularity deciding class possible ne expression assigned 
ne expressions typically belong richer taxonomy ne opening possibility combining information extraction deep knowledge representations ontologies 
area currently exploring collier :10.1.1.20.3061
examples ne classes include person name protein name chemical formula computer product code 
may valid candidates tagging depending contained ontology 
ne viewed type multiple classification task ective studied learning algorithms available hidden markov models hmms rabiner juang error driven learning tbl brill 
new learning paradigm called support vector machines svms vapnik focus intensive research machine learning due capacity learn ectively large feature sets 
svms applied successfully past traditional classification tasks text classification 
promising results reported nlp tasks part speech tagging chunking 
matsumoto 
implemented compared learning methods svm hmm tested data sets 
comparison models informative different nature learning methods 
case hmm learning approach generative positive examples build model ne classes evaluates unseen sentence see words fits model 
svm hand discriminative approach positive negative examples learn distinction classes 
major di erence svm outputs measure distance classification function hmm uses viterbi algorithm viterbi decode maximum likelihood probabilities 
basically expect models quite different strengths weaknesses hopefully complementary allowing eventually combine approaches achieve composite model 
models described performance 
method svm developed method tiny svm package implementation vladimir vapnik svm combined optimization algorithm joachims 
svms inductive learning approaches take input set training examples binary valued feature vectors finds classification function maps class 
points svm models worth summarizing 
svms known robustly handle large feature sets develop models maximize generalizability 
ideal model ne task 
generalizability svms statistical learning theory observation useful misclassify training data margin training points maximized cortes vapnik 
particularly useful real world data sets contain inseparable data points 
secondly training generally slow resulting model usually small runs quickly support vectors need retained patterns help define function separates positive negative examples 
thirdly svms binary classifiers need combine svm models obtain multiclass classifier 
formally consider purpose svm estimate classification function training examples error unseen examples minimized 
classification function returns test data member class 
svms learn essentially linear decision functions ectiveness strategy ensured mapping input patterns feature space nonlinear mapping function 
algorithm described literature cited earlier focus description features tiny svm available cl 
ac jp taku ku software lexical features context considered svm model deciding class tag focus position includes surface word forms part speech orthographic features previous word class tags 
training 
implementation training pattern vector represents certain lexical features 
models surface word orthographic feature collier previous class assignments experiments part speech pos features brill showed pos features inhibited performance molecular biology data set 
probably pos tagger trained news texts 
pos features muc news data set show comparison features 
form vector basically bag words word positions ordering recorded 
experiments report feature vectors consisting di ering amounts context varying window focus word classified ne classes 
full window context considered experiments focus word shown 
pattern formation took iob approach ne chunk identification word assigned class tag class stands chunk tag stands chunk tag stands outside chunk member classes 
due nature svm binary classifier necessary multi class task consider strategy combining classifiers 
experiments tiny svm strategy rest 
example classes tiny svm builds classifiers winning class obtains votes pairwise classifiers 
implemented versions svm 
sv uses window focus word implemented polynomial poly kernel function 
sv directly compare performance svm hmm model described features focus word previous word limited context 
due ects data sparseness hmm di cult train wider context window advantages hope test sv kernel function mentioned basically defines feature space computing inner product pairs data points 
explored simple polynomial function hmm hmm implemented comparison svm version fully described collier basically linear interpolating hmm trained maximum likelihood estimates bigrams surface word orthographic feature deterministically chosen 
part speech formulation model 
consider words ordered pairs consisting surface word word feature 
common practice need calculate probabilities word sequence word name class word di erently initial name class transition 
purposes comparison note tests nobata 
hmm superior decision tree rule learner 
accordingly equation calculate initial name class probability words name classes follows calculated maximumlikelihood estimates counts training data 
current system set constants hand 
current name class conditioned current word feature previous name class previous word feature 
state transition probabilities calculated equations viterbi algorithm viterbi search state space possible name class assignments linear time find highest probability path maximize 
data sets data sets study ne traditional ne 
ne collection bio consists medline abstracts words domain molecular biology annotated names genes gene products 
second muc collection executive succession texts words muc testing 
details shown tables 
class description protein proteins protein groups families complexes substructures dna dna groups regions genes rna rnas rna groups regions genes source cl cell line source ct cell type source mo mono organism source mu multi celled organism source vi viruses source sl source ti tissue table markup classes bio number word tokens class 
results analysis results scores van rijsbergen calculated conll evaluation script score defined pr 
denotes precision recall 
ratio number correctly ne chunks number ne chunks ratio number correctly ne chunks number true ne chunks 
table shows score models collections calculated available lcg www uia ac conll ner bin class date location organization money percent person time table markup classes muc number word tokens class label 
fold cross validation total test collection 
due size collections observe optimal result model clear sustained advantage svm hmm ne task muc ne task bio 
drawback observed svm quite weak low frequency classes rna source mo time hmm usually proved robust 
svm weakest model tested conclude trained similar knowledge hmm svm particular performance advantage observe 
exploiting svms capability easily handle large feature sets including wide context window pos tags results suggest svm perform significantly higher level hmm 
detailed break results class shown table 
obvious tables ect tokenization 
experiments reported svm table fdg parser tapanainen gave better results bio simple tokenization strategy simply split word spaces punctuation marks 
muc advantage clear concluded frequent ambiguous hyphen bio key factor 
ne task bio svm slightly clearly outperformed hmm 
analysis svm results identified types error 
serious type caused local syntactic ambiguities head sharing kd sh sh domain classed protein svm split protein expressions sh sh domain 
particular ambiguous hyphen single chain sc fv antibody parentheses scfv cause svm di culties hmm 
limited contextual information gave svm cause improved grammatical features head noun main verb 
hmm gain advantage viterbi algorithm able partially consider evidence entire sentence 
second minor type error result inconsistencies annotation scheme bio inclusion definite description term name ut cells dependent cell line considered source ct expression 
kernel parameters explore dealt continuing investigation tuning svm parameters 
results provide indication performance trends svm outperform hmm significant margin muc bio data sets wide context window rich feature set 
second svm lacked su cient knowledge complex structures ne expressions achieve best performance bio 
believe tuning svm model prove useful ne allow combine evidence large feature sets order model local structure context 
furthermore training set developed pos tagger ne domain svm strongly benefit 
current configuration svm combined hmm bio data set achieve better performance classes 
acknowledging danger drawing broad ne task domain data set pending analysis cautiously say performance data sets shown muc ne task somewhat easier bio ne task 
despite investigations nature ne task palmer day nobata information theoretical aspects knowledge required task understood considered key area research 
order test method accurately develop composite model building model data set hmm svm poly svm degree degree bio muc muc table scores learning methods test sets fold cross validation data 
svm poly denotes svm trained polynomial kernel function context window svm results context window focus direct comparisons hmm 
results models surface word orthographic features part speech features results models surface word orthographic part speech features 
realistic data set molecular biology full journal articles 
supported part japanese ministry education science 
jun ichi tsujii providing data set bio anonymous referees comments helped improve 
bikel miller schwartz 

nymble high performance learning 
proceedings fifth conference applied natural language processing anlp washington usa pages march april 
brill 

simple rule part speech tagger 
third conference applied natural language processing association computational linguistics trento italy pages st march rd april 
brill 

transformation error driven learning natural language processing case study part speech tagging 
computational linguistics 
collier nobata tsujii 

extracting names genes gene products hidden markov model 
proceedings th international conference computational linguistics coling saarbrucken germany july st august th 
collier takeuchi nobata fukumoto ogata 

progress multi lingual named entity annotation guidelines rdf 
proceedings third international conference language resources evaluation lrec las palmas spain pages may th june nd 
collins singer 

unsupervised models named entity classification 
proceedings joint sigdat conference empirical methods natural language processing large corpora 
cortes vapnik 

support vector networks 
machine learning november 
fukuda tamura takagi 

information extraction identifying protein names biological papers 
proceedings pacific symposium biocomputing psb pages january 
joachims 

making large scale svm learning practical 
scholkopf burges smola editors advances kernel methods support vector learning 
mit press 
matsumoto 

support vector learning chunk identification 
proceedings fourth conference natural language learning conll lisbon portugal pages 
darpa 

proceedings sixth message understanding conference muc columbia md usa november 
morgan kaufmann 
nobata collier tsujii 

comparison tagged corpora named entity model class svm poly hmm data set bio protein dna rna source cl source ct source mo source mu source vi source sl source ti data set muc date location org 
money percent person time table recall precision scores class bio muc 
results calculated fold cross validation available data 
results shown best performing svm models pos features bio pos features muc 
task 
editors proceedings association computational linguistics acl workshop comparing corpora hong kong pages october th 
palmer day 

statistical profile named entity task 
proceedings fifth conference applied natural language processing anlp washington usa march april 
rabiner juang 

hidden markov models 
ieee assp magazine pages january 
tapanainen 

dependency parser 
proceedings th conference applied natural language processing washington association computational linguistics pages 
ohta collier nobata tsujii 

building annotated corpus molecular biology domain 
coling workshop semantic annotation intelligent content th th august 
van rijsbergen 

information retrieval 
butterworths london 
vapnik 

nature statistical learning theory 
springer verlag new york 
viterbi 

error bounds convolutions codes asymptotically optimum decoding algorithm 
ieee transactions information theory 
