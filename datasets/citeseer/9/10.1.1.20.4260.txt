probabilistic approach semantic representation semantic networks produced human data statistical properties easily captured spatial representations 
explore probabilistic approach semantic representation explicitly models probability words occur different contexts captures probabilistic relationships words 
show representation statistical properties consistent large scale structure semantic networks constructed humans trace origins properties 
contemporary accounts semantic representation suggest consider words points high dimensional space 
landauer dumais interconnected nodes semantic network 
collins loftus 
ways representing semantic information provide important insights shortcomings 
spatial approaches illustrate importance dimensionality reduction employ simple algorithms limited euclidean geometry 
semantic networks constrained graphical structure lacks clear interpretation 
view function associative semantic memory efficient prediction concepts occur context 
take probabilistic approach problem modeling documents expressing information related small number topics cf 
blei ng jordan 
topics language learned words occur different documents 
illustrate large scale structure representation statistical properties correspond semantic networks produced humans trace fidelity reproduces natural statistics language 
approaches semantic representation spatial approaches latent semantic analysis lsa landauer dumais procedure finding high dimensional spatial representation words 
lsa uses singular value decomposition factorize word document occurrence matrix 
approximation original matrix obtained choosing singular values thomas griffiths mark steyvers psych stanford edu department psychology stanford university stanford ca usa rank 
component approximation matrix gives word location high dimensional space 
distances space predictive tasks require semantic information 
performance best approximations singular values rank matrix illustrating reducing dimensionality representation reduce effects statistical noise increase efficiency 
methods lsa novel scale subject suggestion similarity relates distance psychological space long history shepard 
critics argued human similarity judgments satisfy properties euclidean distances symmetry triangle inequality 
tversky hutchinson pointed euclidean geometry places strong constraints number points particular point nearest neighbor sets stimuli violate constraints 
number nearest neighbors similarity judgments analogue semantic representation 
nelson schreiber people perform word association task named associated word response set target words 
steyvers tenenbaum submitted noted number unique words produced target follows power law distribution number words reasons similar tversky hutchinson difficult produce power law distribution thresholding cosine distance euclidean space 
shown 
power law distributions appear linear log log coordinates 
lsa produces curved log log plots consistent exponential distribution 
semantic networks semantic networks proposed collins quillian means storing semantic knowledge 
original networks inheritance hierarchies collins loftus generalized notion cover arbitrary graphical structures 
interpretation graphical structure vague connecting nodes activate 
steyvers tenenbaum submitted constructed semantic network word association norms nelson word association data latent semantic analysis left panel shows distribution number associates named target word association task 
right shows distribution number words cosine threshold target lsa spaces dimension threshold chosen match empirical mean 

connecting words produced responses 
semantic network number associates word number edges node termed degree 
steyvers tenenbaum resulting graph statistical properties small world graphs power law degree distribution feature barabasi albert 
fact semantic networks display properties reflects flexibility indication properties emerge representation learned constructed hand 
remainder probabilistic method learning representation word document occurences reproduces large scale statistical properties semantic networks constructed humans 
probabilistic approach anderson rational analysis memory categorization takes prediction goal learner 
analogously view function associative semantic memory prediction words arise context ensuring relevant semantic information available needed 
simply tracking words occur different contexts insufficient task gives grounds generalization 
assume words occur different contexts drawn topics topic characterized probability distribution words model distribution words context mixture topics wi wi zi zi zi latent variable indicating topic ith word drawn wi zi probability ith word jth topic 
words new context determined estimating distribution topics context corresponding zi 
intuitively indicates words important topic prevalence topics document 
example imagine world topics conversation love research 
world capture probability distribution words topics relating love research 
difference topics reflected love topic give high probability words joy pleasure heart research topic give high probability words science mathematics experiment 
particular conversation concerns love research love research depend distribution topics particular context 
formally data consist words 
wn wi belongs document di word document occurrence matrix 
document multinomial dis tribution topics parameters di jth topic represented multinomial distribution words vocabulary word document di zi di parameters wi zi wi predictions new documents need assume prior distribution parameters di dirichlet distribution conjugate multinomial take dirichlet prior di probability model generative model gives procedure documents generated 
pick distribution topics prior determines zi words document 
time want add word document pick topic distribution pick word topic wi zi determined generative model introduced blei 
improving hofmann probabilistic latent semantic indexing plsi 
topics represent probability distributions words documents form dimensionality reduction elegant geometric interpretation see hofmann 
approach models frequencies occurrence matrix arising simple statistical process explores parameters process 
result explicit representation words representation captures probabilistic relationships words 
representation exactly required predicting words 
treat entries word document cooccurrence matrix frequencies representation developed information sensitive natural statistics language 
generative model articulate assumptions data generated ensures able form predictions words seen new document 
blei 
gave algorithm finding estimates hyperparameters prior di correspond local maxima likelihood procedure latent dirichlet allocation lda 
symmetric dirichlet prior di documents symmetric dirichlet prior topics markov chain monte carlo inference 
advantage approach need explicitly represent model parameters integrate defining model simply terms assignments words topics indicated zi 
markov chain monte carlo procedure obtaining samples complicated probability distributions allowing markov chain converge target distribution drawing samples markov chain see gilks richardson spiegelhalter 
state chain assignment values variables sampled transitions states follow simple rule 
gibbs sampling state reached sequentially sampling variables distribution conditioned current values variables data 
sample assignments words topics zi 
conditional posterior distribution zi zi di di assignment zk number words assigned topic wi wi words assigned topic di total number number words document di assigned topic di total number words document di counting assignment current word wi 
free parameters determine heavily empirical distributions smoothed 
monte carlo algorithm straightforward 
zi initialized values determining initial state markov chain 
chain run number iterations time finding new state sampling zi distribution specified equation 
iterations chain approach target distribution current values zi recorded 
subsequent samples taken appropriate lag ensure autocorrelation low 
gibbs sampling simulations order explore consequences probabilistic approach 
detailed derivation conditional probabilities technical report available www psych stanford edu cogsci lda ps simulation learning topics gibbs sampling aim simulation establish statistical properties sampling procedure qualitatively assess results demonstrate complexities language polysemy behavioral asymmetries naturally captured approach 
took subset tasa corpus landauer foltz laham words occurred word association norm data times complete corpus random set documents 
total number words occurring subset corpus number zi sampled 
set parameters model topics 
initial state markov chain established online learning procedure 
initially wi assigned topics 
zi sequentially drawn equation frequencies involved reflected words assigned topics 
initialization procedure hoped start chain point close true posterior distribution speeding convergence 
runs markov chain conducted lasting iterations 
iteration computed average number topics word assigned evaluate sampling procedure large scale properties representation 
specifically concerned convergence autocorrelation samples 
rate convergence assessed gelman rubin statistic remained iterations 
autocorrelation lag iterations 
single sample drawn run markov chain iterations 
subset topics model displayed table words column corresponding topic ordered frequency assigned topic 
topics displayed necessarily interpretable model having selected highlight way polysemy naturally dealt representation 
topics appeared coherent interpretations 
word association data nelson 
contain number asymmetries cases people produce word response 
asymmetries hard ac random numbers simulations generated mersenne twister extremely deep period matsumoto nishimura 
frequent words topics listed www psych stanford edu cogsci topics txt cold trees color field game art body king law winter tree blue current play music blood great rights weather forest red electric ball play heart son court warm leaves green electricity team part muscle laws summer ground playing sing food queen act sun pine white flow games emperor legal wind grass brown wire football poetry bone state snow long black switch baseball band person hot leaf yellow turn field world skin day case climate cut light bulb sports rhythm tissue prince decision year walk bright battery player poem move lady crime rain short dark path coach song stomach castle important day oak gray literature part royal justice spring fall load hit say oxygen man freedom long green little light tennis character thin magic action fall feet turn radio sport audience system court heat tall wide move basketball theater chest heart set ice grow sun loop league tiny golden lawyer woods purple device fun known form knight years great wood pink diagram bat tragedy beat grace free table topics single sample simulation 
column shows words topic ordered number times word assigned topic 
adjacent columns share word 
shared words shown boldface providing clear examples polysemy count spatial representations distance symmetric 
generative structure model allows calculate probability word seen novel context word 
conditional probability inherently asymmetric 
asymmetries predict asymmetries word association norms nelson 
restricted words simulation 
results driven word frequency close asymmetries predicted frequency words subset tasa corpus 
slight improvement performance came cases word frequencies similar polysemy frequency poor indicator frequency particular sense word 
bipartite semantic networks standard conception semantic network graph edges word nodes 
graph type node nodes interconnected freely 
contrast bipartite graphs consist nodes types nodes different types connected 
form bipartite semantic network introducing second class nodes mediate connections words 
example network thesaurus words organized topically bipartite graph formed connecting words topics occur illustrated left panel 
steyvers tenenbaum submitted discovered bipartite semantic networks constructed humans corresponding roget thesaurus share statistical properties semantic networks 
particular number topics word occurs degree word graph follows power law distribution shown right panel 
result reminiscent zipf law meaning number meanings word follows power law distribution 
zipf law established analyzing dictionary entries appears describe property language 
topic topic topic word word word word word left panel shows bipartite semantic network 
right shows degree distribution network constructed roget thesaurus 
probabilistic approach specifies probability distribution allocation words topics 
form bipartite graph connecting words topics occur obtain probability distribution graphs 
existence edge word topic indicates word significant probability occurring topic 
simulations explore distribution bipartite graphs resulting approach consistent statistical properties roget thesaurus zipf law meaning 
particular examine obtain structures power law degree distribution 
simulation power law degree distributions gibbs sampling obtain samples posterior distribution zi occurrence matrices matrix words word association norms simulation second matrix random words topics norm words topics random words topics norm words topics random words topics norm words topics initialization constant frequencies random documents constant documents degree distributions networks constructed simulations 
axes 
words drawn random occurring times tasa corpus 
matrices random documents 
matrix samples taken 
results unaffected number topics focus 
samples obtained separate runs burn iterations samples drawn sample lag iterations 
sample bipartite semantic network constructed connecting words topics assigned 
network degree word node averaged samples 
resulting distributions clearly power law shown 
coefficients remained small range close roget thesaurus 
expected average degree increased topics available generally higher roget semantic networks edges added assignment tend quite densely connected 
sparser networks produced setting conservative threshold inclusion edge multiple assignments word topic exceeding baseline probability distribution represented topic 
probabilistic approach produces power law degree distributions case indicating number topics word assigned follows power law 
result similar properties roget thesaurus zipf observations dictionary definitions 
provides op power law distributions produced averaging exponentials inspected individual samples confirm characteristics 
establish origin distribution see consequence modeling approach basic property language 
simulation origins power law investigate origins power law established initialization procedure responsible results 
matrix random words obtained samples degree distribution immediately initialization 
seen produced curved log log plot higher values simulation 
remaining analyses employed variants occurrence matrix results 
variant kept word frequency constant assigned instances words documents random disrupting occurrence structure 
interestingly appeared weak effect results curvature resulting plot increase 
second variant forced frequencies words close possible median frequency 
done dividing entries matrix frequency word multiplying median frequency rounding nearest integer 
total number instances resulting matrix 
manipulation reduced average density resulting graph considerably distribution appeared follow power law 
third variant held number documents word participated constant 
word frequencies weakly affected manipulation spread instances word uniformly top documents occurred rounded nearest integer giving 
median number documents words occurred documents chosen random words median 
manipulation strong effect degree distribution longer power law monotonically decreasing 
distribution number topics word participates strongly affected distribution number documents word occurs 
examination distribution tasa corpus revealed follows power law 
approach produces power law degree distribution accurately captures natural statistics data constructs representation 
general discussion taken probabilistic approach problem semantic representation motivated considering function associative semantic memory 
assume generative model words occur context chosen small number topics 
approach produces representation word document cooccurrence matrix explicitly models frequencies matrix probability distributions 
simulation showed approach extract coherent topics naturally deal issues polysemy asymmetries hard account spatial representations 
simulation showed probabilistic approach capable producing representations largescale structure consistent semantic networks constructed human data 
particular number topics word assigned followed power law distribution roget thesaurus zipf law meaning 
simulation discovered manipulation remove power law altering number documents words participate follows power law distribution 
steyvers tenenbaum submitted suggested power law distributions language traced kind growth process 
results indicate growth process need part learning algorithm algorithm faithful statistics data 
able establish origins power law distribution model growth processes described steyvers tenenbaum contribute understanding origins power law distribution dictionary meanings thesaurus topics number documents words participate 
representation learned probabilistic approach explicitly representation words word described set features 
representation probabilistic relationships words expressed probabilities arising different contexts 
easily compute important statistical quantities representation probability arising particular context observed complicated conditional probabilities 
advantage explicitly probabilistic representation gain opportunity incorporate representation probabilistic models 
particular see great potential kind representation understanding broader phenomena human memory 
acknowledgments authors supported studentship ntt communications sciences laboratory 
penny smith josh tenenbaum comments tom landauer darrell laham tasa corpus 
shawn wrote mersenne twister code 
anderson 

adaptive character thought 
erlbaum hillsdale nj 
barabasi albert 

emergence scaling random networks 
science 
blei ng jordan 

latent dirichlet allocation 
advances neural information processing systems 
collins loftus 

spreading activation theory semantic processing 
psychological review 
collins quillian 

retrieval time semantic memory 
journal verbal learning verbal behavior 
gilks richardson spiegelhalter 

markov chain monte carlo practice 
chapman hall 
hofmann 

probablistic latent semantic indexing 
proceedings second annual international sigir conference 
landauer dumais 

solution plato problem latent semantic analysis theory acquisition induction representation knowledge 
psychological review 
landauer foltz laham 

latent semantic analysis 
discourse processes 
matsumoto nishimura 

mersenne twister dimensionally uniform pseudorandom number generator 
acm transactions modeling computer simulation 
nelson schreiber 

university south florida word association norms 
www edu 
roget 

roget thesaurus english words phrases 
available project gutenberg 
shepard 

stimulus response generalization stochastic model relating generalization distance psychological space 
psychometrika 
steyvers tenenbaum 
submitted 
large scale structure semantic networks statistical analyses model semantic growth 
tversky hutchinson 

nearest neighbor analysis psychological spaces 
psychological review 
zipf 

human behavior principle effort 
hafner new york 
