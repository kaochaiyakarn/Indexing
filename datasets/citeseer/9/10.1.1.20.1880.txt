hybrid bayesian networks reasoning complex systems dissertation submitted department computer science committee graduate studies stanford university partial fulfillment requirements degree doctor philosophy uri lerner october fl copyright uri lerner rights reserved ii certify read dissertation opinion fully adequate scope quality dissertation degree doctor philosophy 
daphne koller department computer science stanford university principal adviser certify read dissertation opinion fully adequate scope quality dissertation degree doctor philosophy 
stephen boyd department electrical engineering stanford university certify read dissertation opinion fully adequate scope quality dissertation degree doctor philosophy 
ronald parr department computer science duke university approved university committee graduate studies iii parents brother 
iv real world systems naturally modeled hybrid stochastic processes stochastic processes contain discrete continuous variables 
examples include speech recognition target tracking monitoring physical systems 
task usually perform probabilistic inference infer hidden state system noisy observations 
example ask probability certain word pronounced readings microphone probability submarine trying surface sonar data probability valve open pressure flow readings 
bayesian networks compact way represent probability distribution 
extended dynamic bayesian networks represent stochastic processes 
thesis concentrate hybrid dynamic bayesian networks 
contributions fold theoretical algorithmic practical 
theoretical perspective provide novel complexity analysis inference hybrid models show fundamental difference complexity inference discrete models hybrid ones 
particular provide np hardness results inference simple hybrid models 
algorithmic perspective provide suite new inference algorithms designed deal non linearities real world systems scale large hybrid models 
show algorithms outperform current state art inference algorithms hybrid models 
practical perspective apply techniques task fault diagnosis complex real world physical system designed extract oxygen martian atmosphere 
demonstrate feasibility approach data collected actual runs system 
acknowledgments foremost daphne koller 
describe daphne word excellence choice 
standards everybody harder bring students fulfill true potential 
looking back realize little knew came stanford learned daphne years find interesting research problems clearly papers talks teach class short researcher 
am grateful daphne bringing point proud 
excluding daphne ron parr person owe biggest debt gratitude 
ron research associate daphne research group served unofficial second adviser crucial period ph career large extent responsible putting track ultimately led thesis 
working closely ron rewarding enjoyable experiences ph career 
ron time chat traced back chats 
feel extremely fortunate met ron count friend 
ron 
stephen boyd third member reading committee pointed interesting directions connections turned useful 
daphne ron stephen time read long thesis come useful comments 
ross shachter nils nilsson committee 
working rwgs truly group effort am privileged worked amazing group people 
people stanford vi brooks moses truly understood system able come model sheila mcilraith brought problem attention instrumental keeping project right track scott shared workload invaluable crucial moments 
brooks spent hours constructing debugging model 
transformed experience potentially painful enjoyable 
nasa side charlie goodrich help went reasonable expectation 
charlie patiently answer questions sure access data needed kind host home visit kennedy space center 
hoped helpful nicer person charlie 
dan clancy bill larson jon clyde curtis dan keenan help 
zohar manna adviser year stanford transition israel stanford smoother easier 
zohar develop research interests year needed talk believed abilities 
am truly grateful 
shimon ullman weizmann institute science israel helped great deal application process encouraged come stanford 
keep track progress give advice needed 
may come stanford shimon help am debt 
algorithms thesis implemented code infrastructure called frog 
frog initially created lise getoor am convinced known getting done 
beast walks hope various users frog benefited system despite pain caused 
eric bauer xavier boyen scott ben taskar help writing frog 
member dags daphne approximate group student source vii pleasure pride 
year year research coming dags high quality high impact testament extraordinary people members dags daphne vision guidance 
eric bauer xavier boyen barbara engelhardt lise getoor carlos guestrin manfred jaeger alex brian uri dirk ormoneit ron parr avi pfeffer mehran sahami scott eran segal christian shelton ben taskar simon tong 
friends colleagues fortunate get know years eyal amir bjrner michael colon arturo crespo alon efrat erez bernd nir friedman karl pfleger sipma tomas uribe 
different sources funding onr young investigator number aro muri program integrated approach intelligent systems number daah onr muri program decision making uncertainty number nasa number nag 
parents lerner love support count brother proud proud brother 
dissertation dedicated 
viii contents acknowledgments vi bayesian networks 
hybrid network 
classification hybrid models 
contributions 
outline 
bayesian networks notation 
representation 
exact inference 
variable elimination 
clique trees 
sampling techniques 
importance sampling 
markov chain monte carlo gibbs sampling 
probable explanations 
finding mpe 
finding explanations 
ix hybrid bayesian networks normal distribution 
linear gaussians 
definition linear gaussians 
canonical forms 
conditional forms 
conditional linear gaussians 
definition clgs 
representation factors 
message passing strongly rooted trees 
ill defined message passing 
strong triangulation implications 
approximate inference clgs 
discretization 
expectation propagation 
sampling approach 
theoretical analysis hardness results 
discussion 
enumeration algorithm inference algorithms 
setup 
rao blackwellized likelihood weighting 
rao blackwellized mcmc 
enumeration algorithm 
single representative algorithms 
optimization trick 
empirical comparison 
discussion 
error bounds 
comparison lauritzen algorithm 
non linear cpds extended kalman filter approach 
numerical integration 
problem formulation 
gaussian quadrature 
exact monomials unscented filter 
dealing general gaussians 
putting 
encapsulated variables 
moments correction 
example 
combination enumeration algorithm 
augmented clgs representation 
algorithm augmented clgs 
variational approach 
numerical integration approach 
algorithm 
clique integration 
conditional forms 
analysis 
experimental results 
enumeration approach 
dynamic bayesian networks modeling stochastic processes 
definition dynamic bayesian networks 
inference task 
kalman filter 
xi inference discrete dbns 
inference hybrid dbns 
particle filters 
scaling hybrid dbns collapsing algorithm 
hypothesis variable 
empirical comparison 
decomposition belief state 
putting 
smoothing 
application rwgs system rwgs system 
modeling rwgs 
sensor modeling 
sensitivity 
differing time scales 
parameter estimation 
experimental results synthetic data 
testing gaussian approximation 
comparison particle filtering 
comparison rao blackwellized particle filtering 
results real data 
steady state experiments 
shutoff experiments 
discussion 
summary 
limitations directions 
inference issues 
xii modeling long term changes 
active learning 
control 

proofs proof lemma 
proof claim theorem 
proof theorem 
bibliography xiii list tables comparison gaussian approximations var var parameters augmented crop network 
parameters simple model dimensional robot 
probabilities flow sensors gas loop faulty time steps 
xiv list figures simple example hybrid bayesian network 
simple bayes net modeling john weekends 
nodes network weather sunny cloudy rainy deadline true false climbing true false slippery true false injured true false 
variable elimination algorithm 
graphical illustration variable elimination computation equation equation clique tree 
building clique tree bayes net bayes net moralized graph triangulated graph clique tree 
sending message clique tree 
calibration algorithm 
state markov chain 
finding instantiations 
represents possible instantiations 
univariate gaussians oe oe oe note different scale axis 
dimensional gaussians independent variables correlated variables correlation coefficient ae 
cases variances 
contours correspond standard deviations 
linear cpd gamma 
xv example clg model sensor reading depends temperature sensor state joint distribution 
collapsing mixture gaussians 
clg moralized triangulated graph resulting clique tree clique tree strong root clique tree 
calibration strongly rooted tree 
simple clg clique tree strong root 
clgs strong triangulation leads exponential trees 
expectation propagation 
clg ep example 
network proof theorem 
example reduction mixture gaussians 
network proof theorem 
total probability mass taken hypotheses faults possible faults different fault probabilities 
clg example 
speeding gaussian generation previously created gaussians network experiments chapter 
results various algorithms random networks norm percentage runs hypothesis generated algorithm assignments 
norm function prior likelihood discrete assignment prior prior prior top percentage function prior likelihood discrete assignment prior prior prior 
xvi norm function parameter 
top percentage function parameter 
extended kalman filter adapted static bayesian networks numerical integration approach 
comparison gaussian approximations optimal approximation resulting distribution var var 
examples softmax cpds low gas level warning sign thermostat 
generalized softmax cpd 
approximating product gaussian logistic cpd gaussian 
incorporating softmax cpds evidence 
error introduced discrete neighbors clique sigmoid cpds 
error introduced sigmoid cpds entered separately 
inference algorithm augmented clgs 
experimental results error caused inserting cd cpds separately kl divergence discrete variables kl divergence continuous variables 
augmented clgs original crop network extended emission network extended crop network 
experimental results comparison likelihood weighting 
augmented clg reduction theorem distributions jx jx reduction 
highly simplified tbn monitoring vehicle 
unrolled dbn time slices 
xvii unrolling dbn state variables resulting clique tree 
gpb algorithm gpb algorithm imm algorithm 
simple hybrid dbn describing movement robot collapsing mixture different hypotheses 
collapsing mixture gaussians gaussians 
belief state representation belief state representation decomposed probability distribution discrete variables 
model dimensional robot including velocity 
results dimensional robot model kl divergence omniscient kalman filter likelihood correct discrete trajectory 
belief state hypothesis variables 
variables different subsystems assumed independent 
belief state hypothesis variables interdependencies sub systems 
prototype rwgs system 
rwgs schematic 
effects emptying water tank pressure difference 
sensor model 
random samples rwgs model gaussian estimates distribution 
comparison particle filtering simulated data 
heater shutdown enumeration algorithm omniscient kf heater shutdown rbpf omniscient kf 
heater shutdown hypothesis likelihood enumeration algorithm heater shutdown hypothesis likelihood rbpf 
hydrogen shutoff hypothesis likelihood enumeration algorithm xviii hydrogen shutoff hypothesis likelihood rbpf 
hydrogen shutoff probability broken sensor rbpf 
shutoff broken flow sensors hypothesis likelihood 
shutoff broken flow sensors flow estimates 
valves open plotted minutes steady state data 
steps original sequence steps sequence starting 
wrong flow sensor noise level standard deviation large oe standard deviation small oe 
modeling flow sensors bias variables 
flows shutoff entire sequence flows shutoff seconds predicted failures 
predictions flow sensors 
prediction flow sensors readings 
prediction flow sensors readings 
prediction measurements incoming flows 
xix xx chapter bayesian networks intelligent agent able perform reasoning uncertainty 
definitely case humans know lottery ticket buy winner know weather weeks know current blood pressure 
probability theory method choice dealing uncertainty science engineering disciplines control physics economics 
probability theory understood model extremely phenomena physical world 
provides simple clean semantics maintain beliefs face partial information update new evidence 
somewhat surprising years ai community probabilistic models 
important reason phenomenon lack compact ways represent probability distributions reason 
situation changed bayesian networks pea 
bayesian networks represent probability distribution graphical model directed acyclic graph dag 
node graph corresponds random variable domain annotated conditional probability distribution cpd defining conditional distribution variable chapter 
parents 
shall see chapter bayesian networks allow compact natural representation probability distributions 
combined suite inference learning algorithms bayesian networks models bayesian networks popular tool reasoning uncertainty 
successfully models wide variety complex domains medical diagnosis determining needs software users hbh predicting likelihood meeting attendance visual tracking troubleshooting microsoft windows filtering junk email music parsing rap 
fundamental issue bayesian networks problem inference 
bayesian network represents probability distribution required answer probabilistic query form computing probability distribution random variables evidence example medical diagnosis query probability certain disease observed symptoms meeting attendance domain query probability alice attend meeting office meeting scheduled hours troubleshooting domain query probability printer cable disconnected printer printing 
expect bayesian networks focused inference problem 
overwhelming majority considers case discrete bayesian networks networks contain discrete random variables 
progress inference problem fairly understood discrete networks 
high level inference problem divided main classes exact inference approximate inference 
exact inference algorithms designed give exact answer probabilistic query 
exact inference algorithms understand relations 
furthermore understanding complexity exact inference relates structure network structure dag 
chapter 
particular know simple network structures exact inference efficient 
approximate inference designed give approximate answer probabilistic query understanding exact probability crucial 
example medical diagnosis domain exact probability cancer symptoms practical point view probably matter approximate inference algorithm estimate exact value 
approximate inference fairly understood exact inference 
large set approximate inference algorithms useful exact inference intractable growing understanding computational complexity performance respect various properties network 
discrete networks inadequate important domains continuous attributes discrete ones 
examples include visual tracking domain variables location velocity continuous music parsing domain features extracted audio signal continuous 
addition thesis shall motivate domain fault diagnosis physical systems 
domain model physical system set measurements recorded various sensors asked diagnose system 
probabilistic terms need answer questions probability sensor malfunctioning readings readings sensors probability certain valve open sensor readings 
obviously physical systems contain discrete variables valve open closed pump working idle continuous variables pressures flows temperatures 
discretize continuous variables partitioning domain finite number subsets doing transform model discrete 
shall see chapter simple approach problematic lead unacceptable performance 
approach different treat continuous variables continuous trying discretize 
chapter 
level okay sensor flow valve simple example hybrid bayesian network hybrid network shows example simple hybrid bayesian network network contains discrete continuous variables 
discrete variables network represented rectangles continuous variables represented ovals 
particular example models flow liquid container 
representing direct probabilistic dependencies domain shall define formally chapter 
flow liquid represented random variable flow influenced random variables level representing liquid level container discrete variable valve representing valve lets liquid flow container open closed 
addition variable sensor represents value shown flow sensor measuring liquid flow container 
discrete variable okay represents flow sensor working properly 
example probabilistic query assume observe flow sensor indicates flow ask probability valve closed 
stated formally looking valve closed sensor 
note position valve directly influence flow sensor variables independent sensor indicates flow higher likelihood valve closed 
chapter 
example suppose additional information flow sensor faulty 
information position valve irrelevant measurement sensor faulty represent actual flow inferences position valve 
expect valve closed sensor okay false valve closed sensor important realize reasoning examples comes directly joint probability distribution domain variables 
words need create special reasoning rules information need encoded joint probability distribution 
represent probability distribution answer probabilistic queries get type sophisticated reasoning free probabilistic approach 
classification hybrid models classify hybrid models axes expressive power cpds static vs dynamic models 
axis criteria classifications ffl model allow non linear relations continuous variables 
linear relation means variable linear combination parents plus gaussian noise 
example reasonable model flow sensor example sensor flow oe represents white gaussian noise corresponding noise sensor 
hand obvious cases true dependencies non linear 
ffl model allow discrete nodes continuous parents 
example dependency consider discrete sensor indicates liquid level low low gas level chapter 
warning light car 
case natural discrete sensor variable influenced continuous liquid level variable 
popular class hybrid models known conditional linear gaussians clgs 
models answer questions negative answer clgs allow linear relations continuous variables allow discrete nodes continuous parents 
reason popularity models mathematical convenience 
clgs assignment discrete variables distribution continuous variables multivariate gaussian 
joint probability distribution mixture gaussians handled analytical tools 
thesis discuss clgs great detail devote chapters hybrid models violate clg restrictions 
second axis classification static vs temporal models 
example static temporal aspect model 
applications interested stochastic processes static distributions 
certainly case fault diagnosis deal system time 
example expect get sequence measurements sensor past readings current readings diagnosis 
bayesian networks extended dynamic bayesian networks dbns represent stochastic processes 
hybrid dbns impose linearity restrictions clgs called switching linear dynamic systems 
chapters thesis discuss hybrid dbns linear non linear application fault diagnosis 
contributions thesis concentrate inference problem hybrid networks different points view theoretical point view algorithmic point view practical point view 
theoretical point view little known hybrid bayesian networks obvious fact generalization discrete bayesian networks hard 
thesis provide novel complexity chapter 
analysis clgs 
show fundamental difference inference clgs inference discrete networks 
particular show simple network structures exact inference discrete models done linear time approximate inference hybrid models np hard 
algorithmic point view state art algorithm exact inference clgs lauritzen algorithm lau lj 
lauritzen algorithm clique tree algorithm ls ss hd originally developed discrete bayesian networks 
unfortunately generalization clique trees clgs straightforward initially common perception wrong cases lauritzen algorithm intractable simple network structures surprising view theoretical analysis 
approximate inference popular framework stochastic sampling 
framework applies class hybrid models just clgs 
algorithms sampling take long time converge reliable answer suited real time applications 
thesis provide large set new inference algorithms hybrid models 
cases main concern come tractable inference algorithms scale large hybrid models 
particular contributions ffl provide clear coherent presentation lauritzen algorithm discuss computational complexity 
ffl time algorithm inference clgs 
algorithm cases lauritzen algorithm tractable 
show empirically algorithm outperforms stochastic sampling algorithms 
ffl show non linear relations continuous variables handled numerical integration techniques finding gaussian approximation resulting distribution 
provide simple general efficient algorithm 
ffl provide systematic way deal networks involve discrete nodes continuous parents numerical integration techniques 
chapter 
particular show lauritzen algorithm generalized networks 
provide performance guarantees algorithm finds correct moments posterior distribution numerical integration errors 
ffl provide techniques inference large hybrid dbns 
classical inference algorithms scale proposed algorithm models contain tens hundreds variables 
algorithms easily combined numerical integration techniques handle non linear relations continuous variables 
practical point view hybrid models practice limited relatively simple models small number discrete variables 
thesis show real world application hybrid model richer complex model 
particular focus fault diagnosis domain tackle complex real world physical system originally designed extract oxygen martian atmosphere 
show algorithms perform fault diagnosis practically real time useful sensor readings missing 
important emphasize results real data collected actual runs system prototype synthetic data generated model 
outline chapters provide overview discrete hybrid bayesian networks 
chapter provide formal definition bayesian networks describe inference methods discrete networks 
discuss detail exact approximate inference algorithms form basis build thesis 
chapter provide quick overview normal distribution define discuss clg models 
give detailed description lauritzen algorithm exact inference clgs discuss computational complexity 
conclude discussing popular approximate inference algorithms clgs 
chapter 
chapter provide new complexity results clgs discuss implications 
show discrete models approximate inference clgs simple network structure np hard 
practice possible domain specific properties advantage 
chapter efficient approximate inference algorithm takes advantage skewed distributions common domains including fault diagnosis due low probability faults 
compare algorithm existing inference algorithms clgs 
chapters relax clg assumptions show perform approximate inference non linear models numerical integration techniques 
chapter show extend algorithm chapter handle models non linear dependencies continuous variables 
chapter show extend lauritzen algorithm models discrete nodes continuous parents discuss issues come want extend algorithm chapter models 
turn attention dynamic models 
chapter provide overview discrete hybrid dynamic bayesian networks current relevant inference algorithms 
chapter discuss scale algorithms larger complex models 
particular discuss issues come model contains large number variables non linear dependencies continuous variables 
chapter large complex non linear physical system designed extract oxygen martian atmosphere 
discuss various issues come trying model system hybrid dynamic bayesian network 
apply algorithms earlier thesis track state system perform fault diagnosis real data collected actual runs prototype system 
chapter contains discussion open questions concluding remarks 
material thesis appeared previously conference papers 
material chapter parts chapter appeared lp 
chapter new preliminary high level version ideas appeared chapter 
lms 
sections chapter closely follow presentation lsk 
chapter loosely presentation completely re written ideas chapter go content 
parts chapter lms results chapter new 
chapter bayesian networks chapter contains background material bayesian networks compact way representing probability distribution pea 
defining bayesian networks provide overview inference algorithms 
chapter concentrate discrete models models random variables discrete 
rest thesis shall concern hybrid models models contain continuous variables discrete ones 
notation denote random variables upper case letters actual value variables corresponding lower case letters 
denote sets variables bold face upper case letters values corresponding lower case letters 
dom denote set possible values take 
talk hybrid models containing continuous discrete variables convention assigning discrete variables letters alphabet assigning continuous variables letters alphabet 
delta delta delta refer discrete variables model gamma gamma gamma refer continuous ones 
notation denote probability distribution chapter 
bayesian networks probability distribution mass function discrete case density function continuous case 
notation shorthand denote probability takes value usual refers conditional distribution refers conditional distribution denote expectation distribution 
similarly note expectation function 
distribution expectation taken clear context shorthand notation 
bayesian networks notation par denote parents graph clear define bayesian networks section 
representation bayesian networks bayes nets compact representation joint probability distribution set variables bayes net consists parts ffl directed acyclic graph node corresponds random variable ffl set conditional probability distributions cpds node graph graph encodes structure joint probability distribution terms conditional independencies node independent non descendants parents 
node directly depends parents par set cpds parameterizes dependency 
case discrete bayes nets cpd node encodes probability distribution possible combination parents 
joint probability induced bayes net defined chain rule 
assume bayes net nodes representing random variables par chapter 
bayesian networks simple bayes net modeling john weekends 
nodes network weather sunny cloudy rainy deadline true false climbing true false slippery true false injured true false 
continue examine example way certain stanford student called john name changed protect innocent spends weekends 
factors influence john plans weekend weather sunny cloudy rainy john deadline 
factors john decides go climbing 
furthermore weather influences probability rocks slippery 
depending john goes climbing rocks slippery john certain probability get injured 
note john decides go climbing probability injured depend rocks slippery 
john weekend adventures modeled bayes net shown 
bayes net structure encodes conditional independencies cpds encode actual parameters distribution 
example probability injured independent weather independent weather parents climbing slippery 
example note weather deadline independent contrary conventional wisdom deadlines happen weather nice 
cpds encode parameters probability distribution example sunny true chapter 
bayesian networks sunny true 
note fact injured independent slippery john decides go climbing encoded structure bayes net cpd node injured 
examine simple examples probabilistic inference bayesian network 
suppose want know probability certain atomic event weather sunny deadline false climbing true slippery false injured false 
easily compute chain rule equation sunny false true true false sunny delta false delta true sunny false delta true sunny delta false true true delta delta delta delta obviously compute probability possible atomic event answer query 
example interested probability john getting injured weather sunny sunny true sunny true sunny delta delta sunny delta sunny delta true true false 
note sum different events general number terms consider chapter 
bayesian networks exponential size network 
return issue discuss inference bayes nets 
note able compute marginal automatically know compute conditional distributions conditional distribution ratio marginals ready formally define bayesian network 
definition bayesian network random variables fx pair hg directed acyclic graph node variable set conditional probability distributions conditional probability distribution par 
joint distribution represented chain rule equation 
exact inference system uses bayesian networks able perform probabilistic inference find distribution variables certain conditions 
bayes net variables typically need compute distribution called query variables evidence observations 
express conditional distribution ratio marginals start discussion assuming 
unfortunately limited case inference bayesian networks np hard 
theorem bayesian network variables general computing np hard jqj 
theorem proven relatively simple reduction sat problem gj problem bayes net inference coo 
cases take advantage structure bayes net parameters perform probabilistic inference approximate probabilistic inference efficiently 
section discuss methods 
chapter 
bayesian networks variable elimination seen perform probabilistic inference summing relevant terms compute chain rule 
example assume bayes net compute marginal probability distribution injured 
note expression involves types terms constants functions 
example term number term function variable functions called factors represented tables row table corresponds specific instantiation function variables 
example function specific values true false represented true false false true alternatively think terms equation functions involving variable variable view term represented factor chapter 
bayesian networks false false false false false true false true false false true true true false false true false true true true false true true true evaluate equation need able perform operations ffl multiplying factors ffl performing summation variables factor multiply factors need create factor variables union variables original factors 
consider table entry corresponding assignment original factors precisely entry consistent table entry defined product tables entries sum variables factor need create factor remaining variables gamma consider table entry corresponding assignment consistent table entry denoted define value sum matching table entries 
shall soon consider examples multiplication summation crucial observation 
want evaluate equation written need multiply factors resulting factor variables general follow pattern equation bayesian network factor variables domain size exponential number variables 
clearly chapter 
bayesian networks able approach large bayes nets hundreds variables medium bayes nets tens variables 
key observation making bayes net inference tractable rewrite equation re arranging terms pushing summations inside examine evaluation equation 
start evaluating product 
represent factors false true false sunny false false sunny true false cloudy false false cloudy true false rainy false false rainy true true sunny false true sunny true true cloudy false true cloudy true true rainy false true rainy true chapter 
bayesian networks result multiplying factors false sunny false false sunny true false cloudy false false cloudy true false rainy false false rainy true true sunny false true sunny true true cloudy false true cloudy true true rainy false true rainy true example line corresponds line factors accordingly delta 
evaluate result factor false sunny false cloudy false rainy true sunny true cloudy true rainy example second line corresponds third fourth lines accordingly 
left expression chapter 
bayesian networks note eliminated variable appear computations 
early elimination variables key avoiding creation exponentially large factors 
continue fashion 
convert factors multiply resulting factor 
eliminate resulting factor 
step convert multiply resulting 
sum leading answer note entries sum expected marginal distribution false true sum performed computation largest factor deal having variables entries 
better directly equation led factor variables entries 
larger networks savings usually significant 
ready state variable elimination algorithm bb zp 
convert cpds set factors long contains variables appear query choose variable eliminate 
elimination done extracting factors belongs multiplying summing returning result variables left chapter 
bayesian networks variable elimination fp set cpds query variables 
factor representation 
ff set factors 
set random variables 
gamma 

choose variable set gamma fy 
extract factors mentioning 

result summing 
add 

return variable elimination algorithm query variables multiply remaining factors return resulting factor query result 
shows pseudo code variable elimination algorithm 
complexity variable elimination algorithm determined size factors generated way 
turn depends elimination order choose 
example eliminate variable get factor larger factor 
unfortunately simple greedy heuristics choosing elimination order hd finding optimal elimination order resulting smallest factors np hard acp 
furthermore networks optimal elimination order leads factors exponential size bayes net 
cases exact inference methods resort approximations 
variable elimination algorithm especially clique tree algorithm general best known methods exact inference bayesian networks inference non trivial networks practice 
chapter 
bayesian networks handling evidence easy generalize variable elimination algorithm deal evidence 
observation expressed adding indicator function chain rule 
example assume want compute observation true 
write true delta true delta true true true effectively removing terms consistent evidence 
add evidence variable elimination algorithm simply translate indicator function factor observed variable entry consistent evidence add example convert true factor false true result factor false true note entries sum 
reason result represents true probability true 
interested conditional probability true simply need divide result true need normalize chapter 
bayesian networks graphical illustration variable elimination computation equation equation clique tree factor sum 
normalized result false true note efficient ways handle evidence doing variable elimination 
adding extra factor modify factors coming cpds account evidence leading computations efficient time space 
took approach adding extra factor approach clique tree inference discuss 
clique trees clique tree algorithm ls ss hd known join tree cluster tree junction tree algorithm inference algorithm variable elimination algorithm 
main advantages ffl want compute marginal need run variable elimination algorithm multiple times 
clique trees compute multiple marginals performing twice computations needed marginal variable elimination 
ffl data structures clique trees usually lead efficient implementations algorithm direct implementations variable elimination 
chapter 
bayesian networks take look way evaluated equation variable elimination summarized equation equation 
computation interleaving steps 
step equation equation equation multiplied factors 
factors cpds factors generated earlier computations 
second step equation equation equation marginalized results step smaller factors 
think steps operation factors output producing factor input 
convenient visualize graphical illustration shows graphical illustration equation equation 
clique tree tree viewed graph induced variable elimination computation 
node graph corresponds operation multiplying factors associated variables factor 
edge graph corresponds factor generated summing variable connects node factor originated node factor 
edge associated variables summed factor turn intersection variables nodes connects 
example clique tree induced computation illustrated 
definition clique tree bayesian network random variables undirected acyclic graph 
node graph called clique cluster associated variables edge graph associated non empty intersection variables cliques connects 
clique associated factor phi called potential edge associated factor phi called sepset variables 
clique tree obeys properties ffl cpd variables exists clique ffl cliques node path connecting domain 
property known chapter 
bayesian networks building clique tree bayes net bayes net moralized graph triangulated graph clique tree running intersection property 
clique tree performing probabilistic inference clique tree viewed message passing algorithm 
start discussing compute marginal distribution certain clique called root node extend message passing algorithm compute probability distribution cliques 
initialization step incorporating cpds clique tree 
convert cpds factors factor choose clique contains variables factor 
initialize potential clique product cpds associated 
cpd belongs clique set potential value entry 
relevant information tree compute distribution root node 
computation associated variable elimination algorithm performed message passing algorithm messages passed tree starting leaves going root illustrated root 
scheme node needs send message neighbor path root 
needs receive messages neighbors multiply potential sum variables chapter 
bayesian networks example assume want compute distribution 
sequence messages step clique clique operation initialize potentials cpds phi phi delta phi phi phi delta phi phi phi delta phi phi phi delta phi note possible order 
example reverse steps reverse steps send message receiving messages 
assume want compute marginal distribution node clique tree just root 
repeat process just outlined node tree better 
continuing example assume having computed marginal probability interested computing marginal probability done big part steps computation useful computing distribution goal reuse repeat naively think need create message phi multiply phi 
unfortunately incorrect 
reason phi multiplied message note original factor multiplying phi orig phi new factor multiplying need send message phi orig current potential get message phi new 
fortunately simple solution problem phi orig phi orig delta phi orig delta phi new division factors defined just product factors create chapter 
bayesian networks message passing clique tree sepset 
gammas phi sum 
phi divide message sent 
phi store message 
phi phi delta multiply message sending message clique tree calibration clique tree 
initialize potential product relevant cpds 
initialize sepset factor entries set 
directed pairs adjacent nodes hc 

hc got messages neighbors 
send message protocol 
gamma ig 
calibration algorithm factor union variables multiplying relevant entries divide 
furthermore purposes dividing factors define 
equation see order send message backwards tree save message sent root 
recall associated edge sepset factor variables edge 
sepsets store messages sent tree 
summarizes operation sending message node node clique tree 
ready calibration algorithm shown 
message passing protocol node sends message neighbors receiving message neighbors total gamma messages nodes connected gamma sepsets 
note possible send gamma messages example choose node root send gamma messages root send gamma messages root leaves 
sending messages root called upward pass upstream pass 
sending messages leaves called downward pass chapter 
bayesian networks downstream pass 
theorem running calibration algorithm potential tree contains correct marginal distribution variables 
furthermore sepsets contain correct marginal distribution variables 
proof show potentials correct marginal distribution 
recall seen performing upward pass get correct marginal distribution root 
show calibration algorithm multiplies potential factors multiplied root upward pass 
proof goes induction number message 
message claim trivial dividing sepset entries set change message 
th message going cases 
case send message sepset set 
case division change message induction hypothesis correct message 
second case sent message argument equation get message sent message sent upward pass 
left show sepsets correct marginal distributions 
sepset wlog assume sent message sent message set marginal distribution sent message 
point received messages neighbors assumed received message received message contained correct marginal distribution 
follows message stored correct marginal distribution 
clique trees probability distribution clique trees just efficient way implementing variable elimination algorithm 
just bayes nets clique trees graphical model chapter 
bayesian networks represents joint probability distribution pea product potentials divided product sepsets 
definition clique tree variables represents joint probability distribution phi phi note view leads alternative way justify clique tree algorithm 
initially cpds cliques sepsets set 
initialization step distribution represented clique tree product cpds distribution represented bayes net 
easy verify equation invariant message passing described pass messages tree maintain correct joint distribution represented message passing algorithm represents probability distribution represented bayes net 
handling evidence introduce evidence clique tree technique indicator functions variable elimination 
evidence treat multiplying tree indicator functions 
convert function factor find clique contains multiply factor 
calibrate tree clique contain marginal distribution variable observing evidence 
get conditional distribution evidence simply need re normalize cliques sepsets 
bayes net clique tree conclude examining process building clique tree bayes net closely 
build clique tree simply run variable chapter 
bayesian networks elimination algorithm computing factors just determine factors involve variables 
convenient view process different way leading insights choose elimination ordering 
idea transform undirected graph random variable 
edge corresponds variables appear factor 
factors generated variable elimination algorithm correspond maximal cliques definition set variables factor iff clique 
graph constructed steps moralization triangulation 
moralization step build graph gm connecting variables appear cpd appear factor 
obviously edge undirected edge gm addition parents node add edge appear cpd 
example bayes net shows moralized graph 
note example nodes married 
gm contains clique cpd necessarily generate intermediate factors eliminate variables 
building graph add edges gm chosen elimination order variables appearing intermediate factors connected simplest way construct initially set gm pick elimination order 
eliminate variable add edges neighbors eliminated create clique non eliminated neighbors 
reason simple eliminate multiply factors appears 
variables factors exactly neighbors eliminated 
multiplying force factor need put clique 
construction source name clique tree 
connecting parents leads name moralization 
chapter 
bayesian networks process described result graph chordal 
intuitively chordal graph graph cycle length contains shortcut edge non adjacent nodes 
cycles broken cycles length called triangulated 
definition undirected graph chordal triangulated cycles length bigger chord edge part cycle connecting nodes cycle 
words subgraph isomorphic cycle length 
reason chordal graph simple suppose point contains cycle length eliminated variable cycle 
eliminating add edge neighbors non adjacent nodes cycle 
theorem states converse true 
details algorithm extract elimination order chordal graph see ty 
theorem undirected graph contains moralized graph gm em subgraph em maximal cliques correspond set factors variable elimination gm iff chordal 
process undirected graph making chordal adding edges called triangulation 
second phase algorithm triangulating shows example moral graph shown triangulated resulting graph 
finding optimal triangulation induces small cliques np hard surprise finding optimal elimination order np hard 
simple heuristics tend produce triangulation real life bayes net 
simple heuristic triangulating graph elimination order time greedily eliminate variable adds smallest number new edges summarize transforming bayes net clique tree done steps illustrated chapter 
bayesian networks ffl building moralized graph gm ffl triangulating moralized graph variable elimination different algorithm 
ffl finding maximal cliques triangulated graph 
graph triangulated step easy efficient algorithm find maximal cliques triangulated graph ty 
case triangulate graph variable elimination simply record cliques created elimination process discarding cliques subsets cliques 
ffl connecting cliques way preserves running intersection property 
need connect cliques variables common 
done maximal spanning tree algorithm clr weight edge number random variables non empty domain edge connects cliques variables weight jx sampling techniques mentioned exact inference bayesian networks intractable problem np hard coo 
factors involved variable elimination algorithm clique tree algorithm large handle efficiently look alternative approaches 
obvious alternative exact inference approximate inference settle answer approximate 
general approximate inference bayesian networks np hard dl important cases exact inference tractable exists efficient approximate inference algorithm leads provably approximations 
section concentrate sampling approximate inference algorithms 
idea sampling methods randomly pick assignments random variables assignments called samples estimate various properties joint distribution samples 
formally assume joint distribution want chapter 
bayesian networks evaluate expectation function distribution interested evaluating 
important note formulation general 
particular evidence reduce problem computing probability form 
define indicator function recall section indicator function agrees 
write delta return problem estimating 
define random variable 
assume oe oe def var 
assume sequence 
independent identically distributed samples drawn distribution 
consider sum central limit theorem large sum normal distribution mean variance oe sum converges rate convergence proportional words way answer general probabilistic queries drawing samples distribution importance sampling previous section discussed problem estimating expectation joint distribution 
problems involve chapter 
bayesian networks observations interested expectation observations want estimate 
problem defined assume 
straightforward way solve problem sampling methods called rejection sampling 
write expectations compute standard sampling method 
example order compute generate samples discard samples agree remaining samples estimate expectation 
problem approach convergence rate may low 
samples expect consistent evidence effectively np samples compute expectation 
low especially dimension high case rejection sampling inefficient 
furthermore contains continuous variable probability sampling exactly value zero discard samples 
overcome difficulty desirable draw samples directly possible 
generally case distribution sample 
importance sampling general way dealing problem simple observation 
arbitrary distribution 
eq samples drawn define compute evaluate chapter 
bayesian networks cp unknown constant evaluate unknown constant 
case estimator equation slightly different estimator 
define weights notice weight unbiased estimator eq rewriting equation get eq wf eq samples get estimator note known replace return equation 
estimator equation biased estimator division 
hand usually smaller variance estimator equation preferred practice known 
matter choose equation equation obvious question answer order importance sampling choose distribution typical cases estimator unbiased relatively small bias compared variance reasonable criterion choose minimize variance estimator 
analysis estimator variance see example liu shows variance estimator related variance weights general lower variance weights lower variance estimator extreme case variance weights 
analysis implies actual distribution choice importance distribution general close possible chapter 
bayesian networks different variance weights high intuitively small number samples account entire probability mass samples contribute little estimation irrelevant 
worst case small fraction samples useful making importance sampling inefficient compared sampling correct distribution 
likelihood weighting likelihood weighting lw sp popular way apply importance sampling inference bayesian networks 
assume bayes net variables evidence generate samples posterior distribution easy evaluate conditional distribution usually know normalization factor 
evaluating easy inconsistent compute directly chain rule 
function equal unknown constant estimator equation 
remains define sampling distribution bayes net construct new bayes net disconnecting evidence nodes parents set cpd deterministic cpd value example node observed value true parents cpd true 
represents sampling distribution note sampling evidence nodes influence distribution descendents original bayes net influence distribution ancestors 
obvious general represent true posterior distribution evidence intuitively better approximation posterior prior distribution accounts changes induced evidence 
chapter 
bayesian networks consider sample consistent par distribution parents par distribution parents consistent evidence chain rule get par par par par par par observed nodes cpd deterministic par 
hidden nodes par par 
par practice need construct sample directly process nodes topological order 
node unobserved sample value cpd value sampled parents 
node observed set observed value multiply sample weight par 
easy verify process par 
recall general efficiency importance sampling depends close sampling distribution represented likelihood weighting true posterior distribution 
best case evidence root nodes case represents exactly posterior distribution 
case sample correct distribution get best rate convergence 
hand evidence leaf nodes sample prior distribution effectiveness sampling process depends prior distribution approximates posterior distribution 
case get low convergence rate especially evidence making posterior distribution different prior distribution 
chapter 
bayesian networks markov chain monte carlo gibbs sampling main competitor lw markov chain monte carlo mcmc 
mcmc sampling method sensitive probability evidence evidence 
mcmc theory markov chains provide short markov chains 
markov chain stochastic process evolving discrete time steps 
denote state process various time steps 
purposes limit finite chains chains number possible states finite 
distribution initial distribution 
distribution past depends conditional distribution called transition probability 
transition probability depend process called stationary process 
purposes consider stationary processes 
definition invariant stationary distribution markov chain distribution persists forever reached stationary distribution iff state example consider markov chain 
just variable states 
example 
reader verify markov chain stationary distribution 
interested markov chains unique stationary distribution furthermore want process converge regardless initial distribution 
characterize markov chains define concept regular chains 
definition finite markov chain regular exists states probability state getting state chapter 
bayesian networks state markov chain exactly steps larger zero possible get state state exactly steps 
finite chains regularity condition equivalent property called ergodicity define general case 
ready state fundamental theorem markov chains theorem ergodic markov chain converges unique stationary distribution regardless initial distribution 
proof theorem details see example tk nea 
example markov chain ergodic possible get state state exactly steps stationary distribution unique 
markov chain monte carlo algorithm uses markov chains order generate samples approximate inference 
idea construct ergodic chain stationary distribution distribution want sample 
run chain sequence states samples 
issues need addressed formal definition ergodic chains complicated applies infinite state spaces concerned finite state spaces identify regular chains ergodic chains 
chapter 
bayesian networks ffl construct ergodic chain desired stationary distribution 
ffl samples generated chain answer second question 
start choosing random assignment initial distribution 
chain run steps distribution state chain close stationary distribution 
initial phase called burn phase 
pick state chain sample 
chain perform transition sample general samples correlated desire 
alternatively chain run steps picking sample order reduce correlation samples 
samples collect run chain estimate desired quantity just samples lw 
note larger weaker correlation samples pay penalty terms computational cost 
practice chosen cost generating samples low compared cost samples 
turn attention task constructing markov chain desired stationary distribution 
task may sound difficult practical cases possible quite easily 
general technique constructing markov chains called metropolis algorithm mrr 
concentrate specific technique called gibbs sampling gg convenient bayesian networks 
gibbs sampling starts generating random sample consistent evidence likelihood weighting 
run markov chain step randomly choose unobserved variable sample value current assignment variables fact consider cpds involve 
formally sample gamma sampling done efficiently markov blanket known need create factor multiply relevant values chapter 
bayesian networks cpds 
sufficient necessary condition ensure gibbs sampling results ergodic markov chain sample non zero probability assigned possible values ensures positive probability get state state moves 
particular zero entries cpds chain ergodic 
furthermore chain ergodic easy verify equation stationary distribution chain posterior distribution 
crucial factor analyzing markov chains mixing rate rate chain converges stationary distribution 
recall start chain burn phase mixing rate low burn phase long avoid generating samples wrong distribution 
need generate large number samples able 
furthermore mixing rate low samples generate burn phase correlated need large number order get reliable estimate 
example consider chain assume probabilities staying states probabilities leaving just 
start state need expectation samples order move state 
intuitively clear need samples order approximately right proportion samples drawn states 
unfortunately task determining mixing rate gibbs sampler bayesian network quite difficult 
simple decide chain mixed certain burn trajectory 
questions start samples samples needed reliable estimate hard gibbs sampling non trivial task 
note performance gibbs sampling degrade evidence 
gibbs sampling works situations exact inference possible likelihood weighting performs poorly 
chapter 
bayesian networks probable explanations far discussed problem finding marginal probability distribution variables evidence 
consider different problem called probable explanation mpe problem 
mpe problem bayes net evidence asked find configuration remaining variables evidence best explanation evidence 
important generalization mpe problem find probable explanations 
finding mpe note evidence value certain variable value maximizes necessarily part mpe assignment simple greedy algorithms pick value variable time 
example consider simple bayesian network binary cpds value mpe case probability theta assignment involving probability theta 
easy adapt variable elimination clique tree algorithms mpe problem 
consider case 
bayes net variables write max max par note similarity equation equation summation operation replaced maximization operation 
chapter 
bayesian networks technique variable elimination push inside 
example want find mpe bayes net write max max max max note similarity equation equation 
variable elimination algorithm solve mpe problem summation operations replaced maximization operations 
clique tree algorithm variable elimination algorithm idea replacing summations get version clique tree algorithm solves mpe problem 
evidence handled usual way multiplying potentials indicator functions 
calibration algorithm mpe called max calibration potentials clique tree called max potentials 
entry max potential represents probability assignment network variables consistent particular entry 
sepset cliques result maximizing variables sepset maxc gammac maxc gammac proofs similar proofs standard clique tree algorithm daw jen 
performing max calibration easy task extract mpe assignment clique tree 
arbitrarily choose cliques root find mpe downstream pass 
suppose order cliques root 
choose assignment variables clique get clique choose assignment consistent partial assignment gamma ties broken arbitrarily 
easy verify algorithm result assignment mpe 
assume unique mpe probability projection max potential probability larger entry max potential chosen algorithm 
situation cliques similar result returned algorithm argument generalizes case non unique mpe showing algorithm choose mpe assignments key insight chapter 
bayesian networks finding assignments 
build tree perform max calibration 
find mpe 
initialize heap hp 

extract top heap element hp yi 
report th instantiation 
partition gamma fxg subsets 
non empty 
find assignment 
insert hp heap finding instantiations 
represents possible instantiations 
way order cliques algorithm consistent assignment easy verify assignment 
finding explanations possible generalize max calibration algorithm generate assignments just single assignment nil 
assume max calibrated tree cliques sorted way clique path appears order easily find order example dfs traversal tree 
set possible instantiations set instantiations generated algorithm initially 
idea algorithm shown partition gamma subsets ym subset keep instantiation note instantiation gamma instantiation candidates xm keep candidates heap sorted likelihood instantiations step top element heap represents instantiation 
step extract top element heap report instantiation 
chapter 
bayesian networks refine partition scheme partitioning gamma fx new subset find assignment insert heap 
obvious question define partition scheme 
partition scheme properties ffl number subsets large ffl easy find instantiation subset note properties tend conflicting 
extreme keep instantiations set get property second keep small subsets contains just instantiation second property 
heart algorithm identifying partition scheme way maintain 
nilsson nil suggests associate subset clique defined assignment variables cliques gamma set forbidden assignments variables gamma gamma denote subset assign 
important sure subsets empty intersection 
subset assign consistent belongs forbidden assignments example consider tree assume variables binary 
possible subset corresponds defined assign hc hi fag instantiations fha hi fagg 
subset corresponds assign hi fcg 
easy verify empty intersection instantiation instantiation example consider subset assign hi fcg 
empty intersection assignment assignment empty intersection assignment forbidden assignment question course come partition 
recall iteration algorithm assignments subsets 
candidates find assignment chapter 
bayesian networks partition gamma fx subset operations need explained find instantiation partition gamma fxg way sure new subsets empty intersection 
explain find instantiation assign 
assignment variables gamma greedily choose assignment consistent gamma assignment appear set forbidden assignments continue cliques time choosing assignment consistent current partial assignment resulting answer proof similar argument section 
compute likelihood note equation holds max calibration invariant message passing protocol just regular calibration 
compute probability equation time linear number cliques 
turn attention task partitioning gamma fxg assign 
notation denote assignment restricted variables particular note assign gamma partitioning gamma fxg involves creation gamma new subsets 
assign gamma fx gammac gamma words add assignment set forbidden assignments creating subset agrees different assignment obviously may instantiations agree different add subset assign gamma gammac gamma instantiations agree gamma different example consider tree set assign hc hi fag 
assume assignment turned ha hi 
partition set gamma fxg subsets ffl assign hc hi fa ag ffl assign ha hi fbg chapter 
bayesian networks ffl assign ha hi dg variables binary set empty discarded 
second set contains assignments ha hi ha hi 
third set contains just assignment ha hi 
created partition gamma fxg 
step algorithm create jcj new subsets 
order generate assignments create nk assignments 
careful way find assignment compute probability perform operations jxj jxj number variables bayes net 
operations lines done 
inserting element heap extracting element heap done time log number heap elements operations lines done nk log nk 
addition need create max calibrate tree done jt jt total size tree including potentials 
total complexity algorithm jt nk log nk chapter hybrid bayesian networks far considered models include discrete variables 
real world domains include continuous variables discrete ones 
domains known hybrid domains 
examples domains include target tracking continuous variables represent state targets discrete variables model maneuver type visual tracking continuous variables represent positions various body parts person discrete variables type movement speech recognition jel ch discrete phoneme determines distribution acoustic signal 
thesis focus domain fault diagnosis physical system contains continuous variables flows pressures discrete variables failure events 
chapter discuss models represent probability distribution discrete continuous variables 
short review normal distribution forms basis models thesis 
discuss bayesian networks include continuous variables focus bayes nets represent probability distribution mixture gaussians 
chapter 
hybrid bayesian networks normal distribution family normal distributions called gaussian distribution just gaussians far important family distributions probability statistics disciplines 
main reasons account popularity normal distribution ffl normal distribution comes central limit theorem 
random variables define weak technical conditions distribution converges normal distribution 
ffl normal distributions distributions approximated normal distribution arise naturally real world situations play key role fields physics biology social sciences 
ffl mathematical theory family simple tractable 
family closed operations summation multiplication conditioning analytical closed form 
univariate case normal distribution characterized parameters mean variance oe mathematical convenience parameterize distribution variance square standard deviation 
density function form oe oe exp gamma gamma oe note notation oe denote variable normal distribution mean variance oe notation oe explicitly noting random variable 
intuitively mean parameter controls location gaussian value gaussian gets maximum value 
variance parameter oe controls peaked gaussian smaller variance peaked gaussian 
formally mean variance correspond moments chapter 
hybrid bayesian networks univariate gaussians oe oe oe note different scale axis normal distribution oe gamma shows examples gaussians different values mean variance 
note density larger probability distribution discrete variables constraint integral density function 
multivariate case normal distribution characterized parameters mean vector covariance matrix 
precisely random variables said normal distribution sigma vector size sigma symmetric positive definite matrix size theta sigma sigmaj exp gamma gamma sigma gamma gamma shows examples dimensional gaussians 
intuitively contours multivariate gaussians look ellipsoids mean ellipsoid constant density 
mean vector determines center ellipsoids covariance matrix determines shape 
general sigma represents variance sigma sigma represents covariance mean vector covariance matrix correspond moments normal distribution sigma xx gamma give short overview important properties normal distribution 
details proofs theorems see introductory textbook statistics probability 
particular see sections chapter 
hybrid bayesian networks textbook stone sto 
theorem joint normal distribution sigma 
independent iff sigma 
non diagonal elements sigma zero variables independent say gaussian spherical 
show spherical gaussian independent show example correlated note different scale axis 
consider joint normal distributions fx ir ir shall notation sigma xx sigma xy sigma sigma ir ir sigma xx matrix size theta sigma xy matrix size theta sigma xy sigma matrix size theta sigma matrix size theta theorem fx joint normal distribution defined equation 
marginal distribution normal distribution sigma 
important properties family normal distributions closed linear combinations 
theorem joint normal distribution sigma 
fi ir fififi fi fi ir fififi oe 
define random variable fi fififi oe 
ffl conditional distribution normal fi fififi oe ffl distribution normal distribution oe fi fififi oe oe fififi chapter 
hybrid bayesian networks dimensional gaussians independent variables correlated variables correlation coefficient ae 
cases variances 
contours correspond standard deviations 
chapter 
hybrid bayesian networks ffl furthermore joint distribution fx normal distribution cov fififi sigma converse theorem true result conditioning normal distribution linear dependency conditioning variables 
theorem fx joint normal distribution defined equation 
conditional distribution normal distribution sigma sigma sigma gamma xx gamma sigma sigma gamma sigma sigma gamma xx sigma xy particular theorem jy get corollary corollary fx joint normal distribution defined equation 
conditional distribution normal distribution fi fififi oe fi gamma sigma sigma gamma xx fififi sigma sigma gamma xx oe sigma gamma sigma sigma gamma xx sigma xy linear gaussians definition linear gaussians view corollary way convert multivariate gaussian bayesian network 
order variables order corollary find conditional distribution gamma fi gamma fi oe chapter 
hybrid bayesian networks linear cpd gamma create edge iff fi 
cpd called linear cpd form equation dropping zero fi 
note linear cpd root nodes simply univariate gaussian 
bayes net cpds linear called linear gaussian lg 
multivariate gaussian represented linear gaussian 
converse true 
bayes net linear cpds represents normal joint distribution recovered theorem 
canonical forms perform inference lgs ways 
simply convert lg multivariate gaussian operations 
adapt variable elimination algorithm clique tree algorithm section 
main difference factors represented tables anymore 
naively think represent factors gaussians case 
reason linear cpds gaussians conditional chapter 
hybrid bayesian networks distribution number parents zero 
shows example cpd gamma 
shape function ridge line gamma width ridge determined variance case 
deal problem representing linear cpds different representation called canonical form canonical characteristics lau mur 
canonical form represents function form quadratic function 
precisely define omit random variables def exp gamma kx represent gaussian canonical form 
rewriting equation get sigmaj exp gamma gamma sigma gamma gamma exp gamma sigma gamma sigma gamma gamma sigma gamma gamma log sigmaj sigma sigma gamma sigma gamma gamma sigma gamma gamma log sigmaj canonical forms general gaussians invertible canonical form defined inverse legal covariance matrix 
particular represent cpds shown canonical forms recall gamma exp gamma gamma gamma 
possible perform various operations canonical forms ffl canonical form initialized setting 
chapter 
hybrid bayesian networks ffl multiplication easy 
equation get delta ffl division defined similar way multiplication gamma gamma gamma ffl canonical forms extended defined superset variables increasing dimensions setting extra entries zeros 
ffl canonical form fx kxy ky hx hy define marginalization variables dy resulting function integral finite iff ky positive definite inverse legal covariance matrix case result canonical form lau gamma kxy gamma hx gamma kxy gamma hy jy log gamma log ky hy ffl possible enter evidence canonical form set values variables 
result canonical form unobserved variables 
assume canonical form equation setting results canonical form mur hx gamma kxy chapter 
hybrid bayesian networks gamma ky perform operations canonical forms adapt clique tree algorithm section linear gaussians representing factors canonical forms 
issues addressed 
issue possible marginalize canonical forms 
fortunately problem 
recall need marginalize comes sending message clique variables clique variables 
need compute marginal variables need sum gamma canonical form clique just sending message canonical form correct marginal distribution sending message received messages neighbors factors contain domain multiplied quite easy show implies ky submatrices 
marginal distribution gaussian positive definite positive definite 
follows ky positive definite marginalization operation possible 
issue discrete factors instantiate evidence evidence variables longer part canonical form 
easy adapt clique tree algorithm case directly instantiate evidence clique sepset contains observed variables reduce dimension cliques sepsets include observed variables 
conditional forms canonical forms representation clique tree algorithm problems ffl represent deterministic linear relations gamma covariance matrix sigma invertible 
chapter 
hybrid bayesian networks ffl numerically unstable lead serious numerical errors 
second problem serious concern led alternative representation canonical forms introduced lj 
call representation conditional forms 
definition set continuous variables jhj jt conditional form represents conditional distribution form delta bt vector dimension matrix dimensions theta matrix dimensions theta conditional form represents deterministic relation furthermore easily represent linear cpd par conditional forms par 
redundant conditional form represents multivariate gaussian 
operations involved clique tree algorithm performed conditional forms lj leading variant algorithm uses conditional forms main representation 
conditional forms numerical problems canonical forms suffer preferred implementing clique tree algorithms linear gaussian networks conditional linear gaussian networks discussed section 
operations conditional forms complex add extra layer complication algorithms 
shall explain algorithms context canonical forms keeping mind discussion carries conditional forms 
conditional linear gaussians lgs form basis models shall explore thesis 
section discuss model conditional linear chapter 
hybrid bayesian networks gaussians clgs 
clgs bayesian networks combine discrete continuous variables represent richer class distributions lgs discrete bayes nets 
definition clgs definition conditional linear gaussian clg bayesian network containing continuous variables denoted gamma gamma gamma discrete variables denoted delta delta delta restrictions ffl discrete node continuous parent cpds discrete nodes represented discrete bns 
ffl cpd continuous variable linear cpd combination discrete parents 
formally node parents fx gamma gamma gamma fd delta delta delta define cpd parameters dom fi fi oe cpd defined fi fi oe note discrete variables cpds continuous variables linear cpds 
assignment discrete variables clg reduced lg represents normal distribution 
follows joint distribution represented clg mixture gaussians mixture component corresponds instantiation discrete variables 
example simple clg consider network 
thesis convention hybrid bayesian networks discrete variables depicted squares rectangles continuous variables depicted circles ellipses 
simplified model room temperature sensor 
variable room temperature prior distribution mean standard deviation discrete variable ok indicates sensor working properly faulty 
probability ok true 
actual reading sensor depends temperature chapter 
hybrid bayesian networks ok example clg model sensor reading depends temperature sensor state joint distribution sensor state 
sensor working properly approximately equal actual temperature adds gaussian noise standard deviation sensor faulty reading depend actual temperature typically shows values large uncertainty standard deviation 
behavior sensor modeled cpd ok ok true ok false resulting distribution marginalizing ok shown 
distribution mixture gaussians 
peaked gaussian represents case ok true note density gaussian goes scale axis gaussian corresponds ok false 
chapter 
hybrid bayesian networks representation factors usual interested problem probabilistic inference take approach offered clique tree algorithm 
shall develop version clique tree algorithm suited clgs proposed lau called lauritzen algorithm 
key decision adapting clique tree algorithm clgs represent clique potentials sepsets particular represent functions continuous variables 
shows example distribution continuous variable case marginal distribution mixture gaussians 
general marginal distribution subset variables fd xg delta delta delta gamma gamma gamma expressed mixture gaussians assignment sum probabilities mixture components certain represents probability 
note discrete variables network get single gaussian mixture 
leads natural way represent intermediate function fd xg table entry possible assignment entry contains mixture gaussians continuous variables 
mixture gaussians variables represented set pairs hw sigma weight th mixture component 
say mixture normalized case represents probability density function probability normal distribution sigma 
alternatively represent mixture canonical forms defined equation 
case weight part canonical form incorporating parameter add log unfortunately natural representation serious problems ffl size representation fixed vary actual run algorithm 
example function continuous variable simple weight univariate gaussian complex mixture jdom delta delta delta components 
ffl operations clique tree algorithm defined 
chapter 
hybrid bayesian networks gaussians mixture density collapsing collapsing mixture gaussians particular dividing mixtures gaussians result mixture gaussians closed form 
furthermore marginalization defined mixtures canonical forms 
consider alternative approach mixture representation 
represent function fd xg table entry assignment entry contains just canonical form mixture 
refer data structure canonical factors 
canonical factors obvious problem just stated marginal distribution continuous variables mixture gaussians canonical factors allow represent mixtures relevant discrete variables included domain 
represent mixtures discrete variables domain function 
answer 
shall approximate mixtures represent just gaussian 
example shown showing mixture gaussians 
representing actual density shall keep just gaussian result collapsing original gaussians 
collapsing operation defined 
recall gaussians parameterized moments mean vector covariance matrix 
chapter 
hybrid bayesian networks best approximate mixture come gaussian mean vector covariance matrix entire mixture 
theorem lau tells collapse mixture gaussians 
theorem density function normalized mixture gaussians hw sigma sigma normal distribution defined sigma sigma gamma gamma moments means covariances furthermore minimizes kl divergence normal distribution normal distribution sigma equality iff proof easy verify moments directly computing moments prove second part theorem write log dx log dx log gamma log dx gaussian distributions log log quadratic polynomials 
write log gamma log ax moments expectation quadratic polynomial respect log gamma log dx log gamma log dx chapter 
hybrid bayesian networks equality iff note covariance matrix defined collapsing operation terms weighted average covariance matrices mixture components second term corresponds distances means mixture components larger distances larger space mixture components larger variances new covariance matrix 
approximating mixture gaussians gaussian quality approximation depends close mixture density single multivariate gaussian general quite bad 
reasons considered reasonable representation clique tree algorithm ffl shall see results clique tree algorithm correct moments continuous variables correct distribution discrete variables 
applications answer questions interest 
ffl insist finding exact marginal distributions collapsing add relevant discrete variables cliques sepsets cost extra space running time 
view canonical factors representation flexible representation offering spectrum approximations trading efficiency accuracy 
having chosen representation potentials sepsets need verify perform various operations required algorithms 
important exception marginalization shall soon discuss operations natural generalization operations defined discrete factors operations defined canonical form unfortunately shall see efficient approximation impractical 
chapter 
hybrid bayesian networks ffl extend domain canonical factor adding discrete variables just discrete factors adding continuous variables canonical form defined section 
ffl multiply divide canonical factors multiplying dividing relevant entries just discrete factors defined equation equation 
ffl fd xg set observations discrete continuous 
instantiate setting entries consistent zero 
instantiate instantiating canonical form evidence 
remains discuss marginalization 
marginalize continuous variables marginalizing canonical forms 
marginalizing discrete variables subtle involves combining canonical forms 
precisely assume canonical factor defined fa xg delta delta delta gamma gamma gamma want marginalize canonical factor value need find entries consistent combine 
operation combining canonical forms equivalent collapsing operation 
collapsing operation defined mixture gaussians mixture general canonical forms 
combining canonical forms defined iff canonical forms finite moments represented gaussians 
note collapsing operation defined find approximation marginal distribution collapse mixture gaussians just gaussian 
marginalization involves collapsing gaussians called weak marginalization opposed strong marginalization involve approximations 
definition theorem summarize discussion 
definition marginalization canonical factors strong weak 
strong marginalization involve approximations 
performed cases ffl continuous variables marginalized ffl factor defined discrete variables ffl canonical forms identical chapter 
hybrid bayesian networks axy clg moralized triangulated graph resulting clique tree clique tree strong root clique tree conditions hold marginalization weak 
theorem weak marginalization operation defined iff canonical forms represented gaussians 
case weak marginalization operation carried theorem 
weak marginalization involves collapsing results approximation true marginal distribution preserving moments 
section noting replace canonical forms conditional forms defined section leading conditional factors 
discussion regarding canonical factors applies conditional factors 
particular definition applies conditional factors 
analog theorem weak marginalization conditional factors defined case entry conditional factor represents multivariate gaussian 
resulting clique tree algorithm similar described numerically stable lj 
order simplify discussion shall algorithm context canonical factors 
chapter 
hybrid bayesian networks message passing strongly rooted trees able combine general canonical forms forces restrict elimination order variable elimination algorithm clique tree algorithm 
example consider network 
moralization graph triangulated shown 
extract maximum cliques build clique tree get graph shown 
unfortunately point stuck send message tree 
example clique fb contains cpds 
contain cpd canonical forms fx represent linear cpd gaussian 
follows marginalize send message 
similar reasons send message clique fa zg 
way sure message passing possible create clique trees strongly rooted 
strongly rooted tree possible choose clique called strong root weak marginalization sending message example clique tree strongly rooted clique tree strong root clique fa 
having perform weak marginalization upward pass means sending message root continuous variables sepset need sum discrete variables 
definition formalizes idea definition clique clique tree strong root adjacent cliques closer satisfy delta delta delta gamma gamma gamma gamma defer discussion create strongly rooted tree section continue discussion message passing 
tree strongly rooted possible calibrate message passing algorithm 
run upstream pass pass possible weak marginalization involved 
run downward pass 
time perform weak chapter 
hybrid bayesian networks calibration strongly rooted tree 
pick strong root 
initialize canonical factors potentials sepsets 
multiply potentials cpds 
enter evidence 
discrete potential 
continuous potentials sepsets remove observed variables tree 
send upstream messages fig 
protocol 
send downstream messages leaves fig 
protocol calibration strongly rooted tree marginalization possible 
reason downward pass send messages cliques received messages represent probability function 
canonical forms represent gaussians collapsed 
discrete clique trees possible introduce evidence tree 
discrete evidence introduced tree multiplying potentials relevant indicator function case discrete clique trees 
continuous evidence introduced instantiating relevant potential clique tree linear gaussian 
calibration algorithm including evidence instantiation shown 
note evidence introduced calibrating tree case continuous evidence needs instantiated sepsets 
far motivated strongly rooted trees way sure operations canonical factors defined particular collapsing performed canonical forms represent gaussians 
possible find clique trees strongly rooted message passing possible 
example consider clique tree 
tree strongly rooted multiplying cpds clique conditional factors strong root clique contains continuous variables appear sepset neighbor path continuous variables clique chapter 
hybrid bayesian networks fa represents gaussians fx cpds multiplied 
weakly marginalize clique send message fa clique fa zg 
possible weakly marginalize clique fa zg send message back clique fa theorem theorem having strong root sufficient necessary condition operations message passing algorithm defined hybrid clique trees 
reason strongly rooted trees ensures message passing defined leads exact results 
general case trees strongly rooted 
theorem clique tree strong root instantiating evidence running upstream pass followed downstream pass tree calibrated potential contains correct weak marginal 
particular clique contains correct probability distribution discrete variables correct mean variance continuous variable 
proving theorem useful lemma 
lemma sets random variables joint distribution jxj jy bx vector size matrix size theta matrix size theta moments depend moments distribution 
proof lemma appendix 
ready prove theorem 
proof consider algorithm 
cpd potential contains variables multiplications line exact performing product potentials correct joint distribution 
similarly easy show evidence instantiation lines exact chapter 
hybrid bayesian networks similar arguments purely discrete purely continuous clique trees 
note sepsets initialized need perform operations sepsets removing observed variables domains 
left show message passing leads correct weak marginals clique 
upward pass simple marginalization operations involved strong operations exact 
upward pass equivalent running variable elimination algorithm variables strong root correctness follows correctness variable elimination algorithm 
result upward pass correct strong marginal strong root downward pass involves weak marginalization correctness immediate 
show results correct weak marginals cliques 
proof induction distance clique strong root know correct marginal 
assume cliques node path sepset need show sending message correct weak marginal 
denote variables sepset variables gamma gamma denote potentials sending message phi phi denote factor sepset sending message phi 
denote phi potential sending message phi message phi phi summation may involve integrals 
show sending message agrees marginal distribution induction correct weak marginal prove phi represents correct weak marginal phi phi delta phi phi phi phi delta phi phi phi delta phi phi chapter 
hybrid bayesian networks note phi phi holds sepset strong marginal computed upward pass 
strong marginal cliques agree distribution sending message 
left show phi correct weak marginal variables moments correct moments correct 
observe entry canonical form phi represent correct conditional gaussian distribution 
reason upstream pass information variables sent weak marginalization rest tree information variables sepset 
follows entry phi represents gaussian fs correct moments correct 
theorem write linear function plus gaussian noise directly lemma get desired result 
note tree strongly rooted proof breaks places upward pass exact equation hold 
general tree strongly rooted running message passing algorithm lead correct moments tree calibrated operations defined 
fact message passing trees strongly rooted may lead illegal covariance matrices shown section 
ill defined message passing section give example message passing clique tree strong root leads illegal covariance matrices particular negative variance 
consider clg 
cpds discrete variables uniform cpds continuous nodes defined follows chapter 
hybrid bayesian networks simple clg clique tree strong root gammax assume clique tree strongly rooted assume evidence 
follow results message passing algorithm 
presentation easier follow intermediate results moments form canonical form 
assume message passing algorithm sends message clique fa xg clique fb cliques share variable collapse prior distribution theorem get message 
multiply potential fb message get mixture gaussians equal weights gamma gamma chapter 
hybrid bayesian networks instantiating evidence get new mixture gaussians gamma weights gaussians new mixture 
reason marginal original mixture evidence likelihood gaussians 
need send message back clique fa xg 
need collapse gaussians 
result collapsing message 
note example evidence caused variance increase 
incorporate message clique fa xg need divide message multiply entry potential fa xg result division particular need perform operation delta operation carried canonical forms purposes consider second order coefficient recall general sigma gamma oe get oe get oe get new equal gamma gamma represent legal gaussian inverse transformation get oe gamma legal variance 
doing message passing clique tree strongly rooted safe 
chapter 
hybrid bayesian networks strong triangulation implications strongly rooted trees prove useful inference clgs natural question construct strongly rooted trees clg 
address question define strongly triangulated graphs 
definition say chordal graph strongly triangulated corresponds elimination order continuous variables eliminated discrete ones 
recall section nodes clique tree correspond maximal cliques triangulated graph 
theorem lei theorem statement iii tells connection holds strongly rooted trees 
theorem clique tree strong root iff represents maximal cliques strongly triangulated graph 
words order create strongly rooted tree simply elimination order continuous variables appear discrete ones 
turn attention computational implications strong triangulation 
theorem prove important property strongly rooted trees 
definition continuous connected component set continuous nodes path moralized graph nodes path continuous 
maximal continuous connected component continuous connected component proper subset larger continuous connected component 
discrete neighbors continuous connected component denoted dn discrete nodes adjacent node moralized graph 
example continuous nodes fx zg continuous connected component discrete nodes neighbors 
theorem states relation continuous connected components strongly rooted trees 
chapter 
hybrid bayesian networks clgs strong triangulation leads exponential trees theorem maximal continuous connected component dn 
strong triangulation building clique tree exists clique contains nodes domain node proof proof induction jxj 
discrete neighbors adjacent continuous node due strong triangulation eliminate nodes doing create clique contains fx dg 
node eliminated fx dg due strong triangulation node eliminated 
eliminating add edges discrete neighbors continuous neighbors adjacent adjacent gamma fy elimination addition connect continuous neighbors gamma fy continuous connected component 
left continuous connected component size gamma discrete neighbors induction hypothesis 
example consider clg 
continuous variables connected component discrete variables clique strongly rooted clique tree continuous nodes case 
unfortunately size canonical factors exponential number discrete variables theorem implies cases strong triangulation leads clique trees intractable structure clg chapter 
hybrid bayesian networks simple 
example networks shown discrete nodes clique making respective clique trees exponentially large 
point reader may wonder strong triangulation necessary heavy computational penalty pay 
looking data structure canonical conditional factors avoids problems caused weak marginalization trees strongly triangulated prove theorem similar theorem 
alternatively give goal exact inference clgs develop approximate inference algorithm prove approximation version theorem 
answering important questions unfortunately negative answer main contributions thesis discussed chapter 
approximate inference clgs seen exact inference clgs harder exact inference discrete bayesian networks 
surprising cases people resort approximate inference algorithms clgs discuss section 
discretization discretization approach aims transform hybrid models discrete ones 
idea partition domain continuous variable finite number sets 
example continuous variable age partition domain sets 
represent continuous variable age discrete variable takes possible values 
discretize cpds accordingly discrete model solve techniques chapter 
unfortunately discretization problems 
problem partition continuous domain 
discretization scheme fine chapter 
hybrid bayesian networks expectation propagation 
initialize approximations 
compute unnormalized joint 
convergence 
choose 
set gammai 
multiply gammai minimize kl divergence get family 
set gammai 
set expectation propagation resolution posterior probability mass lies perform inference know posterior distribution 
order deal problem kk suggest approach start rough discretization iteratively refine finer partitions areas probability distribution lies 
serious problem discretization problematic need approximate distributions handful discretized variables representation resulting grid exponential number variables 
example number parameters represent gaussian variables 
discretize variables values need parameters describe joint probability distribution 
introduce extra approximations joint probability distribution ended exponential representation polynomial 
situation worse need approximate mixture gaussians single gaussian deciding discretization scheme challenging 
expectation propagation view clique tree approach deciding representation family probability distributions finding best approximation chapter 
hybrid bayesian networks clg ep example joint distribution chosen family 
example consider distribution bayes net 
easy see distribution mixture gaussians mixture component combination different distribution value true 
structure clique tree dictates distribution expressed mixture gaussians mixture component combination just structure tree allow represent correct joint distribution try find best approximation expressed tree done collapsing operation finds best single gaussian approximation mixture gaussians 
general set functions product joint probability distribution case cpds indicator functions evidence 
problem complex represent 
choose family representations try find family member best approximates true joint minimizing kl distance 
key issue find best approximation family distributions 
lauritzen algorithm way puts severe restrictions family distributions 
family correspond strongly triangulated tree exponential size problem 
family representations necessary lauritzen algorithm complex need different approach 
expectation propagation ep min alternative approach finding chapter 
hybrid bayesian networks family member approximation joint probability distribution 
treating term exactly approximating joint includes ep approximates uses product approximation joint distribution 
algorithm iteratively improves approximations assume current approximation 
improve approximation remove define gammai combine gammai exact project result family distributions leading new approximation update approximation contribution leading gammai gamma important property algorithm family distributions exponential family term approximations family 
algorithm 
example consider clg 
assume discrete variables binary 
interested distribution easy see distribution mixture gaussians combination discrete variables induces different multivariate gaussian fx large solve problem lauritzen algorithm theorem strongly triangulated tree clique contains discrete variables 
ep approach 
set terms correspond product cpds 
example suppose decide approximate distribution single gaussian case represent canonical form 
line involves setting prior need approximate 
line simply prior canonical form 
division line easy canonical form 
line multiply gammai resulting mixture gaussians instantiated observed value 
result collapsing mixture gaussian 
division line simple canonical forms 
note convergence easily compute quantities interest probability evidence distribution discrete variables chapter 
hybrid bayesian networks approximation joint distribution 
shown min ep iterations guaranteed fixed point approximations exponential family 
ep leads approximations especially joint distribution approximated member chosen family 
unfortunately algorithm behave arbitrarily badly surprising analysis chapter 
sampling approach discrete bayes nets sampling approach approximate inference algorithm 
particular take lw algorithm section apply hybrid bns modifications 
fact resulting algorithm big advantage rely linearity cpds applied non linear hybrid bns clgs 
case clgs improve lw algorithm technique called rao blackwellization rb 
recall reason resort approximate inference compute analytically distribution random variables domain case clgs mixtures large 
sampling methods approximate distribution generating set assignments random variables estimate various properties posterior distribution 
key observation rao blackwellized sampling necessary sample values random variables domain assignment subset variables distribution variables tractable deal analytically 
formally rb suggests partition domain mutually exclusive subsets purposes shall assume bayes net contain edges variables variables sample called rb sample consists assignment variables distribution 
cases weighted sample case associate sample weight 
chapter 
hybrid bayesian networks rao blackwellized sampling observation 
assume want estimate expectation function evidence xje jx sums replaced integrals 
efficiently compute expectation term need need sample set rb samples drawn assignments fx estimate je jx usual hard sample posterior distribution 
view term jx function view automatically justifies lw mcmc way generate samples notation convenience remainder section partition evidence lw recall weight particle corresponds likelihood evidence rest sample 
variables handled usual way 
sample nodes topological order get node multiply sample weight par 
compute probability distribution compute 
weight sample set par 
addition condition sample distribution evidence distribution keep sample gamma written gamma 
mcmc recall transition distribution determined posterior chapter 
hybrid bayesian networks distribution influenced likelihood evidence 
create possible successor state successor state defined assignment weight defined par probability distribution remaining variables gamma 
renormalize weights possible successor states get probability distribution states choose state probability distribution 
gibbs sampling successor states simply possible values assignments agree values variables gamma fxg 
case clgs natural choice partition random variables sample discrete variables handle continuous variables analytically 
reason assignment discrete variables distribution continuous variables gaussian handle analytically polynomial time space 
expensive generate rb sample generate regular samples rb sample contains information takes regular samples discrete assignment approximation rb sample 
example assume want compute mean continuous variable evidence partition evidence discrete variables continuous ones 
generate samples corresponding assignments discrete variables fx weights fw mcmc weights set 
compute posterior mean evidence jx 
note operation done efficiently closed form assignment distribution gaussian easy condition evidence compute mean 
final estimate rao blackwellization technique limited clgs clgs natural models applied 
clgs chapter 
hybrid bayesian networks different partitions discrete continuous variables 
example decide sample certain discrete variable represent gaussians associated different values analytically mixture gaussians 
general think spectrum extremes exact inference side full sampling 
rb gives points spectrum trading number variables sample complexity exact inference remaining variables 
chapter theoretical analysis clg models commonly surprisingly little formal done analyzing complexity 
obviously clgs generalization discrete bayesian networks difficult 
obvious network structures easy discrete case remain easy clgs 
fact saw theorem complexity lauritzen algorithm exponential size network network structures simple discrete case 
particular discrete case inference done linear time network polytree lauritzen algorithm lead exponentially large cliques 
natural ask perform inference efficiently polytree clgs 
network proof theorem chapter 
theoretical analysis hardness results purposes analysis assume working finite precision continuous variables 
restrict queries involving single discrete variable 
simplest queries ask answer queries efficiently sense consider general queries 
interested solving questions form evidence probability distribution discrete variable phrased decision problem evidence probability discrete variable takes value greater smaller ff 
theorem problem np hard ffl instance polytree clg binary discrete variables evidence ffl question posterior probability discrete variable 
furthermore np exist polynomial approximate inference algorithm absolute error smaller 
proof consider np complete subset sum problem gj 
multiset fs element non negative integer positive integer question exists subset sum elements exactly reduce problem polytree clg model shown 
discrete variables shown squares binary 
decision problem clg 
discrete variables uniform prior cpds continuous variables oe oe gamma oe gamma oe chapter 
theoretical analysis example reduction mixture gaussians gamma oe choose oe cn constant discussed simply assume 
showing details proof helpful consider simple example 
assume 
build network continuous nodes discrete nodes marginal distribution shown 
mode corresponds combination turn corresponds subset example mode corresponds combination turn corresponds subset 
note subsets sum reason bigger mode 
distribution mixture corresponding possible values shown 
get contribution marginal distribution represented solid line get gaussian mean larger variance represented dashed line 
intuitively observe posterior distribution depends observation chapter 
theoretical analysis come solid line dotted line 
solid line dotted line observation come mixture components 
similarly solid line dotted line 
example solid line dotted line exists subset elements sum 
turn task formalizing intuition 
goal show exists subset elements sum iff 
uniform iff compute exp gamma gamma gamma ha assignment ha 
easy show koe direction assume elements sum show 
define iff clearly oe 
delta oe cn chapter 
theoretical analysis case cn conversely assume exist show 
exist get integer different jl gamma 
case get delta oe exp gamma gamma oe cn exp gamma oe cn cn cn gamma explain role constant making large gets arbitrarily close exists arbitrarily close 
hypothetical approximation algorithm absolute error ffl answer decision problem construct problem instance gamma ffl ffl depending existence answer iff algorithm answers 
see correct pa probability distribution generated discrete event jp gamma pa ffl 
assume exists ffl 
pa gamma ffl ffl gamma ffl similarly show exist pa 
np exist polynomial time approximate inference chapter 
theoretical analysis network proof theorem algorithm absolute error smaller 
network shown discrete variables ancestors continuous variable marginal distribution mixture exponentially components 
conjecture restrict marginals continuous variables simple contain components inference problem easier 
theorem tells case 
theorem problem np hard ffl instance polytree clg binary discrete variables continuous node discrete ancestor evidence ffl question posterior probability discrete variable 
furthermore np exist polynomial approximate inference algorithm absolute error smaller 
proof reduction similar previous proof different network structure 
structure shown 
discrete variables binary uniform distribution oe oe gamma gamma oe gamma gamma gamma oe chapter 
theoretical analysis gamma oe query notation 
intuition structure setting evidence having oe small force close gamma gamma depending value resulting similar situation theorem 
fact exactly distribution gamma koe gamma koe simply proof theorem 
get exactly desired distribution get close choosing oe large oe small 
get exactly desired variance ffl away mean 
ffl choose oe oe ffl delta oe oe oe oe induction prove koe gamma see appendix details derivation 
situation theorem 
having distribution noe distribution oe gamma ffl 
choose ffl oe easily modify inequalities theorem prove desired result 
reductions get immediate corollary regarding problem finding mpe assignment discrete variables clg 
corollary problem np hard ffl instance polytree clg binary discrete variables continuous node discrete ancestor evidence chapter 
theoretical analysis ffl question assignment discrete variables 
proof direct result reduction theorem 
exists subset instantiation correspond subset involve 
exist subset instantiation involving instantiation involving 
follows instantiation iff exists solution subset sum problem 
discussion fact exact inference polytree clgs np hard may surprising 
possibly popular clg models switching linear dynamic systems discussed section 
temporal model shall see chapter close relation static clgs particular clgs structure network slightly complex discrete variable depends discrete variable gamma 
continuous variable chain discrete variables ancestors distribution mixture exponentially gaussians 
believed inference case hard proof 
results chapter go giving formal proof previously known intuition 
concentrate queries involving just discrete variables posterior distribution easy represent infer 
reasonable assume hard represent exponential mixture continuous variables case discrete variables obvious 
second theorem prior distribution continuous variable gaussian mixture gaussians 
somewhat surprising seemingly benign structure continuous variables produce np hard decision problem 
important obvious fact proven analysis fact simplest type approximate inference absolute error smaller intractable 
consequently expect chapter 
theoretical analysis find polynomial approximate inference algorithm useful error bound restrictions structure parameters clgs 
technical note observe proof np hardness theorems reduction subset sum solved pseudo polynomial algorithm algorithm polynomial jsj max consequence subset sum problem hard magnitudes numbers set exponential reduce problem clg get network variance exponentially small compared means 
believe problem remains np hard means bounded polynomial currently proof case 
knowing fact advance saved author time 
chapter enumeration algorithm previous chapter saw inference simple clgs np hard approximate inference tractable 
results conclude inference clg models lost cause 
fortunately problems faced practice simpler ones np hardness reductions special structure special features exploited carefully designed algorithms allow efficient inference 
chapter concentrate domains probability distribution discrete variables skewed small number assignments discrete variables dominate rest 
advantage case approximate exponentially large mixture smaller mixture gaussians 
property skewed distribution fairly common practice appears different domains 
consider example fault diagnosis domain 
discrete variables correspond various possible faults priori skewed distribution faults 
null hypothesis corresponding event new faults happening note null hypotheses may contain faults happened past persist 
assuming discrete variables hypotheses corresponding occurrence single new fault hypotheses corresponding pair new faults hypotheses corresponding new faults 
key insight combination new faults extremely 
practice chapter 
enumeration algorithm faults total probability mass taken hypotheses faults possible faults different fault probabilities case priori null hypothesis combined single new fault hypotheses pair new faults hypotheses completely dominate hypotheses 
observation demonstrated 
assume binary fault variables graph shows total probability mass taken hypotheses faults various values example hypotheses fewer faults account probability mass hypotheses fewer faults account probability mass close reasonable hope generating hypotheses small get approximation exponentially large mixture 
unfortunately observation direct implication size mixtures wish approximate care posterior distribution prior 
general assignments priori may posteriori vice versa 
especially case dealing chapter 
enumeration algorithm hybrid domains 
recall hybrid case likelihood evidence corresponds density function bounded 
hypothesis priori extremely evidence hypothesis posteriori 
furthermore evidence balance prior likelihoods leading situation longer small set hypotheses dominate rest 
practice situation appears better 
usually case posteriori small number hypotheses dominate rest 
case expectation hypotheses priori remain posteriori concentrating hypotheses reasonable heuristic shall explore chapter 
fault diagnosis domain idea concentrating hypotheses small number faults natural probabilistic model discrete strong independence assumptions 
note phenomenon small number hypotheses dominate rest unique fault diagnosis domain 
example situation arises visual tracking past evidence hypotheses 
example people having conversation person start jumping start dancing 
inference algorithms presenting general framework enumeration algorithm methods rao blackwellized sampling 
simplicity assume clg just continuous connected component 
case apply algorithm continuous connected component message passing discrete variables 
think building strongly rooted clique tree continuous connected component precisely clique continuous variables discrete neighbors 
algorithms discussed chapter approximate distribution clique calibrate tree message passing 
note chapter 
enumeration algorithm examples interest including rwgs discussed chapter continuous nodes form continuous connected component 
denote discrete variables delta delta delta continuous variables gamma gamma gamma discrete neighbors delta delta delta dn delta delta delta 
joint distribution delta delta delta gamma gamma gamma mixture gaussians gaussian corresponding assignment discrete variables 
gaussians identical assignments delta delta delta agree variables delta delta delta dn gaussian 
setup consider general query form delta gamma delta gamma delta delta discrete gamma gamma continuous 
distribution mixture gaussians note delta delta delta dn delta delta combination delta delta get mixture gaussians single gaussian 
approximating delta gamma delta gamma simpler approximate delta delta delta delta dn gamma delta gamma 
reason including delta delta delta dn sure combination discrete variables just gaussian 
note addition delta delta delta dn query cost case need generate gaussians possible assignments delta delta delta dn delta delta delta delta dn gamma delta gamma easy answer query marginalizing extra variables 
weak marginalization care moments keep original gaussians collapsing potentially leading mixture gaussians combination discrete query variables 
example consider network 
delta delta delta fa gamma gamma gamma fx zg delta delta delta dn fd 
assume delta fa gamma fxg delta fcg gamma fzg interested 
combination fa mixture gaussians corresponding possible values delta delta delta dn get different gaussians different values combinations fa share value gaussians mixtures 
weights mixture components different different values add delta delta delta dn query need approximate chapter 
enumeration algorithm clg example distribution 
order find delta delta delta delta dn gamma delta gamma convenient approximate delta delta delta delta dn gamma gamma delta 
example try approximate 
distribution mixture gaussians variables gaussian gamma gamma conditioning gamma gamma done mixture component time way similar conditioning rb samples evidence see section 
multiply weight mixture component likelihood evidence condition gaussian evidence 
re normalize mixture weights get delta delta delta delta dn gamma delta gamma 
ready discuss inference algorithms evaluate delta delta delta delta dn gamma gamma delta 
fact encountered problem discussed chapter 
obviously lauritzen algorithm feasible particular problem simply 
chapter goal tackle problems lauritzen algorithm leads clique trees large resort approximate inference techniques 
shall possible algorithms 
discussed section 
discuss new algorithm alternative sampling approach 
algorithms observation assignment discrete variables distribution continuous variables gaussian 
algorithms treat discrete continuous variables asymmetric way keep set chapter 
enumeration algorithm weighted particles particle defined assignment discrete variables probability distribution continuous ones 
rao blackwellized likelihood weighting algorithm approximate delta delta delta delta dn gamma gamma delta rao blackwellized likelihood weighting 
recall variables clg partitioned sets sample keep distribution 
choose delta delta delta gamma gamma gamma sample discrete variables sample delta delta delta keep multivariate gaussian sampling delta delta delta lw evidence delta alternatively network structure discrete variables simple perform probabilistic inference sample directly delta delta delta delta 
note generating sample delta delta delta ffi remove ffi discrete variable delta delta delta delta dn similarly remove gaussian variable gamma gamma rao blackwellized mcmc second algorithm approximate inference clgs rao blackwellized mcmc particular rao blackwellized gibbs sampling 
recall gibbs sampling need find conditional distribution discrete variable rest variables delta delta delta continuous evidence 
ffi gammai assignment discrete variables gamma write ffi gammai gamma gamma ffi gammai ffi gammai easy compute ffi gammai just gibbs sampling discrete networks 
find gamma ffi gammai build gaussian corresponds discrete instantiation find likelihood evidence 
having computed gamma ffi gammai ffi gammai possible values normalize probabilities leads probability distribution state 
context rb sampling methods particles identical rb samples 
chapter 
enumeration algorithm enumeration algorithm recall intuition considering hypotheses priori 
rb lw way apply idea context sampling 
third final algorithm applies intuition context deterministic search sampling 
method algorithm uses running time consider distinct hypotheses possible duplicate hypotheses come sampling method 
step assume wish enumerate instantiations delta delta delta delta 
important observation ignore continuous variables bn purposes enumeration care discrete problem enumerating delta delta delta delta 
algorithm section enumerate assignments delta delta delta delta 
instantiation compute relevant multivariate gaussian set weight ffi delta gamma ffi 
note create clique tree generate samples tree discrete variables need strong triangulation lauritzen algorithm 
structure discrete part bayesian network simple fault diagnosis fault variables independent merely influence continuous variables resulting clique tree simple enumeration done efficiently 
possible better 
recall interested enumerating instantiations delta delta delta se 
interested instantiations delta delta delta delta dn useful marginalize clique tree variables delta delta delta gamma delta delta delta delta dn 
unfortunately problem generating hypotheses subset variables significantly difficult generating hypotheses variables par 
cases network structure truly simple clique tree delta delta delta delta dn relatively small possible generate efficiently samples delta delta delta delta dn fact enumerate just instantiations delta delta delta dn weights need re normalized order serve probability distribution 
chapter 
enumeration algorithm delta instantiation discrete bayes net inference find delta delta delta delta dn 
view instance rao blackwellization add variables delta gamma delta delta delta dn set variables 
words assignment delta delta delta dn keep probability distribution continuous variables delta idea lead significant savings faced potential larger clique trees idea possible 
case examples considered thesis delta delta delta delta dn delta gamma delta delta delta dn 
single representative algorithms rb lw rb mcmc algorithms viewed having sources randomness particles generated times particle sampled 
recall possible generate duplicate samples particle effect duplicate samples increase weight particle 
words weight particle corresponding assignment evidence times sampled total samples view term estimator prior probability 
view enumeration algorithm modifies sampling algorithms ways ffl generating particles deterministic way sampling 
ffl avoid generating duplicate samples 
term times sampled particles generated replaced 
view naturally leads intermediate algorithm call single representative algorithm 
single representative algorithm keeps source stochasticity avoids second generates particles stochastically sampling approach ignores duplicate particles uses times sampled particles generated fact define different single representative algorithms chapter 
enumeration algorithm ffl single representative rb lw version particles generated lw sampling section 
ffl single representative rb lw version particles generated mcmc sampling section 
summarize single representative algorithms generate particles lw mcmc sampling algorithms duplicate samples ignored weight particle set 
note computing easy assumption parents sampled variables sampled case computing reduces multiplying relevant cpd entries 
case examples sample discrete variables discrete variables continuous parents 
rb lw rb mcmc single representative algorithms biased 
hope added bias compensated reduction variance stems fact randomness sampling process influence unnormalized weight particle particle generated 
single representative rb lw intermediate algorithm rb lw enumeration algorithm examining performance help understand better difference performance algorithms 
optimization trick algorithms described share common procedure assignment delta delta delta dn need generate multivariate gaussian continuous variables 
certainly possible generate gaussian independently possible speed significantly process previously generated gaussians 
example consider large clg discrete variable influences continuous variable leaf 
assume created multivariate gaussian assignment delta delta delta dn ffi construct new gaussian corresponding assignment ffi ffi ffi represent assignment variable continuous variables influenced gaussians exactly chapter 
enumeration algorithm optimization gaussian generation fhp ffi ffi ig previously generated gaussians ffi new assignment delta delta delta dn 
best 
best gamma gamma gamma continuous variable 

variables different assignments ffi ffi 
continuous descendants 
jxj jx best 
best 
best 


copy best 
recompute moments best theorem speeding gaussian generation previously created gaussians moments involve mean variance covariances variables 
creating scratch reasonable start change equations theorem 
idea generate entries covariance matrix entries number continuous variables 
general assume generated gaussian discrete assignment delta delta delta dn ffi order generate gaussian discrete assignment delta delta delta dn ffi set variables different values ffi ffi start recompute means variances covariances continuous variables descendants variable space keep generated gaussians memory speed algorithm algorithm shown 
previously generated gaussians find gaussian best best needs sense gaussian best largest number continuous variables share moments 
chapter 
enumeration algorithm network experiments chapter copy gaussian order change previously generated gaussian necessary changes copy 
wonder useful idea answer computational gain depends domain way generate gaussians significant 
consider example fault diagnosis domain assume enumeration algorithm generate gaussians systematically 
generating hypothesis corresponding zero faults start generating single fault hypotheses 
differs fault hypothesis just discrete variable 
cases single variable represents sensor fault case typically influences single leaf continuous variable representing sensor reading 
case jaj better jxj get dramatic speedup 
similar situation arises generate hypotheses corresponding faults faults hypotheses differs single fault hypothesis just variable 
idea large real world network chapter achieved speedup factor able consider times hypotheses computation time 
empirical comparison section described algorithms rb lw single representative rb lw rb mcmc single representative rb mcmc enumeration algorithm 
compare algorithms generated random bayesian networks structure shown 
structure resembles np hardness reduction discrete variables independent priori 
domain chapter 
enumeration algorithm discrete variables evidence experiments note dramatic difference clique tree enumeration algorithm strongly rooted tree lauritzen algorithm 
continuous variables continuous connected component strongly rooted tree clique contains discrete variables size tree exponential size network case 
contrast ignore continuous variables discrete variables form simple chain cliques tree contain discrete variables 
size tree enumeration algorithm 
chose run experiments particular network able special ad hoc algorithm compute distribution discrete variables 
recall fa evaluate expression enumerate instantiations computing easy multiply relevant cpd entries 
gaussian order compute compute mean variance done efficiently assignment compute mean variance compute mean variance mean variance 
note simple network structure need compute full covariance matrix 
assignment compute unnormalized probability efficiently 
computed normalize result get exact posterior distribution 
possible instantiations method able compute exact posterior distribution matter seconds 
enabled compare various approximate inference algorithms exact posterior distribution 
parameters network set follows 
set set gamma value determines skewed distribution assignment probability assignments chapter 
enumeration algorithm probability gamma assignments probability gamma 
situation illustrated 
parameters network cpds continuous variables 
recall linear cpd defined gamma ff fi gamma oe parameters fi exist continuous parent 
sampled values ff uniform distribution varied values fi uniform distribution 
set oe 
ran experiments various combinations combination generated sets parameters bayesian network 
sampled instantiation bayes nets sampled value observed value 
computed true posterior distribution discrete variables 
compared algorithms criteria ffl error discrete variables correct distribution distribution generated approximation algorithm jp gamma ffl assignment approximation algorithm assignments correct posterior distribution 
hard come fair comparison algorithms terms number particles general duplicate samples cheaper generating particle time generate duplicate sample need create gaussian just increment counter particle 
compare algorithms relative samples including duplicates give enumeration algorithm unfair advantage particles distinct gaussians expensive compute duplicate particles 
hand count distinct gaussians ignore duplicates give sampling algorithms unfair advantage ignore time chapter 
enumeration algorithm time norm enumeration single rep rb lw rb lw rb mcmc time enumeration single rep rb lw rb lw rb mcmc results various algorithms random networks norm percentage runs hypothesis generated algorithm assignments generate duplicate particles 
order fair comparison algorithms measured performance relative running time 
results different quite similar 
shows results 
shows error shows top criterion percentage runs assignment algorithm top assignments correct posterior distribution 
line single representative rb mcmc top line rb mcmc shown graphs 
reason phenomenon discussed context rb lw shall see performance sampling algorithm poor similar performance single representative algorithm 
striking observation poor performance single representative rb mcmc case experiments ran 
reason space discrete assignments local maxima hard get 
example assume just nodes chapter 
enumeration algorithm assume evidence initial assignment mcmc 
assignment may take chain long time generating sample 
reason gibbs sampling change variable time consider transitions 
assignments initial assignment probability transitioning extremely small 
mixing rate chain small performance mcmc poor 
problem worse nodes local maxima assignments single transition significantly reduces probability evidence 
algorithm get stuck local maxima get assignments reasonable time 
performance mcmc algorithms consistently poor ignore rest section concentrate enumeration algorithm rb lw single representative rb lw 
shows remaining algorithms enumeration algorithm best performance rb lw worst performance single representative rb lw 
explore differences algorithms tested performance algorithms relative likelihood initial assignment mean assignment mean assignment mean 
initial assignment evidence standard deviations away mean assignment standard deviations away mean assignment standard deviations away mean 
initial assignment quite candidates 
chapter 
enumeration algorithm time enumeration single rep rb lw rb lw time norm enumeration single rep rb lw rb lw time norm enumeration single rep rb lw rb lw norm function prior likelihood discrete assignment prior prior prior discrete events 
random networks sampled assignments discrete continuous variables 
partitioned samples subsets likelihood discrete assignments samples subset 
tested performance algorithms subset separately 
rb lw single representative rb lw performed runs assignments 
note assignment fa probability assignments probability delta assignments probability delta probability delta 
note subset just discrete assignment possible generate sample sample includes assignment continuous variables 
show results experiments 
expected algorithms perform better likelihood discrete event higher 
reason discrete events high likelihood enumerated early sampled early sampling process 
note degradation performance rb lw single representative severe degradation performance enumeration algorithm 
reason discrete event priori sampling approaches spend lot time generating duplicate samples assignments 
hand enumeration algorithm waste extra time assignments generated efficiently chapter 
enumeration algorithm enumeration single rep rb lw rb lw time top enumeration single rep rb lw rb lw time top enumeration single rep rb lw rb lw top percentage function prior likelihood discrete assignment prior prior prior spend time generating assignments 
demonstrates interesting phenomenon 
performance algorithms poor performance rb lw single representative similar seen left side 
cases algorithms significantly outperformed enumeration algorithm 
results sampling algorithms reliable single representative rb lw works better rb lw performance similar performance enumeration algorithm seen right side 
conjecture reason phenomenon performance poor stems sampling correct discrete instantiations 
rb lw single representative rb lw generate samples identical way correct assignments correct assignments 
performance better correct samples generated question correct weight 
eliminating sampling noise sample weights single representative rb lw effectively behaves enumeration algorithm case significantly outperforms rb lw algorithm 
final set experiments tested performance algorithms relative skewed distribution recall parameter closer distribution skewed smaller number hypotheses account chapter 
enumeration algorithm time enumeration single rep rb lw rb lw time enumeration single rep rb lw rb lw time enumeration single rep rb lw rb lw norm function parameter probability mass shown closer better algorithms perform expected 
furthermore see matter value enumeration algorithm best performance single representative rb lw outperforms rb lw 
results indicate avoiding stochasticity useful idea skewed distributions 
enumeration algorithm fully deterministic works best 
single representative rb lw stochastic choice particles deterministic determining weight enjoys performance improvement 
discussion section discuss issues relating enumeration algorithm relation rb lw bounding error approximation comparison lauritzen algorithm 
presentation far viewed enumeration algorithm close relative rb lw deterministic alternative lw 
certainly reasonable view argued rb lw enumeration algorithm fundamentally different 
basic level rao blackwellized lw sampling method motivated weak law large numbers number samples grows gets chapter 
enumeration algorithm time top enumeration single rep rb lw rb lw time top enumeration single rep rb lw rb lw time enumeration single rep rb lw rb lw top percentage function parameter accurate limit exact 
algorithm biased estimator directly motivated law large numbers viewed different alternative sampling 
error bounds important issue bounding error approximation 
possible approach bound probability mass hypotheses generated 
partition set instantiations delta delta delta dn represents instantiations generated far represents instantiations generated far 
unnormalized probability mass hypotheses generated far ffi ffi delta gamma ffi bound sum need bound densities gamma ffi may larger possible instantiations delta delta delta dn preferably possible instantiations delta delta delta dn consistent assignment 
find bound write ffi ffi delta gamma ffi ffi ffi delta gamma ffi ffi delta chapter 
enumeration algorithm bound computed efficiently enumeration instantiations note bound decreases monotonically generate instantiations doing move big question course compute bound possible approach bound density gaussian gamma ffi 
assume find density mean gaussian 
matter evidence density larger density mean get upper bound 
furthermore take approach trying bound respect possible assignments delta delta delta dn just consistent assumptions bound efficiently network polytree je gamma 
consider univariate gaussian oe 
density gaussian mean oe order find upper bound density gamma ffi need find lower bound variance gamma assignment ffi 
done easily traversing tree topological order greedily finding smallest variance continuous variable 
want bound density gamma network polytree gamma variable consider covariances variables problem difficult 
simple reduction sat show finding smallest variance single variable possible instantiations delta delta delta dn general networks np hard 
currently know complexity finding upper bound cases ffl polytrees je gamma ffl networks cycles low tree width 
words networks build clique tree small number variables clique perform strong triangulation 
second case quite interesting finding efficient algorithm case enumeration algorithm reliable give indication quality approximation 
chapter 
enumeration algorithm comparison lauritzen algorithm conclude chapter comparing enumeration algorithm lauritzen exact inference algorithm 
give algorithm time enumerate possible discrete instantiations continuous connected component come exact answer 
simple networks possible enumerate discrete instantiations various continuous connected components complexity algorithm comparable complexity lauritzen algorithm 
algorithms enumerate number hypotheses directly enumeration algorithm creating gaussian factor lauritzen algorithm 
main differences ffl algorithm creates gaussians continuous variables continuous connected component opposed lauritzen algorithm continuous variables strong root 
disadvantage approach creates larger gaussians 
doing algorithm computes exact distribution weak marginals avoids costly operations involved message passing algorithm matrix inversion 
ffl algorithm overhead creating clique tree discrete variables enumerating instantiations 
cases extra complexity negligible compared operations involved manipulating gaussians 
furthermore know advance time enumerate instantiations need enumerate prior likelihood completely avoid overhead involved enumeration prior 
algorithms computational resources come exact answer performance comparable differ polynomial factor 
choice algorithms depend particular network structure implementation details need weak strong marginals 
algorithm designed small networks lauritzen algorithm tractable large networks 
case algorithm enjoys chapter 
enumeration algorithm important advantages ffl algorithm anytime algorithm give answer albeit approximate request 
ffl algorithm better space complexity 
recall lauritzen algorithm results exponentially sized cliques simple networks polytrees discussed chapter 
contrast storage requirements dictated size query exponentially smaller 
example query involves discrete variables just keep factor update generate gaussians keeping gaussians generate 
example want know moments continuous variables keep single gaussian iteration simply collapse current gaussian generated enumeration algorithm 
want true marginal may keep mixture gaussians space requirements significantly larger 
real life networks lauritzen algorithm impractical enumeration algorithm get approximate answer 
shall exactly order perform fault diagnosis real life physical system chapter 
chapter non linear cpds far concentrated clg models continuous cpds linear cpds 
unfortunately real world domains non linear dependencies variables 
rwgs discussed chapter contains examples non linear dependencies 
chapter concentrate non linear cpds continuous variables 
chapter consider non linear cpds involve continuous parents discrete children 
presence non linear cpds resulting joint distribution longer mixture gaussians mixture distributions general non gaussian represented closed form 
simplicity start discussion purely continuous model discrete variables 
shall see presence discrete variables techniques chapter 
standard approach joint distribution non gaussian approximate gaussian task find gaussian distribution close possible original distribution 
discuss traditional approach extended kalman filter see improved viewing problem numerical integration problem 
chapter 
non linear cpds extended kalman filter static bns 
sort nodes topological order 

cpd linear 
approximate cpd linear taylor series expansion 
compute moments theorem extended kalman filter adapted static bayesian networks extended kalman filter approach extended kalman filter ekf algorithm developed context dynamic models static networks 
approach static bayesian networks shall context 
variable bayes net parents assume known gaussian distribution sigma 
assume cpd non linear deterministic function 
assumption deterministic restrict generality method add source stochasticity extra variables example oe view deterministic function variables 
task find gaussian approximation 
approach taken ekf replace function simpler function 
idea simple finding gaussian approximation easy 
popular choice linear function appealing property linear approximation distribution gaussian computed theorem 
ekf uses standard linear approximation taylor series expansion mean rf gamma chapter 
non linear cpds taylor series approximation basis ekf algorithm shown 
sort nodes topological order increasingly build multivariate gaussian nodes order 
note get node gaussian approximation parents taylor series expansion defined equation 
small addition need compute covariances node nodes appear topological order directly parents 
node 
simple way finding cov add parent represent fi fi fi theorem 
problem approach get variables parents algorithm inefficient 
discuss alternative approach section 
important emphasize taylor series approximation lead optimal order approximation sense resulting gaussian approximation distribution sub optimal 
example consider function 
mean easy find 
variance easy compute closed form var gamma gamma gamma 
best gaussian approximation 
order taylor series approximation mean value leading gaussian approximation delta function mass located 
obviously extremely poor approximation 
ekf serious disadvantages 
accuracy lack thereof 
quality approximation depends approximates local area mean size local area determined variance 
ekf approximation approximation second higher order terms area negligible 
practical situations case ekf leads poor approximation just seen case 
chapter 
non linear cpds second problem ekf need compute gradient 
nonlinear functions may differentiable max function preventing ekf 
furthermore differentiable computing derivatives may hard depending way represented 
function may explicitly analytical function lookup table function implemented programming language general black box simply generates value inputs 
case simple way compute derivatives taylor series expansion resorting numerical differentiation methods 
improve accuracy ekf possible extend account higher order terms taylor series expansion formulas involved tend complex 
complication compute partial derivatives example second order approximation compute hessian 
second complication computing resulting distribution gaussian theorem derive different complicated set formulas 
practice second order approximation commonly higher order approximations 
numerical integration problem formulation extended kalman filter uses may considered indirect approach simplifying non linear function computing resulting distribution 
direct approach try directly approximate distribution resulting original non linear function 
start case non linear deterministic function goal approximate gaussian 
note quantities looking expressed chapter 
non linear cpds integrals gamma dx gamma dx gamma dx note notation gamma dx represent integral ir evaluate integrals correct second order moments able construct best gaussian approximation resulting distribution 
dealing non linear continuous cpds reduced integration problem question solve integrals 
note possible solve integrals closed form leading efficient optimal way computing best gaussian approximation 
fact saw example considered function equation reduces computing equation reduces computing equation reduces computing closed form formulas 
unfortunately cases closed form solutions resort numerical integration techniques 
vast literature techniques potentially purposes 
particular integrals form product gaussian function particularly suitable gaussian quadrature exact monomials methods dr 
gaussian quadrature discussion dimensional integrals 
gaussian quadrature approximates integrals form dx known non negative function case gaussian 
note approach general set 
function choose points chapter 
non linear cpds weights approximate integral dx points weights chosen integral exact polynomial degree gamma 
consider example case assume integration rule chosen exact represent polynomial form ff ff ff ff linear combination ff get dx ff dx ff dx addition ff ff integration rule exact exact general polynomials degree 
order rule exact case dx 
assuming gamma get set non linear equations gamma dx gamma gamma dx gamma dx solution equations swapping chapter 
non linear cpds gamma 
get integration rule gamma dx gamma general get set non linear equations existence solution obvious 
fortunately possible construct different set equations set orthogonal polynomials respect atk come efficient procedure solve equations generate integration points 
resulting integration rules accurate efficient 
point major advantages numerical integration approach compared ekf approach 
recall extended kalman filter relied taylor series expansion compute relevant derivatives 
need case need evaluate function set points 
property particularly advantageous differentiable implicitly function lookup table easy evaluate point hard compute derivatives 
suppose integral ir ir weight function show relax assumption gaussian weight function section 
gaussian quadrature method points grid points resulting rule accurate polynomials degree gamma 
approach reasonably small obviously practical integrals high dimensions 
problem numerical integration harder high dimensions fortunately better points choosing different set points 
exact monomials unscented filter avoid exponential number points gaussian quadrature method multi dimensional integrals try choose points directly ir grid points chosen ir dr 
goal find rules exact monomials particular consider sets monomials degree monomials form powers chapter 
non linear cpds nonnegative integers rules said precision degree formally monomial degree rule exact 
note integration rule uses combination dimensional gaussian quadrature rules precision exact larger set monomials monomials form example combine dimensional gaussian quadrature rules precision get rule exact monomials precision exact higher degree monomials exact degree monomial 
general distinct monomials precision 
points nd parameters choose weights nd coordinates points 
expect order achieve precision points 
case 
get set non linear equations guarantee solution exists 
hand possible find rules significantly fewer points called rules 
example rule uses points free parameters exact monomials degree 
mcnamee ms suite exact monomial rules fully symmetric weight functions 
say function fully symmetric obtained permutations changes sign coordinates example weight function fully symmetric 
mcnamee develop rules precisions general procedure find rules precision 
points 
show rules precisions weight function examples 
section show points transformed general gaussians 
definition point ir called generator denoted sigmau sigmau sigmau 
represents set points obtained permutations changing sign coordinates 
example generator contains point ir generator chapter 
non linear cpds sigma sigma ir represents set points gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma shorthand notation sigmau sigmau sum sigmau sigmau sum applied points represented generator sigmau sigmau 
shall consider integration rules set generators rules form consider monomial odd generator sigmau symmetric sigmau exists sigmau sign ith coordinate switched 
gammaf summing points get sigmau result integration rule 
consider integral gamma dx 
weight function fully symmetric particular symmetric get gamma dx gamma delta delta delta gamma dx delta delta delta dx gamma delta delta delta gamma gamma dx dx delta delta delta dx gamma dx delta delta delta dx gamma delta delta delta gamma delta dx delta delta delta dx gamma dx delta delta delta dx follows weight function fully symmetric integration rule form equation exact monomial odd power 
consider monomials powers 
develop precision rule points 
rule developed possible derive rule just points rule numerically stable 
chapter 
non linear cpds fully symmetric function form gamma dx sigmau monomials degree odd power monomials lead equation just equations dw gamma dx gamma dx parameter free parameter choosing get gamma get precision rule value different choices lead different approximations 
small values lead local approximations behavior near mean gaussian affected higher order terms julier uhlmann ju suggest choose general different values lead better worse approximations depending function precision look rule form gamma dx sigmau sigmau sigmau note get point generator points generator sigmau gamma points sigmau sigmau generator total points 
time sure rule exact monomials precision odd powers 
due symmetry generators consider monomials get set equations dw gamma gamma dx gamma gamma dx chapter 
non linear cpds gamma gamma dx gamma dx example second equation term gamma gamma points coordinate sigmau generator sigmau sigmau 
divide third equation second obtain values follow immediately 
get gamma gammad technique exact monomial rules provides new useful perspective unscented filter uf ju suggested alternative ekf tracking non linear dynamic systems 
uf extremely accurate cases ekf leads poor approximation ju 
point unscented filter exactly precision rule equation superior performance compared ekf instance generally better accuracy numerical integration approach compared ekf accurate numerical integration rules 
view unscented filter immediate practical consequences trade accuracy computation computational requirements 
example interested precise rule unscented filter willing evaluate function points exact monomial rule precision 
depending function may represent significant gain accuracy 
dealing general gaussians integrals appear equation equation weight function sigma general represent different weight function 
directly set points chosen general gaussians 
fortunately transform points general gaussian keeping accuracy guarantees 
chapter 
non linear cpds theorem sigma 
square matrix aa sigma gamma dx gamma ax dx proving theorem consider usefulness 
sigma positive definite possible find matrix called square root sigma 
example cholesky decomposition find lower triangular 
theorem lets transform integral involves general gaussian integral involves standard gaussian 
set points compute integral gaussian apply linear transformation ax point 
intuitively linear transformation corresponds scaling point covariance matrix sigma shifting mean proof note aa sigma follows gamma sigma sigma gamma gamma note jaj ja jaj sigmaj 
write ay sigmaj exp gamma ay gamma sigma gamma ay gamma jaj exp gamma sigma gamma ay jaj exp gamma iy jaj compute integral gamma dx perform change variables ay get gamma dx gamma ay ay dy chapter 
non linear cpds numerical integration non linear cpds 
sort nodes topological order 

cpd linear 
find moments par approximate gaussian 
find covariances fx gamma gamma par equation 

compute moments theorem numerical integration approach gamma jaj jaj ay dy gamma ay dy putting adapt algorithm numerical integration approach shown 
sort variables topological order linear cpds simply theorem 
non linear cpds compute moments cpd variable parents integration rules simple grid approach gaussian quadrature rule low dimensions higher dimensions exact monomial rules equation equation 
left explain compute covariances variables fx gamma gamma par 
add cpd include integrals compute 
approach leads integrals high dimensions forcing relatively inaccurate integration rules 
fortunately alternative approach 
variable non linear cpd variable appeared topological order 
interested finding chapter 
non linear cpds cov 
processed gaussian approximation 
corollary order represent linear combination fi fi representation cov zy gamma fi fi gamma fi fi fi fi wy gamma fi gamma fi gamma fi gamma gamma fi cov reason wy independent independent note representing linear combination involves solving set linear equations ax covariance matrix cov cov 
need perform computation node efficiently decomposing cholesky decomposition solve set linear equations see details 
computing cov equation exact conditions ffl errors cov ffl true joint probability distribution gaussian words truly gaussian ancestors non linear cpd get correct cov able find correct moments 
reason approximation suffer errors resulting chapter 
non linear cpds approximation 
possible rely gaussian estimate integrals involve variables ancestors practice integrals higher dimensions inaccuracies arise due accurate integration rules negate potential improvement gaussian approximation 
encapsulated variables just structure bayesian network decompose dependency various variables cases possible decompose non linear dependency 
example may able decompose non linear function directly approximating gaussian fx define extra variables 
find gaussian approximation 
approximate gaussian equation approximate gaussian 
approximate gaussian equation find gaussian approximation 
accuracy tradeoffs discussed previous section apply reducing dimension integrals solve accurately may introduce errors interactions extra variables non linear 
principle add bayesian network treat regular variables 
doing increases number variables gaussian denoted increases algorithm space complexity need represent covariance matrix 
better treat extra variables local variables encapsulated cpd unknown rest network 
computing gaussian approximation example consider gas flow sensor rwgs system described chapter 
gas flow flow sensor gives different readings depending gas type 
assume random variables represent total flow compositions different gases ff ff function may product ff representing net flow gases 
function weighted sum flows weights correspond sensor response different gases 
chapter 
non linear cpds cpd variables simply marginalize encapsulated variables compute covariance network variable 
example reduce gaussian gaussian 
terms algorithm operation viewed implementation line encapsulated variables considered variables bayes net 
note encapsulated variable belongs just cpd non encapsulated child 
approach similar local computations oobn model kp variables local variables encapsulated cpd 
moments correction unfortunately numerical integration approach significant problem guarantees resulting covariance matrix positive definite positive semi definite 
example estimated variance difference gamma underestimate overestimate negative variance possible way dealing problem accurate integration rule solves problem practice 
problem may persist accurate rule 
furthermore accurate rule computationally expensive may undesirable 
alternative approach find closest positive definite covariance matrix 
cast problem convex optimization problem bv ber 
consider problem approximating multivariate gaussian nonlinear function parents fx 
assume results numerical integration rule 
denote recall var gamma cov gamma 
covariance matrix chapter 
non linear cpds sigma gamma gamma gamma sure sigma legal covariance matrix positive definite 
theorem useful proof appendix 
theorem sigma symmetric matrix form sigma positive definite matrix dimension theta vector dimension scalar 
sigma positive definite iff gamma gamma 
distribution known assumed legal equation fixed allowed change change little possible order sigma positive definite 
formulate optimization problem norm express closeness minimize gamma gamma gamma subject gamma gamma gamma gamma gamma ffl note ffl demand sigma semi positive definite 
sure positive definite ffl needs small positive number 
note original estimates result legal covariance matrix solution optimization problem done 
gamma positive definite functions convex convex optimization techniques solve chapter 
non linear cpds problem 
lagrange multiplier form lagrangian gamma gamma gamma gamma gamma gamma gamma ffl take standard approach solving dual problem maximize def inf subject function called dual function 
convex respect find inf setting partial derivatives gamma gamma gamma gamma gamma gamma gamma gamma setting equation get gamma gamma gamma gamma gamma set equation get set equation gamma gamma chapter 
non linear cpds plugging equation equation get gamma gamma gamma gamma gamma gamma gamma gamma gamma solving equation get gamma gamma gamma gamma equations define infimum value plug equations equation get function defined equation 
function get concave bv ber hard find value maximizes 
particular find solving equation 
finding may appear cumbersome fact lemma 
lemma consider convex optimization problem minimize subject convex differentiable 
define lagrangian dual function define function arg inf 
monotonically decreasing function 
proof concave know monotonically decreasing 
show note rf delta rx rf delta rx rx delta rf rf chapter 
non linear cpds defined infimum 
convex gradient respect infimum zero rf rf 

find optimal value need solve equation defined equation functions defined equations 
monotonically decreasing solve equation bisection 
find value non negative value non positive 
value note violate constraint equation know simply choose 
find value convenient define function gamma gamma gamma gamma ffl monotonically decreasing write gamma gamma gamma gamma gamma gamma follows choose 
lower upper bound actual value bisection order solve equation 
equations compute plug equation chapter 
non linear cpds variance kl divergence ekf rule rule optimal ekf rule rule comparison gaussian approximations optimal approximation resulting distribution var var get new covariance matrix 
represents efficient algorithm correct integration results guaranteeing legal covariance matrix 
terms algorithm plug procedure lines correcting moments par 
note par represents legal gaussian gamma represents legal gaussian generated line legal gaussian 
example compare ekf approach exact monomial rules simple nonlinear function simplicity assume independent oe oe 
computed gaussian approximation distribution ekf exact monomial rule precision exact monomial rule precision 
example precision rule led legal covariance matrices precision rule resulted illegal covariance matrices oe 
results reported precision rule include correction technique described section 
chapter 
non linear cpds mean variance kl divergence optimal ekf rule rule table comparison gaussian approximations var var shows kl divergence optimal approximation various methods 
optimal approximation accurate gaussian quadrature rule grid theta integration points 
general quality approximation method degrades increase oe expected methods accurate low order polynomials larger oe larger contribution higher order terms 
small medium variances ekf exact methods 
large variances precision rule accurate precision rule maintains better precision ekf 
reason precision rule behavior illegal covariance matrices oe moments correction capable dealing problem long oe large oe inaccuracies significant 
important note high variances gaussian approximation approximation important close optimal gaussian approximation 
low variances approximation corrected precision rule significantly dominates ekf approximation 
shows distribution oe precision rule quite accurate corrected precision rule significantly accurate ekf 
results summarized table including kl divergence optimal approximation 
combination enumeration algorithm far focused networks contain continuous variables saw approximate joint distribution multivariate gaussian 
hybrid chapter 
non linear cpds networks ideas combined easily enumeration algorithm section 
recall enumeration algorithm enumerates discrete assignments prior computes corresponding gaussian 
adding non linear cpds change prior distribution discrete variables exact algorithm enumerating discrete assignments 
discrete assignment back case distribution continuous variables cpds linear 
discrete instantiation numerical integration approach ekf order approximate induced distribution continuous variables gaussian 
moments correction algorithm order sure covariance matrices positive definite 
note approach combine rb lw algorithm generates discrete assignment techniques deal non linear cpds 
chapter augmented clgs main restrictions clgs graphical model allow discrete variables continuous parents dependency arises domains 
example consider feedback control loop involving thermostat controls room temperature turning heating device cooling system 
thermostat modeled discrete variable heating cooling idle depends continuous variable representing room temperature 
example consider modeling sensors warning sign appears gas level car gas tank gets low 
discrete variable sign sign depends continuous gas level 
chapter define class augmented clgs discuss inference models 
augmented clgs cpds continuous nodes linear cpds allow discrete nodes depend continuous parents 
cd refer cpds 
representation possible functional forms represent cd cpds 
useful softmax logistic function 
discrete node chapter 
augmented clgs empty full gas level sign level sign level action heat action idle action cool examples softmax cpds low gas level warning sign thermostat possible values am parents 
define exp exp case discrete parents modeled clgs define different softmax function combination discrete parents 
possible eliminate linear combinations dividing numerator denominator exp leaving gamma sets parameters th set get division 
particular binary new form simplifies standard sigmoid function 
define gamma 
exp exp shows examples softmax cpds 
softmax function binary variable corresponding low gas level warning sign 
tank full sign tank empty chapter 
augmented clgs generalized softmax cpd sign 
parameters softmax cpd determine slope transition location determines slope gammaw determines location 
models thermostat 
temperature thermostat turn heating turn cooling system 
thermostat keeps appliances 
location transitions determined relative values slope transition corresponds noise level magnitude 
possible generalize softmax functions express fairly complex distributions 
intuitively generalized softmax cpd defines set regions parameter choice 
regions defined set linear functions continuous variables 
region characterized part space particular linear function larger 
region associated distribution values discrete child distribution variable region 
actual cpd continuous version region idea allowing smooth transitions distributions neighboring regions space 
precisely discrete variable continuous parents fx assume possible values fa amg 
regions defined vectors parameters vector chapter 
augmented clgs vector weights specifying linear function associated region 
vector fp probability distribution am associated region 
cpd defined ff ff exp exp words distribution weighted average region distributions weight region depends exponentially large value defining linear function relative rest 
power choose number regions large wish key rich expressive power generalized softmax cpd 
demonstrates expressivity 
example different regions 
choice determines regions slope transitions choice determines distribution defining region 
algorithm augmented clgs approach inference augmented clgs observation gaussian approximation product gaussian softmax cpd mur 
quality approximation depends slope logistic function compared variance gaussian demonstrated 
sharper logistic function worse approximation cases accuracy approximation high 
observation product gaussian logistic function approximated gaussian suggests extend lauritzen algorithm enumeration algorithm case augmented clgs 
unfortunately case continuous non linear cpds augmented clgs enumeration algorithm impractical shall discuss section 
section chapter 
augmented clgs concentrate extending lauritzen algorithm augmented clgs numerical integration approach 
goal come algorithm exact terms moments numerical integration errors 
variational approach review algorithms murphy mur wie extending lauritzen algorithm augmented clgs variational approach 
roughly speaking idea build strong clique tree introduce variational parameters clique potentials canonical factors defined section parameterized extra parameters 
try find set parameters represents approximation distribution represented bayesian network 
variational approach limit networks discrete variables continuous parents binary cd cpds form equation 
formally oe sigmoid function oe def exp gammax murphy mur suggests bound oe log oe log oe gamma gamma take value gamma oe combining bound equation representation binary softmax cpd equation get bound softmax cpds gamma kx gamma ww gamma chapter 
augmented clgs logistic cpd gamma logistic cpd gamma exact gaussian exact gaussian product product approximating product gaussian logistic cpd gaussian chapter 
augmented clgs log oe gamma delta gamma gamma 
equation quadratic form 
value variational parameter get bound softmax cpd 
multiply quadratic form potential strongly rooted tree tree represent distribution parameterized variational parameters 
important note variational parameters representation mixture gaussians 
goal find best variational parameters give best possible approximation 
variational approach minimizing kl divergence distribution represented tree true underlying distribution done iterative algorithm 
variational parameter corresponding equation update rule bw moments computed lauritzen algorithm 
murphy mur discusses choose initial values variational parameters important problem bad values lead poor local maximum 
wie shows approximation significantly improved different variational parameters gaussian canonical factor just variational parameter softmax cpd 
variational approach quite elegant suffers problems 
currently limited binary softmax distributions quite hard extend generalized softmax cpd 
importantly note method optimizing bound equation offer real guarantees bound may tight finding lower bound log oe guaranteed result approximation posterior distribution 
furthermore iterative algorithm optimize parameters natural minimize kl divergence harder 
chapter 
augmented clgs guaranteed converge global maximum poor local maximum 
currently method estimating close result best possible approximation expressive power distribution approximates posterior mixture gaussians correct moments mixture component 
numerical integration approach possible extend lauritzen algorithm deal augmented clgs performing variational approximation direct approach numerical integration 
resulting algorithm exact errors introduced numerical integration 
simplify presentation take approach chapter algorithm context canonical forms 
simple matter adapt algorithm conditional forms shall show section 
simple motivating example 
consider network gaussian distribution oe cpd softmax ax 
clique tree single clique factor contain product cpds 
contain continuous functions product gaussian sigmoid 
approximate resulting distribution mixture gaussians need compute marginal distribution moments conditioned different values gamma dx gamma xp dx gamma xp dx gamma dx chapter 
augmented clgs gamma dx basic idea leads outline algorithm 
roughly follow lauritzen algorithm diverging cases clique contains cd cpds case approximate factor mixture gaussians mixture gaussian correct second moments instantiation discrete variables 
section fill details algorithm addressing subtleties arise 
algorithm integrable distributions difficulty applying algorithm arises observation equations compute expectations relative evaluate expressions clique probability distribution clique 
unfortunately message passing algorithm guarantee distributions available initially 
consider network continuous nodes discrete 
clique tree network consists cliques 
lauritzen algorithm clique tree initialized incorporating cpds corresponding cliques 
case incorporate clique computing relevant expectations 
initialization phase message passing performed 
canonical form represent gaussian distribution preventing performing integration multiply cpd clique stage 
address problem introducing preprocessing phase serves guarantee cliques contain integrable distribution gaussian distribution relative compute relevant expectations non gaussian canonical form 
build standard clique tree bn initialize clique potentials 
insert cpds cd cpds 
resulting network equivalent clg calibrate chapter 
augmented clgs exact evidence softmax incorporating softmax cpds evidence lauritzen algorithm resulting probability distributions clique 
insert remaining cd cpds re calibrate tree 
note cliques tree designed accomodate insertion operation 
integrable distributions perform approximation 
example calibrate clique tree cpd obtaining distribution introduce cpd clique distribution integration distribution compute necessary expectations 
ensure defined distribution clique tree 
approach raises question distribution clique tree integration distribution 
unfortunately reasons distribution approximation lead errors 
discuss show correct 
conditioning evidence lauritzen algorithm possible condition distribution evidence part algorithm 
need deal non linear cpds chapter 
augmented clgs longer true 
consider example network assume observed 
minimal cliques 
current algorithm enter cpds calibrate tree 
insert cpd calibrate tree approximating distribution mixture gaussians 
enter observed evidence incorporating approximate distribution true potentially leading sub optimal approximation 
shows example phenomenon approximation obtained integrating cd cpd conditioning evidence sub optimal approximation 
optimal approximation uses posterior directly integration distribution 
solution problem straightforward ensure clique gaussian distribution ensure gaussian distribution accounts evidence 
incorporate evidence propagate entering cd cpds 
note evidence continuous variables cause difficulties approximation evidence discrete variables merely changes probabilities mixture components incorporated integration 
restricting integration cliques subtle problem choice integration distribution relates collapsing lauritzen algorithm 
consider network assume clique tree cliques note tree inconsistent strong triangulation 
current algorithm calibrate clique tree cpds insert cpd clique 
distribution clique correct prior distribution correct prior distribution modes value appear clique lauritzen algorithm collapses modes single gaussian losing bimodal nature 
lemma ensures approximation introducing new errors functions linear hold chapter 
augmented clgs optimal approximation suboptimal approximation error introduced discrete neighbors clique sigmoid cpds non linear functions 
cpd linear may get optimal approximation moments 
shows example approximation worse collapsed distribution integration distribution 
solution enter cd cpd clique integration distribution continuous parents correct 
variables maximal continuous connected component 
denote cd variables continuous discrete participate cd cpd 
definition dn discrete neighbors discussed cd mode assignment dn 
want represent exact multimodal distribution cd necessary sufficient clique containing variables cd dn 
course requirement result larger clique tree overhead large 
discussed section dn clique strongly rooted tree 
worst add continuous variables cliques 
representation canonical forms multivariate gaussians quadratic number variables chapter 
augmented clgs sharp separate sharp combined flat separate flat combined error introduced sigmoid cpds entered separately size tree grow polynomial factor worst 
note modification clique tree necessary want guarantee optimal approximation 
algorithm remains defined approximate integration distribution quality approximation degrade 
clique tree clique contains cd contains subset variables dn 
obtain spectrum approximations tradeoff complexity accuracy 
simultaneous insertion cd cpds final problem arises cd cpd 
simply insert cd cpd sequentially 
insert cd cpd approximate resulting joint distribution mixture gaussians proceed mixture basis inserting cd cpd 
unfortunately approach problem integration distribution cd cpds inserted approximation correct non gaussian distribution resulting insertion earlier cd cpds 
chapter 
augmented clgs solution problem integrate cd cpds continuous connected component operation 
show difference network tried inserting softmax cpds clique containing separately 
experimented step transitions sharp sigmoids smoother transitions 
allows better approximation gaussian error doing approximation step step 
difference clearly manifested 
idea joint integration expensive note relevant cpds clique discrete neighbors increase size tree 
pay price computing integrals higher dimensions 
reduce cost integrating cd cpds 
get spectrum approximations tradeoff complexity accuracy want avoid high dimensional integration approximate inserting cpds separately small groups 
possible exact monomials methods discussed section get efficient integration rules high dimensions 
full algorithm inference augmented clgs 
hybrid bayesian network evidence query wish compute 
note cd cpds integrated achieve best approximation integrated separately discussed 
clique integration having defined structure algorithm remains discuss integration process cliques 
integrals form product gaussian function integration techniques discussed chapter 
desirable reduce dimension integrals involved possible doing speed computation alternatively achieve better accuracy spending time computation integral 
reducing dimension integrals special importance case cliques algorithm contain chapter 
augmented clgs extending lauritzen algorithm augmented clgs 
construct strongly rooted tree maximal connected components dn cd belong clique 
insert cpds softmax cpds 
insert continuous discrete evidence 
calibrate tree lauritzen algorithm 
instantiate cd cpds continuous evidence 
maximal connected component 
find softmax cpds go 
insert multi dimensional integration 
re calibrate tree 
return distribution inference algorithm augmented clgs continuous variables particularly keep clique contains entire continuous connected component 
fortunately properties augmented clgs significantly reduce computational burden 
observation exploits fact dealing gaussian distributions similar trick discussed section 
assume continuous variables partitioned sets variables appear cd cpds 
recall represent multivariate gaussian variables fy zg linear gaussian network structure edges variable variable cd cpds change distribution variable depends linearly variables lemma ensures having moments infer moments numerical integration 
incorporate variable cpd required integration dimension exactly number continuous parents interestingly substantially improve idea case cpd softmax 
softmax node soft threshold defined set linear functions continuous parents function value eliminate discussed section 
shall assume fm jaj left chapter 
augmented clgs gamma functions ff fm gamma define set new variables deterministic linear function variables reinterpret softmax cpd parents variables note linear functions parents gaussian distribution induces gaussian distribution distribution integration distribution 
represent variables linear combination apply lemma find correct distribution moments 
dimension integrals perform jaj gamma jaj number values choice methods perform insertion clique integration dimension min jaj gamma jy jy number continuous parents dealing binary variables approach result dramatic savings 
generally set variables linear combinations represented linear combination integration dimension jzj 
particular choose ff fm gamma recall fm choose jzj min jaj gamma jy 
possible represent functions linear combinations represent vectors linearly dependent may find smaller dimension 
example assume jaj jy 
notation equation set parameters gamma gamma gamma gamma gamma chapter 
augmented clgs step sure subtracting coefficients dividing exponents exp 
get gamma gamma note set parameters represent exactly softmax distribution original set 
principle define fz nonzero functions 
fact gamma define fz third function defined gamma define gamma variables define softmax cpd parents new set parameters represents distribution gamma gamma example exp gamma gamma exp exp exp gamma gamma integrals defined 
reduced dimension integration 
course construct networks integration dimension large networks discrete variable values continuous parents cd cpds need integrated clique 
cases choose exact monomials method relatively low precision rule 
chapter 
augmented clgs conditional forms discussed section canonical forms suffer known numerical instabilities extension lauritzen algorithm inherit 
lauritzen jensen modified algorithm lj improves original conditional forms numerically stable incorporate deterministic cpds 
enjoy benefit numerical stability adapt algorithm conditional forms 
version lauritzen algorithm clique potentials conditional gaussians strong root potential multivariate gaussian 
need ensure cd cpds inserted clique strong root guaranteeing integrable distribution 
algorithm property forced line 
note tree strong root contain continuous variables cd cpds case algorithm defined canonical forms guaranteed give correct moments change clique tree push operations lj 
operations push continuous variables strong root ensure property line algorithm holds 
possible consequence addition continuous variables cliques 
added complexity worst case polynomial number continuous variables 
analysis show algorithm exact errors caused numerical integration 
exact sense lauritzen algorithm computes correct distribution discrete nodes correct second moments continuous ones 
theorem augmented clg 
query clique tree evidence 
algorithm computes distribution exact discrete variables chapter 
augmented clgs correct moments continuous variables inaccuracies caused numerical integration 
proof start showing algorithm exact clique tree contains just clique 
step involves inserting discrete conditional linear cpds clique tree 
variables clique need collapsing get clique potential represents exact product cpds inserted 
note product clique potential cd cpds exact prior distribution 
step involves incorporating evidence 
saw section discrete evidence viewed extra factors multiplied tree entry factor corresponding evidence rest 
multiplying factors tree exact operation introduce inaccuracies 
consider continuous evidence 
setting evidence equivalent setting relevant values function product tree cd cpds 
clique potential simply condition canonical factors conditional factors evidence 
cd cpds guaranteed continuous evidence variables appear cpd parents simply set value get new conditional distribution involving cd cpd just discrete factor 
step insertion cd cpds clique potential 
able represent answer exactly clique potential exact posterior distribution 
practice true posterior distribution mixture non gaussians approximate mixture components gaussian 
third step produces correct moments inaccuracies caused numerical integration 
remove assumption single clique tree assuming general strongly rooted tree 
clique defined line algorithm 
consider network get removing cd cpds 
clg clique strong root note contains discrete neighbors maximal connected components represent exact distribution continuous variables chapter 
augmented clgs collapsing 
theorem know line algorithm distribution represents correct posterior distribution variables similarly single node case multiply cd cpds numerical integration get correct distribution discrete variables correct moments continuous variables inaccuracies caused numerical integration 
perform downward pass tree send messages outward 
proof get correct moments downward pass numerical integration inaccuracies arbitrary clique similar proof theorem 
induction length path know correct marginal 
node define node path define sepset induction hypothesis canonical factor represents correct discrete distribution correct moments numerical integration errors 
idea proof theorem equation show variables belong weak marginal variables correct weak marginal numerical integration errors holds left prove claim variables gamma strong triangulation variables gamma continuous variables discrete case message passing operation exact claim trivial 
observe way tree built line continuous variables appear cd cpd clique variables gamma appear cd cpd variables gamma linear dependency variables dependency linear lemma applies correct moments variables numerical integration inaccuracies 
point numerical integration errors magnified discrete evidence 
recall order get conditional distribution renormalize discrete weights sum dividing chapter 
augmented clgs kl flat sigmoid kl sharp sigmoids kl flat sigmoid kl sharp sigmoids experimental results error caused inserting cd cpds separately kl divergence discrete variables kl divergence continuous variables likelihood evidence 
divide small number slight errors approximation significant 
possible reduce effect evidence step process run algorithm obtain estimate posterior discrete variables rerun precise expensive integration rules mixture components posterior distribution 
experimental results inconvenient part extension lauritzen algorithm need perform integration cd cpds simultaneously order get optimal approximation 
interesting examine important insert cd cpds versus time 
experimented network 
show kl divergence discrete continuous variables distributions obtained inserting softmax cpds versus time 
error depends strength correlation considered different correlation values ranging 
chapter 
augmented clgs crop price buy waste type filter state burning regime metal waste emission light efficiency dust emission sensor metal emission metal sensor dust sensor policy rain crop price buy profit augmented clgs original crop network extended emission network extended crop network changed parameters softmax cpds allowing smooth transitions case flat sigmoid step transitions case 
see error larger strong correlation variables influenced different cd cpds sigmoids sharp making gaussian approximation worse 
interestingly accuracy high cases insert cd cpds separately associated computational benefits get accurate results 
compared performance algorithm examples considered researchers 
tested algorithm crop network shown 
compared results results reported murphy mur running gibbs sampling bugs gts 
turns murphy variational algorithm performs quite poorly posterior distribution multi modal achieving errors binary discrete variables 
hand algorithm bugs performed simple network giving correct result instantaneously 
note wie reports results network variational approach 
test algorithm larger network emission network lau models emission heavy metals waste 
original chapter 
augmented clgs network clg 
augmented extra discrete binary variables shown 
additional variables correspond various emission sensors cd cpd dust sensor gamma sensor metal emission sensor gamma 
experimented various queries algorithm compared runs gibbs sampling bugs 
bugs unstable produced results differed significantly 
example queried distribution emission dust setting metal emission sensor sensor high 
algorithm returned mean variance 
bugs converged samples mean variance change substantially samples 
understand discrepancy likelihood weighting query 
samples estimated mean estimated variance agree quite closely results produced algorithm 
algorithm able converge gaussian quadrature numerical integration technique points dimension highest integration dimension 
algorithm instantaneous faster bugs likelihood weighting 
final example tried algorithm network containing non softmax cd cpds 
augmented crop network variables shown 
profit variable depends product crop price variables 
parameters extended network appear table profit negative profit investigated discrepancy discovered bugs returns answers disagree appearing lau original emission network cd cpds 
example standard deviation converged mean correct 
chapter 
augmented clgs node distribution policy rain drought liberal drought conservative average liberal average conservative floods liberal floods conservative crop drought average floods price gamma gamma buy gamma profit sub buy exp pc exp pc sub buy exp exp sub buy exp pc exp pc sub buy exp exp table parameters augmented crop network time seconds kl error lw error price lw error rain profit alg 
error rain profit experimental results comparison likelihood weighting chapter 
augmented clgs profit positive having experienced problems bugs compared results likelihood weighting 
tested scenarios evidence evidence profit compared accuracy algorithms various queries 
numerical integration points dimension ground truth 
ran lw algorithm amount time measured kl divergence ground truth results 
lw averaged runs short runs number samples smaller variance larger averaged results larger number runs 
shows results kl error price evidence rain profit 
lines visible kl error algorithm case evidence close zero visible graph 
clear algorithm converges faster lw especially evidence 
expected discussed sampling methods take lot time converge performance lw deteriorates evidence 
enumeration approach main problem approach generalizing lauritzen algorithm augmented clgs saw section lauritzen algorithm scale case regular clgs 
recall chapter dealt problem enumeration algorithm saw section extend algorithm non linear cpds involving continuous variables 
hope similar ideas augmented clgs resulting distribution augmented clgs mixture distribution approximate mixture component gaussian 
unfortunately enumerating discrete hypotheses prior harder task augmented clgs clgs 
fact finding just discrete instantiation polytree augmented clgs np hard 
intuitively reason discrete variables depend continuous variables chapter 
augmented clgs xn xn augmented clg reduction theorem distributions jx jx reduction longer ignore continuous parts network purposes enumeration prior pay price terms significant added complexity 
theorem problem np hard ffl instance polytree augmented clg binary discrete variables 
ffl question assignment discrete variables proof reduction similar theorem 
show reduction subset sum problem set non negative integers fs positive integer asked exists subset reduce problem augmented clg polytree structure shown 
going details explain general idea proof 
variables distribution theorem 
variables represent selector function determines 
assignment variables induces gaussian mixture gaussians mode iff exists variables noisy indicators value noisy indicator gamma noisy indicator 
note chapter 
augmented clgs part assignment iff exists solution subset sum problem 
nodes sure solution subset sum problem assignment assignment 
show exists solution subset sum problem iff discrete assignment augmented clg 
formally define cpds variables 
discrete variables binary 
variables uniform distribution continuous variables oe oe oe gamma gamma oe gamma oe cpds logistic cpds shown 
note jx exp gammaff gamma jj jx exp ff gamma gamma jj cpds jb jb set values constants ff values chapter 
augmented clgs clear proof 
just assume ff positive 
assume exists solution subset sum problem 
show part discrete assignment 
ha assignment ha corresponding iff note similar arguments ones theorem ja noe consider assignment ha denote ha gamman delta delta need find 
note independent gamma dx gamma dx gamma exp gammaff gamma exp ff gamma gamma gamma exp ff gamma gamma exp ff gamma gamma exp gammaff gamma dx exp gammaff phi gamma gamma phi gamma gamma exp gammaff phi gamma phi gammac chapter 
augmented clgs notation phi gamma dx 
choices ff get delta delta gamman consider assignment 
assignment likelihood assignment delta gamman delta gamman delta gamman case assignment 
show exist solution subset sum problem assignment 
consider assignment ha involving ha assume gamman fact gamma gamma get gamma dx gamma exp gammaff gamma jjp dx dx gamma exp gammaff gamma jjp dx dx exp ff gamma dx dx chapter 
augmented clgs exp ff phi gammac choices ff get probability assignment delta gamman realize assignment need remember matter assignment choose combinations combination combination delta gamman delta gamman get assignment 
case similar enumeration prior hard simple augmented clgs general part efficient approximate inference algorithm 
imply different version enumeration algorithm augmented clgs 
enumeration prior heuristic tries achieve enumeration order close order induced posterior distribution 
possible come heuristics try approximate order induced posterior distribution tractable augmented clgs 
leave problem finding tractable heuristics open issue address thesis 
chapter dynamic bayesian networks far considered static environments environments evolve time 
perform tasks including fault diagnosis need model stochastic processes various variables different values time 
chapter discuss extend bayesian networks dynamic case important inference algorithms 
sections draft textbook koller friedman 
modeling stochastic processes stochastic process convenient view state terms snapshot encodes process state instant 
state represented usual assignment values set random variables 
denote random variables represent state time accordingly represents entire state time 
representational simplification model time proceeding discrete regularly scheduled time slices 
viewing time continuous assume take integer values state initial time slice state single time increment 
assumption consistent way sensors practice value sensor sampled constant frequency frequency natural candidate chapter 
dynamic bayesian networks represent discrete time units model 
assumption represent process probability distribution 
done directly representing distribution representing conditional distribution gamma 
note representation encodes fact time moves forward state time depends history system time second simplifying assumption 
letting depend entire history system time step way time step gamma assume dependency goes back time steps past assume gamma gammak gamma thesis strongest form assumption 
special case called markov assumption 
definition say dynamic system variables satisfies markov assumption gamma gamma way state markov assumption independent gamma independent past 
conditional independence assumption markov assumption may reasonable 
example assume tracking moving object variable location 
location time independent history location time need know direction object moving velocity 
markov assumption realistic adding variables model 
model accurate adding variables represent acceleration object 
general transform model depends gammak gamma fixed markov model treating variables gammak gamma part system state system state memory chapter 
dynamic bayesian networks drunk vel pos weather sensor failure terrain drunk vel pos weather sensor failure terrain sensed pos drunk vel pos weather sensor failure terrain drunk vel pos weather sensor failure terrain sensed pos drunk vel pos weather sensor failure terrain sensed pos highly simplified tbn monitoring vehicle 
unrolled dbn time slices gamma time slices past 
process unbounded need represent infinitely conditional probabilities gamma 
usually simplifying assumption definition say markovian process time invariant homogeneous stationary case represent model transition model represents variables current time slice represents variables 
definition dynamic bayesian networks markov stationarity assumptions key representing stochastic dynamic processes representation transition model conditional probability distribution 
represent model time slice bayesian network tbn 
definition time slice bayesian network tbn process state variables bayesian network fragment defined follows 
graph structure directed acyclic graph nodes nodes parents 
nodes annotated cpds par 
tbn chapter 
dynamic bayesian networks represents conditional distribution chain rule par shows simple example tbn vehicle monitoring application 
tbn models behavior vehicle environment board camera tries estimate vehicle position order keep lane 
stationarity property model defines probability distribution put distribution initial states defines distribution arbitrarily long sequences time slices resulting dynamic bayesian network model dk definition dynamic bayesian network dbn pair hb 
bayesian network representing initial distribution states 
tbn process 
distribution defined delta gamma desired window length easily compose initial bayesian network transition network 
define bayesian network variables window 
process called unrolling dbn 
example shows bayesian network obtained unrolling dbn time slices 
edges time slice ones shown separately 
cpds cpd cpd copied cpd 
note edges go time slices nodes time slice 
intuitively depends temporal aspect interaction 
effect immediate shorter difference time slices influence manifested roughly time slice 
effect longer term influence manifested time slice 
addition chapter 
dynamic bayesian networks arcs time slices called persistence arcs representing tendency certain variables weather persist time high probability 
just bayes net dbn encode sophisticated reasoning patterns long explicit part model 
example explicitly encoding sensor failure reach sensor failed 
example get reading tells unexpected car suddenly feet left thought seconds ago addition considering option car suddenly consider option sensor simply failed 
note model considers options built 
sensor failure node sensor reading depend current location way explain sensor readings trajectory 
inference task inference dbns usually categorized possible types queries ffl prediction probability distribution current state compute distribution set variables ffl monitoring tracking observation comes time slice maintain distribution current state evidence seen far 
distribution def called belief state time ffl probability estimation sequence observations distribution intermediate state ffl explanation initial state observations sequence sequence states explaining observations 
dbn version mpe problem section 
chapter 
dynamic bayesian networks unrolling dbn state variables resulting clique tree prediction monitoring probability estimation solved clique tree inference 
simply unroll dbn perform inference resulting bayes net 
example want perform monitoring unroll dbn time set observations inference engine find probability distribution problem approach inefficient 
suppose monitor condition patient information various sensors 
process run long unrolled dbn large example monitor patient time intervals seconds days 
wasteful practical unroll dbn perform probabilistic inference scratch results time gamma order speed computations time understand online version algorithm useful think dbn having just state variable observation variable special case dbns called hidden markov models hmms rab 
shows example dbn rolled time slices shows resulting clique tree 
easy verify monitoring task equivalent upstream pass clique tree upstream left right pass message passed fx gamma clique fx clique probability downstream evidence get appropriate conditional probability simply renormalize factor 
key point messages clique tree simply unnormalized belief state leading simple line algorithm avoids full chapter 
dynamic bayesian networks unrolling dbn time slice 
assume induction calculated compute evidence propagate state forward def simplification step due markov assumption 
condition evidence bayes law get simply multiply renormalize 
process computing called filtering 
solve probability estimation task need calibrate tree find clique potentials various standard clique tree algorithm doing upstream pass doing downstream pass 
fact algorithm independently developed hmm community theory clique trees developed called forward backward algorithm obvious reasons 
principle computations extend easily general dbns difference belief state defined joint distribution state variables 
key question doing inference dbns represent joint distribution 
shall see sections answer question strongly depends type dbn dealing continuous discrete hybrid 
matter type dbn belief chapter 
dynamic bayesian networks state unfortunate property dbn relatively small number time slices belief state conditional independence structure usually variables correlated past ast 
example consider network consider variables terrain pos part belief state intuitively variables independent fact 
reason terrain correlated influenced terrain turn correlated pos parents observed variable 
pos correlated pos get terrain correlated pos correlations come past variables terrain pos conditionally independent variables time belief state 
kalman filter important type dbns simplest type variables continuous cpds 
linear cpds 
models called linear dynamic systems simply linear systems models investigated communities long dbns introduced 
example tracking linear systems heart target tracking systems tracking sonar data 
widely computer vision computer graphics economics control 
popularity linear models comes simplicity importantly simplicity tracking algorithm 
key property linear systems belief state multivariate gaussian 
show induction 
linear gaussian initial belief state gaussian 
assume gaussian 
propagate belief state equation get gaussian cpds 
linear 
similarly condition evidence cpds linear gaussian 
belief state kept compact representation mean vector covariance matrix representation size quadratic number state chapter 
dynamic bayesian networks variables 
propagating belief state involves various manipulations gaussians linear gaussians done time cubic number state variables 
polynomial algorithm monitoring linear systems 
algorithm known kalman filter kf kal 
variables continuous cpds linear general belief state gaussian 
standard approach approximate belief state gaussian ideas discussed chapter ekf algorithms numerical integration rules unscented filter 
inference discrete dbns turn attention representation belief state discrete dbns 
seen section structure dbn simplify representation belief state 
belief state conditional independencies better way represent table number possible combination discrete variables 
obviously practical method reasonably sized networks variables 
exact inference usually possible dbns resort approximations 
section concentrate specific type approximation analyzed boyen koller bk 
idea belief state fully correlated correlations weak approximated independencies 
recall example 
seen variables terrain pos correlated correlation weak may reasonable approximate independence 
example consider monitoring physical system engine 
eventually belief state variables correlated may sense approximate state various subsystems gas subsystem cooling subsystem independent alternatively conditionally independent state electric subsystem 
case belief state approximated product chapter 
dynamic bayesian networks separate distributions approximate various subsystems fully independent conditionally independent 
precisely define set disjoint clusters maintain approximate belief state propagate approximate belief state forward variables correlated 
perform process time take propagate forward time condition resulting distribution evidence 
result new distribution note get correct belief state started approximate belief state 
distribution may factored product independent marginals clusters approximate 
approximation step straightforward simply compute product marginals clique tree algorithm implement process 
generate bn distribution bn fully connected subgraph variables cluster edges variables different clusters connections time slices original tbn 
generate clique tree sure variable cluster clique 
set evidence time variables calibrate clique tree 
calibration complete read appropriate cliques tree 
marginals clusters give consider case overlapping leading conditional independencies simple direct independencies 
idea think various nodes clique tree add sepsets gamma tree 
belief state represented product restrict subsets cliques clique tree arranged tree running intersection property holds 
chapter 
dynamic bayesian networks cliques divided product sepsets gamma propagation step carried clique tree algorithm 
build bayes net represents better plug belief state directly clique tree fx appropriate structure bk 
immediately clear algorithm provides useful approximation 
intuitively process introduces errors belief state time step 
errors accumulate results relevant 
luckily case 
consider parallel processes hypothetical process exact tracking correct belief states actual process approximate tracking 
analyze far diverge 
opposing forces 
stochastic transition time time adds noise belief states causing difference decrease 
conditioning evidence causes error decrease expectation 
approximation opposing force causes error increase 
shown process stochastic opposing forces remain balanced bk 
error remains bounded indefinitely time 
inference hybrid dbns third type dbns discuss chapter hybrid dbns containing discrete continuous variables 
particular interested clg dbns dbns 
clgs 
models called switching linear systems switching linear dynamic systems 
bar shalom provide models ghahramani hinton gh provide date literature survey 
reason name discrete variables system linear switches value discrete variable correspond switches system behavior 
chapter 
dynamic bayesian networks collapse propagate propagate propagate prop prop prop prop prop prop prop prop prop collapse collapse collapse collapse collapse collapse propagate propagate propagate gpb algorithm gpb algorithm imm algorithm unfortunately just situation static bayesian networks combination discrete continuous variables dbns significantly harder separately 
consider example belief state discrete variables values continuous variable continuous variable observed 
edges distribution gaussian 
propagate belief state time slice get different gaussian value belief state mixture gaussians value propagate time slice gaussians time slice results different gaussians value mixture gaussians 
hard verify belief state mixture gaussians gaussian corresponds particular combination similar reasons time slice binary variables exact belief state time mixture gaussians 
conclude hope representing exact belief state chapter 
dynamic bayesian networks trivial small number time slices 
trying deal problem ss kim gh 
assumes number discrete combination time slice relatively small difficulty comes exponential dependence section review techniques approximate belief state conditions 
chapter discuss case number discrete combinations time slice large preventing directly ideas 
propagation algorithm practical sure mixture gaussians represented belief state get large 
main techniques pruning collapsing 
pruning algorithms reduce size belief state discarding gaussians 
standard pruning algorithm simply keeps gaussians highest probabilities discards rest probabilities sum 
collapsing algorithms partitioning mixture gaussians subsets collapsing gaussians subset gaussian 
belief state partitioned subsets result belief state exactly gaussians 
different collapsing algorithms usually differ number subsets chosen exactly collapsing performed 
section algorithms gpb gpb imm 
general pseudo bayesian gpb algorithms limit number gaussians belief state gamma number possible system modes number discrete combinations induce different behavior continuous variables tbn positive integer 
example jdom note belief state time gamma gaussians step propagation gaussians 
gpb algorithms partition gaussians subsets elements subset trajectory gamma time steps past 
example gaussians subset correspond values gammak understand better examine special cases 
chapter 
dynamic bayesian networks case get gpb algorithm keeps exactly gaussian belief state 
assuming different modes behaviors gaussian time belief state get gaussians time collapsed gaussian 
algorithm shown case circle represents gaussian 
note collapsed gaussian correspond different discrete values time trajectories differ current discrete instantiation 
consider case corresponds gpb algorithm 
keep gaussians belief state 
propagation get gaussians corresponding discrete values time discrete values time 
partition mixture subsets subset gaussians discrete value different time values 
subset collapsed resulting belief state gaussians 
entire process shown 
note trajectories collapsed gaussians agree time step 
problem gpb algorithm propagation step generate gaussians computationally expensive 
gpb better gpb interacting multiple model imm algorithm efficient gpb accurate gpb 
just gpb algorithm imm algorithm keeps gaussians belief state time gpb gaussians collapsed propagating time 
give behavior gpb difference collapse gaussians different ways depending discrete value time 
values discrete variables time compute different set probabilities time mixture collapse gaussians probabilities 
example consider 
belief state distribution gaussian 
consider possible values values chapter 
dynamic bayesian networks compute distribution value mixture gaussians different probabilities belief state probabilities 
intuitively new probabilities better correspond specific mode operation 
note computing new probabilities involves discrete variables simple long large 
collapse mixture new probabilities propagate gaussian time resulting just gaussian time variables 
doing value gaussians time new time belief state 
process illustrated 
collapse belief state different ways imm algorithm create gaussians time opposed gaussians gpb 
extra compared gpb computation new mixture probabilities collapsing mixtures just usually extra computational cost small relative cost computing gaussians time 
computational cost imm algorithm slightly higher gpb algorithm significantly lower gpb 
practice imm performs significantly better gpb gpb bbs 
imm algorithm appears compromise complexity performance 
particle filters seen previous chapters sampling techniques offer alternative approach approximate inference algorithms bayesian networks 
situation similar dynamic bayesian networks 
popular sampling technique dbn inference particle filters gss kit described section 
chapter 
dynamic bayesian networks naive way sampling techniques dbns apply lw see section unrolled dbn sample corresponds possible trajectory system generate sample initially sample sample 
weigh sample likelihood evidence continue process sampling variables time slice time multiplying sample weight probability evidence 
possible modify procedure get online algorithm monitoring 
maintain set samples time slice 
associated weight 
time slice take samples propagate forward sample variables time adjust weight suit new evidence time 
set time samples answer various queries 
unfortunately simple approach practice 
recall evidence leaves lw generates samples prior distribution evidence affects weights 
example car surveillance dbn evidence leaves 
generate completely random trajectories car evidence influence weights 
unfortunately random trajectories evidence 
sample low weight 
problem regular bns dbns leads complete breakdown algorithm time evolves trajectories deviate true system trajectory corrected evidence 
samples reliable approximation system hidden state lw diverges rapidly time goes 
particle filtering pf algorithm way deal problem 
key intuition think samples time approximation belief state time truly want propagate sample propagate belief state take samples belief state propagate new samples forward 
assume belief state samples associated weights 
sample samples chapter 
dynamic bayesian networks called particles belief state propagate particles time weigh time evidence 
new set samples weights serves approximation time belief state 
note sample large sampled sample low sampled 
way view algorithm samples better 
particular samples larger weights explain evidence far better closer current state 
want sample starting point propagation large weight 
particle filtering works better lw practice quality approximation degrade time 
models natural combine particle filtering techniques section leading rao blackwellized particle filter rbpf dga 
idea sample discrete variables 
assignment discrete variables continuous variables normal distribution compute represent exactly 
particle represented assignment time discrete variables gaussian time continuous variables 
tradeoffs similar rb tradeoffs rbpf particle expensive generate contains information 
practice rbpf particles worth extra computational complexity needed compute 
chapter scaling hybrid dbns section discussed standard techniques inference models 
recall binary discrete variables time slice time steps exact belief state mixture gaussians 
techniques described section assumed small number 
assumption mixture gaussians assumed small tractable concern deal temporal blowup raising power unfortunately domains realistic assume small 
example fault diagnosis domain discrete variables represent different faults broken sensor various modes behavior valve open valve closed 
obviously reasonably sized system variables 
upshot deal case large ignore temporal blowup 
subject chapter 
collapsing algorithm section discussed belief state hybrid dbns represented mixture gaussians 
recall gpb belief state just gaussian gpb imm belief state gaussians 
unfortunately options useful 
gaussian needs fault chapter 
scaling hybrid dbns ok ok mixture collapsing simple hybrid dbn describing movement robot collapsing mixture different hypotheses diagnosis important track distinct hypotheses time order decide correct afford collapse hypotheses just gaussian 
hand mixture gaussians large needs practical keep large mixture 
fact keep gaussians belief state collapsing scheme gpb may ideal 
gpb collapse gaussians share assignment time discrete variables different assignments time 
gaussians collapse may different worthwhile keeping distinct hypotheses 
hand possible gaussians gpb keeps similar collapsed significant loss accuracy 
example assume simple hybrid dbn shown models movement robot dimensional world 
variable indicates location robot 
depending value discrete variable robot move left right 
sensors measure robot location represented sensors chapter 
scaling hybrid dbns properly broken behavior represented discrete variable ok 
sensor working ok true get noisy reading robot location 
sensor broken get reading get information robot position 
sensors differ accuracy reliability sensor accurate tends break lot second sensor noisy rarely gets broken 
consider hypotheses agree time robot moved left sensors left ok false ok false 
note hypotheses agree time variables collapsed gpb 
assume hypotheses start different robot locations time robot assumed move left cases hypotheses represent different locations time sensor reading change estimates hypothesis 
example hypotheses may may collapse hypotheses assuming likelihood get hypotheses shows mixture original hypotheses collapsed hypotheses 
see lost distinct hypotheses collapsed hypotheses believe robot original hypotheses 
especially problematic dealing physical systems 
reason values may violate physical constraints 
original mixture may reflect constraints collapsing get hypothesis probability mass forbidden zone 
desirable avoid collapsing distinctly different hypotheses 
hand consider hypotheses time originate robot location time direction robot movement sensor accurate sensor working differ sensor working 
hypotheses may similar estimate robot location realistic model sensor faults persistent time 
discuss models chapter 
chapter 
scaling hybrid dbns time sensor working noisy reading sensor little effect estimate robot location 
may sense collapse similar hypotheses keeping separate hypotheses gpb 
belief state keep gaussians gpb collapsing scheme may optimal 
may want consider gaussians similar different deciding collapse 
formalize intuition need define similarity gaussians sigma sigma 
choose define similarity terms kl divergence gaussians define distance measure desired properties ffl symmetric ffl equality iff ffl computed closed form ct tr sigma gamma sigma tr sigma gamma sigma gamma gamma sigma gamma sigma gamma gamma dimension gaussians 
having defined distance measure ready collapsing algorithm shown 
assume mixture gaussians collapse smaller mixture gaussians 
algorithm partitions mixture subsets gaussians 
subsets represent gaussians collapsed gaussians subset collapsed gaussian resulting mixture 
gaussians subset discarded 
question course create partition scheme 
greedy approach takes consideration likelihood chapter 
scaling hybrid dbns collapsing algorithm mixture gaussians maximal size mixture collapsing threshold distance matrix 
sort decreasing order weights 
mark gaussians unused 
set 
jbj unused gaussians 
hw unused gaussian 
find hw unused 
mark hw hw 

gaussian resulting collapsing hw hw equations 
ig 
collapsing mixture gaussians gaussians different gaussians similarity 
sort different hypotheses likelihoods 
starting gaussian find gaussians similar create subset fp find gaussian list far continue manner 
gaussians original list generating subsets 
define gaussians similar note importance parameter set gaussians similar collapsed 
effectively get gpb algorithm 
hand different gaussians similar subset contain exactly gaussian 
algorithm behaves simple pruning algorithm keeping gaussians discarding rest 
example assume binary discrete variables gaussian combination original chapter 
scaling hybrid dbns mixture corresponding gaussian false false false false false true false true false false true true true false false true false true true true false true true true suppose collapsing algorithm mixture size resulting mixture bounded 
gaussian look similar gaussian 
assume similar leading subset fp move remaining gaussian assuming remaining gaussians similar get subset fp remaining gaussian case similar remaining gaussian leading subset fp generated subsets 
output algorithm subsets fp fp fp gaussians discarded 
assume dimensional gaussians original mixture collapse mixture gaussians 
sorting gaussians weights takes log 
compute kl divergence gaussians necessary invert covariance matrices done original gaussians 
covariance matrices inverted evaluating equation takes 
inverting original gaussians takes nd 
equation line needs evaluated iterations gaussians iteration leading complexity 
collapsing operation line done number gaussians collapsed iteration collapsing operations done nd 
total complexity algorithm log nd 
chapter 
scaling hybrid dbns belief state representation belief state representation decomposed probability distribution discrete variables hypothesis variable previous section discussed collapsing algorithm implicitly assuming variables belief state continuous 
mentioned assumption realistic belief state contains persistent discrete variables continuous ones 
need discuss represent discrete continuous variables belief state relation 
gpb gpb representation quite simple 
gpb gaussians collapsed represent belief state product multivariate gaussian continuous variables factor discrete variables keeping just gaussian continuous variables assumed independent discrete variables 
gpb hypothesis corresponds full instantiation discrete variables belief state represented gaussian factor keep gaussian combination discrete variables 
clear belief state look collapsing algorithm 
case gaussian corresponds set discrete assignments weights 
words gaussian induces probability distribution discrete variables assignment collapsed probability zero assignments involved probability chapter 
scaling hybrid dbns proportional weight 
example consider example section 
assume discrete variables belief state 
gaussians correspond subsets fp fp fp gaussians induce distributions discrete variables renormalization fp fp fp false false false true true false true true false false false true true false true true false false false true true false true true obvious question represent type belief state 
convenient introduce new random variable called hypothesis variable 
hypothesis variable discrete variable value gaussian belief state 
define belief state graphical model shown continuous variables belief state discrete variables dm discrete continuous variables depend hypothesis variable value hypothesis variable gaussian distribution defined continuous variables factor defined discrete variables 
important note hypothesis variable correlates continuous variables discrete ones gaussian mixture corresponds value hypothesis variable turn induces probability distribution discrete variables 
chapter 
scaling hybrid dbns model dimensional robot including velocity empirical comparison compare collapsing algorithm gpb gpb slightly extended dimensional robot model model shown 
sure possible run gpb kept belief state minimum discrete states 
continuous variables belief state robot location robot velocity 
positive velocity implies robot moving right negative velocity implies robot moving left 
time step robot take actions influence velocity accelerate right increase velocity accelerate left decrease velocity adding negative constant decelerate decrease absolute value velocity 
note depending current velocity result accelerating direction similar 
sensors measure robot location sensors possible statuses failed okay 
sensor working sensor working reading noisy 
observed nodes network exact parameters network table 
example see action acc left action acc right 
chapter 
scaling hybrid dbns node discrete parents distribution action status status location velocity action action slow action acc right action acc left status status failed status okay status status failed status okay location velocity action slow action acc right action acc left gamma table parameters simple model dimensional robot unfortunately extremely small example exact inference intractable reasonably long sequence 
wanted conduct experiments sequences hundreds time steps evaluate algorithms criteria computed exact posterior distribution 
criteria 
criterion comparison omniscient kalman filter 
omniscient kalman filter gets observe discrete variables 
variables left simple linear system kalman filter algorithm 
clearly omniscient kalman filter better job tracking system method imperfect partial information comparing provides indication quality approximation 
sampled trajectory omniscient kalman filter compute gamma gamma gamma belief state continuous variables inference algorithm computed pa gamma gamma gamma belief state computed kl divergence pa averaged computing chapter 
scaling hybrid dbns gaussians discrete likelihood gpb gpb collapsing collapsing gaussians gpb gpb collapsing collapsing results dimensional robot model kl divergence omniscient kalman filter likelihood correct discrete trajectory kl divergence collapsed pa single gaussian order simple expression kl divergence 
second criterion likelihood discrete variables 
generated data knew ffi assignment discrete variables time second criterion pa ffi averaged likelihood algorithm assigned correct discrete event 
emphasize criteria perfect events happen 
example certainly possible discrete event exact belief state actual discrete assignment 
cases likelihood correct discrete event small maximizing second criterion right thing 
holds true criterion events correct ones omniscient kalman filter may poor approximation belief state 
average criteria provide reasonable indication quality algorithms 
treat results noisy sensor quality algorithms 
results shown shows kl divergence criterion shows criterion likelihood discrete trajectory 
graphs results averaged trajectories time chapter 
scaling hybrid dbns steps 
graphs show performance gpb gpb algorithms collapsing algorithm 
collapsing algorithm show performance values threshold parameter 
axis represents number gaussians belief state axis relevant collapsing algorithm gpb gaussian gpb gaussians gpb gpb shown horizontal lines 
graphs show gpb beats gpb expected 
performance collapsing algorithm poor just gaussian gpb gaussians 
note collapsing algorithm behave gpb get exact performance gpb just gaussian 
gaussians belief state performance collapsing algorithm comparable gpb cases slightly better 
interestingly kl divergence criterion collapsing algorithm go gaussians go high gaussians 
conjecture system gaussians usually keep correct hypothesis belief state adding gaussians increases kl divergence omniscient kalman filter add hypotheses different correct 
symptom kl divergence criterion compares distribution correct hypothesis comparing true belief state indication problem collapsing algorithm 
noise criteria hard say gpb performs better worse collapsing algorithm gaussians 
gpb collapsing algorithm outperform gpb performance comparable 
win collapsing algorithm achieves similar performance gpb smaller belief state 
reasonable believe number gaussians required gpb large collapsing algorithm provides useful alternative offering comparable performance gpb outperforming gpb alternative 
chapter 
scaling hybrid dbns decomposition belief state potential problem belief state representation described section size factor discrete variables exponential number variables 
number discrete variables belief state large may practical keep factor represents joint distribution 
case similar ideas bk algorithm discussed section 
impose independence conditional independence assumptions probability distribution discrete variables order get compact representation 
example probability distribution discrete variables decomposed 
keeping factor fd keep smaller factors fd fd fd leading significant savings space 
example subset discrete variables independent subsets hypothesis variable 
just bk algorithm discrete dbns choose overlapping subsets leading conditional independencies subsets direct independencies 
decomposition distribution discrete variables natural ask useful analogous decomposition continuous variables 
answer exactly analogous decomposition useful somewhat different decomposition useful 
exactly analogous decomposition keeping multivariate gaussian value hypothesis variable decompose product smaller gaussians want keep dependencies continuous variables product conditional forms 
discrete factors grow exponentially number variables space required order represent multivariate gaussian quadratic number variables gaussian representation belief state contains tens hundreds continuous variables 
system extremely large feasible represent belief state continuous variables hypothesis variable multivariate gaussian real need decomposition 
chapter 
scaling hybrid dbns different type decomposition decompose hypothesis variable continuous variables 
type decomposition lets represent larger mixture gaussians effectively keeping hypotheses belief state 
see consider mixture gaussians fx independent 
assume marginal probability mixture gaussians gamma weight gaussian 
easy see joint distribution fx mixture gaussians 
weight mixture component gamman covariance matrix mean vector looks sigma sigma absolute value coordinate mean vector 
naively represent joint distribution need represent different gaussians space requirement 
hand independencies variables represent joint distribution product marginals 
marginal represented mixture univariate gaussians space requirement 
independencies continuous variables dramatically reduce space requirements 
reason savings different need different collapsing schemes done keep marginals full joint distribution 
example assume joint distribution mixture gaussians gamma gamma gamma gamma 
consider marginal collapsing change marginal consider marginal collapse operate full joint distribution gaussians similar collapse 
case subsets continuous variables close independent useful decompose distribution define separate collapsing scheme subset continuous variables 
insight leads decomposition belief state 
partition variables belief state subsystems subsystem contains chapter 
scaling hybrid dbns belief state hypothesis variables 
variables different subsystems assumed independent 
discrete continuous variables 
example model car subsystem variables relating electric system subsystem variables relating gas system engine 
assign hypothesis variable subsystem represent distribution subsystem graphical model ones shown 
note subsystem different collapsing scheme 
simplest case shown assume variables different subsystems independent 
relax assumption various subsystems conditionally independent graphical model shown variables subsystem influence variables different subsystems 
continuous variables represent dependency conditional gaussians 
formally define belief state graphical model hypothesis variables partition variables belief state subsets subset influenced hypothesis variables 
addition discrete variables subsystem influence discrete variables subsystem similarly continuous variables subsystem influence continuous variables subsystem 
constraint impose graphical model contains cycles 
subtle difference representation representation discrete version bk 
discrete case bk algorithm represents chapter 
scaling hybrid dbns belief state hypothesis variables interdependencies sub systems conditional independencies having overlapping sets variables various subsystems making sure subsystems agree marginal distribution intersection 
problematic case 
decomposition useful mainly different collapsing schemes different subsystems 
different collapsing schemes get different mixtures continuous variables general marginal distributions intersection different different subsystems 
keep overlapping sets continuous variables different collapsing schemes different subsystems joint distribution may defined 
contrast semantics graphical model defined collapsing scheme fx defined different hypotheses subsystem different values represent different dependencies fx fx note conditional gaussians depend hypothesis variable hypotheses subsystem 
left discuss modifications collapsing algorithm account conditional gaussians 
consider example subsystem 
conditional gaussians mixture defined variables fx call variables relevant variables denote definition relevant variables subsystem denoted variables belief state subsystem direct parents decomposed belief state excluding hypothesis variables 
example relevant variables subsystem chapter 
scaling hybrid dbns fd denote continuous variables gamma gamma gamma partition gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma continuous variables subsystem gamma gamma gamma gamma continuous variables subsystem relevant subsystem example gamma gamma gamma fx gamma gamma gamma fx gamma gamma gamma gamma fx similarly denote discrete variables delta delta delta partition delta delta delta delta delta delta delta delta delta gamma delta delta delta discrete variables subsystem delta delta delta gamma discrete variables delta delta delta example delta delta delta fd delta delta delta fd delta delta delta gamma fd recall collapsing algorithm collapsed gaussians small kl divergence gaussians similar probability distributions 
interested collapsing gaussians similar conditional distribution joint distribution significantly different 
formally gaussians demand gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma similar gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma close allowing possibility gamma gamma gamma gamma gamma gamma gamma gamma significantly different 
fortunately natural metric conditional kl divergence defined log easy verify ct gamma intuitively expression means conditional kl divergence ignores differences leaves differences 
collapsing algorithm shown replace metric defined equation new metric defined gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma chapter 
scaling hybrid dbns gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma decide collapsing scheme need transform distribution conditional distribution gamma gamma gamma gamma delta delta delta gamma 
delta delta delta gamma transform factor delta delta delta distribution delta delta delta delta delta delta gamma conditional distribution delta delta delta delta delta delta gamma 
simply dividing delta delta delta delta delta delta gamma marginal delta delta delta gamma 
similarly gamma gamma gamma gamma transform gaussian gamma gamma gamma gamma gamma gamma gamma conditional gaussian gamma gamma gamma gamma gamma gamma gamma 
dividing gamma gamma gamma gamma gamma gamma gamma marginal gamma gamma gamma gamma 
need represent belief state canonical forms section conditional forms section 
putting tools necessary scale algorithms discussed section large scale hybrid dbns 
crucial components propagation algorithm ffl representation belief state discussed sections 
ffl enumeration algorithm section 
ffl numerical integration techniques discussed chapter enumeration algorithm 
ffl collapsing algorithm sections propagation step decomposed belief state time tbn 
evidence time 
task create decomposed belief state time 
create full bayesian network combining belief state tbn 
create decomposed belief state subsystem time 
subsystem enumeration algorithm order approximate distribution collapsing algorithm resulting mixture distance metric equation equation depending gamma gamma gamma gamma 
get different collapsing scheme different chapter 
scaling hybrid dbns step inference time belief state tbn evidence time 
plug 
create bn 
subsystem 
create subnetwork ancestors 
enumeration algorithm section create mixture gaussians fx 
condition evidence 
collapsing algorithm create collapsing scheme hypothesis variable subsystem 
generate time belief state discussed section 
needed generate conditional distributions hypothesis variable subsystem 
gamma gamma gamma gamma generate conditional gaussian delta delta delta gamma generate discrete factor represents conditional distribution 
issue discussed take advantage subsystem decomposition generating gaussians 
simple approach generate mixture gaussians variables network condition evidence subsystem variables leading mixtures gaussians mixture defined perform collapsing algorithm independently subsystem 
different subsystems get gaussian source original mixture system variables define different collapsing schemes different approach subsystem generate gaussians 
approach useful subsystems relatively small compared entire system 
size covariance matrix quadratic number variables variables may efficient generate relatively small gaussians subsystems gaussians variables case subsystem generate gaussians relevant variables observation variable independent denoted define chapter 
scaling hybrid dbns bayesian network nodes ancestors dbn 
edges nodes edges dbn cpds 
words original dbn restricted ancestors 
generate gaussians subsystem enumeration algorithm ignoring rest dbn 
small compared full dbn approach computationally efficient 
resulting algorithm shown 
unfortunately practice case contains relatively variables contains observations system forcing large entire dbn 
example rwgs system discussed detail chapter chemical reactor creates chemical reaction gases flowing system 
chemical reactor modeled relatively small subsystem 
efficiency reaction depends incoming flow turn depends flows pressures system 
dependent flow pressure sensor system 
result network reactor subsystem include variables influence flows pressures system variables model 
cases observation nodes influence variables subsystem case dependencies extremely weak ignored changing posterior distribution 
chemical reactor example measurement incoming flow may able ignore flow pressure measurements system introducing significant errors 
may reasonable approximation ignore nodes order reduce size decrease time takes generate gaussian 
doing may able generate gaussians amount computation time compensating errors introduced ignoring observations 
note flow truly independent measurements system measurement measurement noisy uncertainty actual incoming flow measurements influence belief chapter 
scaling hybrid dbns smoothing far concentrated problem tracking evidence available observations current applications interested probability estimation task defined section 
sequence observations find distribution probability estimation comes perform offline inference order learn parameters model 
restrict online applications possible look 
example assume get new observation second 
allowed short delay second required give estimate state system observations order improve estimate 
process observations order improve estimate called smoothing 
smoothing quite important tracking applications supporting evidence hypotheses may 
consider example case leak water tank assume measure water level tank 
leak starts effect water level leak hypothesis supported evidence 
looking evidence realize leak hypothesis currently 
discrete dbns perform smoothing message passing 
consider dbn resulting clique tree 
assume need find belief state addition evidence evidence available 
perform smoothing algorithm called forward backward algorithm identical clique tree calibration algorithm 
send messages forward time clique fx clique fx clique fx send messages backward time 
send message fx back clique fx clique fx correct posterior distribution evidence 
chapter 
scaling hybrid dbns unfortunately algorithm hybrid dbns 
reason clique tree strongly rooted definition hold clique tree 
intuitively strongly rooted tree perform upstream pass collapsing gaussians send forward messages clique tree resulting hybrid dbn collapse gaussians belief state avoid exponential blowup 
forward backward algorithm hybrid dbns may defined 
example easy convert example section resulted negative variance tree represents dbn 
possible salvage part smoothing algorithm 
problems backward pass involve illegal gaussians ignore continuous variables backward pass send messages involve discrete variables 
result belief state time discrete variables distribution reflects evidence past evidence 
smoothing discrete variables change actual gaussians belief state change weights 
new distribution discrete variables ways ffl improve estimate state system 
seen leak example need evidence order come accurate estimate current state 
ffl improve collapsing scheme 
recall collapsing scheme influenced likelihood hypotheses attempts keep hypotheses relatively unchanged belief state 
sort hypotheses new weights keep belief state hypotheses may evidence evidence 
obviously keeping hypotheses belief state desired feature 
important sure double count evidence 
belief state propagation step reflect past chapter 
scaling hybrid dbns evidence evidence 
distribution discrete variables sent forward distribution smoothing step 
collapsing algorithm smoothed distribution heuristic deciding hypotheses remain belief state collapse hypotheses weights smoothing step 
words new weights line original weights lines 
chapter application rwgs system far considered theoretical aspects hybrid bayesian networks developed algorithms tested relatively simple examples synthetic data 
chapter put algorithm real test performing fault diagnosis rwgs system complex real world physical system 
quick overview system discuss issues addressed modeling system 
test inference algorithm model synthetic data actual data collected system runs 
rwgs system reverse water gas shift rwgs system shown complex physical system designed extract oxygen carbon dioxide 
nasa number possible uses rwgs system including producing oxygen atmosphere mars converting carbon dioxide oxygen closed human living quarters 
mars mission rwgs supposed operate days human intervention lg making fault diagnosis system necessary component 
prototype system constructed nasa kennedy space center 
rwgs works decomposing carbon dioxide abundant mars oxygen carbon 
system schematic chapter 
application rwgs system prototype rwgs system rwgs schematic chapter 
application rwgs system shown goo contains loops gas loop converts hydrogen water water loop produce normal operation line combined returned line mixture reactor recycle line 
mixture enters reactor heated ffi approximately react form condensed stored tank 
remaining gas mixture passes separation membrane sends fraction vent directing remaining mixture recycle line 
compressor maintain necessary pressure differential membrane 
water loop tank dissolved detrimental process 
remedy pumped second tank purge pumped separates portion re enters gas loop remaining goes tank mixture cooled separated 
returns leaves system 
entire process controlled computer program opens closes valves turns water pumps controls gas flow reactor temperatures 
addition normal operating mode system may operate water pumps 
mode reaction supplied supply line paralleling supply line 
option feasible operation mars proven useful testing physical system development 
rwgs interconnected nonlinear system various components influence complicated unexpected ways 
example runs necessary empty water tank chapter 
application rwgs system time sec flow time sec effects emptying water tank pressure difference periodically prevent water accumulating eventually overflowing tank 
causes gases tank expand creates significant sudden pressure drop affects flow system 
phenomenon demonstrated whi 
graph shows flow vent evolves time spikes correspond emptying water tank 
modeling rwgs model rwgs hybrid dbn 
tbn nodes discrete continuous 
discrete variables persistent belong belief state discrete variables belief state 
discrete variables correspond various modes operation compressor valves open closed 
discrete variables correspond sensor faults 
note variables indicating mode operation represent faults example possible valve supposed open stuck closed 
model keeping discrete variables chapter 
application rwgs system hidden valve supposed open high probability valve open possible valve closed 
continuous variables model capture continuous valued elements system pressure various points system flow rates temperatures gas composition 
continuous variables represent time belief state represent variables time 
variables belief state variables time variables encapsulated variables discussed section variables observed variables rest transient variables 
variables time step cpds variables non linear cpds 
examples non linear functions cpds include multiplication function division function max function special purpose functions gas loop equations discussed section 
sensor modeling real system rwgs sensors record underlying state exactly 
addition important quantities gas compositions measured existing sensors noisy biased 
noise level sensors depends factors change time 
example shown difference readings pressure sensors located plotted time 
main reason noise time steps physical proximity sensors compressor sends pressure waves system 
sensors synchronized compressor take measurements various phases pressure waves measure significantly different values 
seconds compressor shuts noise level decreases dramatically 
interestingly note sensors placed close average difference zero 
plot demonstrates case indicating sensors calibrated bias 
furthermore bias depends system state shown change sensor noise literally noise heard pressure waves sound waves generated compressor 
chapter 
application rwgs system ok bias bias flow measurement ok sensor model average difference compressor shuts 
deal noisy sensors obvious way increasing variance predicted measurement values match noise level data 
sensor biases interesting modeling problem 
biases easily modeled simple parameter unknown drift time 
address problem adding hidden variables belief state model different biases sensors 
biases start zero mean reasonably large variance persist time cpd bias linear cpd form bias bias variable represents gaussian noise relatively small variance allowing amount drift occur time 
shows sensor model flow sensor example 
belief state contains variables sensor status sensor working failed bias variable 
sensor working reading time sum actual quantity flow sensor bias 
idea works quite tends overfit data letting bias account discrepancy model predictions actual sensor measurements tracking algorithm settle incorrect steady state 
example assume flow sensors bias 
add constant biases flow measurements explained flows smaller true value flow sensor bias measures flow predict actual flow gamma leads state consistent measurements fully consistent chapter 
application rwgs system sensors may indicate discrepancies different actual state 
fix problem sure model biases reflect true sensor biases biases kept small possible allowed grow real reason 
implement idea introducing contraction factor fl empirically set bias formula bias fl delta bias biases tend go zero doing introduces systematic discrepancy predicted system state 
sensitivity challenging aspects rwgs system parts system state sensitive input conditions measured directly existing sensors 
example consider compositions gas loop 
normal operating conditions gas flow reactor equal amounts added line recycle line supply 
reactor takes amounts back mixture 
aside small difference quantities changes compositions gas loop arise small amounts leave system vent line 
changes compositions gases slow 
system takes long time reach equilibrium equilibrium exceptionally sensitive input conditions behavior membrane 
matters worse gas compositions measured directly real time 
reason existing sensors capable making sort measurement applicationspecific require specific types lasers complicated optical setups significant amounts time specialists order set system 
sensors entirely impractical prototype system may feasible actual system needs autonomously different planet 
exact form equations governing balance gas loop model level sensitivity chapter 
application rwgs system slight variations input conditions 
addition model predictions suffer high sensitivity inherent errors model small errors physical parameters approximate equations complex components separation membrane 
equations balance contain intentionally non physical component stabilizing term reduces sensitivity 
term drives balance pre determined point case expected value balance 
words variable representing composition certain gas time weighted average wm gamma represents composition predicted model represents pre determined stabilization term 
weight manually adjusted provide tradeoff physical accuracy model stability set prediction model contributes stabilizing term 
differing time scales rwgs system contains different phenomena manifest different time scales 
pressure waves rwgs propagate essentially instantaneously speed sound 
time takes gas flow circulate gas loop order seconds 
just discussed gas compositions gas loop take long time reach steady state order hours 
sensors system collect data sampling rate second 
naive approach model dbn finest time granularity ms time step 
doing enables model pressure waves go system principle 
idea useful model system time steps ms get observation steps 
find way create accurate model hard data sparse effectively training sensor data correct predictions finer time granularity help achieve accurate predictions 
words point modeling system chapter 
application rwgs system finer granularity sensor data 
choose model dbn time granularity second time step 
approximate pressure waves occurring instantaneously modeling transient behavior model quasi steady state system pressure waves traveled system reached equilibrium 
equations case substantially simpler require far fewer empirical constants 
difficulty change part system affect parts faster rate time steps happen instantaneously point view model 
fact situation leads cyclical influences variables 
example gas flow entering membrane influences pressure 
change flow change pressure causing instantaneous pressure wave gas loop 
change pressures change gas flows loop including flow 
situation flow influences pressures influence flow 
changes occur time step get cycle dbn legal framework 
solution solve equilibrium equations gas loop simultaneously 
variables represent pressures flows gas loop denoted value determined know outside conditions denoted gas flows system pressure vent pipe 
equations compute include compressor equation approximation membrane equations developed whi 
fairly large nonlinear direct simultaneous solution form exists 
solve equations iteratively set fixed point equations 
combine fixed point equations dbn 
insert fixed point equations nonlinear cpd dbn 
solve equations simultaneously variables fact vector cpd variables parents variables definition cpd essentially procedural value parents executes iterative fixed point computation convergence outputs values chapter 
application rwgs system variables view defining non linear function 
function complicated form implemented procedure code 
approximating distribution induced may daunting task 
ekf algorithm section difficult task easy compute derivatives function represented numerical integration methods particular exact monomials method section difficulty cpd exact monomials method need evaluate function set points need solve fixed point equations points determined numerical integration algorithm 
plug procedural cpd numerical integration algorithm estimate moments relevant variables 
algorithm unchanged exception cpds defines moments variable 
parameter estimation model rwgs quite complex includes parameters 
useful classify parameters different categories helps understand different types parameters involved model different methods estimate values 
partition parameters classes 
known physical constants 
unknown physical constants 
unknown physical quantities change time 
noise level parameters induced physical system 
noise level parameters induced model inaccuracies 
persistence parameters 
model correction parameters chapter 
application rwgs system class probably simplest 
involves constants gravity earth constants convergence physical units 
convergence parameters useful want represent variables convenient units sensors units 
example may convenient represent temperatures kelvin sensors celsius 
model sensor readings convert kelvin celsius 
parameters category known simply set actual value 
second class unknown physical constants includes parameters physical lengths pipes system estimate rate reactor heater surface area separation membrane 
parameters class estimated nasa team length pipes data squares curve fitting techniques 
example plotted cooling curve reactor tried find low order polynomial fit curve 
third class includes parameters resistance gas flow turned change time bias variables discussed section 
quantities change time modeled simple model parameters 
add hidden variables model initial distribution heuristic 
example initial mean bias parameters set zero initial resistance membrane set rough estimate produced data collected steady state runs 
inference algorithm take care rest adjust values variables time 
fourth fifth classes correspond variances certain variables model 
fourth class corresponds noise exists physical system 
examples include noise levels sensors noise levels various set values pressure valve located supposed set pressure system pre defined value may set value slightly different 
noise levels usually easily estimated data 
example order estimate sensor noise take short sub sequences chapter 
application rwgs system steady state data actual measured quantity stays constant sub sequence 
compute variance sensor readings sub sequences estimate sensor noise level 
noise level certain sensor depends outside conditions compressor working working perform estimation procedure independently operating mode 
fifth class accounts model errors 
example equations model separation membrane approximation actual membrane behavior outside conditions known exactly uncertainty predicted flows pressures gas loop 
similarly know exactly temperature reactor gas composition give approximate estimate efficiency reaction uncertainty added predictions 
parameters estimated trial error tried different values chose ones led best results 
note adding parameters model artificially introduce noise compensate model inaccuracies 
sixth class includes persistence parameters discrete continuous variables 
example need parameter defines probability sensor time working time example persistence parameter relates continuous variables uncertainty sensor bias time know sensor bias time parameters estimated various methods tuned trial error 
example persistence discrete parameter estimated sensors break start working data 
training data improve value parameter setting 
class includes parameters various stabilization mechanisms added model 
examples parameters include weight stabilization term described section decay parameter fl sensor biases described section 
parameters estimated trial error 
chapter 
application rwgs system parameter estimation turned difficult task involved modeling rwgs 
reason relatively little data parameters estimate 
furthermore system modes behavior data making estimation parameters hard 
example data case flow system turned flowing system 
feel point coming model parameters single biggest problem solved order hybrid models fault diagnosis practice 
discuss issue section 
experimental results synthetic data tested algorithm real data simulated data generated model 
running real data real test approach running simulated data interest 
reason sources errors real data model inaccuracies errors induced algorithm 
simulated data errors second type better test performance algorithm 
section results experiments synthetic data 
experiments run pentium iii mhz 
testing gaussian approximation test belief state defined model approximated gaussian particular approximation algorithm uses numerical integration approach 
represent belief state approximating gaussian generated set samples model 
introduce evidence samples sampled correct joint distribution 
show results particular variables flow point pressure point 
variables chosen chapter 
application rwgs system flow integration samples random samples rwgs model gaussian estimates distribution dependency non linear cpd membrane variables produced similar results 
samples appear drawn distribution gaussian close 
furthermore estimate joint distribution depicted contours standard deviations close gaussian estimated directly samples 
reasonable expect techniques lead approximations belief state 
comparison particle filtering popular alternative inference algorithm particle filters pf section addition rao blackwellization 
reason may want simple pf rbpf system non linear pf impose restrictions belief state samples represent distribution matter non gaussian compare algorithm pf generated trajectory time steps chapter 
application rwgs system time step truth algorithm particle filtering comparison particle filtering simulated data model tested algorithms 
task particularly simple assumed discrete variables observed shall see simple case quite challenging pf 
algorithm took ms time step included computing gaussian approximation belief state numerical integration necessary conditioning evidence 
comparison generating sample particle filtering took ms 
step algorithm took time generating samples 
just samples pf performed extremely poorly experiments samples time step giving pf somewhat unfair advantage 
shows estimates percentage flow point computed algorithm particle filtering actual value known simulated data 
report results particular variable gas compositions measured sensors potential challenge algorithm 
axis represents time axis chapter 
application rwgs system percentage flow point 
increase readability shifted estimates generated algorithm left generated particle filtering right 
error bars represent uncertainty estimates plus minus standard deviations 
algorithm simply variance gaussian compute standard deviation 
particle filtering computed standard deviation variance induced weighted samples set particles fhx hx variance estimated gamma 
demonstrates estimates particle filtering estimates algorithm 
entire sequence average error algorithm average error particle filtering setup particle filtering slower algorithm factor 
dramatic difference estimates variance 
estimated variance particle filtering extremely small estimated value accurate time steps 
fact entire sequence estimated distribution algorithm correct value composition standard deviations time consistent fact probability mass standard deviations gaussian mean 
comparison particle filtering true value estimated standard deviations time 
difference apparent considered average loglikelihood true value possible estimates belief state 
note particle filtering estimate log likelihood directly samples probability ground truth exactly equal value samples zero 
samples generated particle filtering estimate moments belief state distribution 
moments approximate distribution gaussian compute loglikelihood ground truth 
algorithm average log likelihood particle filtering gamma delta reason problem relatively high dimension evidence leads high variance weights samples 
generated chapter 
application rwgs system samples time step small number significant effect estimate 
entire sequence time steps sample accounted total probability mass sample accounted mass sample accounted 
obviously cases sample completely dominates rest estimates pf reliable particular variance estimates extremely small misleading 
algorithm faster particle filtering samples factor estimates reliable 
comparison rao blackwellized particle filtering results previous section strong indication rbpf preferred pf rwgs domain 
compared performance rbpf performance enumeration algorithm 
direct comparison algorithms cases algorithm inference hybrid dbns difference case implemented line enumeration algorithm case rb lw generate gaussians 
belief state represented decomposed belief state described section 
hypothesis variables subsystems modeled independent 
sub systems corresponded partition model ffl reactor subsystem subsystem contained variables describe state reactor area mainly variables relate temperature reactor 
smallest subsystem just belief state variables discrete continuous 
ffl water subsystem subsystem contained variables describe state water tanks 
specifically involved variables areas 
subsystem contained belief state variables discrete continuous 
chapter 
application rwgs system ffl gas loop subsystem subsystem contained variables belief state correspond gas flows system 
compromised areas 
largest subsystem belief state variables discrete continuous 
sake having fair comparison gave algorithms amount time generate gaussians seconds gas loop subsystem seconds water subsystem seconds reactor subsystem allocated time gas loop subsystem far largest 
ran synthetic trajectories corresponding specified sequence faults 
methods evaluate algorithms 
examine algorithm converged correct discrete hypothesis quickly 
second comparison omniscient kalman filter described section 
omniscient kalman filter gets observe discrete variables expect algorithm get observe discrete variables perform omniscient kalman filter evaluate quality algorithm close gets omniscient kalman filter 
faults sequence sequence simplest possible sequence faults occurred 
case rbpf enumeration algorithm performed gave high probability fault hypothesis distribution continuous variables belief state indistinguishable omniscient kalman filter 
turning heater second sequence involved single fault time step reactor heater turned 
shows results tracking variable tr representing temperature reactor enumeration algorithm omniscient kalman filter plot actual value tr known chapter 
application rwgs system time step ground truth omniscient kf algorithm heater shutdown enumeration algorithm omniscient kf time step omniscient kf run run run run run heater shutdown rbpf omniscient kf chapter 
application rwgs system synthetic data 
create plots ran algorithms time step computed mean tr belief states 
time step estimates mean tr computed belief states enumeration algorithm omniscient kalman filter 
time step estimates diverge identical time step remain sequence looking graph 
behavior best understood considering shows probability heater computed enumeration algorithm 
initially probability heater low 
remains heater turned time step 
reason initial drop reactor temperature better explained noise sensors model equations physical system fault heater device 
seconds temperature consistently keeps dropping heater heater probability 
time steps enumeration algorithm believes wrong hypothesis heater estimated mean tr time steps agree omniscient kalman filter 
words early run time step enumeration algorithm gives high probability correct discrete hypothesis 
omniscient kf knows correct discrete hypothesis agree belief state 
enumeration algorithm know correct discrete hypothesis disagrees omniscient kf 
ran experiment rbpf generated different runs rbpf sequence 
rbpf runs behaved initially enumeration algorithm identical omniscient kf diverged 
different rbpf runs disagreed omniscient kf different periods time 
shows trace rbpf runs agrees omniscient kf time step just enumeration algorithm agree omniscient kf time step fact estimates agree 
reason model contains built delay time heater turned time reactor temperature starts change result 
chapter 
application rwgs system time step heater heater shutdown hypothesis likelihood enumeration algorithm time step heater heater shutdown hypothesis likelihood rbpf chapter 
application rwgs system converge omniscient kf time step shown order graph readable 
behavior best understood likelihood correct discrete hypothesis shown 
graph plots heater different runs rbpf 
different runs algorithm discovers heater different time steps 
best runs algorithm discovers state heater just enumeration algorithm 
worst run belief state indicates heater late 
understand behavior rbpf remember change temperature single time step explained noise 
persistent drop temperature time steps indicates heater 
order algorithm discover heater needs generate hypothesis propagate time steps 
conditions problematic rbpf 
recall discrete persistence factor set samples duplicate samples heater 
small fraction samples represent heater hypothesis fact guarantee hypothesis generated 
correct hypothesis generated weight significantly larger weights hypotheses heater evidence single time step explained noise 
heater hypothesis belief state time may re sample propagate belief state time case heater hypothesis lost 
intuitively low probability generating hypothesis representing fault rbpf standard version unreliable algorithm fault diagnosis 
interesting phenomenon 
runs discovered heater significantly reduce probability heater runs keep probability close 
believe reason variability sampling process hypotheses generated number duplicate samples interestingly knowing correct discrete hypothesis mean variable diverge measurements sure temperature estimate stays close actual value 
chapter 
application rwgs system time step hydrogen valve closed hydrogen shutoff hypothesis likelihood enumeration algorithm hypothesis 
note problem exist describing behavior enumeration algorithm 
instance enumeration algorithm robust rbpf 
shutting hydrogen flow third sequence involved fault hydrogen valve simulating valve getting closed time step 
sequence algorithms information flow sensor located hydrogen pipe 
valve gets closed indicates flow approximately zero fault easier diagnose heater turned 
figures show performance enumeration algorithm rbpf case 
enumeration algorithm diagnoses fault soon happens 
hand performance rbpf varied different runs 
best run rbpf diagnosed fault time step just enumeration algorithms 
runs fault diagnosed sequence chapter 
application rwgs system time step valve closed hydrogen shutoff hypothesis likelihood rbpf runs fault diagnosed time steps 
results similar results seen heater example 
reason high discrete persistence correct hypothesis sampled right time 
particular sequence possible way explain sensor readings broken 
hypothesis valve closed broken sensor show flow value fact shows coincidence indicates flow stopped 
reason enumeration algorithm chooses valve closed hypothesis probability broken goes time step drops back zero 
hand suspect rbpf may consider broken sensor hypothesis quite runs closed valve hypothesis generated 
shows case 
plot probability hydrogen sensor broken runs rbpf closed valve fault generated relatively late sequence 
chapter 
application rwgs system time step hydrogen sensor broken hydrogen shutoff probability broken sensor rbpf sensor faults trajectory involves related fault hydrogen valve fault flow sensor failing 
sensor faults quite easy diagnose model principle 
model failed sensor normal distribution large standard deviation oe distribution 
trajectory giving readings consistently close suddenly generated reading gamma 
behavior explained sensor fault enumeration algorithm problem fault estimated failed 
rbpf hand serious problems sequence 
faulty sensor hypothesis usually generated 
case generating hypothesis worse previous examples 
model noise level working standard deviation 
working sensor hypothesis expect get reading 
reading chapter 
application rwgs system standard deviations away working sensor hypothesis probability evidence low precisely exp gamma delta gamma number small represented standard bits double representation real numbers rounded 
effect likelihood hypothesis include sensor fault zero 
order propagation algorithm defined generate hypothesis positive probability inference algorithm able continue 
rbpf model discrete persistence factor generate faulty sensor hypothesis case probability hypotheses zero algorithm fails continue 
important emphasize value discrete persistence factor tuned realistic model produce best fault diagnosis results model easy hard certain inference algorithm 
going back enumeration algorithm observe reliably diagnose fault single sensor point algorithm break 
example sensors fail simultaneously enumeration algorithm generate correct hypothesis reasonable amount time hypothesis 
case hypotheses generated zero likelihood algorithm fail just rbpf single sensor 
question far push enumeration algorithm simultaneous faults reliably diagnose 
optimization techniques described section able generate gaussians second gas loop subsystem gaussians second heater water subsystems 
gas loop subsystem contains discrete variables subsystem bottleneck generating simultaneous faults hypotheses 
seconds true faults independent 
note dbns model system certain faults happen simultaneously common cause 
case enumeration algorithm generate hypothesis corresponds faults happening simultaneously generates hypotheses correspond fault happening 
chapter 
application rwgs system time step gas loop subsystem enumeration algorithm able generate double fault hypotheses relatively small portion triple fault hypotheses 
setup experiments algorithm diagnose independent simultaneous sensor faults real time 
note requested diagnose simultaneous faults things 
option faster computer generate gaussians second 
option divide gas loop subsystem smaller subsystems containing discrete variables 
may able improve inference algorithm discuss ideas section 
hand faults happen sequentially algorithm faults happen consecutive time steps 
final synthetic trajectory describes catastrophic scenario lose flow sensors just time steps flow sensor located fails 
flow sensors located fail 
flow sensors located fail 
time steps lost flow sensors gas subsystem flow sensor measures flow located 
algorithm problems diagnosing faults relevant probabilities jumped appropriate time steps 
final challenge sequence included shutoff 
note point sequence working flow sensors give useful information diagnose fault working flow sensor continues report amount flow going system 
information comes sensors system pressure sensors gas loop making diagnosis task quite hard 
plots likelihood hydrogen shutoff hypothesis estimated algorithm 
algorithm reaches correct albeit fairly long time expected little sensor data available sequence 
seen shows actual flow combined estimates omniscient kf algorithm 
usual algorithm diverges omniscient kf long gives high likelihood chapter 
application rwgs system time step hydrogen valve closed shutoff broken flow sensors hypothesis likelihood time step flow omniscient kf algorithm ground truth shutoff broken flow sensors flow estimates chapter 
application rwgs system wrong discrete hypothesis shutoff algorithm infers correct discrete events converges omniscient kf behaves point 
indicates ground truth trajectory sampled model quite noisy fact noisier actual data data section 
reason extra noise added model compensate inaccuracies discussed section sampling trajectory model adjusted noise levels creating jagged trajectories 
results real data turn attention experiments run real data collected team 
data available water pumps working water subsystem part model tested real data 
effect making model large 
data sets recorded sensor readings frequency seconds 
model time step second data sets limited solely model estimation 
remaining data sets taken gas loop operating compressor rest compressor gas flow went system ventilation pipe 
experiments section reported data sets 
long sequence seconds included seconds steady state operation seconds shutoff transition 
second included seconds just seconds steady state seconds shutoff transition 
steady state results reported data set 
order data training testing model seconds steady state sequence training rest testing 
shutoff transition treated data set chapter 
application rwgs system sensor flow flow flow flow flow table probabilities flow sensors gas loop faulty time steps training set second data set test set 
steady state experiments set experiments real data performed steady state data 
tested capability model recognize faulty sensors 
data supplied included faulty sensor flow sensor located 
experiment initialized sensors including faulty probability working broken 
indicated synthetic data results confirmed sensor faults easy diagnose 
table shows probabilities flow sensors gas loop faulty time steps 
inference algorithm believes sensors working general model worked expected steady state data correct discrete hypothesis having high probability continuous quantities tracked 
steady state data challenging removed data collected flow sensors measure incoming flow 
sensors measurements gas flowing gas loop making various diagnoses harder 
particular diagnosing valves open closed difficult problem 
shows results running model steady state data plot probability valves open correct discrete hypothesis 
exception small drop sequence probability practically entire sequence note chapter 
application rwgs system time step valves open valves open plotted minutes steady state data sequence represents seconds minutes system steady state 
better understand drop sequence shows just time steps sequence 
conjecture drop caused initial conditions represent correct initial distribution 
caused factors ffl quantities model directly measured gas compositions rough estimates probably somewhat wrong 
ffl initial values sensor biases zero know wrong 
ffl model correlations variables initial belief state just defined mean variance variable setting covariances zero 
know incorrect 
test initial drop caused incorrect initial belief state chapter 
application rwgs system time step valves open time step steps original sequence steps sequence starting sequence time started time step 
modified initial distribution reflect situation knew approximation correct initial belief state 
results experiment shown 
note probability correct hypothesis drops sequence going back 
original experiment problems view indication problem initial conditions 
clearly system practice necessary better approximation initial belief state task estimating belief state sequence heater turned gas flows system may significantly easier 
general model worked steady state data test various aspects model 
report experiments test sensor model 
recall sensor model includes important components sensor noise sensor bias 
tested effect components 
demonstrate effect noise level sensors 
standard deviation flow sensor noise estimated data 
tried chapter 
application rwgs system time step valves open time step wrong flow sensor noise level standard deviation large oe standard deviation small oe increase decrease standard deviation data plot probability valves open estimated modified models 
clear performance different noise levels quite poor predictions robust 
noise level small model sensitive noise data causing change discrete hypothesis order explain data noise 
noise level high model insensitive data adequately flow sensors readings correct predictions 
belief state poor estimation state system leading choose wrong discrete hypothesis 
important model correct noise level sensors bad estimates parameter lead serious degradation quality model predictions 
set standard deviation noise level back removed bias variables flow sensors model 
setup removing readings plotting probability valves open 
results shown showing dramatic degradation performance algorithm 
intuitively bias variables belief chapter 
application rwgs system time step valves open modeling flow sensors bias variables state reliable estimation state system 
example consider hidden flow variable 
bias variable flow sensor force value close value measured significant bias value predicted close actual flow predictions reliable 
shutoff experiments set experiments involves shutoff sequence 
treat shutoff unknown event fault need diagnose 
results part generated test set 
adjust model test set modeling initial conditions sequence 
better understand results examining data generated sequence 
shows readings recorded flow sensor measures incoming flow membrane flow sensor measures outgoing flow membrane 
chapter 
application rwgs system time step inflow outflow flows shutoff entire sequence time step inflow outflow flows shutoff seconds chapter 
application rwgs system focuses part sequence 
interesting phenomena data 
qualitative behavior flows 
initially flows dropping just expect turning flow decrease flows system 
seconds transition flows start increase go original values 
reason different response separation membrane different gases 
supply cut compositions gases gas loop change 
gradually system composition flow increases 
resistance membrane smaller resistance flows start increase 
eventually system stabilizes 
notice model predict subtle behavior system synthetic trajectory shown ground truth similar qualitative behavior dropping rising stabilizing discussed noise levels model larger simulated transition happen slightly faster 
second interesting phenomenon relation flows measured general flow equal sum flows 
flow measured large flow measured actual sequence flow larger flow seconds see 
transition readings larger readings starting time step difference significant 
reason behavior apparently sensors calibrated react differently changes gas compositions 
actual flow slightly larger flow sensors indicate significantly smaller 
shows readings reflected belief state model 
plot joint probability distribution status sensors 
dominating hypothesis sensors working 
model explains higher readings compared adjusting bias variables sensors accordingly 
bias variables encoded model large values 
model chapter 
application rwgs system time step probability failed okay okay okay predicted failures dependency bias variables gas compositions model reflect change biases gas compositions change 
model explain behavior 
possible explanation sensors faulty exactly diagnosis system 
initially hypothesis faulty 
flows level system prefer flow measured changes diagnosis faulty high probability 
seconds gives high likelihood sensors faulty infers working 
diagnosis quite reasonable explicitly model influence gas compositions flow rates bias variables 
words sensor model behavior considered fault system reasonable inference expect assuming sensors broken 
examine performance model inference algorithm chapter 
application rwgs system valve closed predictions flow sensors time step valve closed prediction flow sensors readings chapter 
application rwgs system time step valve closed prediction flow sensors readings diagnosing position valves 
indicates correct diagnosis reached discussed case synthetic data long data flow sensor located easy diagnose shutoff 
interesting test ignore data test diagnose shutoff 
results experiment shown 
data takes little time reach right time steps reach correct diagnosis consider extremely rest sequence 
previous section tried diagnose shutoff event flow sensor working 
tried similar experiment real data sequence included shutoff shutoff removed sensors sensor 
results shown quite similar results synthetic data synthetic data noisy discussed 
fact get qualitative chapter 
application rwgs system time step probability open closed hydrogen closed closed prediction measurements incoming flows behavior real simulated data case positive indication quality model inference algorithm 
positive indication fact able come useful diagnosis real data set sensors 
final challenging test ignored data flow sensors eliminating measurement incoming gas flows 
coming correct diagnosis challenging infer valves closed hard infer effects shutoff shutoff similar 
results experiment plotted 
shortly transition probability valves open goes quickly hypotheses valve closed valve closed 
short time hypothesis shutdown considered seconds transition probability shutoff gets close 
remains practically short periods sequence 
interestingly period correspond time step model infers chapter 
application rwgs system faulty 
second corresponds short period model gives high probability hypothesis faulty 
results quite encouraging obviously done 
discuss extensions framework chapter current model inference algorithm able diagnose fairly complex faults manifested directly sensor readings 
particular graphs important flow sensor readings missing demonstrate power hybrid dbns coming complex diagnoses probabilistic model system 
discussion previous fault diagnosis main approaches 
engineering community fault diagnosis framework usually models ai community concentrated constraint methods discretizing system 
section discuss approaches contrast approach hybrid dbns 
fault detection isolation engineering community research focused modeling system defined section fra ise ger 
non linear systems linearized usually ekf 
approach quite similar hybrid dbns approach main difference decomposition probability distribution model uses just discrete variable represents possible faults 
result need explicitly encode behavior continuous variables value discrete variable forcing model relatively small number possible faults 
main alternative approach discretize variables system resulting discrete model expressed set constraints 
violated constraint indicates fault task fault diagnosis find explanations violated constraint 
classical approach ai community ct mci 
chapter 
application rwgs system discussion concrete focus livingstone system wn kn fault diagnosis control system developed nasa 
livingstone defined slightly different ways purposes livingstone models system tuple mx ffl set discrete state variables representing state system 
just dbns set variables time step particular represents state system time continuous quantity interest discretized small number possible values 
examples typical discretizations values zero nonzero values low medium high 
note sensor observation discretized pre processing step 
ffl mx set state constraints formalized propositional logical formulas defined constraints define subset possible states feasible states system 
example constraint closed flow zero represents state valve flow represents flow ffl set constraints constraining possible transitions defined set propositional formulas example state sensor broken remains broken broken broken livingstone uses simple heuristic model probabilities possible trajectories system number faults transition faults assumed independent 
transition faults considered believable trajectories faults ruled due evidence constraints mx livingstone considers nominal faults trajectory actual trajectory system consistent chapter 
application rwgs system constraints 
possible trajectories system defined single fault trajectories consistent constraints 
trajectories double fault trajectories considered 
actual algorithm find trajectories kn quite similar enumeration algorithm chapter 
algorithm maintains belief state time variables enumerates transitions time order likelihood ignoring transitions consistent constraints 
fact livingstone uses discrete variables important difference livingstone approach 
livingstone belief state defined set possible states opposed approach maintains probability distribution states 
consequently approach uses evidence states livingstone uses evidence hard constraints making states feasible 
livingstone applied nasa systems including propulsion system cassini spacecraft rwgs 
livingstone successful application gk 
reason different nature systems 
model propulsion system basic modes operations determined configuration valves flow fuel system thrust produced flow thrust 
typical faults system cause flow start unexpectedly causing flow increase decrease flow 
questions answered simple discretization flows zero nonzero 
situation rwgs complex 
seen faults manifest changes flows pressures flow going zero 
zero nonzero discretization rwgs entire graph reduced flow constantly nonzero fault impossible diagnose 
possible define finer discretization faced similar problems discussed section hard find discretization scheme inference significantly expensive 
addition defining model harder 
example assume partition possible flows ranges chapter 
application rwgs system decide define flow range sensor reading fault express fact fault resulting reading 
diagnosing faults slow drop temperature reactor result heater turned extremely hard framework 
deep reason problems rwgs truly hybrid system actual continuous quantities important question zero non zero 
classical constraint approach may useful cases systems rwgs 
feel results chapter indicate modeling systems hybrid dynamic bayesian networks natural alternative potential overcoming limitations classical approach 
chapter summary thesis tried achieve main goals develop better understanding hybrid bayesian networks show hybrid models useful practical applications particular order perform fault diagnosis complex real world physical system 
overview inference discrete hybrid bayesian networks 
showed insights discrete networks hybrid networks important subtle differences 
particular saw possible adapt clique tree algorithm developed discrete networks clgs 
discrete case elimination order clgs 
elimination orders operations involved message passing defined operations defined lead illegal distributions distributions negative variance 
fix problem necessary add crucial restriction elimination order called strong triangulation 
saw strong triangulation get strongly rooted clique tree message passing defined guaranteed find correct weak marginal clique 
unfortunately complexity analysis showed strong triangulation dramatically increase chapter 
size clique tree 
simple networks including polytrees strong triangulation get compact clique trees strong triangulation leads impractical clique trees 
observation led important natural questions strong triangulation avoided 
words possible find algorithm clique trees suffer problems induced strong triangulation 
alternatively compromise approximate inference exact inference find approximate inference algorithm efficient networks polytrees provides approximation posterior distribution 
questions answered theoretical analysis chapter unfortunately general answer 
showed polytrees approximate inference clgs np hard discrete case exact inference polytrees done linear time 
proved np hardness holds restricted class polytrees marginal prior distribution continuous variables mixture gaussians 
corollary finding discrete instantiation observations continuous variables np hard simple network structures 
result appears quite discouraging context fault diagnosis part task find states system sensor readings 
corollary tells finding single state system certainly states np hard polytrees 
chapter showed practice situation bad 
real life domains look np hardness reductions take advantage properties order come useful efficient algorithms 
particular domains distribution discrete variables skewed 
example fault diagnosis domain probability faults low combination simultaneous independent faults extremely 
time algorithm takes advantage phenomenon deterministically enumerating possible states system order prior likelihood 
probability distribution skewed hope chapter 
considering fairly small number hypotheses provides approximation actual distribution 
algorithm works hypotheses priori hypotheses posteriori considering produces approximation posterior distribution 
algorithm time algorithm space complexity exponentially smaller lauritzen algorithm get answer albeit approximate cases lauritzen algorithm completely impractical 
furthermore empirically showed algorithm outperforms classical sampling algorithms networks skewed distribution 
turned attention non linear models 
introduce non linear relations model resulting distribution longer gaussian mixture gaussians arbitrarily complex 
cases distribution non gaussian approximated gaussian 
approach approximate non gaussian distribution gaussian 
chapter described detail numerical integration techniques gaussian quadrature exact monomials 
techniques proved useful purposes allowed compute efficiently moments non gaussian distribution high precision 
showed approach outperforms extended kalman filter approach 
showed approach presents simple coherent view successful unscented filter turns special case exact monomials method 
viewed numerical integration technique clear extend unscented filter achieve better precision 
numerical integration approach naturally combined enumeration algorithm chapter 
chapter concentrated augmented clgs clgs include discrete nodes continuous parents 
systematic way deal networks extending lauritzen algorithm numerical integration techniques 
algorithm computes exact distributions discrete nodes exact second moments continuous ones inaccuracies resulting numerical integration procedure 
achieve properties address subtle technical issues modify lauritzen algorithm chapter 
non trivial ways 
showed special case softmax cpds integration done efficiently moments leads particularly accurate approximation 
discussed extend enumeration algorithm augmented clgs 
proved enumeration prior distribution done efficiently clgs simple structure polytrees np hard augmented clgs similar structure 
enumeration algorithm directly augmented clgs heuristics necessary 
point thesis concentrated static models 
pointed interested tracking stochastic processes time 
turned attention temporal models dbns 
chapter described standard inference algorithms hybrid dbns gpb gpb imm 
unfortunately algorithms suitable needs number gaussians kept belief state 
gpb keeps gaussian belief state purposes fault diagnosis gpb imm keep gaussians assuming belief state binary variables 
clearly algorithms practical large 
particular models fault diagnosis domain easily tens discrete variables belief state 
example rwgs system discussed chapter discrete variables belief state 
chapter discussed scale inference hybrid dbns 
observed gaussians collapsed gpb may different better kept distinct hypotheses 
hand gaussians gpb keeps separate hypotheses may similar collapsed significant loss accuracy 
insight developed new collapsing algorithm 
algorithm takes parameter representing maximal number gaussians belief state tries identify distinct hypotheses keep belief state 
key part algorithm collapses gaussians similar terms kl divergence 
chapter 
discussed implications collapsing algorithm representation belief state 
described belief state graphical model includes special hypothesis variable 
value hypothesis variable corresponds exactly gaussians belief state induces probability distribution discrete variables belief state 
hypothesis variable convenient way correlate mixture gaussians resulting collapsing algorithm distribution discrete variables 
addition approximating belief state collapsing hypotheses may beneficial decompose belief state keep factored way just discrete dbns 
discussed various ways doing including decomposing mixture gaussians hypothesis variable 
point thesis described techniques necessary perform fault diagnosis rwgs model 
section tracking algorithm hybrid dbns combines set techniques ffl enumeration algorithm chapter 
ffl numerical integration techniques chapter 
ffl collapsing algorithm representation belief state chapter 
applied techniques rwgs system chapter 
rwgs system complex physical system designed extract oxygen martian atmosphere carbon dioxide rich environment 
described system discussed interesting issues addressed order model rwgs system hybrid dbn including problem parameter estimation 
experimental results synthetic data real data collected actual runs system 
experiments synthetic data compared enumeration algorithm sampling approach 
believe experiments demonstrate enumeration approach real advantages sampling approach 
successful experiments synthetic data demonstrate chapter 
approach modeling system hybrid dbn practice 
real world domains model just approximation system 
real data quite fits model working presents extra challenge synthetic data 
feel experiments real data sets demonstrate feasibility approach real world conditions 
limitations directions hope thesis demonstrates usefulness hybrid dynamic bayesian networks provides useful algorithms inference models 
obviously answered questions come models room done 
section review exciting research directions build top thesis 
inference issues heart algorithm tracking rwgs system enumeration algorithm chapter combined techniques chapter 
algorithm proven capable reliably dealing complex real world system improved ways 
combination cd cpds currently enumeration algorithm handles non linear relations continuous variables 
chapter showed easy combine algorithm cd cpd cpds discrete nodes continuous parents 
need exists discussed chapter cases cd cpds natural way model system 
finding way incorporate cd cpds enumeration framework open problem 
chapter 
combination outside experts enumeration algorithm uses information available directly probability distribution 
cases get information may useful improve performance algorithm 
example may outside expert uses algorithms come possible hypotheses state system 
natural combine expert algorithm combining hypotheses generated expert hypotheses enumerated algorithm 
defining architecture studying theoretical empirical properties may prove significant advance hybrid dbns fault diagnosis 
adaptive enumeration order enumeration algorithm enumerates hypotheses determined prior distribution discrete variables discrete observations exist 
enumeration order take account observations continuous variables 
discussed chapter view similar approach rb lw generates samples distribution take account continuous observations 
adaptive importance sampling approach ok cd attempts improve sampling distribution time 
roughly speaking idea samples generated order get better approximation posterior distribution 
generate samples improved approximation posterior samples improve approximation 
possible idea enumeration algorithm 
start enumerating hypotheses distribution take account continuous evidence 
enumerating hypotheses estimate new distribution better approximation posterior distribution 
enumerating hypotheses start enumerating hypotheses enumerate hypotheses compute continue chapter 
fashion 
approach may prove useful dealing simultaneous faults 
assume independent faults happen simultaneously 
probability faults low take long enumeration algorithm reaches hypothesis corresponds faults 
hypotheses generated discover probabilities faults larger original estimates 
learn new distribution faults 
new distribution hypothesis corresponding simultaneous faults appears earlier enumeration order may enable enumeration algorithm get reasonable time 
exploiting approximate conditional independencies important improvement enumeration algorithm combine collapsing process 
example consider rwgs system 
gas compositions gas loop depend temperature reactor depend discrete variable representing heater turned 
include discrete variable representing heater status variables enumerate hypotheses 
know current temperature reactor dependency disappears 
practice know exact temperature sensors noisy know temperature high accuracy ignore dependency gas compositions heater status 
temperature sensor working properly may need enumerate different hypotheses heater status similar purposes query collapse pick completely ignore part network 
hand temperature sensor working important consider heater status case want enumerate different hypotheses 
observation suggests sophisticated algorithm takes advantage approximate conditional independencies query variables parts network partial instantiation discrete variables chapter 
network 
implement algorithm address technical issues order partial discrete instantiations enumerated identify approximate conditional independence relations deal variables approximately independent query variables combine techniques non linear dependencies 
coming algorithm takes advantage observation challenging problem feel potential improvement performance algorithm dramatic justify time spent problem 
reason find partial instantiations large parts network approximately independent query get exponential reduction number hypotheses 
fact approach may able effectively enumerate distinct hypotheses query variables 
modeling long term changes rwgs experiments concentrated diagnosing abrupt changes valve getting closed 
physical systems important type change called drifts 
drifts occur slowly long period time 
example drift slow accumulation dust chemical reactor causing slow drop efficiency reactor 
drifts cause noticeable change short period time significant consider accurate time granularity 
short period time drifts detected changes cause negligible compared sources variability system 
motivating example consider process aging human beings 
thought experiment assume sequence snapshots person face taken time intervals hour 
snapshots obviously different due different light conditions different facial expressions 
look snapshots taken hours days apart indication aging process chapter 
able tell chronological order snapshots taken 
look snapshots taken decades apart results aging process clear 
drifts challenge modeling perspective diagnosis perspective 
modeling perspective clear best model drifts induce perceptible change time step 
diagnosis perspective clear standard techniques tracking belief state order identify drifts need different mechanism equivalent comparing current snapshot snapshots taken long time ago 
active learning parameter estimation process rwgs system described section complicated time consuming process 
quite certain investment model parameters greatly improved 
example currently know rwgs model shutoff sequence available possible predictions model case quite poor 
obviously need systematic way parameter estimation model learning general 
possibility em algorithm dlr designed estimate parameters model incomplete data data variables observed 
feel order learn model physical systems important direct data collection process include sequences useful modeling purposes sequence includes shutoff rwgs 
words perform active learning active learning setup possible direct data collecting process collect data specific system configurations 
active learning framework learning algorithm ask data collecting process set variables values generate sample values 
chapter 
active learning context discrete bayesian networks tk tk 
order successfully active learning needs interesting problems addressed ffl perform active learning hybrid networks discrete ones 
ffl perform active learning dbns bayesian networks 
note requesting sample setting variable dynamic setup ask sequence describing system behavior initial conditions impose interventions way 
ffl real life systems rwgs take account safety constraints 
example may able ask test behavior system pressure grows system explode case 
order active learning systems rwgs model safety constraints combine active learning algorithm 
control thesis focused task fault diagnosis 
practical cases interested fault diagnosis part control system want just know system faults want take actions fix minimize damage cause 
obvious need control component hand hand fault diagnosis tools 
control difficult problem state system fully known 
classical control theory usually deals continuous systems ai community focuses discrete systems usually modeled markov decision processes put 
done problem creating control component complex hybrid systems tps ps problem open issue especially state system fully known 
chapter 
hope thesis demonstrates hybrid dynamic bayesian networks powerful tool reasoning complex realistic domains 
combine explicit representation uncertainty proved useful bayesian networks expressive power model discrete continuous phenomena world 
thesis concentrated hybrid models fault diagnosis hard come domains call hybrid models visual tracking speech recognition robotics 
thesis answered important fundamental questions hybrid models naturally led questions just important 
importantly thesis go theoretical analysis testing algorithms synthetic data generated toy examples take bull horns prove applicability techniques real world system actual data 
believe strong indication hybrid bayesian networks intellectual exercise hold practical importance 
hope serve motivation researchers pursue research questions related hybrid bayesian networks combination testing applicability hybrid models domains 
appendix proofs proof lemma proof fx fy 
prove theorem induction assume case write alternatively 
appendix proofs zx get expectations depend moments required 
induction step similar 
linear write linear combination fx gamma plus gaussian noise 
induction moments gamma argument case show moments involve determined directly moments gamma proof claim theorem proof recall network shown parameters defined oe oe gamma gamma oe gamma gamma gamma oe gamma oe oe delta oe oe oe oe goal prove oe gamma appendix proofs prove claim induction show oe 
general conditional independencies induced network write gamma gamma gamma dx gamma gamma gamma gamma gamma gamma gamma dx gamma gamma gamma gamma gamma dx gamma induction hypothesis gamma gamma gamma gamma oe easy explicitly write gamma gamma gamma gamma gamma gamma gamma gamma ff oe gammaoe oe oe gammaoe oe oe oe oe ff ff 
marginalizing gamma trivial get gamma gamma gamma gamma ff oe oe oe oe oe oe find need condition distribution equation find new mean variance corollary posterior mean oe oe oe oe gamma gamma gamma gamma ff appendix proofs oe oe oe oe oe oe oe oe oe oe oe oe oe oe oe oe oe oe oe oe oe oe oe oe oe ffl nm oe oe ffl nm combining equation equation get fi fi fie gamma gamma ff fi fi fi fi fi fi fi fi gamma ffl nm gamma gamma gamma ff gamma gamma ff fi fi fi fi fi fi fi fi fi fi gamma ffl nm gamma gamma gamma ff fi fi fi fi fi ffl nm ffl nm delta ffl nm delta ffl inductive hypothesis know fi fi fis gamma gamma gamma fi fi fi gamma ffl furthermore choice ff gamma ff 
fi fi fis gamma fi fi fi fi fi fis gamma ff gamma gamma gamma gamma fi fi fi fi fi fis gamma gamma gamma fi fi fi fi fi fi gamma ff gamma fi fi fi gamma ffl ffl appendix proofs turn attention computing posterior variance conditioning 
corollary get var oe gamma oe oe oe oe oe gamma oe oe oe oe oe oe oe oe oe oe oe oe oe oe oe oe oe oe oe oe oe oe oe oe oe oe oe oe oe oe oe oe oe proof complete 
proof theorem proof sigma positive definite iff vector ir 
ir vector 
describe vector dimension scaler 
appendix proofs jy ax yx quadratic function positive definite function convex find minimum setting derivative respect ax yu gamma plugging equation equation fact gamma gamma get sigma positive definite iff gamma gamma gamma equation holds iff gamma gamma bibliography acp arnborg corneil 
complexity finding embeddings tree 
siam journal algebraic discrete methods 
ast astrom 
optimal control markov decision processes incomplete state estimation 
journal mathematical analysis applications 
atk atkinson 
numerical analysis 
wiley 
bb 
dynamic programming 
academic press new york 
bbs blom bar shalom 
interacting multiple model algorithm systems markovian switching coefficients 
ieee transactions automatic control august 
ber bertsekas 
nonlinear programming 
athena scientific belmont massachusetts second edition 
bk boyen koller 
tractable inference complex stochastic processes 
proceedings th annual conference uncertainty ai uai pages 
binder koller russell kanazawa 
adaptive probabilistic networks hidden variables 
machine learning 
bibliography bar shalom li 
estimation application tracking navigation 
john wiley sons 
bv boyd vandenberghe 
convex optimization 

appear 
cd cheng druzdzel 
ais bn adaptive importance sampling algorithm evidential reasoning large bayesian networks 
journal artificial intelligence research 
clr cormen leiserson rivest 
algorithms 
mit press cambridge massachusetts 
coo cooper 
probabilistic inference belief networks np hard 
artificial intelligence 
ct cover thomas 
elements information theory 
wiley 
ct cordier thi 
event diagnosis systems 
technical report irisa cedex france 
daw dawid 
application general propagation algorithm probabilistic expert systems 
statistics computing 
dga doucet godsill andrieu 
sequential monte carlo sampling methods bayesian filtering 
statistics computing 
dk dean kanazawa 
model reasoning persistence causation 
computational intelligence 
de kleer williams 
diagnosing multiple faults 
matthew ginsberg editor readings nonmonotonic reasoning pages 
morgan kaufmann los altos california 
de kleer williams 
diagnosis behavioral modes 
proceedings th international joint conference artificial intelligence ijcai pages 
bibliography dl dagum luby 
approximating probabilistic inference bayesian belief networks np hard 
artificial intelligence march 
dlr dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society series 
dr davis 
methods numerical integration 
academic press 
fra frank 
fault diagnosis dynamic systems analytical knowledge redundancy survey new results 
automatica 
ger 
fault detection diagnosis engineering systems 
marcel dekker new york 
geweke 
bayesian inference econometric models monte carlo integration 
econometrica november 
gg geman geman 
stochastic relaxation gibbs distributions bayesian restoration images 
ieee transactions pattern analysis machine intelligence november 
gh ghahramani hinton 
variational learning switching statespace models 
neural computation 
gj garey johnson 
computers intractability 
freeman new york 
gk goodrich kurien 
continuous measurements quantitative constraints challenge problems discrete modeling techniques 
proceedings 
bibliography goo goodrich 
reverse water gas shift system presentation 
stanford university april 
gss gordon salmond smith 
novel approach nonlinear non gaussian bayesian state estimation 
iee proceedings april 
gts gilks thomas spiegelhalter 
language program complex bayesian modelling 
statistician 
hbh horvitz breese heckerman 
lumiere project bayesian user modeling inferring goals needs software users 
proceedings th conference uncertainty artificial intelligence pages 
heckerman breese 
troubleshooting uncertainty 
technical report msr tr microsoft research 
hd huang darwiche 
inference belief networks procedural guide 
international journal approximate reasoning 
horvitz koch kadie jacobs 
coordinate probabilistic forecasting presence availability 
proceedings th annual conference uncertainty ai uai pages 
ise 
supervision fault detection fault diagnosis methods 
control engineering practice 
jel jelinek 
statistical methods speech recognition 
mit press cambridge ma 
jen jensen 
bayesian networks 
ucl press london 
ju julier uhlmann 
new extension kalman filter nonlinear systems 
proceedings aerosense th international symposium aerospace defence sensing simulation controls 
bibliography kal kalman 
new approach linear filtering prediction problems 
journal basic engineering 
kim 
kim 
dynamic linear models markov switching 
journal econometrics 
kit kitagawa 
monte carlo filter smoother non gaussian nonlinear state space models 
journal computational graphical statistics 
kjaerulff 
triangulation graphs algorithms giving small total state space 
technical report tr department mathematics computer science aalborg denmark 
kk koller 
nonuniform dynamic discretization hybrid networks 
proceedings th annual conference uncertainty ai uai pages 
kn kurien nayak 
back consistency trajectory tracking 
proceedings th national conference artificial intelligence aaai pages 
kp koller pfeffer 
object oriented bayesian networks 
proceedings th annual conference uncertainty ai uai pages 
lau lauritzen 
propagation probabilities means variances mixed graphical association models 
jasa 
lau lauritzen 
graphical models 
oxford university press new york 
lei 

triangulated graphs marked vertices 
andersen editors graph theory memory dirac volume annals discrete mathematics pages 
elsevier north holland 
bibliography lg larson goodrich 
intelligent systems software human mars missions 
international aeronautical foundation congress 
liu liu 
independent sampling comparisons rejection sampling importance sampling 
statistics computing 
lj lauritzen jensen 
stable local conditional gaussian distributions 
statistics computing 
lms lerner moses scott mcilraith koller 
monitoring complex physical system hybrid dynamic bayes net 
proceedings th annual conference uncertainty ai uai pages 
lp lerner parr 
inference hybrid networks theoretical limits practical algorithms 
proceedings th annual conference uncertainty ai uai pages 
lerner parr koller biswas 
bayesian fault detection diagnosis dynamic systems 
proceedings th national conference artificial intelligence aaai pages 
ls lauritzen spiegelhalter 
local computations probabilities graphical structures application expert systems 
journal royal statistical society 
lsk lerner segal koller 
exact inference networks discrete children continuous parents 
proceedings th annual conference uncertainty ai uai pages 
mci mcilraith 
explanatory diagnosis actions explain observations 
proceedings th international conference principles knowledge representation reasoning kr pages 
bibliography min minka 
expectation propagation approximate bayesian inference 
proceedings th annual conference uncertainty ai uai pages 
mrr metropolis rosenbluth rosenbluth teller teller 
equations state calculations fast computing machines 
journal chemical physics 
ms mcnamee 
construction fully symmetric numerical integration formulas 
numerische 
middleton shwe heckerman henrion horvitz lehmann cooper 
probabilistic diagnosis reformulation internist qmr knowledge base ii evaluation diagnostic performance 
methods information medicine 
mur murphy 
inference learning hybrid bayesian networks 
technical report csd department computer science berkeley 
mur murphy 
variational approximation bayesian networks discrete continuous latent variables 
proceedings th annual conference uncertainty ai uai pages 
nea neal 
probabilistic inference markov chain monte carlo methods 
technical report crg tr university toronto 
nil nilsson 
efficient algorithm finding probable configurations probabilistic expert systems 
statistics computing 
ogata 
modern control engineering 
prentice hall upper saddle river nj fourth edition 
bibliography ok ortiz kaelbling 
adaptive importance sampling estimation structured domains 
proceedings th annual conference uncertainty ai uai pages 
par park 
map complexity results approximation methods 
proceedings th annual conference uncertainty ai uai pages 
pea pearl 
probabilistic reasoning intelligent systems networks plausible inference 
morgan kaufmann san mateo california 
pradhan provan middleton henrion 
knowledge engineering large belief networks 
proceedings th canadian conference artificial intelligence pages 
pavlovic rehg 
cham murphy 
dynamic bayesian network approach tracking learned dynamical models 
proc 
iccv 
ps pappas sastry 
continuous abstractions dynamical control systems 
antsaklis kohn nerode sastry editors hybrid systems iv volume lecture notes computer science pages 
springer verlag berlin germany 
press teukolsky vetterling flannery 
numerical recipes cambridge university press 
put puterman 
markov decision processes discrete stochastic dynamic programming 
wiley new york 
rab rabiner 
tutorial hidden markov models selected applications speech recognition 
proceedings ieee 
rap raphael 
hybrid graphical model rhythmic parsing 
artificial intelligence may 
bibliography sahami dumais heckerman horvitz 
bayesian approach filtering junk mail 
aaai workshop learning text categorization 
sp peot 
simulation approaches general probabilistic inference belief networks 
proceedings th annual conference uncertainty ai uai pages 
ss shenoy shafer 
axioms probability belief function propagation 
proceedings th annual conference uncertainty ai uai pages 
ss shumway stoffer 
dynamic linear models switching 
journal american statistical association september 
sto stone 
course probability statistics 
duxbury press 
tk taylor karlin 
stochastic modeling 
academic press san diego third edition 
tk tong koller 
active learning parameter estimation bayesian networks 
advances neural information processing systems pages 
tk tong koller 
active learning structure bayesian networks 
proceedings th international joint conference artificial intelligence ijcai volume pages 
tps tomlin pappas sastry 
conflict resolution air traffic management study multi agent hybrid systems 
ieee transactions automatic control april 
ty tarjan yannakakis 
simple linear time algorithms test graphs test acyclicity hypergraphs selectively reduce bibliography acyclic hypergraphs 
siam journal computing august 
whi 
operation modeling analysis reverse water gas shift process 
nasa cr press 
wie 
variational approximations mean field theory junction tree algorithm 
proceedings th annual conference uncertainty ai uai pages 
wn williams nayak 
model approach reactive systems 
proceedings th national conference artificial intelligence aaai pages 
zp zhang poole 
simple approach bayesian network computations 
proceedings th canadian conference artificial intelligence pages 
