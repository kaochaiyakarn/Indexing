improving performance learning locally weighted regression thesis graduate school university florida partial fulfillment requirements degree master engineering university florida acknowledgments completed guidance members thesis committee prof antonio prof mike prof eric schwartz 
prof keith deserves credit helping formulate original question regarding improvement learning 
am grateful facilities machine intelligence laboratory university florida gave ability carry research 
fellow members lab deserve providing enriching experience years graduate student 
ii table contents acknowledgments 
chapters problem description 
thesis organization background reinforcement learning 
description reinforcement learning 
definition markov decision process mdp 
policies mdps 
optimizing policy optimality criteria 
value functions 
derivation policy value iteration rules bellman equations 
optimization learning limitations discrete state learning 
progression rl learning advancements sarsa 
getting dimensionality factor 
approximating value function 
applying mdp methods pomdp problems description perceptual aliasing 
dealing pomdps 
approximation learning complete locally weighted regression approximation 
iii page simulator design implementation choice physical platform 
description platform 
simulator considerations 
ii simulation implementation generalities 
arena obstacles infrared calculation 
infrared algorithm 
collision detection algorithm 
robot extraction algorithm 
simulator features 
ease porting code 
experimentation 
learning parameters table generation 
approximating value function neural network 
approximating value function locally weighted regression weighting functions operation different environments discussion results predicted vs observed table convergence 
vector quantization 
perceptual aliasing 
neural networks map problem value function basis performance comparison mechanism performance enhancement directions 
summary contributions 
directions 
appendix 
sketch 
iv thesis graduate school university florida partial fulfillment requirements degree master engineering improving performance learning locally weighted regression chair antonio major department electrical engineering december thesis describes method dealing large continuous state spaces encountered large reinforcement learning problems 
problem approached learning coarse quantization states 
value function approximated extend results original large continuous state space 
second step method improves problem performance allowing agent originally available state information 
performance step method significantly improved compared performance coarsely quantized problem 
nature method directly applicable practical reinforcement learning problems perceptual aliasing learning times problem 
simulator demonstrate method task obstacle avoidance complicated environment moderate amount perceptual aliasing 
significant performance enhancements observed learned policy dependent state history 
differentiates method described current methods dealing perceptual aliasing involve memory previous states distinguish identical states 
vi chapter problem description oftentimes problem faced researchers applying reinforcement learning nontrivial robotics problem run head curse dimensionality 
particular problem researchers discrete state algorithms number states exponentially increases complexity problem 
various methods dealing large state spaces proposed experimentally demonstrated methods effectively dealt practical situations difference states perceivable robot 
shown useful reinforcement learning algorithms experience theoretical difficulties conditions algorithms explicitly account incomplete perception lack generality computationally awkward effective 
thesis demonstrates method performance discrete state algorithm improved applied continuous state problem combination function approximator 
method consists steps 
step consists learning value function small number discrete states 
second step involves function approximator generalize discrete states continuous state space allowing previously discarded state information effectively problem solution 
thesis organization thesis divided chapters problem background related simulator design implementation experimentation discussion directions 
appendix containing information obtain simulator source code associated user guide concludes thesis 
problem background covers relevant background reinforcement learning leading approach problem 
background material order specifically define value functions markov decision processes mdps measure utility particular policy mapping states actions 
go theoretical backing useful applications learning dimensionality problem evident 
methods literature approach topic leading utility specific approach increasing real world performance learning 
simulator design implementation chapter describes simulator necessary specifications implementation details 
interest keeping thesis focused experimentation implementation details confined describing novel elements code 
chapter experimentation covers generation table followed descriptions methods approximate value function 
results approximation neural network nn followed positive results approximation locally weighted regression 
discussion chapter analyzes experimental results issues surrounding 
summation results followed chapter takes look practical utility method 
short exploration directions taken 
chapter background reinforcement learning description reinforcement learning principle reinforcement learning rl easy conceptualize 
general observable rl problem agent able fully perceive current state st state st determination state agent environment agent position environment time step part discussion assumed state member set discrete states suppose agent robot determines particular state 
agent choose execute action set discrete possible actions 
agent observe inputs determine new state st 
action agent perceives state st receives bounded reinforcement signal reward rt reflects value agent executing action state st goal agent find policy mapping states probabilities executing actions maximize long term reward agent 
reinforcement learning methods generally describe algorithms agent learns behavior policy feedback generated trial error 
reinforcement learning methods solving behavior policy particularly lend situations world model known advance 
reinforcement learning problem agent immediate reward signal told actions best long run 
actions give large immediate rewards may optimal long run 
consider example problem driving truck steep valley shown 

truck hill suppose engine truck powerful accelerate steep valley side get goal 
order reach goal solution move away goal opposite slope left 
car build momentum carry steep slope right 
agent choose sequence actions look worse terms getting closer final goal 
agent choose actions negative immediate rewards able achieve ultimate objective 
problems delayed rewards example modeled markov decision processes mdps 
definition markov decision process mdp 
mdp consists tuple finite set states finite set actions bounded reward function rt st state transition function st st 
policies mdps rl goal agent maximize long term reward environment 
state agent choice possible actions 
action agent elects execute state affects agent long term reward influences action selections 
order discuss method action selection necessary define concept policy 
agent policy maps states probabilities selecting different actions denotes probability selecting action agent state time st object rl find policy maximizes agent long term reward words want find optimal policy 
possible scenarios problem 
scenario agent access complete description mdp state transition probabilities values state forth 
collection methods known dynamic programming address paradigm 
second scenario assume priori knowledge underlying mdp scenario primary focus rl 
dynamic programming methods useful case adaptations form basis rl methods 
optimizing policy looking state action policy covering mdp primary concerns questions value goodness policy 
optimal policy 
optimality criteria meant optimal long term policy 
ways set defining optimal policy 
general definition optimal generate policies differ greatly 
common methods defining optimality shown rt criterion considers rewards ignores rewards time steps ahead 
denotes expected value policy 
second optimality criterion infinite horizon discounted model rt discount parameter discounts rewards obtaining greater value temporally close rewards mathematically necessary ensure sum bounded 
third optimality criterion average reward model rt average reward optimality criterion difficult distinguish policies different reward profiles time 
immediate rewards weighed heavily rewards distant 
optimality criteria infinite horizon discounted criterion tractable analytically practice 
mathematically analyzing rl problems important realize varying optimality criteria place different emphasis desirability delayed rewards determining optimal policy 
reason discussion optimality restricted infinite horizon discounted case 
value functions find value particular policy value function defined expected long term reward mdp policy 
jt st words denotes expected long term reward policy agent starts state time step subsequently follows policy 
considering infinite horizon discounted case optimality criterion obtain eq 
rt action value function defined rt st words denotes expected long term reward policy agent starts state executes action time step subsequently follows policy 
define optimal action value function max possible states actions 
bellman showed stationary deterministic optimal policy exists 
problem faced rl efficiently find policy 
derivation policy value iteration rules bellman equations deriving optimal policy important realize best policy value states equal expected long term reward 
steps show derivation iterative policy update equation converges optimal policy 
max max rt st max rt rt st notation defined discussion markov problems max max note bellman showed optimal policy stationary deterministic 
equations define optimal policy argmax compare arbitrary policy determine value utility policy 
case optimal policy value determined iteratively method initialize value 
repeatedly perform max maximum change epsilon close previous value value function converged close desired optimal value function 
point optimal policy easily calculated eq 

conversely optimal policy iteratively obtained initial arbitrary policy 
repeat evaluate improve policy arg max improvement epsilon small 
point arbitrarily close optimal policy 
optimization learning watkins combined steps single step update algorithm named learning 
result learning computationally tractable proved converge convergence proof lacking separated steps 
learning algorithm initialize arbitrarily states actions 
set initial start state repeat select action policy derived current function 
take action observe reward state 
update step just taken 
eq 
set state new state quantity xi predicted value particular state action pair 
selecting max states result ideal value function 
course method begins imperfect estimate values state iterative evaluation prediction update algorithm applied 
max algorithm proven converge probability true value problem mdp states visited infinitely values represented discrete values look table limitations discrete state learning kearns singh demonstrated convergence bounds finite sequence transition probabilities mixed 
learning proven converge optimality proven convergence rate non optimal conditions 
practice depending learning parameters convergence optimality near optimality extremely slow 
central difficulty convergence rate dependent number discrete states 
reinforcement learning discrete states inherent problem arising known curse dimensionality number state variables increases number discrete states increases exponentially 
matters worse sufficient memory available store state values frequency agent visits particular state proportionately decrease 
rl methods generally require repeat visits states order estimated value converge true value convergence time generally increase exponentially 
difficulty tends inhibit meaningful experimentation discrete state rl problems having free state variables 
example consider robot infrared sensors capable bit precision plus able turn different directions move different speeds 
order apply learning need assume inputs completely distinguish states hidden states remain problem 
assumption rarely true practice implications dealt section 
set states problem possible combination sensor values times number possible actions different states 
obtaining value function covering state problem table learning algorithm quite impossible 
state values stored somewhat reasonable current technology roughly memory required sheer size problem inhibit convergence due long sequences actions needing passes propagate values back start sequence 
fact bounds established kearns singh eq 
take nearly steps approach true values 
assuming mixed transition probabilities 
weren case number transitions greater 
log transition log log log values resulted considerations ir sensors bit resolution combinations possible rotation states possible actions learning advancements progression rl years watkins originated learning algorithm received attention researchers due ease implementation sound theoretical backing 
encouraging results come efforts reduce storage space number updates required 
learning proven certain conditions converge ideal model policy 
point generally proceeded directions 
direction focused reducing number updates state transitions required state approach near optimal values 
watkins proved convergence alternate learning algorithm updates values multiple states iteration 
sarsa rummery niranjan came algorithm named sarsa difference standard learning value state action taken value selected max operator 
changes algorithm line algorithm line algorithm 
proved online td methods converged results hold mdps 
littman results show sarsa converges defined value function performance asymptotically optimal 
gordon showed sarsa easily fail converge partially observable mdp pomdp 
general pomdp problems proven difficult analyze researchers done decrease po character problem introducing state sequence memory 
examples refer :10.1.1.45.8935:10.1.1.56.7115
singh established utility sarsa eligibility traces certain classes pomdp problems near optimal memoryless policies exist 
jordan results regarding memoryless policies 
getting dimensionality factor general area focused tackling dimensionality problem updating multiple states information single action 
approach presupposes states similar values state action pairs 
update algorithm updates actual value values neighboring states 
area performed singh 
alternative approach discrete state approach simply abandon practice finely dividing states approximate continuous valued conditions simply adopt continuous valued approach 
difficulty approach technique depends policy memory happened specific state previously visited fail utter prior visit information available generate action values state 
continuous value function produced map states values greatly simplify generation optimal policy 
approximating value function considerable effort spent generating useful approximation value function observed state transitions estimate obtained value unvisited state 
reasoning attempts described manner finely discretized state space logical reason states closely resemble values likewise resemble 
unfortunately efforts rest sound theoretical background general efforts met mixed results successes specific applications failing 
tesauro met eyebrow raising success neural net methods 
suspect particular application backgammon abnormally suited nn methods game depends probabilities dice combinations probabilities smoothly vary 
researchers shown linear approximations capable approximating value functions proofs exist nonlinear methods despite positive experimental results 
linden pointed examples nn methods incapable mapping value functions contain discontinuities 
example linden obstacle avoidance forbidden regions inside obstacles produces discontinuities value function borders obstacles 
interesting note difference linden approach gridworld approach thesis reactive policy sensory inputs 
approach guarantees existence forbidden regions state space discontinuities value function approach assumes point state space reachable 
tsitsiklis roy describe proof convergence line function approximators finite non terminating markov processes 
addition reconcile varying results literature regarding convergence lack thereof function approximators showing convergence depends sampling frequency various states 
convergence guaranteed states sampled distribution steady state markov chain 
doing tends generate inaccurate state action values leading non optimal policies forth 
applying mdp methods pomdp problems convergence properties learning related algorithms currently fairly understood expect wide application real robot situations 
slight problem moving simulation reality 
stems fact various rl algorithms dependent fact markov process learned entirely observable hidden states 
real applications difficult ensure 
applying algorithm learning real problem researcher typically hoping acceptable results obtained despite partially observable characteristics situation 
common sense reasoning hope performance mdp methods decline gracefully po character increases 
demonstrated gordon researchers reasoning proved false 
hidden state mdp cause divergence 
easy mathematical situations mdp methods perform miserably examples show worst case performance algorithms 
actual applications mdp methods experiments problems exist memory policies main returned experimenters consider acceptable results 
acceptable understood necessarily perfect behavior significant increase performance course experiment 
common factor rl simulations gridworld agent knows exactly state state equivalent location 
contrast typical experimental setup involves distinguishing states basis agent sensory input infrared reflections measured agent 
outside mechanism beacon triangulation state transition memory ensure location mistaken risk perceptual aliasing 
description perceptual aliasing ballard whitehead showed agent observational inputs define states introduces possibility different situations appearing agent 
quite issue doing experiments discrete valued states perceptual resolution suffers due state minimization 
instance quantizing ir sensor different input values shown significant amount information lost 
figures demonstrate examples perceptual aliasing 
dealing pomdps method dealing pomdps add memory agent different states may distinguished state sequence taken reach view solutions undesirable working reactive policy tend agent dependent exact environment 
instance suppose agent generated table object avoidance problem possesses considerable pomdp character memoryless policy 
obstacles moved purely reactive method perform performance memory method degraded 

proceeding counterclockwise upper right see situation robot approaching corner 
diagram upper left see robot detect corner ideal sensors 
diagram just see robot observe corner forced quantize inputs levels 
lower right gone step reduced number sensors sensor gives average reading detection area 
unclear lies ahead robot 
way thinking exists pomdp algorithms probabilistic estimate state agent occupies 
drawback type technique computationally expensive reliant problem information regarding exact number hidden states 
addition whitehead ballard suggest method accessing additional sensory data practice method somewhat impractical typically agent sensors computationally unable handle extra data 
control policy followed algorithms solving pomdps affects probabilities agent occupy particular state 
problem learning control policy followed agent affect eventual convergence 
states pomdp persistently exciting possible establish convergence 
case learning algorithm function values underlying states combined probability occupancy particular observation pa max meaning draw learned value state summed value underlying states weighted probabilities 
stated earlier treating agent sensory inputs state values generally adds hidden states 
particular state defined particular range sensor values approximation learned value function conceivably better estimation values hidden states learned value observable state assuming true value function covers states behaved 
approximation learning complete approximating value functions focused maintaining approximation function learning approximation control 
examples refer 
positive results reported performed fully observable mdps 
results reported sensor values mapped states introducing pomdp character problem 
order avoid theoretically ground decided perform learning problem shown converge mean type problem 
performing approximation learning complete avoids theoretical problem generated altering state occupancy probabilities 
locally weighted regression approximation locally weighted regression lwr suited generating smoothed line data points having non uniform distribution 
kernel regression suited problems having irregular distribution data points 
nearest neighbor methods unsuited due irregularity data density table 
neural networks having produced results applications expected function approximating function varies widely comparatively data points 
lwr successfully variety rl applications 
general lwr create local model function query point 
particular advantage local model simple linear functions required approximate global function neighborhood query point 
avoids difficult task generating global model nonlinear function 
regards rl applications lwr quite useful problems value function control required continuous discrete samples available estimate value function 
forbes andre obtained results lwr automobile lane centering task form prioritized sweeping 
moore schneider deng authored covering algorithmic enhancements lwr 
compared lwr favorably techniques regarding problem estimating distance door 
tadepalli ok local linear regression improve performance learning 
atkeson moore schaal number practical results lwr devil sticking showing lwr suited control applications demand fine precision 
chapter simulator design implementation choice physical platform tj pro robot chosen platform simulated 
tj pro lab previous rl experiments proved versatile simple system 
small number sensors actuators lends easy simulation robot shown perfectly capable physical rl platform 
description platform tj pro robot seen circular roughly cm diameter cm tall 
khz modulated ir emitters detectors front 
collision detection accomplished bump sensor composed front back bumper rides switches 
motors mated rubber wheels mounted side body 
tj pro generally runs pack aa batteries mounted holder inside body 
brains robot provided motorola hc microcontroller ram 
speed robot moves controlled pwm signals motors 
maximum speed fully charged batteries cm sec 
practical length experiment limited battery lifespan roughly half hour 
ram contents remain stable battery place swapping batteries middle experiment invitation problems 
addition platform pair side looking ir detectors simulated tj pro 
necessary reduce amount chattering values 
modification performed physical robot minor amount electronics 

tj pro simulator considerations quick comparison predicted convergence rate number table updates second obvious converged table needed project obtained practical manner physical platform 
simulator needed order shorten duration experiments 
simulation accurately model ir sensing proximity 
simulation detect collisions simulated robot obstacles sides arena 
respects model correct movement robot 
simplifications respect motion 
physical robots don move perfectly straight line 
wheels generally somewhat center slightly greater diameter imperfection motors 
net result isn straight line somewhat arc motion simplified straight line motion faster calculate 
simplification effect results movement actions swamp deviation 
simplification deals behavior robot actual physical contact obstacle 
robot contacts obstacle oblique angle tends turn perpendicular collision due friction torque contact point 
collision robot tends slide face obstacle turning depending wheels slip 
behavior somewhat difficult model accurately decided leave simulator 
version simulator dos program evaluation genetically optimized learning parameter selection 
tracking bugs simulator proved problematic convenient interface determine robot behaving 
discovering bugs robot able escape arena enter obstacles decided develop graphical interface 
learned program windows environment developed application visually simulate robot motion providing means recording data simulation runs 
arena obstacles simulation implementation generalities meter meter arena shaped seen 
functional difference arena walls sides obstacles 
simply contained program list line segments robot cross 
reshaping arena trivial merely involves changing list line segments 
obstacle list handle length obstacles increasing number course program run slowly 
corners arena obstacles specified rotated degrees clockwise 
done decrease occurrence infinite slopes ir calculation section 
infrared calculation infrared emissions tj pro robot simulated array line segments extend outwards simulated meters somewhat produce detectable reflection real robot 
guiding principle better long short 
actual length ir line segments doesn really matter terms processing overhead drew long calibrated return observed white objects various distances 
manner ir calculated lends simulating various ir simulating specular diffuse reflection 
obstacles physical arena fairly coarse surfaces diffuse reflection calculated 
simple extension simulation calculate increased ir return smooth perpendicular surface 

ir lines seen 
note difference resolution side ir sensors compared front ir sensors 
full resolution needed straightforward alter ir routines full resolution sensors 
infrared algorithm obstacle segment checked intersection entire ir segment list intersection point distance obstacle robot stored memory 
multiple obstacle segments intersect ir segment list closest intersection point 
simulated return value determined applying inverse square law 
simulation agrees reality constant intensity scaling coefficient needed 
exact value determined calibration order match reality 
agreement shows ok calculate diffuse reflection reflection return perpendicular obstacles matched reality 
collision detection algorithm robot checked collision obstacle segments manner 
robot circular equation form segment equation form intersections determined manner 
equations set equal little mathematical massage put classic quadratic form 
application quadratic solution equation straightforward determine intersection existed seeing roots imaginary 
robot extraction algorithm robot partially inside object moved edge tangent obstacle 
direction retreat opposite initial direction motion allows extraction backwards collision 
code allowed possibility multiple contacts occurring 
observing program dealt difficult situations acute angle decided drop portion algorithm rarely long runs 
program check collision 
robot returned old location bump detector triggered 
simulator features interesting program features completely described users guide accompanying simulator possible pause simulation place robot arena useful testing robot behaves approaching corners instance 
simulation speed slowed pace desired 
visible ir lines toggled increase speed visual appeal 
lastly filename progression feature enabling copies program multiple computers write back results central location having worry overwriting data previous runs 
ease porting code code run physical robot ported change simulator 
simulator framework performs various ir movement functions form little different actual robot code 
physical robot control code performs exactly simulator slight differences format functions dealing simulated arena 
chapter experimentation learning parameters representation robot arena experimental procedures seen fig 

arena designed complex environment composite collision avoidance behavior required 
wide open regions narrow passages acute angles intended trap robot 
environment intended duplicate complexity real world environment 
collision avoidance behavior stayed far possible wall obstacle fail explore confined areas wall behavior fail explore open space 
acceptable collision avoidance behavior robot covering reachable area arena colliding little possible seen 
specification rules ideal collision avoidance behaviors spinning place advance retreat determining required reinforcement schedule involved tweaking rewards desired behavior emerged 
reward values collision forward step turn backing 
learning parameters learning rate alpha discounting factor gamma ensure sufficient exploration states random move selected time greedy maximum value selection remaining actions 

main robot arena 

picture showing area covered simulated robot steps 
note unpopular areas near upper left bottom center 
attempting avoid generating backup forward behavior reward backing set negative 
generated behavior appeared acceptable collision avoidance performance ceased improve noticeably experiment wore 
general noticed reinforcement schedules penalties particular action resulted quick collision avoidance results long term performance suffered comparison schedules positive rewards 
due low values unrepresentative true utility actions prematurely suppressing exploration 
table generation reinforcement schedule generated acceptable behavior simulator run long reasonably possible generate table approached convergence 
day run simulator performed updates saving table steps 
graph absolute value sum table states plotted appears values come reasonably close convergence 
chattering values tables saves averaged obtain representative table 
seen table sum smoothly approach convergence 
gordon described state value behavior genesis lies problem perceptual aliasing different robot positions determined state 
ordinarily wouldn problem action appropriate positions 
perceptual aliasing causes difficulties different actions appropriate 
see chapter discussion illustration issue 
number steps 
graph value convergence showing absolute value sum states 
table generated necessary convert quantized input ranges specific values sample points function approximation 
merely middle state vector quantization performed 
accomplished mean actual input value state large number steps 
accounted possibility actual average inputs displaced middle quantized ranges 
see section detailed analysis vector quantization step 
approximating value function neural network step extending collision avoidance behavior continuous valued inputs implement neural network approximated value function table data points 
neural net modified freely available neural network simulator authored phil goodman david rosen allen plummer 
despite extensive effort useable approximation 
best performance marginally better having controller 
see section discussion neural network failed 
approximating value function locally weighted regression step approximate value function locally weighted regression lwr 
paucity sample points roughness value function unnecessary include values distant query point regression calculation 
nearest points various distance weighting formulas tested performance 
shows graph collision avoidance performance plotted weighting scheme 
experimental runs evaluated time task failure basis measuring number steps collision 
comparison best neural network controller average run collision roughly steps wall contacted basically amounts distance nearest obstacle random controller run steps drifted wall 
increased run length random controller primarily attributable fact average actions largely cancel leaving robot roughly location 
analogous example movement dust glass water brownian motion 

performance lwr compared standard learning arena shown runs carried multiple computers generate samples runs reasonably possible minimum samples 
set runs different weighting function 
functions form distance positive real number range investigated 
values greater increasingly reproduced quantized values original table identical action selection 
values resulted rapid performance decrease reasons discussed section 
weighting functions distance weighting functions systematically investigated spot checks types functions showed object avoidance performance depended weights extremely large near table data points having broad shape gaussian distribution 
somewhat contrary reported atkeson choice weighting function generally critical performance note existence exceptions 
high degree roughness value function demand narrow weighting function order diminish influence irrelevant data distant query point 
operation different environments experimentation showed lwr improves collision avoidance performance specific environment step examine learned value function specific environment 
case rl algorithms state sequence memory distinguish states appear identical 
learning step learned memoryless policy policy probably perfectly adapted environments usefully serve controller having re computationally expensive learning step 
seen controller performed quite comparison original performance learned environment 
fact performed better 
reasons examined chapter explanation boils original environment problem hard detect obstacle corners 
furthermore continuous valued inputs concert lwr generated approximation value function resulted substantial improvement case showing method practical utility task discretized table information control continuous valued problem 
arena arena arena 
comparison collision avoidance performance different environments 
arena arena arena shown arena shown 
environments lwr significantly enhanced performance 

arrangement second arena 
learning learning lwr 
arrangement third arena 
chapter discussion results predicted vs observed table convergence formula derived kearns singh mixed transition probabilities values experimental setup approach factor true values steps 
referring apparent convergence proceeds somewhat slowly steps corresponds axis 
takes nearly steps axis table sum visibly levels 
extended convergence length reasonable considering robots states occupied instance ir return sensors bump detector triggered 
chattering table values noticeably impact rate convergence chattering occurs faster time scale convergence rate 
vector quantization vector quantization describes process state space partitioned regions defined representative vector 
representative vectors chosen minimize total mean squared distance sample points nearest vector 
vector quantization generally iterative process starting division state space arbitrary regions calculating centroid data distribution region redefining regions location nearest centroid 
shows random distribution data dimensional state space arbitrary estimate region borders centroids regions marked black dots 
region borders defined equidistant nearest centroids 
step seen recalculation centroid region region boundaries redrawn accordance changed centroid positions 
data distribution example random see little change original centroids boundaries 

random distribution points dimensions estimated centroid region shown black dot 
example data distribution random 
seen original guesses regarding location centroids wildly inaccurate recalculation centroids results substantial change position boundaries 

centroids regions fig 
calculated region boundaries redrawn accordance 
values obtained learning phase action values input range defined state 
order obtain data points table necessary condense state defining input ranges discrete points 
difficulty simply mean input range resulted regression performance worse expected 
apparent distribution continuous valued inputs non uniform resembling 

starting estimate regions 

centroids recalculated boundaries moved accordance 
example input range left front ir sensor corresponded lowest ir level extended range 
robot roaming wide open areas input value common meant value corresponding state actions taken low input range 
weighted average median input range shifted average lower range 
simple step performed step vector quantization procedure recalculating centroids 
performing multiple iterations involved determining new value altered state computationally expensive 
iteration markedly improve performance combination lwr 
perceptual aliasing stated earlier thesis pretending sensory input values define discrete states mdp solution methods opens oneself problems 
problems lie fact identical input values reality different states demand different actions 

inputs action 
inputs different action figures illustrate situations robots positions identical inputs believe identical states 
doesn matter action obviously appropriate turn left 
different actions appropriate 
robot position easily proceed forward robot position 
fact robot position collide corner situation approaching corner ir returns balanced occur position values state heavily biased expected reward actions taken position versus actions taken position guarantee learned value stable gordon showed perceptual aliasing lead indefinite oscillation chattering values associated indistinguishable states 
example average learned value primarily reflect experiences taken position value quite converge experiences position 
time position encountered learned value away value learned values long period time averaged order obtain true estimate observed state 
figures show exactly happened simulation 
figures show position collision long run 
note 
collisions arena collisions corners exactly expected reasoning 

collisions second arena 
neural networks map value function size neural network approximate function defined sample points related complexity function 
rule thumb determining maximum number hidden units result overfitting sample points degree freedom weight network 
fewer sample points hidden units likelihood overfitting increases 
neural network successfully reproduces sample points generally overfit invalid approximation sample points 

collisions third arena 
problem faced neural network approximate function sample points exist define function relatively smooth 
cursory inspection values shows samples extremely jagged function subject useful approximation neural network constrained number weights 
despite trying variety neural network morphologies proved impossible generate neural network fewer degrees freedom number sample points exactly reproduce sample points 
likewise proved impossible accomplish rigorous task selecting correct actions states defined learning process 
faced failure reasonably sized neural network generate acceptable approximation attempt unreasonably sized network somewhat weights sample inputs 
network able reproduce values 
network controller expected incapable generating correct actions input equal sample points train 
predicted consequence overfitting virtually assured 
basis performance comparison question basis various runs compared 
time task failure selected order rule presence artifacts due feature weighting 
state bumper depressed seen neighboring states untouched bumpers influence outcome regression 
influence depends feature weighted 
clearly simply euclidean distance bump bump states invites adding apples oranges analogy overlooking fact distance type distance ir sensor points 
easiest way deal problem add scaling factor bump distance add distance query point sample points 
inspection raises question best scaling factor 
attempt deal determining ideal scaling factor time determining correct distance weighting factor decided determine best distance weighting meant behavior dealing collision stood chance poor feature scaling interacted favorably distance weighting formula 
effort compare distance weighting functions basis proved necessary time runs collision happened repeat procedure 
experiment wouldn test behavior actions bump proved convenient consider un bumped states separately bumped states effect giving infinite feature weighting 
determining useful range distance weighting factors experiment performed test lwr controller recover collision 
distance weighting equation experiment center range improvement noted 
conditions bumped un bumped considered separately regression including un bumped states un bumped queries vice versa 
essentially infinite feature weighting bump value 
experiment unfolded controller showed recover bumping object performing action sequences spinning place retreating advancing collision repeatedly forth 
mechanism performance enhancement shows simplified interpretation smoothes values boundary states shown curved lines lwr approximation affect action selection basis highest action value near boundaries table states 

boundary smoothing collision points arenas compared seen collisions corners acutely angled corners having collisions angled corners having fewer collisions 
self evident statement change action selection resulted improved performance result fewer collisions corners 
analogy say state represents condition robot nearing corner 
robot idea getting dangerously close corner inputs receives mimic condition safe distance perpendicular wall 
state action values learned state represent composite conditions robot colliding corner robot coming somewhat close perpendicular wall 
states action go forward action turn away 
discrete state action selection method example inputs range belonging state seen action selected 
time input values risen threshold state late turn away collision occurred 
lwr smooth values boundary states robot known approaching undesirable state input values state boundary turned away collision occurred 
value function presumed behaved function discontinuities 
smoothing state boundaries reproduce value function exactly 
unfortunately priori method determine smoothing start little smoothing large distance weighting factor slowly decrease weighting factor collision avoidance performance began decline implied smoothing 
chapter directions summary contributions thesis demonstrated mdp algorithm learning pomdp problem memory policy determined exist 
performance algorithm enhanced completion learning generation value function continuous state space lwr 
significance method lies procedure demonstrated allows continuous valued problem approached discrete state problem order minimize computational complexity sacrificing possible accuracy obtainable additional state information 
benefits method straightforward possible attempt characterization complex continuous state problems previously immune theoretical massage 
directions step taken method apply function approximation control learning 
convergence proven possibility exists results obtained experimentally class problem 
gordon done investigation online fitted approximations results encouraging show large classes approximators including lwr divergent situations 
somewhat discouraging lwr fitting type approximator task 
method improve performance perform multiple learning episodes intervening recalculation state classification boundaries vector quantization 
help compensate fact researcher chosen boundaries optimal locations 
dam performed related area self organizing input state quantization procedure obtained results 
littman moore reinforcement learning survey journal artificial intelligence research vol 
pp 

bellman dynamic programming 
princeton new jersey princeton university press 
reinforcement learning algorithms methods temporal differences 
master thesis institute computer science warsaw university technology 
watkins learning delayed rewards 
ph thesis king college cambridge 
watkins dayan technical note learning machine learning vol 
pp 

ring continual learning reinforcement environments 
phd thesis university texas austin 
jaakkola jordan singh 
convergence stochastic iterative dynamic programming algorithms neural computation pp 

kearns singh finite sample convergence rates learning indirect algorithms advances neural information processing systems kearns solla cohn eds cambridge ma mit press pp 
barto bradtke singh learning act real time dynamic programming artificial intelligence vol 

rummery niranjan line learning connectionist systems cambridge university engineering dept technical report cued infeng tr 
singh jaakkola littman convergence results single step policy reinforcement learning algorithms univ col dept comp 
sci boulder tech 
rep 
littman 

combining exploration control reinforcement learning convergence sarsa 
available www cs duke edu 
gordon chattering sarsa cmu learning lab pittsburgh pa internal report 
wiering schmidhuber hq learning adaptive behavior vol 
pp 

lin tom mitchell memory approaches reinforcement learning nonmarkovian domains carnegie mellon university pittsburgh pa tech 
rep cs 
chrisman reinforcement learning perceptual aliasing perceptual distinctions approach proceedings tenth international conference artificial intelligence pp 

singh eligibility traces find best memoryless policy partially observable markov decision processes proc 
icml pp 

jordan singh jaakkola learning state estimation partially observable markovian decision processes proceedings eleventh machine learning workshop pp 

ri littman unified analysis value function reinforcement learning algorithms neural computation pp 

singh jaakkola jordan reinforcement learning soft state aggregation advances neural information processing systems tesauro touretzky eds vol 
pp 
tesauro practical issues temporal difference learning machine learning vol 
pp 

rummery problem solving reinforcement learning 
phd thesis university cambridge 
linden 

discontinuous functions reinforcement learning 
available anonymous ftp archive cis ohio state edu directory pub 
tsitsiklis van roy analysis temporal difference learning function approximation cambridge ma tech 
rep lids 
whitehead ballard learning perceive act trial error machine learning vol 
pp 

visual memory learning mobile robot navigation proceedings second international conference computational intelligence neurosciences vol 
pp 
boyan moore generalization reinforcement learning safely approximating value function advances neural information processing systems pp 

forbes andre practical reinforcement learning continuous domains computer science division university california berkeley tech 
rep ucb csd 
approximating values basis function representations fourth connectionist models summer school hillsdale nj 
cleveland loader smoothing local regression principles methods discussion 
statistical theory computational aspects smoothing eds 
heidelberg physica verlag pp 
pp 
pp 

hastie loader local regression automatic kernel carpentry statistical science vol 
pp 
atkeson moore schaal locally weighted learning artificial intelligence review pp 

moore schneider deng efficient locally weighted polynomial regression predictions proceedings th international conference machine learning fisher ed pp 

tadepalli ok scaling average reward reinforcement learning approximating domain models value function proc 
th int 
conf 
machine learning pp 

atkeson moore schaal locally weighted learning control artificial intelligence review special issue lazy learning algorithms 
comparison reinforcement learning techniques automatic behavior programming aaai pomdp oct pp 

zapata kinetics robotics development universal metrics robot swarms florida conference advances robotics miami fl 
antonio creating table parameters genetic algorithms florida conference advances robotics gainesville florida 
fu neural networks computer intelligence 
st ed new york ny mcgraw hill 
gordon approximate solutions markov decision processes 
ph thesis carnegie mellon university 
van dam adaptive state space quantisation reinforcement learning collision free navigation proc 
ieee int 
workshop intelligent robots systems pp 

appendix copy simulator thesis user guide obtained mil ufl edu publications 
simulator compiled visual known windows 
probably versions windows hasn tested 
source code probably compile windows compilers supplementary code libraries needed 
user guide contains instructions set files edit simulator code order adapt uses 
biographical sketch born ca attended dartmouth college university california san diego undergraduate 
received bachelor science degree biochemistry ucsd switched career paths electrical engineering grad student university florida active machine intelligence laboratory 

