integrating value functions policy search continuous markov decision processes nicholas roy robotics institute carnegie mellon university pittsburgh pa nicholas roy ri cmu edu sebastian thrun department computer science carnegie mellon university pittsburgh pa sebastian thrun ri cmu edu value function approaches markov decision processes successfully find optimal policies large number problems 
findings demonstrated policy search effectively reinforcement learning standard value function techniques overwhelmed size dimensionality state space 
demonstrate substantial benefits achieved combining approaches approximate value function solution low dimensional space seed policy search continuous high dimensional space 
demonstrate approach motion planner mobile robot 
show combination practice find policies efficiently policy search capable solving complex problems value functions 
value function approaches markov decision processes applied successfully large number problems 
unfortunately finding policies value function rapidly intractable large possibly infinite state spaces 
consequently approximate value functions frequently 
baxter bartlett demonstrated policies derived approximate value functions fail arbitrarily badly cases principled search policy directly maximizes long term average reward 
policy search comparison suffer unacceptably high variance local minima effects efficiency concerns 
real world domains highlight drawbacks methods 
particular domain interested control mobile robot 
robot real world lives continuous high dimensional space constraints demand efficient solutions 
implementations motion planners compromise approximate value function solution find high level plan re planning locally approximation fails 
local planner fail arbitrarily badly global information return finer grain local information 
approach combine value function policy search approaches 
value function approximation low dimensional discrete space 
employ policy representation high dimensional continuous space allows mapping project policy higher dimension space includes orientation velocity state features 
represent policy sequence waypoints local controller attempts drive waypoints sequence 
high dimensional projection value function policy seed perform gradient ascent policy space modifying waypoints increase expected reward policy 
number simple physically motivated assumptions demonstrate approach finds policies conventional means fail realistic amount time 
central assumption value function policy close optimal solution provides reasonable starting point 
mobile robot navigation problem assume value function provides topologically correct solution refined policy search 
provide guarantees quality final policy initial value function policy allowed arbitrarily bad 
provide empirical evidence approach successful real world domains 
related policy search methods solving pomdps promised greater convergence rates value iteration policy value function methods date retained typically intractable inner loop dynamic programming 
baird moore demonstrated vaps framework policy search value function approximations specific instances general formulation reinforcement learning 
necessarily highlight formulation bootstrap second formulation 
ng implemented mdp pomdp control density estimation continuous domain restrict posterior probability density system particular model :10.1.1.42.6193
baxter bartlett proposed gradient ascent policy search algorithm reinforcement learning pomdps single execution trace provide gradient information 
results require parameterized stochastic policy 
comparison able domain knowledge perform gradient descent deterministic policy show section reward function domain contains necessary gradient information 
davies proposed algorithm similar spirit mobile robot application projected low dimensional approximate value function solution higher space explicitly perform policy search minimize expected reward 
heuristic search find admissible trajectory deterministic agent 
contrast allows noisy action model find admissible path attempt find best path objective function 
markov decision processes markov decision process mdp describes problem agent mobile robot take actions world parameterized policy 
model outcome actions non deterministic probability distribution 
assumption true state fully observable action state independent state action 
mdp described tuple state space initial state set actions available agent 
transitional probability matrix describes state changes actions 
js probability state starting state action rewards agent 
general reward function mapping states actions numerical rewards 
rmax simplicity loss generality assume reward depends current state 
value function approach goal agent mdp setting maximize cumulative long term expected reward 
value function setting value assigned state value function policy represents long term cumulative expected reward 
optimal policy maximizes value expected reward state 
optimal value function bellman equation jsj jaj js js current policy discount factor determines contribution reward current state value 
rest assume loss generality undiscounted finite horizon problem termination state 
furthermore assume deterministic policy js 
value iteration finds optimal policy computing value function state policy iteratively updating policy value maximized state 
nature value iteration highlights computational cost mapping state value action preserved iteratively updated 
order find policy continuous high dimensional problem need different compact representation policy way evaluate policies representation 
policy search policy search operates evaluating long term cumulative reward current policy usually parameterized manner 
gradient ascent search estimates gradient reward adjusts policy parameters increase expected reward maximum reached 
order perform search policy space need parameterized continuous policy representation 
addition need map low dimensional policy high dimensional continuous space implications choice policy representation 
number different choices policies bayes nets neural nets take continuous valued state features return control action mapping discrete grid world complex representations easy :10.1.1.42.6193
represent policy series ordered set waypoints associated local controller 
controller emits actions action moves agent closest waypoint 
series waypoints viewed forming approximately piece wise linear path start state goal 
execution time action time function current state waypoint 

policy additional free parameter approach determines controller switches waypoint iff jjs jj approach noted principle sacrifice type policy final policy longer truly universal plan 
value function policies highly desirable property policies contain optimal action possible state 
optimal policy described exhibits property close expected trajectory quality policy arbitrarily bad greater deviation expected path goal 
model local controller accurately reflects true state distribution result policy search policy states truly matter 
initial policy estimate order search policies efficiently avoid problems local minima want estimate policy search 
reasonable policy low dimensional state value function remains convert value function policy policy larger continuous space 
extracting set maximum likelihood states describe expected trajectory lower space 
project states higher space generate large set waypoints 
unnecessarily dense policy prune merging neighbours value function policy shown 
endpoints line segments represent policy high dimensional space policy initial estimate search 
merge project projecting value function policy left higher dimension space right merge neighbouring states policy single line segment 
endpoints line segments constitute initial estimate policy search 
computing policy value optimal policy maximizes expected reward execution policy start state goal state max jsj dt js dt ds action dictated policy state integral simply expectation reward state distribution time dt continuous form cf 
equation 
final simplifying assumption local controller unbiased 
local controller keeps state distribution centred expected trajectory distribution parameters local controller 
dictates result action maximum likelihood line computing expected distribution easier 
modify integral equation transforming state distribution dt js function centred maximum likelihood state dt 

allows simplify equation dt 
dt dt maximum likelihood state time dt policy 
onwards dt assumed agent distribution state space parameters assumed 
constrain policy described series waypoints euclidean space expand recurrence equation 
dt 
ds maximize equation differentiating travelling positive gradient 
differentiating gives rv 
ds 
ds note case purely deterministic action models perfect controller drop 
term simply integrate reward path 
integrating expected reward respect 
capture noise inherent local controller subject assumption bias 
larger noise local controller conservative eventual policy 
depending particular domain individual gradient terms computed closed form sampling techniques 
domain mobile robot control reward contains gradient information directly computed closed form 
determining policy size initial policy estimate provided value function consist number waypoints 
optimal policy may require arbitrary number waypoints 
consequently procedure required introduce new waypoints needed 
number waypoints needed represent policy similar concept problem estimating number clusters expectation maximization consequently borrow popular split merge technique 
perform policy search convergence consider inserting new waypoint appropriate 
algorithm terminates policy value increase inserted waypoint 
heuristic inserting waypoints immediate reward waypoints drops immediate reward waypoints insert table algorithm summary 
run dynamic program extract policy value function section 
determine maximum likelihood trajectory convert set waypoints section 
policy search waypoint determine total policy value contribution trajectory previous waypoint waypoint section measure gradient endpoints move waypoint gradient policy value changes repeat waypoints convergence 
add new waypoints section find lowest immediate reward trajectory insert new waypoint repeat step 
repeat waypoint insertion convergence mobile robot navigation demonstrate approach mobile robot navigating indoor environment 
shows mobile robot pearl interacting elderly people assisted living residence 
nature particular environment emphasizes need optimal policies confined quarters slower agile humans surroundings control policies approximations shape orientation robot increasingly brittle 
importantly stated value function planning exact state space computationally intractable 
represent robot state time tuple gives position robot orientation 
translational rotational velocities 
assume state space continuous dimensions 
kinematic model robot coupled local controller gives state distribution 
goal robot pearl interacting residents health care facility 
maximize expected reward navigation problem designed robot incurs small negative reward 
traveling distance receives large positive reward reaches goal 
furthermore robot receives large negative reward 
attempting travel distance obstacle closer certain safety range 
knowledge state space compute gradient terms approximately 
reward function seeks minimize travel time minimizing likelihood robot hit obstacle 
expected travel time point trajectory function direction previous waypoint waypoint 
likelihood hitting obstacle function direction distance nearest obstacle 
compute gradient rv term moves waypoint closer immediate neighbours second term moves waypoint away obstacle experimental results compared integrated value function policy search method existing approximate value function planner uses local collision avoidance module refine approximate value function realizable trajectory 
expected trajectory sample problem shown phases dynamic programming policy projection dp solution dimension continuous space initial phase policy search final optimized policy 
image depicts map indoor environment lab part corridor outside lab 
robot starts inside lab attempts drive corridor 
black areas obstacles walls white areas free space 
opening top map door way corridor cm wide robot width safety margin cm leaving cm clearance side 
figures demonstrate value function dynamic program solution projected dp solution policy search dp solution additional waypoints 
black areas obstacle white area free space 
show trajectory sense topologically correct policy clearly suboptimal respect real world 
reduced state space means model notion size robot consequences different orientations 
result policy free move robot arbitrarily close obstacles 
figures show results policy iteration searching initial set waypoints waypoint set size converged 
policy initial set waypoints clear improvement odd motions generated compensate impoverished policy representation 
waypoints added policy converges straight line path narrow doorway 
effect additional dimensions velocity seen middle doorway robot velocity low entering doorway reduce likelihood hitting door 
robot part way door additional waypoints change robot heading allow velocity increase kinematics robot dictate danger impact passed 
table shows quantitative comparison existing planner approximate value function algorithm planner value function seeding policy search 
table experimental results algorithm planning time execution time distance travelled prior value function planner sec cm policy search sec cm results table indicate planning time increases substantially adding policy search total time taken execute trajectory drops significantly compared value function approach 
trajectory acceptable intuitive sense controller previous planner spent significant amount time driving back forth narrow doorway aligned enter lab 
worth noting relatively easy improve planning time policy search current implementation optimized manner 
shown seeding policy search solution approximate value function generate efficiently policies high dimensional continuous space 
allows combine efficiency value iteration higher performances compact representations policy search higher dimensions 
key assumption low dimensional value function suboptimal respect reality biases policy search correctly lead arbitrarily bad controller 
demonstrated real robot navigating lab particular focussing goal requires highly constrained trajectory 
preliminary results indicate able handle real world problems appropriately able outperform existing approximate value function style planner constrained situations 
currently assume fully observable markov decision process model draw policy search pomdps better model noisy robot sensors 
certainly approximate value function methods cassandra roy thrun directly applied method 
acknowledgments authors drew useful discussions improvements execution control loop 
pineau jonathan baxter provided valuable suggestions early version 
michael montemerlo assistance invaluable developing software controlling robot hardware 
andrew jeff schneider 
autonomous helicopter control reinforcement learning policy search methods 
proceedings international conference robotics automation 
baird andrew moore 
gradient descent general reinforcement learning 
advances neural information processing systems volume 
jonathan baxter peter bartlett 
reinforcement learning pomdp direct gradient ascent 
proceedings seventeenth international conference machine learning 
buhmann burgard cremers fox hofmann schneider thrun 
mobile robot rhino 
ai magazine 
anthony cassandra leslie kaelbling james kurien 
acting uncertainty discrete bayesian models mobile robot navigation 
proceedings ieee rsj conference intelligent robotic systems iros 
scott davies andrew ng andrew moore 
applying online search techniques continuous state reinforcement learning 
proceedings fifteenth national conference artificial intelligence aaai 
andrew ng ronald parr daphne koller :10.1.1.42.6193
policy search density estimation 
advances neural international proceedings systems volume 
christos papadimitriou john tsitsiklis 
complexity markov decision processes 
mathematics operations research august 
dan pelleg andrew moore 
means extending means efficient estimation number clusters 
proceedings international conference machine learning 
puterman 
convergence policy iteration stationary dynamic programming 
mathematics operations research 
nicholas roy sebastian thrun 
coastal navigation mobile robots 
advances neural processing systems volume pages 
