learning predictive compositional hierarchies karl eger computer science department stanford university stanford ca usa kp eger cs stanford edu explores vital overlooked problem learning compositional hierarchies predictive models presents new sequential learning paradigm study models 
hierarchical compositional structure taxonomic structure critical representation tool arti cial intelligence 
prominent existing hand built systems demonstrates potential predictive models compositional hierarchies making inferences smoothly integrate bottom top uences enabling processing representations spanning multiple levels spatial temporal resolution 
additionally taxonomic hierarchies compositional hierarchies learned purely primitive data general unsupervised fashion subsequently predictions unseen data 
taxonomies numerous foundational learning algorithms exist analogous foundational learning predictive compositional hierarchies 
core aim learning models identify bottomup fashion frequently occurring repeated patterns enabling discovery larger patterns 
process holds potential scale automatically ne grained low level data coarser high level representations bridging gap proved biggest stumbling blocks way creating signi cantly complex intelligent autonomous agents 
ai systems operate representations levels raw environmental data 
called states operators features rules schemata scripts name successes failures systems depended critically human choices instantiating representational units choices algorithms designs representational languages 
order design highly intelligent autonomous agents exibility term agent course 
grand sense denoting entire computational system aptitude far surpassing arti cially created today nilsson newell vast number representational units spanning levels resolution abstraction needed 
researchers advocated undertaking great ort needed necessary choices hand guha lenat hayes davis evident machine learning play useful role hold best promise scaling raw low level data higher level concepts 
vague notion continuum low level high level representations con ates correlated measures representational level distinct types structure 
taxonomic structure relates entities relationships controls level abstraction 
compositional structure relates entities part relationships controls level granularity 
types structure chained create hierarchies 
structures ubiquitous real world existing ai systems commonly structures representations 
machine learning ml related elds study learning long history producing systems learn taxonomic hierarchies including hierarchical clustering algorithms cobweb fisher autoclass iv hanson stutz cheeseman known agglomerative hierarchical clustering techniques anderberg chapter 
researchers continue today devote deal research ort hierarchical clustering vaithyanathan dom 
unfortunately eld corresponding progress building compositional hierarchies 
dietterich michalski noted discrepancy years ago situation signi cantly changed 
fact building compositional hierarchies surfaced learning related areas case learning depended critically specialized features 
example learning consideration embedded environment interest 
focus multi agent systems refer small modules system agents 
compositional operators speedup learning shavlik dietterich mitchell keller kedar laird rosenbloom newell depends critically problem solving planning system learning embedded system domain knowledge 
specialized contexts compositional hierarchy learning useful foundational basic techniques inducing compositional hierarchies needed complement analogous taxonomic 
learning formulation allows separate domain theory problem solving context extraneous information limiting learning depend exclusively observed low level primitives patterns exhibit 
presents detailed discussion problem compositional hierarchy learning ort stimulate foundational research area 
prediction basic operation learned models concentrate prediction performance metric drive learning 
obvious discussion additional underlying motivation ability resulting high level structures variety ways transfer 
believe reason compositional hierarchy learning received attention dominant learning paradigms eld tend favor investigation issues 
example unsupervised prediction standard iid independently identically distributed data favors concentrating clustering taxonomic structure 
fundamental quite general learning paradigm unbounded data sequences compositional hierarchies particularly appropriate respects directly analogous standard unsupervised prediction paradigm iid data 
dietterich dietterich discusses number phases model research process 
rst exploratory phase 
previous discussed fall areas midst exploratory phase contrast number areas ml 
properties exploratory phase research falls outside established paradigms literature describes new ways looking problems familiar intuitive readers leading lack clarity 
ai cognitive science motivated ideas similar underlie notions hierarchy particular thrown literature 
set areas ideas umbrella elds diverse language describe hierarchies imprecise 
remarkable papers basic distinction taxonomic compositional hierarchies clear abstracts introductions typical ambiguity exploratory phase compounded somewhat area 
hand exploratory research spirit dietterich secondarily literature review attempts disambiguate characterize diverse compositional hierarchies lesser extent sequence prediction 
brie examine nature types hierarchies section 
new learning paradigm section 
section presents basic idea attacking paradigm learning predictive compositional hierarchies 
section review relevant explaining previous research appropriate overdue time pursue ideas pointing existing research falls short addressing problem suciently adequate general ways 
section includes discussions involves hierarchies compositional structure discussions non hierarchical approaches relevant learning paradigm laid section 
midst review brie note multiple promising directions research indicate directions research 
section concludes discussing transfer bene ts compositional hierarchy learning framing research problem larger research landscape 
taxonomies vs compositional hierarchies section presents brief analysis properties di erent types hierarchies 
purposes hierarchy types denote partial orderings represented directed acyclic graphs edges pointing parents lower level children 
taxonomic hierarchy taxonomy class hierarchy de ned population individuals instances 
important property taxonomy children non leaf node form set respect parent 
unordered duplicates 
non leaf node class corresponds subset individuals speci cally subset comprises descendents graph 
speci base level population possible classes xed 
individuals nodes highest node corresponds entire population 
emphasize taxonomies bounded height xed population base level entities 
compositional hierarchy henceforth ch de ned population atomic level primitives atoms 
ch children node part parent 
call non leaf nodes chunks miller 
chs di er notably taxonomies 
edges linking parent children structural relationships 
example structural relationships simple order relations issue question 
th ing th ing thinking example compositional hierarchy chunks th ing thinking 
example parse 
complex dimensional spatial relationships 
may edge linking parent child structural relationships distinguishing 
non leaf node represents structural aggregation atoms descendents graph 
duplication possible small set atoms compositional hierarchies stretch unbounded heights incorporating arbitrarily complex patterns repeated atoms 
third type hierarchy see corresponds parse particular con guration atomic tokens terms ch 
parse hierarchy typically exhibit repeated substructure corresponds directly structure ch 
hierarchy shares properties types 
instance bounded xed atomic token data links structural annotations ordering 
interested learning compositional hierarchies 
potential learning hierarchies unbounded heights primary reason nd ch learning appealing contrast taxonomy induction 
simplistic characterizations 
complex formulations composition taxonomy combinations integrating representational building blocks 
goal advance knowledge representation area pursue learning 
reasonable learning performance necessitates representationally simple versions types structures 
simplicity prudent early history compositional hierarchy learning 
unbounded unsupervised learning foundational learning problem formulations involve data bounded interdependencies iid identically independently distributed data consisting xed width records 
noted compositional hierarchies capable representing patterns unbounded size 
necessitates learning paradigm involving unbounded data sense primitive variables probabilistic view conditionally dependent arbitrarily large number variables 
unbounded sequences provide simplest example concentrate sequences mind generalization higher dimensional analogs 
note sequences need inherent directionality temporal spatial 
new foundational learning paradigm seen natural analog standard unsupervised predictive paradigm iid data 
paradigm set xed attributes predicted whichever attributes available 
standard supervised iid learning seen special case set attributes predicted just xed 
task outlined simply obvious analogous prediction task set attributes xed unbounded 
common sequential predict task special case proposed problem variable predicted symbol sequence 
predict task respects analogous supervised iid prediction 
symbol sequence serves teaching label earlier symbols 
supervised prediction iid data received research attention unsupervised prediction surprising sequential context symbol prediction received attention arbitrary prediction received 
follows detailed description proposed learning problem including data assumptions qualitative requirements 
consider data discrete position occupied symbol discrete nite alphabet 
instance sequence random variables nite arbitrary length 
values variables may missing 
assume way sequence simply subsequence potentially unbounded larger sequence 
contrast grammar assume sequence constitutes complete sentence string equivalently larger sequence subsequence drawn comes segmented independent sections see section discussion grammar language 
task unsupervised line learning arbitrary prediction performance element 
learner instances time 
instance learner adjust model environment capable making predictions 
prediction task corresponds arbitrary probabilistic inference 
new sequence ar missing values learner predict speci ed subset missing values targets values 
generalizes symbol prediction target just uses task varied including ltering noise lling missing data context detecting anomalies 
possible measures targets matched 
concentrate prediction accuracy loss frequently learner guesses correct values targets 
require learner output full probability distribution targets concentrates generative probabilistic models capable doing 
note accuracy unsupervised contexts seen evaluating model usefulness associative memory 
intuitively building compositional hierarchies frequent patterns useful producing associative memory experimental evidence supports frequent patterns particularly appropriate achieving high accuracy scores see common alternative simple predictive accuracy provided entropy measures relative entropy kullback leibler divergence cover thomas take account full distribution output learner 
iid data accuracy common supervised ml entropy metrics common unsupervised learning langley provan smyth 
entropy dominates sequence learning 
arti cial pairings tradition necessity worth spending moment question 
supervised models viewed approximating conditional probability distribution unsupervised models unconditioned distribution judged basis full distributions just modes aspects non quantitative orderings 
performance measure universally superior 
appropriate measure context dependent context provided application learning algorithm deployed directly larger computational system learning embedded 
research general techniques strive understand performance learners measures 
requirement line learning impose constraint size learned representations grow slowly amount data seen learner learner simply remember seen 
batch formulations problem equivalent drop line restriction possible information line learning sublinear space constraint practical necessity long lived autonomous learning 
assume data stationary invariant unconditioned joint distribution sequences length identical 
joint distributions smaller sequences appropriate marginals larger joints 
suppose common environment parameterized simple nite set parameters generate arbitrarily long sequences markov models learner job parameters possible 
similar view stated ron singer tishby sequences study sequences generally simple underlying statistical source 
notion attempting scale arbitrary heights complexity incompatible simple xed complexity view 
assume environment characterized nitely wide stationary joint distribution job learner approximate degree marginal distribution wide width possible 
data seen learner approximate widening marginals nitely wide joint distribution nite amount training data allow learner possibility fully characterizing environment making relevant predictor features 
combination arbitrary targets unbounded predictors may task learner dicult compared learning paradigms believe task analogous type perceptual inference imposed real world day 
important aspects world masked sensors 
loud noises frequently parts auditory signals transmissions drop signals brief periods 
important parts visual scenes occluded obstacles hidden poor lighting conditions 
missing information inferred context size relevant context unclear 
sequential time dimension important just predict happen infer happen unobserved period observed information period 
may obvious auditory signals evidence people visual systems sejnowski patel 
human perceptual system evolved deal everyday problems order understand speech listen music read interpret visual scenes autonomous intelligent agents need similar abilities 
assume uence nearby variables generally stronger uence farther away 
words strength interdependence variables tends fall increased separation 
assumption prevents seemingly hopeless learning situation context nitely complex environment 
assumption ron singer tishby implicitly explicitly similar characterization environment 
intuitively property hold aspects real world 
book chorales chorales chorales cond mutual information see cover thomas bits vs positional distance 
words chart plots usefulness sense symbols helping predict symbol function distance predicted symbol 
unconditional mutual information thomas hardy novel far crowd book calgary data compression corpus bell cleary witten reduced letter alphabet eliminating non alphabetic characters 
unconditional mi dataset bach chorales nevill manning consisting half note semitone pitch changes 
di erent chorales concatenated single large sequence string unknown symbols padded 
pitch change reduced simple symbol alphabet signifying increasing decreasing constant pitch 
fully conditioned mutual information plotted 
unconditional mi signi es usefulness symbol prediction absence information conditional mi signi es average usefulness symbol presence information intervening symbols 
known symbols available prediction need model ultimately wide utilize information may signi cant help 
ts abilities natural learners 
known ability people animals notice correlated perceptual cues fall cues separated holyoak nisbett 
empirically evidence wide variety sequence types english letters musical notes supports assumption variety dependence measures mutual information 
see examples 
note assume trend general tendency 
uence variables fall monotonically distance necessarily fall quickly insigni cant levels 
assumptions choices underlying learning paradigm represent common alternative possible choices case justi cation choice takes similar stance 
aim making choices de ne general learning paradigm possible natural unbounded analog standard unsupervised predictive paradigm 
general learning strategy general strategy incrementally building compositional hierarchies conditions outlined repeatedly combine chunk frequently occurring patterns higher level aggregates enabling combination patterns larger higher level aggregates 
learned chs lead naturally smooth integration bottom topdown processing mediate lateral inferences type described 
section explains approach particularly appropriate learning problem described 
obvious alternative approach learning problem typical xed width unsupervised density estimation algorithm applied sliding window fashion 
clear disadvantages compared methods grow width model data seen 
xed width model inherently limited information predictions 
recall shown uence distant variables necessarily fall quickly rate fall varies different data representations 
dicult know appropriate width ahead time argues strongly widening models needed 
models increase width number parameters data trade bias variance error geman bienenstock doursat reduce simply lowering variance error 
ch grows height width increases width width highest widest chunk 
restricted model space may able fully capture aspects data learning problem 
types widening models underlying hypothesis widening models chs capture important characteristics data useful 
bias variance perspective building chs frequently occurring patterns structurally introducing new parameters representations frequent patterns allows models concentrate parameters exactly data 
accuracy evaluation metrics appears exactly parameters 
extensive discussion small point place see eger experiments detailed explanations 
readers bene brief description simple experiment consider sparse unconditional gram model single xed counts probability estimates maintained possible patterns tuples cross product space possible pattern 
complete distribution purposes making predictions uniform distribution remaining probability mass assumed rest patterns 
number patterns parameters maintained held xed selection patterns varied accuracy scores maximized set patterns included frequently generated environment 
repeated chunking speci cally frequent patterns particularly apt strategy building predictive compositional hierarchies 
main issues embody ch representation allows predictive inference incrementally grow ch response new data 
section ers brief mentions approaches taken 
point take strong position questions 
possible approaches 
majority rest concentrates illustrating closely related directly address fundamental questions discussing great bene ts importance nding reasonable answers 
related section serves main purposes review characterize number research orts disparate areas tied certain overlapping motivations abilities show sum total adequately address feature set described sections 
section discusses current state research involves hierarchies compositional structure 
section reviews non hierarchical approaches learning prediction sequences 
brief summary table section 
note reviewed motivated slightly di erent concerns 
descriptions shortcoming respect goals set forth taken general shortcomings outright excellent 
merely illustrates remarkable lack progress date eld fundamental achievable goals just outlined 
state ch research prediction programmed chs blackboard systems early nii eger hayes roth notably hearsay ii provide early examples hand crafted compositional hierarchies operating manner intend 
blackboard held interpretation perceptual auditory input encoded parse hierarchy 
hand crafted computational modules called knowledge sources implicitly represented ch updated blackboard representation inferences edges underlying ch parts parts 
example interactive activation model context ects letter perception iam mcclelland rumelhart directly inspired initial landmark connectionist system remarkably successful anderson rosenfeld modeling human perceptual inference mathematical psychology 
model structurally encoded parse hierarchy implicitly ch individual letters letter words symmetric recurrent relaxation style neural network similar boltzmann machine 
basic behavior network inferences letters bottom top ow activation similar types inferences blackboard systems 
fact iam viewed degenerate blackboard system eger hayes roth 
blackboards iam hand crafted systems learning 
iam su ered lim level hierarchy 
widespread success lines decade respective elds signi cant uence remarkable serious attempts years learn structure parameters models analogous behavior 
precisely advocate see discussion graphical models section 
learning chs predict nevill manning witten introduced elegantly simple algorithm called sequitur discovering compositional structure sequences 
sequitur greedily combines repeated substrings creating ch 
speci cally time sees substring repeated second time create node hierarchy substring 
sequitur systems builds ch type data described section 
sequitur hierarchies data analysis dictionary style data compression segmentation information retrieval 
directly prediction encode statistical information structure hierarchy 
creation new chunks linked frequency underlying pattern insofar occurs 
earlier created hybrid system sequitur induced hierarchy encoded iam style network eger 
succeeded creating system automatically built predictive network desired type due iam lack parameter learning sequitur statistical insensitivity parameters structure resulting model tuned statistical properties data 
mk wol similar sequitur contains frequency information representation 
greedy parsing count resets forming new chunks call question accuracy completeness frequency information 
unclear mk representation arbitrary inference inference middle context sides 
may possible get mk sequitur system perform arbitrary inferences derive equivalent joint distributions models clear directions authors taken systems 
believe bringing tunable parameters appropriate probabilistic semantics type agglomerative algorithm promising avenue investigation see discussion grams section 
chs linguistic sequitur mk viewed grammar induction techniques 
nonterminals grammars viewed chunks chs 
context free grammars powerful types chs discussed due branching recursion possible non terminal expend multiple ways expand string includes techniques inducing probabilistic grammars build chs 
concerns prevent fact solving problem 
probabilistic grammars directly handle types predictions describe techniques deriving gram probabilities directly grammar possible see example stolcke segal stolcke :10.1.1.113.983
second unfortunately little inducing structure stochastic grammars manning schutze 
exist stolcke omohundro langley utilizes batch learning line structure learning narrow restricted sense 
techniques depend critically input data consisting short independent strings allows top techniques bottom agglomeration 
bottom chunking operator depend inherently top behavior incorporate training strings verbatim expansions starting non terminal depending input data separate strings 
choice chunk function global metric bayesian posterior stolcke omohundro mdl score langley evaluated training data 
completely batch algorithm 
algorithm stolcke omohundro performs line structure learning clearly stolcke line updates accomplished virtue restricted nature structure change operators combined top technique incorporating new training instances :10.1.1.113.983
data incorporation builds new structure model structure change operators merge operators case structure search collapse structure insure sucient statistics approximating likelihood function maintained structure changes 
due restricted set structure change operators models jump bad parts structure space get back bad structure change happens look moment 
richer set structure change operators adding operator reverses merge discussed stolcke chapter render line learning impossible structure search framework employed :10.1.1.113.983
line structure learning depends critically top incorporation nature data 
major di erence grammar induction techniques ch learning discussed grammar induction techniques generalized easily handle higher dimensional data core ch learning ideas ch learning systems generalize straightforward way 
chs interpret data parse hierarchies 
parse hierarchies conversely viewed hierarchical segmentations 
noted aspects grammar induction depend having input sequences marked sentence segmentation boundaries information required type ch learning proposed available 
brent studied technique learning segmentations takes segmentation boundaries input data extreme utilizing information brent cartwright 
discusses context children learning word boundaries words uent speech utterances 
utterances word boundaries directly observable boundaries individual utterances available due instance long pauses 
great demonstration far get utterance sentence boundaries ignoring statistical information patterns symbols sequences 
issue raise viewed complement question 
far get statistical information boundaries 
important understanding di erent sources potential power boundaries observable dealing situations boundaries available 
fact developmental word learning community brent falls achieved success simple statistical analyses boundary markers 
example sa ran newport shows infants learn segment uent speech dips conditional probabilities symbol immediately preceding symbol symbols speech syllables case due fact word transitions tend higher probabilities transitions 
idea low points transitional probabilities segmentation similar high transitional probabilities chunking 
high transitional probabilities correlate high joint probabilities frequent patterns idea segmentation similar idea proposed frequent patterns chunking 
deals segmentation level words sequence syllables extension multilevel hierarchies ignoring issues common pre xes suxes roots compound words short phrases brent orts concentrate single level segmentation brent 
mechanism described brent cartwright implicit hierarchy elements lexicon include multi word phrases compositional relationships represented directly learning mechanism segment nely words words boundary utterance boundary 
statistical natural language processing detecting collocations common word pairs manning schutze shares idea nding frequent patterns adding representation 
done exclusively batch fashion done hierarchically 
important note uses letters basic symbols gures section letters linguistic units examples types data mean address 
primarily interested linguistic data problems general mechanisms may applicable types data share properties 
letter sequences convenient domain discussion readers conscious familiarity common letter sequences 
letters come historically ancestry iam mcclelland rumelhart 
sa ran 
sa ran newport note potential general mechanisms say remains unclear statistical learning observed indicative mechanism speci language acquisition general learning mechanism applicable broad range distributional analyses environmental input 
researchers share view 
example redlich redlich see uses segmentation letters words experimental domain motivated study vision wol wol searching general mechanisms intelligence 
course real sequences written letters contain space characters punctuation provide strong extra clues syntax aspects data 
extent general hierarchical chunking mechanisms capture useful information letter sequences information uncovered primarily morphological level level grammar 
types data uent speech discussed music sequences observable events environment don contain specialized segmentation cues cues identi ed specialized techniques required 
important understand general mechanisms function di erent areas far get raw unsegmented symbol data 
composition parameterized models researchers begun investigate xed parameter unsupervised learning systems highlevel internal representations uence lower levels top feedback order resolve lowlevel ambiguity high level representations develop automatically parameter learning mechanisms compositional relationships lower level representations 
current systems characteristics lewicki sejnowski typically generative probability models embodied neural network graphical models 
perform recognition similarly iam black feedforward systems develop layered representations embody compositionality satisfy rst property due lack feedback 
boards sound probabilistic semantics capable parameter learning 
develop layered representations including compositional relationships entities di erent layers 
compositional hierarchy learning described structure learning 
xed parameter models operating xed sized inputs scale larger larger compositional patterns reuse previously learned patterns doing trade bias variance error data seen described section 
action operator paradigms creating hierarchies involve compositionality occurs number specialized areas involve acting representing environment 
areas uses hierarchies hand built constructed deterministically non data dependent methods automated domain analysis 
minority involves learning hierarchies partially compositional learning depends critically specialized aspects domain larger problem context learning embedded 
learning techniques employed applicable general contexts pure prediction 
reinforcement learning rl example area hierarchies frequently 
currently inclusive publication summarizing hierarchy rl sun sessions summarizes prominent systems hierarchical rl point systems hand constructed hierarchies :10.1.1.11.226:10.1.1.11.226
provide great simpli cation eld summarizes relevant case prominent systems 
general build hierarchies states actions independently build hierarchies complex units combining aspects state action reward 
units termed skills behaviors similar typically referred temporal abstraction 
example macro operators category consist conditional action sequences intermixed sensing construction learning depends reward structure environment 
approaches category automatically depend interplay states actions reward applicable simpler unbounded data 
little area involves learning hierarchies data interaction environment 
examples include mcgovern andre early stages :10.1.1.52.5775
reported sun sessions farther determines hierarchy learning rl individual modules hierarchies quite exible :10.1.1.11.226:10.1.1.11.226
number levels hierarchies number modules level xed ahead time system fully address structural learning problem 
common hierarchies built exclusively states operators independently hierarchical decompositions state space commonly referred state aggregation 
technically parse hierarchies degenerate sort correspond interpretations chs reused substructure 
words repeated patterns di erent buildings identical occupying di erent regions state space 
partitioning typically driven solely computational issues example moore atkeson 
relevant reinforcement learning ring ring cited hierarchical reinforcement learning literature 
discussed section schemes ring easily separated paradigm rewards action selection viewed pure sequence learning algorithms second system way ring 
straddles multiple research areas treated separately note motivations qualitative abilities share learning hierarchies rl 
hierarchies macro operators show closely related elds primarily concerned learning including search planning 
elds reinforcement learning hierarchy generation done people done deterministically static analyses data statistical properties experiences 
experience speedup learning shavlik dietterich fikes hart nilsson laird rosenbloom newell mitchell keller kedar 
focus minimizing needed experience usually single instance utilizing extra information domain theories knowledge associated machinery problem solvers complex cognitive architectures soar laird rosenbloom newell 
drescher drescher studied tabula rasa learning paradigm involving actions distinct reinforcement learning primary learning goal model ects actions optimize reward 
learning mechanisms able create new actions new units representing state environment involved compositional structure 
systems paradigms discussed section mechanisms creating hierarchical compositional structure depended interplay actions environment appropriate general passive predictive task 
note attempting argue unfortunate indicative need ch learning research term chunking seldom ai outside context soar 
hierarchy creation action paradigms worthy study 
important concepts discovered examining interplay actions environment clearly begun 
point concepts discovered interplay requires research 
ai investigates paradigms involving actions operators areas ai majority machine learning related elds concentrate fundamental problems involving pure modeling 
includes discussed outside section 
salient subclass pure modeling category involves perception 
amazing thing structural learning hierarchies predictive models concentrated exclusively taxonomic relationships ignoring compositionality 
sequence prediction sequence learning involve explicit compositional structure dealt section 
ring ring provides rare example sequence prediction models build compositional structures hierarchically 
viewed reinforcement learning area separated issues reinforcement action selection treated purely method sequence learning 
ring connectionist systems called behavior hierarchies temporal transition hierarchies 
designed forward prediction behavior hierarchies explicit compositional structure 
despite limitation forward prediction behavior networks strong connections general strategy outlined section 
networks learn chs actions chunks macro operators 
network fully connected single layer feedforward autoassociative neural network 
number input output units initially primitive action 
actions primitive boolean sensing operations 
timestep action just executed active unit input layer sensing operation unit active sensing detected property currently true environment 
network attempts predict action action selected weighted sampling network outputs 
weights trained simple delta rule modi ed slightly take rewards environment account 
compositionality enters large weights trigger creation new nodes added note distinction perception action pure modeling complex computation modeling action 
predictive modeling lines problem section instance function motor skills actions absence higher level computations planning 
input output layers corresponding macro action previous actions connected weight 
selecting macro action deterministically execute primitive actions sequence 
assumes uniform zero reward structure actions simply symbols observed predicted clear reinforcement learning aspects system removed system capable growing ch bottom frequently occurring sequences symbols 
basically system operates rstorder markov model dynamically adds new states outputs 
weight triggering new node creation similar independently conceived chunking mechanism new node creation boltzmann machines symmetric recurrent networks hebb weight modi cation rule eger 
prediction abilities ring networks forward direction extremely limited notion network unrolling weight sharing temporally extended models discussed section eger 
extension hidden markov models hmms called hierarchical hidden markov models hhmms fine singer tishby type model capable prediction sequences explicit hierarchical structure 
hhmm structure representationally richer simple compositional hierarchies include compositionality 
state hhmm hhmm markov model view ring capable output 
compositionality structure reuse lower level structures hhmm tied parameters appear multiple places higher level hhmms 
generative probabilistic models models reasonable predictive abilities ring behavior networks area concentrated exclusively inference parameter learning 
currently reported learning structure models 
dictionary compression redundancy reduction eld universal lossless data compression dictionary opposed model compression bell cleary witten includes number systems similar sequitur mk discussed 
famous family algorithms bell cleary witten nevill manning popular compression utilities including gzip unix compress 
ziv lempel proposed di erent schemes lz builds compositional chunks hierarchically 
sequitur mk lz schemes statistical parameters tune predictions 
chunking extremely greedy sensitivity chunk frequency 
sequitur mk lz build new chunks combining arbitrary existing chunks adding single primitive symbol right side existing chunk 
lz schemes need remember entire input said line learning 
worth noting similarity data compression schemes ideas proposed motivated needs autonomous intelligent agents coincidence 
miller noted early usefulness chunks increasing short term memory capacity 
viewed compressing contents short term memory ways similar operation dictionary compression schemes 
computer agents memory architecture humans computers share need di erent sets memories faster accessible memories necessarily having smaller capacities 
chunking seen way process increasingly expansive concepts xed computational resources 
point discussed section 
di erent research context redlich redlich investigated called redundancy reduction means unsupervised feature detection 
essentially redlich algorithm method doing batch single level segmentation discrete input data doing global optimization nice local computability properties 
presents linguistic examples letters grouped words included redlich motivated language problems vision 
sees techniques applied stages stage performing redundancy reduction essentially compression help deal enormous volume information impinging visual sensors maximizing amount information owing retina stages processing 
miscellaneous research areas deal compositional structure gotten learning structure predictive chs 
areas include connectionist symbol processing hierarchical graphical models constructive feature induction model object recognition 
don deal structure learning problem appropriate examine areas detail 
areas complementary cases shares motivations long term goals 
section brie mentions peripherally related compositional structure areas particularly come mind response ideas 
bienenstock geman potter potter bienenstock geman potter investigated visual scene analysis essentially parsing compositional grammars inherently hierarchical 
representationally ambitious couple ways learning structure parameters data 
detailed latest potter presents representation language probabilistic extensions suitable de ning prior compositional grammars sets compositional rules chunks chunk templates algorithms computing certain useful results distributions 
provides likelihood function parameter optimization learning mechanisms states speci cally learning structure composition rules line 
uses similar language title learning compositional hierarchies inducing structure objects data 
surface similarity run deep learning paradigm extremely di erent 
algorithm learns compositional hierarchy single type object data representing example instances object 
hierarchies learned contain reuse substructure 
algorithm despite language describe closer spirit mixture model clustering algorithms 
implied resulting models model object recognition type object recognized bit di erent hierarchical clustering examples experiments applying techniques way described 
state sequence learning models research far concentrated discussing related compositional hierarchies 
turn relevant addressing learning task de ned section compositional hierarchies 
involves learning prediction context sequences involves models grow additional parameters learning 
explosion clustering research ml dietterich michalski sparc system dietterich michalski observation research concentrated taxonomic learning compositional learning 
referred instance class induction part induction respectively 
refers supervised learning 
clustering third category consider taxonomic unsupervised 
noted little done 
despite observation representation contain explicit compositional hierarchical structure 
sequence prediction take narrow approach 
paradigm section fact sequence factored representation elements attaching set attributes element 
main di erence issues discussion sparc assumes simple model determines set allowable elements sequence rules apply entire sequence things determine sequence human generated rules game 
noise assumed 
dietterich michalski point reminiscent psychological prediction dietterich michalski 
things restrictive simple rule deterministically generating rest sequence 
learning lines results forming hypotheses sequence generation rules di ers substantially building statistical models occurs sequence prediction described rest section 
model lossless data compression bell cleary witten represented algorithms ppm family deals similar unbounded discrete data model concerns learning forward model predicting symbol 
laird saul tried spark interest task ml community presenting algorithm similar ppm algorithms 
models essentially forward predicting grams markov models variable context sizes 
ppm cleary teahan capable unbounded width contexts trained line accomplishes remembering training data 
model essentially elaborate data structure organizes data seen ecient forward prediction 
minor di erence ideas data compression concerns exclusively entropy performance metrics translate directly compression performance 
similar variable width gram occurs area statistical natural language processing nlp manning schutze 
contrast data compression statistical nlp uses batch style learning 
result literature ers mechanisms growing width models data seen 
underlying goals eld force strict adherence predicting symbol entropy performance measures norm evaluating grams 
existing area compositional relationships patterns tuples di erent widths 
believe promising avenue research involves variable width grams encourage hierarchical structure relationships gram patterns di erent widths 
implies models represent possible patterns width 
intimate connection compositionality sparseness representation data redundancy noted bienenstock geman potter 
investigated sparse gram models compositional relationships help decide patterns represent model eger 
space ecient representations scheme similar mk sequitur representations see means adding tunable parameters mk sequitur style algorithms 
variable width sparse models called prediction suf trees psts studied statistical nlp schutze singer ron singer tishby pereira singer tishby compression communities willems shtarkov 
shares characteristics sparse gram ideas mentioned see eger detailed comparison 
simple view markov models variable context lengths encoding gram counts trees deeper nodes represent longer contexts 
similar ppm see interleaved sequences publications 
major di erence current pst algorithms utilize repeated substructure extend tree symbol sequences exist high counts tree 
bias compositional patterns focus critical line learning frequency arbitrary potential new patterns conveniently queried large training dataset 
compositional ideas applied psts hope saw undirected representations natural initial investigation 
psts example generalize easily dimensional data models 
big problem faced sparse models psts width increases number patterns included model grows exponentially 
pst algorithms deals problem choose patterns include batch training ron singer tishby schutze singer 
tree mixtures willems shtarkov pereira singer tishby uses incremental updates described line algorithms perform line structure learning pattern selection sense described 
build psts ways 
depth bound add nodes su ciently small strings occur training bound add nodes strings occur 
case model simply reorganizes remembers training data ppm impractical contexts 
case model grow recognize patterns data wider prespeci ed bound may require memory exponential bound 
data model able represent arbitrarily large patterns provided demonstrated strongly data require space data seen space sub exponential largest pattern represented 
tree mixture focused time ecient methods weighting mixtures best predictions 
deal space eciency challenge just line methods adding new representational units aid predictions needed similar mentioned direction pereira singer tishby 
compositional aggregation proposed solution 
grams equivalently plain markov models studied hidden markov models hmms mediate unbounded interactions nite parameters 
graphical models perspective models xed structure approximate widening joint distributions described section 
true temporal dynamic graphical models dean kanazawa saul jordan bring new parameters bear periodic creation new hidden variables 
commonly studied models area utilize long term learning scale low level high level representations automatically 
see area exploration 
models successfully demonstrated hierarchical chunking characteristics described sections form temporally extended graphical model 
explicitly study online creation new hidden variables structures mimic chs iam rst concentrating optimizing useful low level structural representations factored state structures 
specifically network undirected graphical model boltzmann machines hinton sejnowski 
symmetric recurrent neural network similar operation iam viewed direct descendent iam addressing lack learning system 
chunking mechanism correlations indicated large hebbian weights similar spirit mechanism ring see section 
system chunks hierarchically improves predictive performance time low level prediction recognition high level chunks 
system brie described eger detailed preparation 
summary related table summarizes compares systems classes just discussed terms characteristics described section 
system summarized qualities representation operation learning 
table characterizes system representation terms exhibits compositionality chunks segments representation hierarchical multilevel single level 
operation columns describe prediction statistical inference abilities 
unbounded prediction refers ability training data predict symbol arbitrarily large number predictor variables 
arbitrary prediction refers ability predict arbitrary target variable predictor variables arbitrary relative positions 
learning parameter learning signi es tuning parameters model operation 
chunking structure learning means non parametric method discovering chunks segments hierarchical 
structural chunking performed statistical chunking refers chunk creation sensitive statistical properties chunked patterns frequency 
qualify line learning column system line learning model ectively reproduce entire input 
bottom data driven requires information low level data drive learning 
learning unsegmented data implies data string embedded boundary markers 
table review show diverse collection done overlapping issues philosophies prior system aware addresses full generality problem unsupervised learning predictive compositional hierarchies 
broader view update section need talk amount data stats ml concerned optimizing small datasets simple underlying true distributions data sparsity biggest problem bayesian prediction really contexts problem isn little data fact way data think long lived autonomous agent real environment decent sensors robot raw sensor data overwhelming processed remembered batch processes certainly people animals information sensor overload problem underlying assumption data far complexity environment problem build useful representations allow deal data overload scale qualitatively di erent levels dealing information requirements environment note amount data interacts line vs batch question lots data goes line sparse models go lots data line see gram nearly eld arti cial intelligence researchers idea computers learn learned learn 
andre despite age pervasiveness idea thought staged layered hierarchical learning majority learning research ml related elds way :10.1.1.52.5775
important distinction hierarchical learning 
flat learning amounts optimization search 
line hierarchical structure learning considered search characterization apt 
view capture quality long term evolution signi cant representational change introduce qualitatively di erent abilities terms operation table summary relevant existing systems terms characteristics discussed 
see section exact meanings column 
unclear signi cant cells remain blank 
note number cells row taken measure relevance discussed assessment quality signi cance 
authors di erent biases concentrated issues re ected table 
system class systems representation operation learning chunking name authors section discussed compositionality chunks segments hierarchical prediction unbounded prediction arbitrary prediction parameter learning structure learning statistical chunking line learning bottom data driven unsegmented data hearsay ii learning iam learning sequitur prediction mk prediction stolcke omohundro di erent kind langley di erent kind brent cartwright prediction sa ran chunking collocations prediction lewicki sejnowski chunking mcgovern andre sun sessions chunking soar prediction drescher ring behavior nets hhmms chunking lz lzw lzc prediction redlich ppm chunks chunking stat nlp grams chunks chunking psa pst chunks chunking hmms chunks chunking feature level line segment units described original considered primary parts system receive feedback served fancy way set environmental activation letter level units 
small sensitivity threshold xed parameter generally greater 
line richer set structure change operators ability reverse bad merge 
separated rl paradigm studied way 
reuse lower level structure 
lzc variant lzw lz family bell cleary witten unix compress utility 
prediction xed width accomplished model class sucient xed model xed threshold predict relevant predictor variables 
typically trained way statistical nlp community easily 
perform arbitrary predictions clear equally predictions 
learning algorithm contain strong forward directed inductive bias 
prediction terms learning potential 
terms search view search space vastly larger typical learning majority space inaccessible time 
small portion space quickly accessible slowly changes data seen new structures built 
reasons learning overwhelmingly dominated eld 
flat learning systems easier study experimentally analyze theoretically 
easier deploy engineered applications applications having human loop problematic structural decisions conveniently cost ectively handled human 
hierarchical learning important scienti engineering perspectives 
people animals learn hierarchically 
reviewed motivated attempting directly address fact 
example drescher drescher meant model piaget stages human development 
practical perspective hierarchical learning crucial automatic development systems large quantities high level knowledge tuned properties environment 
type knowledge necessary autonomous agents achieve extremely broad competencies 
engineering knowledge hand systems complex strategy harder harder level complexity ability increases infeasible long reaching agents near human level abilities 
furthermore hierarchical learning best way achieve environmentally sensitive knowledge environments question dicult human knowledge engineers interact directly dangerous distant virtual symbol grounding growing body associated phrase symbol grounding attempts address problem representations derive meanings environment way low level data 
thrust correspondence available raw data usually continuous newly created symbolic data higher level representations 
higher level representational units said grounded result meanings clearer directly useful environment 
example oates schmill cohen demonstrates clustering multivariate time series data derived mobile robotic sensors range 
resulting clusters de nite implications state robot changing 
demonstrating similar continuous discrete clustering singer tishby provides inverse mapping continuous pen trajectories strings discrete set symbols eciently encode cursive handwriting fine singer tishby 
examples area include coradeschi siskind orts involve richer representations presents di culty learning 
see section discussion richer representations 
agree sun high level representational units grounded environment 
includes implications representation environmental interaction important 
agree bottom learning process necessary create high level units 
noted section agree decisions actions necessary type environmental interaction creation forms high level concepts 
important question ground representational units level lowest level grounding representational units levels learned 
tying representations levels directly raw environmental data easier correspondence high level units tying low level representations turn tied raw environmental data 
ability learn hierarchical representations bottom fashion low level symbolic representations including speci cally ability learn hierarchies compositional structure critical 
discrete symbols nite alphabet seen entirely reasonable format input data autonomous intelligent agents oates schmill cohen singer tishby provide explanations symbols arise 
time type bottom hierarchical learning described provides plausible means representations increasingly expansive signal patterns grounded 
processes need happen isolation 
sophisticated combinations ideas allow interesting interplay mechanisms 
note issues span diverse set domains 
example provided word learning domain infants generate discrete symbolic representations syllables raw acoustical data believe models brent cartwright newport build higher level structures symbols form grounded representations words 
additional importance chs transfer semi supervised learning prediction valuable practical uses helpful force driving unsupervised learning process creating compositional hierarchies 
resulting chunks may utilized ways larger system form transfer areas memory communication action selection additional learning including called semi supervised learning 
strong correlation chunks useful prediction bene cial trans fer 
section reviews areas learned chs chunks play useful roles 
memory learned chunks act aids increase memory capacity 
simple substitution recoding stand parts data compression schemes touched section 
memory capacity system increased long term memory store chunks ectively increase capacity working short term memory allowing store chunks numerous primitive symbols 
process recognized studied academic settings decades ago 
classic discussing relatively xed capacity immediate human memory miller stated recognize importance grouping organizing input sequence units chunks 
memory span xed number chunks increase number bits information contains simply building larger larger chunks chunk containing information 
went describe experiments small sequences binary digits memorized purpose aiding memory large sequences digits 
experiments described di erent comprehensive sets chunks speci sizes memorized set covering possibilities length triples binary digits 
reasonable arti cial task digits remembered probably uniformly distributed 
clear non uniform data sets chunks di erent lengths biased frequently occurring patterns ectively see discussion chess 
important recognize increasing working memory capacity general importance simply able remember longer lists miller recognized little dramatic watch person get binary digits row repeat back error 
think merely mnemonic trick extending memory span important point implicit nearly mnemonic devices 
point recoding extremely powerful weapon increasing amount information deal 
form recoding constantly daily behavior 
time stress ability trade memory capacity longer term shorter term memories useful people 
computer systems typically numerous storage subsystems varying capacities accessibility parameters speed silberschatz 
inherent tradeo accessibility capacity smaller capacity means memory new items push older items quickly older items constantly refreshed 
ability manipulate coarse representations smaller memories vital ecient computation intelligent thought 
important note ect limited sequential dimensional data 
known chase simon speaks point number points just mentioned 
chase simon showed chess masters long term memories chunks representing con gurations playing pieces knowledge remember board con gurations drawn actual chess games better beginners shown board seconds 
experience chess better able memory task 
experience chess unhelpful remembering random board con gurations 
patterns pieces similar occur actual play familiar chunks picked 
consistent point chunks frequently occurring patterns prove useful uniformly selected chunks 
timing results multiple trials done chase simon directly support experts chunks shortterm memory 
researchers segmented recalled pieces observed chunks thresholding amount time recall successive pieces large pauses mean chunk boundaries pieces chunk tended recalled trials order pieces chunks arbitrary 
result suggests chunk stored memory single compound representation 
glass holyoak showed experts experience larger chunks consistent bottom discovery chunks time 
similar experiments repeated complex player board game called go glass holyoak 
context chess type recoding binary digit experiments miller expect chess experts show improvement beginners random boards chunks useful substitutes 
happen probably due signi cantly larger space possible patterns chess domain context small sequences binary digits 
chess masters estimated chunks glass holyoak number insigni cant compared number possible patterns 
masters slightly worse novices random boards probably due automatic chess related processing masters help devote cognitive resources 
consistent fact masters slower respond stimuli clicks performing task glass holyoak 
go point time simon ability perceive directly represent chunks units working memory collections components ects simple memory tasks 
simon suggested example chunks conditions production rules lead faster processing glass holyoak 
chess go exhibits expert simply thinking right move 
words perception familiar perceptual chunk leads directly appropriate action 
glass holyoak miller summed view memory experiments discussed am convinced process general important 
miller reported quarter half century ago simon miller researchers eld cognitive science amazing today dozens alternative computational mechanisms addressing aspect process especially chunks learned rst place 
associative thought researchers appear neglect importance association associative memory intelligence researchers realize pervasiveness association wide variety intelligent thought 
associative memories particularly prominent models speci areas cognitive modeling diverse set suggests memories may underlie great variety human thinking intelligence eger 
researchers proposed computational association mechanisms basis architectures thought generally 
include primarily psychological models come ai community 
ideas span great deal time classic rumelhart 
general experiments claim possible systematically associative structures perform reasoning forming chains associations 
orts highlight importance associative inference 
leaves open question needed associations initially formed 
learning algorithms viewed learning associations uniformly associations learned prede ned representations severely limits associations discovered 
representations ne grained noticing association complex patterns dicult 
hierarchical chunking mechanisms outlined advocated directly provide associations wholes parts kinds associations necessary intelligent thought 
hierarchical chunking provide means identify prevalent high level concepts candidates associations kinds 
greatly reduces complexity discovering high level associations 
summary transfer summary hierarchical chunking similar discussed may keys automatically developing rules schemata rumelhart scripts serve important building blocks knowledge may critical enabler forming associations play pervasive role intelligent thought 
potential areas transfer nearly boundless 
richer representations relevant dating back years compositional hierarchy learning infancy 
real potential techniques tapped eld learns integrate induction compositional taxonomic structure 
believe important give isolated treatment compositional hierarchies rst better understand issues involved added complexity taxonomy order complement extensive research taxonomic induction bene cially isolated compositional learning 
discussed section single level taxonomic learning useful underneath bottom level ch order generate reasonable set atomic level symbols arbitrary interleaving taxonomic compositional structure valuable 
ability learn classes wholes parts turn classes wholes ability learn simpli ed semantic networks arbitrary relations extremely powerful 
worth putting idea perspective 
number representational formalisms powerful compositional taxonomic combination hierarchies learning formalisms 
obvious examples involving learning provided inductive logic programming ilp genetic programming gp communities 
ilp capable learning hierarchical relationships rst order logic including automatic creation new reusable predicates 
gp capable learning computer programs eld concentrates automatic creation reusable subroutines 
see mitchell introductions appropriate level ilp gp pointers see angeline kinnear number papers reviews major threads learning reusable subroutines gp 
order logic general computing machines equivalent representational power fully cable representing chs combination compositional taxonomic hierarchies fact recursive structure 
existing ilp gp render compositional hierarchy learning obsolete just subsume hierarchical clustering representationally weaker learning sub elds 
ilp gp valuable research areas su er number diculties 
existing techniques elds capable learning representations achievable learning notably recursive structure techniques elds practice usually fall short achieving full representational generality rst order logic turing equivalence inspires elds 
second generality comes price far dif cult learning 
elds impressive successes great diculty overcoming huge search spaces representations 
phenomenon pervades learning 
context primarily simpler ch learning systems nevill manning experimented recursive structure sequitur nevill manning techniques developed perform nearly simpler ch building 
techniques branching taxonomic structure investigated diculties 
help overcome problems learning ilp gp goal directed 
ilp target concept learned learner positive negative exemplars 
addition learner aided extra background knowledge 
gp learner tness function 
furthermore current gp automatic subroutine generation falls classes new subroutines created randomly selected existing program additional tness metrics provided evaluate usefulness partial program pieces 
ilp gp techniques capable learning structures contain hierarchical compositionality learning problems address fundamentally di erent unbounded unsupervised learning discussed 
learners require extra goal directed information ipl gp techniques appropriate identifying compositional structure higher level aggregate concepts implicit unlabeled low level data 
depicts various representations discussed organized representational power 
point signi cant needed hierarchical clustering taxonomy node ilp gp 
intelligence learned hierarchical representation elephants don learn domain theories 
play chess elephants representations 
research philosophy underlying surprisingly shares ideas espoused brooks late subsumption example brooks :10.1.1.12.1680
obvious differences overlap spans related points 
salient brooks advocated layered hierarchical systems scaling higher levels complexity bottom fashion nature nevill manning personal communication 
computer programs gp order logic ilp taxonomies hierarchical clustering compositional hierarchies combination compositional taxonomic hierarchies hierarchy hierarchy number learnable hierarchical representations hierarchy representational power 
taxonomies positioned slightly lower compositional hierarchies grow bound see section 
higher formalisms challenging learning problems 
chs simplest hierarchical representation scale arbitrary heights 
layered systems di erent mechanism new layers added previous layers engineering vs learning 
brooks writing clearly reveals goal human level intelligence 
described section expanded goal include agents signi cantly general abilities far achieved fall slightly short human level intelligence 
large gap closed 
addition shared goal picture brooks paints underlying landscape ai respect goal nearly identical motivations introductory section 
early ai researchers believed high level thinking reasoning cognition sort computation necessary play chess represented bulk ai problem 
majority early eld involved creating high level systems interfaces hard crafted high level representational units 
gradually decades slow realization ai greater cognitive science communities perceptual motor systems closer ties external environment lower level signi cant portion puzzle initially appreciated 
brooks merely vocal advocate extremes position timescales evolution back argument 
stated process creating high level representations raw perceptual data essence intelligence hard part problems solved 
brooks motivations views similar underlie interest symbol grounding discussed :10.1.1.12.1680
complained input ai programs restricted set simple assertions deduced real data humans 
brooks having correctly identi ed problem brooks took unfortunate approach representation altogether :10.1.1.12.1680
problem existence representations lack computational understanding increasingly high level representations develop low level data available interaction environment 
extreme view brooks position characterize pointing bottom knowledge poor processes necessary just knowledge intensive representations top computations enable simple approach vein go farther researchers ready realize extreme claims 
brooks raise eyebrows extremely sided view reality believe knowledge intensive processes top learning part important picture 
additionally independent practical applications drives modern research ort 
areas receiving adequate attention 
share brooks view simple important mechanism neglected potential people realize 
important see just far go 
lastly brooks driven problem application distinction important 
brooks passes bias 
particular interest applications clear goals met range applications resulting systems limited imagination 
brooks share sentiment long term result countless applications shy away dedication speci application way fear danger investigating extremely general mechanisms context single speci application :10.1.1.12.1680
performance speci application case usefully optimized non transferable specialized techniques biases hard decouple rest system usually detract improvement understanding general mechanisms 
despite similarities deep rooted philosophical di erences research advocated brooks subsumption style 
noted believe learning engineering key scaling layered systems required complexity 
representation critical key representational bias missing learning hierarchical compositional structure 
believe mobility acute vision ability carry survival related tasks dynamic environment provide necessary basis development true intelligence 
brooks things achieved brooks subsequent decade passed indication easier bits falling place naturally core pieces :10.1.1.12.1680
learning hierarchically environmental data vital largely unexplored ingredient 
introduced goal learning predictive compositional hierarchies independently motivated particularly relevant new learning paradigm 
advocate see continuation early predictions hand crafted compositional hierarchies needed complement foundational learning taxonomic structure 
compositional aggregation keys bridging gap low level ne grained representations high level concepts critical long lived autonomous intelligent systems 
acknowledgments barbara hayes roth nils nilsson richard fikes pat langley patrick doyle craig dan shapiro helpful feedback discussions 
anderberg 
cluster analysis applications 
academic press 
anderson rosenfeld eds 

neurocomputing foundations research 
mit press 
andre 
learning hierarchical behaviors 
nips workshop abstraction hierarchy reinforcement learning 
angeline kinnear jr 
advances genetic programming 
mit press 
bell cleary witten 
text compression 
prentice hall 
bienenstock geman potter 
compositionality mdl priors object recognition 
advances neural information processing systems 
brent cartwright 
distributional regularity phonotactic constraints useful segmentation 
cognition 
brent 
ecient probabilistically sound algorithm segmentation word discovery 
machine learning 
brooks 
intelligence representation 
arti cial intelligence 
chase simon 
perception chess 
cognitive psychology 
cleary teahan 
unbounded length contexts ppm 
computer journal 
coradeschi 
anchoring symbols sensor data preliminary report 
proceedings seventeenth national conference arti cial intelligence 
cover thomas 
elements information theory 
wiley 
davis 
naive physics 
ai magazine 
dean kanazawa 
model reasoning persistence causation 
computational intelligence 
dietterich michalski 
learning predict sequences 
machine learning arti cial intelligence approach volume 
morgan kaufmann 
chapter 
dietterich 
exploratory research machine learning 
machine learning 
drescher 
schema mechanism 
hanson rivest eds machine learning theory applications 
springer 

sejnowski 
motion integration visual awareness 
science 
fikes hart nilsson 
learning executing generalized robot plans 
arti cial intelligence 
fine singer tishby 
hierarchical hidden markov model analysis applications 
machine learning 
fisher 
knowledge acquisition incremental conceptual clustering 
machine learning 
reprinted readings machine learning shavlik dietterich 
geman bienenstock doursat 
neural networks bias variance dilemma 
neural computation 
glass holyoak 
cognition 
random house second edition 
guha lenat 
cyc midterm report 
ai magazine 
hanson stutz cheeseman 
bayesian classi cation correlation inheritance 
th international joint conference arti cial intelligence 
morgan kaufmann 
hayes roth 
building integrated cognitive agents review allen newell uni ed theories cognition 
arti cial intelligence 
hayes 
second naive physics manifesto 
readings knowledge representation 
morgan kaufmann 

hinton sejnowski 
learning relearning boltzmann machines 
parallel distributed processing explorations cognition volume foundations 
mit press 
chapter 
holyoak nisbett 
induction 
psychology human thought 
chapter 
whitney sejnowski 
position moving objects 
science 
letters reply 
laird saul 
discrete sequence prediction applications 
machine learning 
laird rosenbloom newell 
chunking soar anatomy general learning mechanism 
machine learning 
langley 
learning contextfree grammars simplicity bias 
proceedings european conference machine learning 
langley provan smyth 
learning probabilistic representations 
machine learning 
editors special issue 
lewicki sejnowski 
bayesian unsupervised learning higher order structure 
advances neural information processing systems volume 
manning schutze 
foundations statistical natural language processing 
mit press 
mcclelland rumelhart 
interactive activation model context ects letter perception part 
account basic ndings 
psychological review 
mcgovern 
acquire macros algorithm automatically learning macro actions 
nips workshop abstraction hierarchy reinforcement learning 
miller 
magic number plus minus limits capacity processing information 
psychological review 
mitchell keller kedar 
explanation generalization unifying view 
machine learning 
reprinted shavlik dietterich 
mitchell 
machine learning 
mcgraw hill 
moore atkeson 
parti game algorithm variable resolution reinforcement learning multidimensional state spaces 
machine learning 
nevill manning witten 
identifying hierarchical structure sequences linear time algorithm 
journal arti cial intelligence research 
nevill manning 
inferring sequential structure 
ph dissertation university waikato new zealand 
see sequence rutgers edu nevill 
newell 
uni ed theories cognition 
cambridge mass harvard university press 
nii 
blackboard systems blackboard model problem solving evolution blackboard architectures 
ai magazine 
nilsson 
eye prize 
ai magazine 
oates schmill cohen 
method clustering experiences mobile robot accords human judgements 
proceedings seventeenth national conference arti cial intelligence 
patel sejnowski 
flash lag ect di erential latency 
science 
letter reply 
pereira singer tishby 
word grams 
proceedings third workshop large corpora 
eger hayes roth 
blackboard style systems organization 
technical report ksl stanford cs 
see www cs students stanford edu kp eger publications 
eger 
associative networks reasoning problem solving possibly memory 
unpublished 
copies available request 
eger 
learning compositional hierarchies modeling context ects 
technical report ksl stanford cs 
see www cs students stanford edu kp eger publications 
eger 
learning compositional hierarchies data driven chunking 
proceedings sixteenth national conference arti cial intelligence 

eger 
line learning undirected sparse grams 
technical report ksl stanford cs 
see www cs students stanford edu kp eger publications 
potter 
compositional pattern recognition 
ph dissertation brown university 
redlich 
redundancy reduction strategy unsupervised learning 
neural computation 
ring 
learning sequential tasks incrementally adding higher orders 
advances neural information processing systems 
ring 
continual learning reinforcement environments 
unchen wien oldenbourg verlag 
ron singer tishby 
power amnesia learning probabilistic automata variable memory length 
machine learning 
rumelhart smolensky mcclelland hinton 
schemata sequential thought processes pdp models 
parallel distributed processing explorations cognition volume psychological biological models 
mit press 
chapter 
sa ran newport 
statistical learning month old infants 
science 
saul jordan 
boltzmann chains hidden markov models 
advances neural information processing systems 
mit press 
schutze singer 
part speech tagging variable memory markov model 
proceedings nd annual meeting association computational linguistics acl 
shavlik dietterich eds 

readings machine learning 
morgan kaufmann 
silberschatz 
operating system concepts 
wiley 
singer tishby 
dynamical encoding cursive handwriting 
biological cybernetics 
siskind 
visual event classi cation force dynamics 
proceedings seventeenth national conference arti cial intelligence 
stolcke omohundro 
inducing probabilistic grammars bayesian model merging 
grammatical inference applications 
springer 

stolcke segal 
precise gram probabilities stochastic context free grammars 
proceedings nd annual meeting association computational linguistics 
stolcke 
bayesian learning probabilistic language models 
ph dissertation university california berkeley 
sun sessions 
self segmentation sequences automatic formation hierarchies sequential behaviors 
ieee transactions systems man cybernetics 
press 
sun 
symbol grounding new look old idea 
philosophical psychology 

feature selection incremental learning probabilistic concept hierarchies 
proceedings seventeenth international conference machine learning 

learning compositional hierarchies inducing structure objects data 
advances neural information processing systems 
morgan kaufmann 
vaithyanathan dom 
hierarchical unsupervised learning 
proceedings seventeenth international conference machine learning 

associative computation 
ph dissertation university ulm germany 
see www informatik uni ulm de ni html 
willems shtarkov 
context tree weighting method basic properties 
ieee transactions information theory 
wol 
algorithm segmentation arti cial language analogue 
british journal psychology 
