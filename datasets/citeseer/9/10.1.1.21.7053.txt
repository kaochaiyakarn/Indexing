practice branching program boosting elomaa department computer science box fin university helsinki finland elomaa cs helsinki fi 
branching programs generalization decision trees 
viewpoint boosting theory appear exponentially cient 
earlier experience demonstrates results necessarily translate practical success 
develop practical version mansour mcallester algorithm branching program boosting 
test algorithm empirically real world synthetic data 
branching programs attain prediction accuracy level 
contrary implications boosting analysis signi cantly smaller corresponding decision trees 
corroborates earlier observations way boosting analyses bear practical signi cance 
weak learning model boosting theory able er analytical explanation practical success top induction decision trees subsequently dts short 
earlier attempts explain success dt learning theoretical models successful 
weak learning framework may better suit analyzing designing practical learning algorithms pac model variants exercise care drawing practical implications boosting analyses 
provide evidence support consideration 
branching program bp model computation takes form directed acyclic graph dag 
bps studied theoretical computer science 
view classi ers 
empirical machine learning similar representation formalism decision graphs studied extent 
bps strict generalization dts 
learning computational learning frameworks hard 
mansour mcallester devised boosting algorithm bps 
main advantage obtained bps dts training error guaranteed decline exponentially square root size program 
dt learning algorithms viewed boosting algorithms training error declination polynomial size tree 
learning algorithm bps basically similar algorithms top induction dts 
greedily searches splits nodes level evolving program 
central di erence bp dt branches may grow separate branches unite 
experiment practical learning algorithm results mansour mcallester 
clarify algorithm test data sets uci repository synthetic data 
experiments bps attain prediction accuracy level unpruned dts 
domain speci di erences observed particular synthetic domains di erences clear 
bps appear slightly smaller unpruned dts clear di erence 
section recapitulate weak learning boosting introducing time framework subsequently 
review kearns mansour analysis dt learning boosting algorithm 
section presents boosting algorithm bps motivation details 
brie touch related learning decision graphs 
section empirical experiments bp algorithm reported 
results contrasted obtained 
lessons learned experimentation re ected section 
concluding remarks 
weak learning boosting decision tree induction boolean target function input space set base classi ers predicates ful lls weak learning hypothesis respect contains distribution classi er prd words weak learning hypothesis guarantees existence predicate predictive advantage random guessing 
boosting algorithms exploit weak learning hypothesis combining di erent predicates di erent ltered distributions original sample 
amplify small predictive advantages random guessing iteratively obtain combined function training error desired error threshold 
natural type boosting algorithm voting algorithm uses number iterations assemble collection weak classi ers decision stumps uses weighted voting determine classi cation instances 
example adaboost algorithm :10.1.1.32.8918
interpreting node predicates weak classi ers apply boosting framework dt learning 
top induction dts predicates assigned leaves evolving tree 
usually predicates test value single attribute 
lter subsets original sample forward tree 
splitting criterion rank candidate predicates 
favors predicates reduce impurity class distribution subsets result splitting data 
ltered distributions gradually lose impurity usually easier go tree 
nal predictor combined node predicates 
viewing node predicate selection form weak learning enables explain learning behavior successful dt learners cart 
weak learning hypothesis choosing suitable predicate class appropriate splitting criterion training error dt driven polynomially size 
bound close optimal dt learning algorithm exponentially worse adaboost :10.1.1.32.8918
empirically error reduction respect classi er size adaboost essentially better 
binary dt constructed basis sample denote set leaves 
assume leaves labeled majority class examples reaching 
node set examples reaching 
denote set positive training examples reaching node fraction sample reaching node denoted js jsj proportion positive examples reach denoted js js splitting criterion mapping 
assigns impurity value observed class frequency distribution 
pure distributions impurity 
mixed distribution maximal impurity 
addition properties kearns mansour required permissible splitting criterion symmetric concave 
commonly impurity functions ful lling properties include binary entropy gini function cart algorithm 
criterion bp learning algorithm permissible 
index tree dt consisting single internal node labeled leaves corresponding values 
denote index measured respect change index node split predicate 

selecting split replace node entails choosing attribute gives decrease value splitting criterion 
evaluation attributes part entails determining best binary partition domain attribute 
empirical error decision tree weighted sum error leaves minf bounded index minf properties required permissible splitting criterion 
kearns mansour showed splitting criterion connection dt learning assuming weak fig 

minimal bp dt computing exclusive bits arcs black arrow head correspond value white head correspond value 
learning hypothesis sample exists 

result mansour mcallester de ned satisfy weak index reduction hypothesis sample exists 

approximating weak learning hypothesis reduction index obtained growing size tree bound empirical error tree polynomial size jt best known bound respect obtained splitting criterion branching program boosting polynomial reduction dt error ine cient analytical point view 
mansour mcallester showed compact dag representation bps exploit weak index reduction hypothesis ciently dts 
concerned leveled bps 
bp depth nodes form dimensional lattice 
levels individual width 
arcs nodes go nodes rst level consists root program nodes level leaves 
leaves may appear earlier levels 
internal nodes bp contain predicate 
leaves labeled classes 
instance unique path root program leaves internal node containing predicate decide follow corresponding arc level 
internal nodes leaf reached 
label leaf determines class prediction fig 
gives example bp dt recognizing function 
bp learning algorithm start sketching main points learning algorithm 
ll missing details describe analytical background subsequently 
analysis mansour mcallester constant levels 
constant available practice 
algorithm level current index value determines width grid potential nodes 
ones get connected nodes previous level potential nodes help merge leaves class distribution close 
discuss intuition potential nodes detail subsection 
algorithm initialize consist root node bp 
associate sample root 
single node program 
main loop de ne follows 

leaves consist pure nodes set leaves empty terminate 

predicate selection choose predicate reduces value splitting criterion words check attributes determine optimal binary splits respect choose best attribute optimal binary split technically choose predicate maximizes 


program expansion bp obtained splitting leaves determine average reduction index obtained 
potential node grid choosing determine width number subintervals dimensional grid potential nodes corresponding interval 
determine widths intervals 

leaf merging associate leaf potential node corresponds subinterval contains proportion positive examples 
potential nodes incoming arc potential nodes discarded 
resulting program parts fully elaborated algorithm steps 
comprise theoretically demanding points algorithm 
subsection review analytical motivation steps 
analytical background key di culty learning bps su ciently nely spaced grid subsets nodes get merged loss index reduction large 
hand order restrict growth hypothesis size limit number potential nodes grid 
go parts analysis mansour mcallester motivate choice grid potential nodes 
consider rst growing leveled bp nodes get merged 
leveled dt growing yields polynomial error declination respect tree size dt learning algorithms 
improve error reduction respect classi er size nodes merged bp 
consider happens index program expanded phase process nodes rst split produce eq 
index expanded program 
expansion grid potential nodes chosen 
helps merge leaves produce nal nodes potential nodes nw correspond division consecutive intervals 
uw 
denote leaves nodes belonging merged potential node node discarded 
see index bp changes due merging nodes 
operations 
merging node receives examples arrived leaves belonging proportion examples received examples 
denote usual fraction positive examples reaching index change index bp expressed inf sup concave see fig 

index increases increase small provided maps points interval close 
order lose index reduction obtained increase index due small respect 
obtained setting um um restricted 
sup inf fig 

leaves fraction positive examples interval gathered potential node combined leaves index proportion positive examples 
point convex combination points falls region bounded points line connecting points 
index value assigned higher keep width grid control nw handled special cases 
setting ln ln holds 
rst potential node represent subset leaves 
grid chosen inf pw hand eq 


words half index reduction obtained retained merging nodes 
remains show jp grows slowly 
rst assume equals xed index reduction conforming analysis 
index increase width depends jp grows quadratically observations imply case error reduction bp exponential square root size 
allowed analysis complicated may vary 
mansour mcallester show error declination holds general xed analysis extended show program produced uniform lower bound holds jp related decision graph induction dag obvious generalization tree learning decision graphs dgs just isolated strand empirical machine learning 
dg induction algorithms developed solve particular subtree replication data fragmentation problems inherent top induction dts 
dgs resemble bps exactly alike 
oliver iterative hill climbing algorithm uses minimum message length mml criterion determine split leaf join pair leaves evolving dg 
experiments algorithm attained accuracy level mml dt learners 
dgs observed give particularly results learning disjunctive concepts 
kohavi originally proposed constructing dgs bottom manner 
despite empirical success algorithm able cope numerical attributes lacked methods dealing irrelevant attributes 
subsequently kohavi li method post processes dt top dg 
special requirements put initial dt required oblivious test variable node level tree 
approach proposed alternative bottomup pruning dts 
reported experiments exhibit classi cation accuracies comparable smaller classi er sizes 
despite approaches learning dag formed classi ers thoroughly studied 
su cient analysis 
empirical evaluation test practice 
splitting criterion kearns mansour function analysis covers class problems 
algorithm handle predicates 
restrict attention domains classes 
nominal variables values search optimal binary grouping respect 
numerical value ranges binarized 
classi er sizes compared see bps zero empirical error smaller corresponding dts analysis assuming index reduction hypothesis 
order evaluate practical applicability bps compare prediction accuracies bps dts 
su ces test relation adaboost known 
order comparison fair bps contrast unpruned dts built 
force drive empirical error zero possible 
order comparison fair dts avoid repetitive counting leaves measure classi er sizes number internal nodes 
test domains known binary data sets uci repository synthetic domains 
uci repository relatively large domains included 
adult approx 
connect examples changed binary problem uniting classes draw lost 
data sets manipulated rid ects missing attribute values 
synthetic data sets majority function bits denoted maj maj respectively multiplexor function address bits exclusive function bits xor xor 
synthetic data sets di erent sized random samples generated 
test strategy fold cross validation repeated times 
table gives average prediction accuracies sizes bps dts 
records observed di erences statistically signi cant measured tailed student test signi cance level 
results uci data real world data sets accuracies learning algorithms close 
total fteen uci domains statistically signi cant di erences prediction accuracies observed 
cases di erences favor cases unpruned dts 
domains bps may bene fact due subset split decisions larger population training examples corresponding decisions dts 
obtained accuracy levels lower pruning dramatically 
hypothesis sizes fact pruning disabled course observed 
classi er average sizes large 
domains bps smaller unpruned dts remaining uci domains unpruned dts smaller corresponding bps 
algorithm produces smaller hypotheses tends better prediction accuracy 
exceptions rule 
results synthetic data synthetic data bps systematically accurate majority problems 
addition signi cant di erence favor 
results majority domains explained utility merging leaves evolving program 
consider leaves merged 
fractions positive examples close 
di erent bits tested en route leaves subsets associated possible set parameters release produce perfect unpruned trees 
comparison release 
table 
average classi cation accuracies classi er sizes standard deviations test domains 
statistically signi cant di erences learning algorithms indicated double asterisk 
data set bp size dt size adult breast chess connect diabetes euthyroid german glass heart hepatitis ionosphere liver sonar tic tac toe voting maj maj xor xor leaves consist examples similar numbers positive negative bits 
hand disadvantageous 
synthetic domains race smaller hypothesis tied 
time exception rule algorithm produces smaller hypothesis better prediction accuracy 
summary performance bps unpruned dts similar measured parameters 
bps perform slightly better dts 
discussion empirical observations comparison adaboost explained di erent advantage sequences weak learning parameter changes round round 
depends algorithm data 
parameter characterizes di culty classi cation problems posed weak learners 
produces increasingly re ned partitions sample obtaining increasing advantage adaboost concentrates examples falsely classi ed earlier weak classi er producing harder harder ltered distributions losing advantage 
advantage sequences directly analyzing chooses weak classi ers time 
reason believe sequences bps worse dts merging may task separating classes di cult 
advantage grows original sample re ned subset lead slow growth 
consider exclusive bits 
assume grown leaves merged di erent bits tested paths root splitting leaves testing remaining bit produces pure leaves gives large reduction index 
hand merged examples reaching get mixed impossible separate positive negative examples testing single bit 
obtain smaller value larger bp information lost gathered anew 
may note width grid potential nodes explodes respect width corresponding program level 
program levels typically nodes wide width grid potential nodes may close intervals 
analysis mansour mcallester potential nodes determine size program larger actual program size 
quite programs produced dts node 
bp consists consecutive trees nodes gather remaining examples program construction starts anew 
compared bps unpruned dts better relate sizes 
dts easily pruned enhance classi er compactness 
respect empirical setting fair pruning easily beat classi er size 
hand wanted obtain understanding basic properties new interesting learning approach test boosting analysis re ected practice 
seen explicit merging approach dg learning algorithms leaves evolving program rst expanded merged heuristically 
grid potential nodes analytically motivated heuristic 
heuristics 
constructing dag formed classi ers tree formed ones natural idea 
harder interpret prune popular 
evident advantage bps known 
new analysis gives promise concrete advantage bear fruit practice 
empirical evaluation indicates theoretical advantage directly materialize experiments 
results comparable obtained unpruned dts 
altogether learning bps dgs interesting new idea deserve empirical attention 

bergadano learning branching programs small depth circuits 
ben david 
ed computational learning theory proc 
third european conference 
lecture notes arti cial intelligence vol 
springer verlag berlin heidelberg new york 
blake merz uci repository machine learning databases 
department information computer science university california irvine 
www ics uci edu mlearn mlrepository html 
breiman friedman olshen stone classi cation regression trees 
wadsworth paci grove ca 
dietterich kearns mansour applying weak learning framework understand improve 
saitta 
ed proc 
thirteenth conference machine learning morgan kaufmann san francisco 
kumar rubinfeld learning bounded width branching programs 
proc 
eighth annual conference computational learning theory acm press new york 
freund boosting weak learning algorithm majority 
inf 
comput 

freund schapire experiments new boosting algorithm 
saitta 
ed proc 
thirteenth international conference machine learning 
morgan kaufmann san francisco 
freund schapire decision theoretic generalization line learning application boosting 
comput 
syst 
sci 

kearns mansour boosting ability top decision tree learning algorithms 
comput 
syst 
sci 

kohavi bottom induction oblivious read decision graphs 
bergadano de raedt 
eds machine learning proc 
seventh european conference 
lecture notes arti cial intelligence vol 
springer verlag berlin heidelberg new york 
kohavi li oblivious decision trees graphs top pruning 
proc 
fourteenth international joint conference arti cial intelligence 
morgan kaufmann san francisco ca 
mansour mcallester boosting multi way branching decision trees 
solla leen mller 
eds advances neural information processing 
mit press cambridge ma 
mansour mcallester boosting branching programs 
cesa bianchi goldman 
eds proc 
thirteenth annual conference computational learning theory 
morgan kaufmann san francisco ca 
oliver decision graphs extension decision trees 
proc 
fourth international workshop arti cial intelligence statistics 
society arti cial intelligence statistics 
quinlan programs machine learning 
morgan kaufmann san mateo ca 
schapire strength weak learnability 
mach 
learn 

