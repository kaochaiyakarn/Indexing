applying training methods statistical parsing propose novel training method statistical parsing 
algorithm takes input small corpus sentences annotated parse trees dictionary possible lexicalized structures word training set large pool unlabeled text 
algorithm iteratively labels entire data set parse trees 
empirical results parsing wall street journal corpus show training statistical parser combined labeled unlabeled data strongly outperforms training labeled data 
current crop statistical parsers share similar training methodology 
train penn treebank marcus collection sentences labeled corrected parse trees approximately word tokens 
explore methods statistical parsing combine small amounts labeled data unlimited amounts unlabeled data 
experiment reported sentences bracketed data word tokens 
methods attractive reasons bracketing sentences expensive process 
parser trained small amount labeled data reduce annotation cost 
creating statistical parsers novel domains new languages easier 
combining labeled data unlabeled data allows exploration unsupervised methods tested evaluations compatible supervised statistical parsing 
introduce new approach combines unlabeled data small amount labeled bracketed data train statistical parser 
training method yarowsky blum mitchell aravind joshi mitch marcus mark liberman srinivas david chiang anonymous reviewers helpful comments 
partially supported nsf sbr aro daah darpa 
anoop sarkar dept computer information science university pennsylvania south rd street philadelphia pa usa anoop linc cis upenn edu goldman zhou previously train classifiers applications word sense disambiguation yarowsky document classification blum mitchell named entity recognition collins singer apply method complex domain statistical parsing :10.1.1.114.9164:10.1.1.114.3629
unsupervised techniques language processing machine learning techniques exploit annotated data successful attacking problems nlp aspects considered open issues adapting new domains training domain testing 
higher performance limited amounts annotated data 
separating structural robust aspects problem lexical sparse ones improve performance unseen data 
particular domain statistical parsing limited success moving unsupervised machine learning techniques see section discussion 
promising approach combining small amounts seed labeled data unlimited amounts unlabeled data bootstrap statistical parsers 
machine learning technique training successfully classification tasks web page classification word sense disambiguation named entity recognition 
early combining labeled unlabeled data nlp tasks done area unsupervised part speech pos tagging 
cutting reported high results brown corpus unsupervised pos tagging hidden markov models hmms exploiting hand built tag dictionaries equivalence classes 
tag dictionaries predefined assignments possible pos tags words test data 
impressive result triggered follow studies effect hand tuning tag dictionary quantified combination labeled np pierre nnp nnp md join vb vp np vp dt board nn pp np dt non executive jj director nn example kind output expected statistical parser 
data 
experiments merialdo showed specific cases hmms effective combining labeled unlabeled data 
brill showed aggressively tag dictionaries extracted labeled data bootstrap unsupervised pos tagger high accuracy approx wsj data 
exploit approach tag dictionaries method see section details 
important point attacking problem parsing similar machine learning techniques face representational problem difficult define notion tag dictionary statistical parser 
problem face parsing complex assigning small fixed set labels examples 
parser generally applicable produce fairly complex label input sentence 
example sentence pierre join board non executive director parser expected produce output shown 
entire parse reasonably considered monolithic label usual method parsing decompose structure assigned way join np vp join np pierre vp join vp join vp join join np board pp recursive decomposition structure allow simple notion tag dictionary 
solve problem decomposing structure approach different shown uses context free rules 
approach uses notion tree rewriting defined lexicalized tree adjoining grammar ltag formalism joshi schabes re lexicalized version tree adjoining grammar joshi joshi 
tains notion lexicalization crucial success statistical parser permitting simple definition tag dictionary 
example parse generated assigning structured labels shown word sentence simplicity assume noun phrases generated single word 
tool described xia convert penn treebank representation 
np pierre np vp vp vp join np pp np vp vp np board np non executive director parsing tree classification attachment 
combining trees rewriting nodes trees explained section gives parse tree 
history bi lexical dependencies define probability model construct parse shown 
history called derivation tree 
addition byproduct kind representation obtain phrase structure sentence 
produce parse phenomena predicate argument structure subcategorization movement tic treatment 
join pierre board director derivation indicating attachments trees occurred parse sentence 
generative model stochastic ltag derivation proceeds follows schabes resnik 
initial tree selected probability init trees selected words sentence combined operations substitution adjoining 
operations explained examples 
operations performed probability attach valid start derivation pinit substitution defined rewriting node frontier tree probability attach said proper attach indicates tree substituting node tree example operation substitution shown 
adjoining defined rewriting internal node tree tree 
recursive rule adjoining operation performed probability attach proper attach na attach probability rewrites internal node tree adjoining na occurs node additional factor accounts adjoining node required probability formed 
example operation adjoining 
ltag derivation built starting tree subsequent attachments probability pr init attach np pierre np pierre np vp vp join np join np example substitution tree pierre tree join join np pierre 
vp vp np vp np vp join np vp join np example adjoining tree tree join join vp 
note assuming tree lexicalized word derivation corresponds sentence words 
section show exploit notion tag dictionary problem statistical parsing 
training methods parsing supervised methods learning treebank studied 
question want pursue unlabeled data improve performance statistical parser time reduce amount labeled training data necessary performance 
assume data input method characteristics 
small set sentences labeled corrected parse trees large set unlabeled data 

pair probabilistic models form parts statistical parser 
pair models able mutually constrain 

tag dictionary backoff smoothing strategy labels covered labeled set 
pair probabilistic models exploited bootstrap new information unlabeled data 
steps ultimately agree utilize iterative method called training attempts increase agreement pair statistical models exploiting mutual constraints output 
training applications word sense disambiguation yarowsky web page classification blum mitchell identification collins singer :10.1.1.114.9164:10.1.1.114.3629
cases unlabeled data resulted performance rivals training solely labeled data 
previous approaches tasks involved identifying right label small set labels typically relatively small parameter space 
compared earlier models statistical parser large parameter space labels expected output parse trees built recursively 
discuss previous combining labeled unlabeled data detail section 
training blum mitchell yarowsky informally described manner pick views classification problem :10.1.1.114.9164
build separate models views train model small set labeled data 
sample unlabeled data set find examples model independently labels high confidence 
nigam ghani confidently labeled examples picked various ways 
collins singer goldman zhou take examples valuable training examples iterate procedure unlabeled data exhausted :10.1.1.114.3629
effectively picking confidently labeled data model add training data model labeling data model 
lexicalized grammars mutual constraints representation parsing lexicalized grammar done steps 
assigning set lexicalized structures word input sentence shown 

finding correct attachments structures get best parse shown 
steps involves ambiguity resolved statistical model 
explicitly representing steps independently pursue independent statistical models step 
word sentence take different lexicalized structures 
introduce statistical model disambiguates lexicalized structure assigned word depending local context 

word assigned certain set lexicalized structures finding right parse tree involves computing correct attachments lexicalized structures 
disambiguating attachments correctly appropriate statistical model essential finding right parse tree 
models agree trees assigned word sentence 
right trees assigned predicted model fit cover entire sentence predicted second model represents mutual constraint model places 
tag dictionaries words appear unlabeled training data collect list part speech labels trees word known select training data 
information stored pos tag dictionary tree dictionary 
important note frequency distributional information stored 
information stored dictionary tags trees selected word training data 
count cutoff trees labeled data combine observed counts unobserved tree count 
similar usual technique assigning token unknown infrequent word tokens 
way trees unseen labeled data tag dictionary assigned probability parser 
problem lexical coverage severe unsupervised approaches 
tag dictionaries way problem 
approach unsupervised part speech tagging brill seed data pos tags selected word input unsupervised tagger 
see discussion relation approach supertagging srinivas interesting extend models unknown word handling machine learning techniques clustering learning subcategorization frames creation tag dictionaries 
models described treat parsing step process 
models 
selects trees previous context tagging probability model 
computes attachments trees returns best parse parsing probability model tagging probability model select trees word examining local context 
statistical model decide trigram model srinivas supertagging model srinivas 
model assigns best lattice tree assignments associated input sentence path corresponding assignment elementary tree word sentence 
details see srinivas 
wn tn tn wn ti tn sequence elementary trees assigned sentence wn 
get bayes theorem obtain ignore denominator applying usual markov assumptions 
output model probabilistic ranking trees input sentence sensitive small local context window 
parsing probability model words sentence selected set elementary trees parsing process attaching trees give consistent bracketing sentences 
notation stand elementary tree lexicalized word part speech tag init introduced earlier stand probability root derivation tree defined follows pinit including lexical information written pr pr pr pj top pr wj top variable top indicates tree begins current derivation 
useful approximation init pr pr label label root node pr count top label count top number bracketing labels constant smooth zero counts 
attach introduced earlier stand probability attachment attach na attach including lexical information written pr pr decompose components pr pr pr node pr jp node similar decomposition 
equations backoff model handle sparse data problems 
compute backoff model follows stand original lexicalized model backoff level uses part speech information node node init attach count 
backoff model computed follows diversity number distinct counts 
attach smooth probabilities 
example handled way 
pr count node count node count node count node diversity adjunction number different trees attach node 
set trees possibly attach node tree experiments value set training algorithm position describe training algorithm combines models described section section order iteratively label large pool unlabeled data 
datasets algorithm labeled set sentences bracketed correct parse trees 
cache small pool sentences focus iteration training algorithm 
unlabeled large set unlabeled sentences 
information collect set sentences tree dictionary tree dict part speech dictionary pos dict 
construction dictionaries covered section 
addition datasets usual development test set termed dev test set called test evaluate bracketing accuracy parser 
training algorithm consists steps repeated iteratively sentences set unlabeled exhausted 

input labeled unlabeled 
update cache randomly select sentences unlabeled refill cache cache empty exit 
train models labeled 
apply cache 

pick probable run add labeled 

pick probable 
experiment reported andk set iteration 
ran algorithm iterations covering sentences unlabeled added best parses remaining sentences 
experiment setup experiments report done penn treebank wsj corpus marcus 
various settings training algorithm section follows labeled set sections penn treebank wsj sentences unlabeled sentences section treebank stripped annotations 
tag dictionary lexicalized trees labeled unlabeled 
novel trees treated unknown tree tokens 
cache size sentences 
expensive run parser cache multiple times pruning capabilities parser 
iterations set beam size value prune derivations large portion cache ones 
allows parser run faster avoiding usual problem running iterative algorithm thousands sentences 
initial runs limit length sentences entered cache shorter sentences beat longer sentences case 
beam size reset running parser test data allow parser better chance finding parse 
results scored output parser section wall street journal penn treebank 
aspects scoring useful comparision results scored including sentence final punctuation 
empty elements scored 
evalb written satoshi sekine michael collins scores par black standard parameter file standard practice part speech brackets part evaluation 
adwait ratnaparkhi part speech tagger ratnaparkhi tag unknown words test data 
obtained labeled bracketing precision recall respectively defined black 
baseline model trained sentences labeled data performed precision recall 
results show training statistical parser cotraining method combine labeled unlabeled data strongly outperforms training labeled data 
important note previous studies method moving unsupervised parsing directly compared output supervised parsers 
certain differences applicability usual methods smoothing parser cause lower accuracy compared state art statistical parsers 
consistently seen increase performance training method baseline trials 
emphasised result data usually parsers 
experimenting smaller set labeled data investigate learning curve 
previous combining labeled unlabeled data step procedure training method statistical parsing incipient srinivas statistical model tagging sentences elementary lexicalized structures 
particularly lightweight dependency analyzer lda shortest attachment heuristics initial supertagging stage find syntactic dependencies words sentence 
statistical model attachments notion mutual constraints steps exploited 
previous studies unsupervised methods parsing concentrated inside outside algorithm lari young carroll rooth 
limitations inside outside algorithm unsupervised parsing see marcken experiments draw mismatch minimizing error rate iteratively increasing likelihood corpus 
approaches tried move away phrase structural representations dependency style parsing lafferty fong wu 
inherent computational limitations due vast search space see pietra discussion 
approaches realistically compared supervised parsers trained tested kind representations complexity sentences penn treebank 
chelba jelinek combine unlabeled labeled data parsing view language modeling applications 
goal get right bracketing dependencies reduce word error rate speech recognizer 
approach closely related previous training methods yarowsky blum mitchell goldman zhou collins singer :10.1.1.114.9164:10.1.1.114.3629
yarowsky introduced iterative method increasing small set seed data disambiguate dual word senses exploiting constraint segment discourse sense word 
unlabeled data improved performance disambiguator purely supervised methods 
blum mitchell approach gave name training :10.1.1.114.9164
definition training includes notion exploited different models constrain exploiting different views data 
prove pac results learnability 
discuss application classifying web pages method mutually constrained models 
collins singer extend classifiers mutual constraints adding terms adaboost force classifiers agree called boosting :10.1.1.114.3629
goldman zhou provide variant training suited learning decision trees data split different equivalence classes models hypothesis testing determine agreement models 
experiment ideas incorporated model 
explore entire words wsj penn treebank labeled data larger set wsj data input training algorithm 
addition plan explore points bear understanding nature training learning algorithm contribution dictionary trees extracted unlabeled set issue explore experiments 
ideally wish design training method information unlabeled set 
relationship training em bears investigation 
nigam ghani study tries separate factors gradient descent aspect em vs iterative nature training generative model em vs conditional independence features models exploited training 
em successfully text classification combination labeled unlabeled data see nigam 
experiments blum mitchell balance label priors picking new labeled examples addition training data :10.1.1.114.9164
way incorporate algorithm incorporate form sample selection active learning selection examples considered labeled high confidence hwa 
proposed new approach training statistical parser combines labeled unlabeled data 
uses training method pair models attempt increase agreement labeling data 
algorithm takes input small corpus sentences word tokens bracketed data large pool unlabeled text tag dictionary lexicalized structures word training set ltag formalism 
algorithm iteratively labels unlabeled data set parse trees 
train statistical parser combined set labeled unlabeled data 
obtained labeled bracketing precision recall respectively 
baseline model trained sentences labeled data performed precision recall 
results show training statistical parser training method combine labeled unlabeled data strongly outperforms training labeled data 
important note previous studies method moving unsupervised parsing directly compared output supervised parsers 
previous approaches unsupervised parsing method trained tested kind representations complexity sentences penn treebank 
addition byproduct representation obtain phrase structure sentence 
produce parse phenomena predicate argument structure subcategorization movement probabilistic treatment 
black abney flickinger grishman harrison hindle ingria jelinek klavans liberman marcus roukos santorini strzalkowski 

procedure quantitatively comparing syntactic coverage english grammars 
proc 
darpa speech natural language workshop pages 
morgan kaufmann 
blum mitchell 

combining labeled unlabeled data training 
proc 
th annual conf 
comp 
learning theory colt pages 
brill 

unsupervised learning disambiguation rules part speech tagging 
natural language processing large corpora 
kluwer academic press 
carroll rooth 

valence induction head lexicalized pcfg 
xxx lanl gov abs cmp lg may chelba jelinek 

exploiting syntactic structure language modeling 
proc 
coling acl pages montreal 
collins singer 

unsupervised models named entity classification 
proc 
emnlp pages 
cutting kupiec pedersen sibun 

practical part speech tagger 
proc 
rd anlp conf trento italy 
acl 


baum welch re estimation help taggers 
proc 
th anlp conf pages stuttgart october 
fong wu 

learning restricted probabilistic link grammars 
wermter riloff editors connectionist statistical symbolic approaches learning natural language processing pages 
springer verlag 
goldman zhou 

enhancing supervised learning unlabeled data 
proc 
icml stanford university june july 
rebecca hwa 

sample selection statistical grammar induction 
proceedings emnlp vlc pages 
joshi schabes 

tree adjoining grammar lexicalized grammars 
nivat podelski editors tree automata languages pages 
elsevier science 
joshi levy takahashi 

tree adjunct grammars 
journal computer system sciences 
joshi 

tree adjoining grammars context sensitivity required provide reasonable structural description 
dowty karttunen editors natural language parsing pages 
cambridge university press cambridge lafferty sleator temperley 

grammatical trigrams probabilistic model link grammar 
proc 
aaai conf 
probabilistic approaches natural language 
lari young 

estimation stochastic contextfree grammars inside outside algorithm 
computer speech language 
de marcken 

lexical heads phrase structure induction grammar 
yarowsky church editors proc 
rd pages mit cambridge ma 
marcus santorini 

building large annotated corpus english 
computational linguistics 
merialdo 

tagging english text probabilistic model 
computational linguistics 
kamal nigam ghani 

analyzing effectiveness applicability training 
proc 
ninth international conference information knowledge cikm 
kamal nigam andrew mccallum sebastian thrun tom mitchell 

text classification labeled unlabeled documents em 
machine learning 
della pietra della pietra lafferty ure 
inference estimation long range trigram model 
carrasco oncina editors proc 

springer verlag 
ratnaparkhi 

maximum entropy part speech tagger 
proc 
emnlp university pennsylvania 
resnik 

probabilistic tree adjoining grammars framework statistical natural language processing 
proc 
coling volume pages nantes france 
schabes 

stochastic lexicalized tree adjoining grammars 
proc 
coling volume pages nantes france 
srinivas 

complexity lexical descriptions relevance partial parsing 
ph thesis department computer information sciences university pennsylvania 
xia palmer joshi 

uniform method grammar extraction applications 
proc 
emnlp vlc 
yarowsky 

unsupervised word sense disambiguation rivaling supervised methods 
proc 
rd meeting acl pages cambridge ma 
