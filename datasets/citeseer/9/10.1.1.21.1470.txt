weighting features dietrich wettschereck david aha german national research center computer science sankt augustin germany dietrich wettschereck gmd de navy center applied research ai naval research laboratory washington dc usa aha aic nrl navy mil 
case reasoning algorithms retrieve cases derivative nearest neighbor nn classifier similarity function sensitive irrelevant interacting noisy features 
proposed methods reducing sensitivity parameterize nn similarity function feature weights 
focus methods automatically assign weight settings little domain specific knowledge 
goal predict relative capabilities methods specific dataset characteristics 
introduce dimensional framework categorizes automated weight setting methods empirically compare methods dimensions summarize results hypotheses describe additional evidence supports 
investigation revealed methods correctly assign low weights completely irrelevant features methods performance feedback demonstrate advantages methods require pre processing better tolerate interacting features increase learning rate 
case retrieval nearest neighbor variants nearest neighbor nn classifier frequently case retrieval case reasoning cbr algorithms 
nn assumes case fx xn defined set numeric symbolic features class value 
query case library nn retrieves set similar distant cases predicts weighted majority class class distance defined distance theta difference parameterized weight value assigned feature difference jx gamma feature numeric feature symbolic international conference case reasoning iccbr 
tr aic 
probability member class defined distance distance small shown prevent zero division 
classification tasks class largest output 
set leave cross validation loocv numeric features normalized subtracting mean dividing standard deviation ensure range expected impact equation 
nn address cbr issues case retrieval requires extension applied case representations defined flat set features 
equation nn assigns equal weights features fw 
bias nn allowing redundant irrelevant imperfect features influence distance computations 
nn perform poorly features 
variants proposed assign higher weight settings presumably relevant features case retrieval 
feature weighting variants nn reported improve retrieval accuracy tasks aha kelly davis wettschereck relative merits known previous comparisons focussed specific algorithm pairings wettschereck dietterich kohavi case study results mohri tanaka 
introduce framework feature weighting methods section empirically compare specific subset section 
case studies particularly informative 
section hypotheses explaining results investigate empirically 
section includes summary discussion 
framework examples feature weighting methods reviewed modify standard nn similarity function allowing feature weights different values 
briefly introduce framework section detail examples dimension section 
wettschereck 
discuss details dimensions 
framework feature weighting distinguish feature weighting methods dimensions table 
dimension feedback dimension concerns feature weighting method receives feedback nn variant 
discuss dimension detail section 
dimension weight space concerns size space feature weights searched algorithm 
feature weighting methods perform table 
dimensions distinguishing feature weighting methods dimension possible values feedback weight space representation generality knowledge feature selection constrains search space binary values 
feature selection long history pattern recognition devijver kittler 
researchers reported accuracy speed improvements nn variants cbr systems cardie moore lee skalak aha bankert 
feature selection methods reduce task dimensionality eliminate irrelevant features 
kohavi 
evidence continuous weighting increases accuracy features vary relevance classification requires searching larger space 
dimension representation concerns feature set transformed replaced different set weighting cbr algorithms interacting correlated features 
example mohri tanaka quantification method ii transform feature set 
symbolic features replacing feature defined values gamma binary features computes weights maximize ratio variance class cases variance cases 
mohri tanaka introduced nn variant named reported performed best algorithms tested 
dimension generality weighting methods learn settings single set global weights assumption feature relevance invariant domain constraining inappropriate 
methods assume weights differ local regions case space different values feature stanfill waltz ricci case specific basis domingos press 
case specific weighting provides great flexibility assessing feature relevance required model subject data accurately aha goldstone 
dimension knowledge important dimension distinguishing feature weighting methods 
domain specific knowledge example constrain case representation stanfill waltz guide feature transformation aha assign case specific weight settings cain 
focus automated algorithms receive knowledge 
comparison knowledge intensive methods weighting features cbr algorithms complement contribution 
details feedback dimension dimension refers feature weighting method uses feedback nn variant assign weights refer feedback ignorant methods 
kohavi 
favorable comparative evidence feedback method 
review sets feedback methods sets ignorant methods section 
feedback methods 
incremental hill methods modify feature weights increase similarity query nearby cases class decrease similarity nearby cases classes 
process training case sensitive presentation ordering 
salzberg method 
correct classification occurs feature matches weight incremented delta mismatching features weights decremented amount 
incorrect classifications weights mismatching features incremented weights matching features decremented 
salzberg reported different values delta worked better different datasets 
wettschereck dietterich argued weighting method insensitive skewed concept distributions problem ib aha addresses 
computes weights max gamma expected asymptote seemingly irrelevant features 
higher observed frequency concepts incremented ae gamma difference theta gamma difference theta gamma incremented gamma 
aha reported results ib tasks involving irrelevant features 
ib assumes uniform distribution irrelevant feature values 
kira rendell removed constraint relief 
selects random training case similar positive case similar negative case updates feature weights gamma difference difference kira rendell reported results relief parity tasks 
evaluate hill climbing variant relief kononenko extension relief experiments 
continuous optimizers feedback methods iteratively update feature weights randomly selected training cases 
example ga kelly davis uses genetic algorithm update feature weights search guided genetic operators fitness training accuracy recency :10.1.1.95.9953
ga attained lower error rates nn datasets 
skalak mutation genetic operator select features binary weights nn 
algorithm retains best performing bit sequence terminates fixed number iterations finding new best string 
attained higher accuracies nn datasets halving number features compute distances 
continuous optimization algorithms knowledge function gradient increase learning speed works reasonably smooth target functions 
lowe employed approach variable kernel similarity metric vsm optimizes feature weights conjugate gradient algorithm minimize summed loocv error training set 
derivative error respect feature weight guide search 
vsm performed better algorithms datasets required far training time algorithms 
experimented nn sm wettschereck variant vsm isolates feature weighting method 
nn sm computes distances pairs training cases equation assigns value weight fw optimizes value uses conjugate gradient optimize feature weights minimize loocv training error 
error function set classes ae gamma ignorant methods 
conditional probabilities group consists methods assign feature weights simple conditional probabilities discretize numeric features symbolic features 
category feature importance pcf method assigns high weight values features highly correlated class weight feature class conditional probability case member value algorithm tends classify cases majority class mohri tanaka 
cross category feature importance ccf method averages classes 
computes weight settings mohri tanaka reported results ccf included experiments 
class projection sophisticated value difference metric vdm stanfill waltz symbolic features 
computes similarity individual symbolic feature values defining distance distance theta difference difference gamma equation computes weight feature value 
number times cases class case library value value summed set classes 
vdm assigns higher weights features distribution values classes highly skewed 
equation computes difference values assigns greater differences values corresponding sets cases highly disparate class distributions 
cases similar feature values respective projections training library similar class distributions 
included vdm experiments 
mutual information third ignorant approach assigns feature weights mutual information mi shannon values feature class training cases 
mi variables reduction uncertainty variable value knowledge value computed delta log delta probability class arbitrary training case probability value feature set possible values daelemans van den bosch reported mi significantly improved nn accuracy task 
wettschereck dietterich reported mi significantly increased salzberg accuracy 
experiments examined variant approach discretizes continuous features fayyad irani algorithm 
table 
algorithms selected experimentation weighting method name category subcategory relief feedback incremental hill climber sm feedback continuous optimizer ccf ignorant conditional probabilities vdm ignorant class projection mi ignorant mutual information table 
characteristics selected datasets 
boolean continuous symbolic 
relevant features datasets located horizontal divider approximately equally relevant 
domain set size number number type irrelevant training test features features classes banded sinusoidal gauss band parity led display led waveform waveform cleveland nettalk phonemes comparing feedback ignorant weighting methods section reports empirical comparison motivates selection summary hypotheses section 
focus dimension framework 
selected algorithm table represents subcategory methods described section differ assign weights features 
shown nn control algorithm 
selected datasets table test algorithms capabilities tolerating irrelevant interacting redundant features 
cases selected randomly uniform distribution datasets 
banded relevant irrelevant feature equal sized classes 
second sinusoidal dimensional concept boundary sine curve peaks vertical dimension nearly irrelevant distinguishing cases classes 
third gauss band interacting features 
extends sinusoidal additional features define gaussian distributions fifth determines class determined second pair features 
fourth dataset parity problem binary see wettschereck extensive evaluation 
table 
mean accuracy feature weighting algorithms relative nn feature weight learning algorithm feedback method ignorant method dataset nn relief sm ccf vdm mi banded sigma sinusoidal sigma gauss band sigma parity sigma led display sigma led sigma waveform sigma waveform sigma cleveland sigma nettalk sigma features positive cases odd number features set 
interacting irrelevant features 
remaining datasets obtainable uci repository murphy 
led relevant features roughly equally relevant waveform vary relevance 
waveform led identical waveform led addition continuous irrelevant features 
cleveland dataset contains redundant features nettalk dataset irrelevant redundant features wettschereck 
dataset randomly partitioned times disjoint training test sets 
table lists algorithms average test set accuracy relative nn standard error listed 
significant differences highlighted boldface tailed tests confidence level 
loocv tune algorithms free parameters large nettalk dataset optimized parameter settings cross validated subset training set 
optimal value estimated methods 
nn sm number epochs limited number required minimization conjugate direction 
fayyad irani algorithm discretize continuous features relief ccf vdm mi 
evaluating summary hypotheses results motivated test hypotheses addressed section 
wettschereck 
describe extensive analysis 
feature weighting methods tolerate irrelevant features highly interacting features 
results generally support hypothesis sinusoidal task see 
size case library considered 
example nn average performance decreased small amount adding irrelevant relevant features irrelevant features fig 

feature weights computed mi waveform task table 
average accuracies waveform tasks relative nn feature weight learning algorithm training feedback method ignorant method dataset size nn relief sm mi waveform sigma sigma waveform sigma sigma features waveform task 
relative accuracies methods increased expected computed low weights irrelevant features see relief significantly improved accuracy 
smaller case libraries prevent nn performing algorithms 
results follow study table support hypothesis 
ignorant methods suffer substantially data carefully pre processed 
hypothesized ignorant methods performed comparative poorly sinusoidal task discretization procedure task revealed subsequent inspections 
accuracies ignorant methods improved significantly table followup experiments user provided discretizations 
discretization performed nn nn sm positive effect relief 
feedback methods attain higher accuracies ignorant methods tasks interacting features 
evidenced performance differences mi nn sm tasks interacting features parity led 
tested parity concept varying number relevant binary features dimensional space 
vertical relevant dimension generally split intervals interval covered nearly entire range 
exist equally sized intervals 
table 
average accuracies sinusoidal task relative nn feature weight learning algorithm discretization feedback method ignorant method method nn sm ccf vdm mi sigma fayyad irani manually set generalization accuracy number training cases nettalk learning curve relief mi number training cases led learning curve relief mi fig 

learning curves mi relief tasks 
nn sm significantly outperformed mi parity features performed equally poorly larger numbers interacting features 
feedback methods faster learning rates ignorant methods 
selected computationally efficient methods category mi relief investigate hypothesis 
learning curves experiments tasks methods attained approximately equal accuracies table indicate relief learning rate faster mi 
curves shown 
summary results provide confirming incomplete evidence hypotheses designed help determine methods perform task characteristics 
reason perform differently due difference similarity function particular feature weighting method 
implications focussed empirical evaluations knowledge poor feature weighting methods cbr algorithms 
dimensional framework relate methods weighting features motivate experiments distinguish comparative abilities suggest selection followed continuous weighting locally estimating weighting profitably explored 
new weighting methods categorized framework simplify comprehension 
investigations focus dimensions 
evidence hypotheses limited algorithms datasets tested 
example suggests feedback methods preferred tasks interacting features contradicted designing ignorant method account features 
examined binary feedback ignorant distinction gradations dimension 
scope investigation limited ignores nn methods incorporate domain specific knowledge dimension framework section task appropriate case representations 
designing practical cbr applications feature weighting method selected context crucial design decisions 
anonymous reviewers suggestions 
aha 

study instance learning algorithms supervised learning tasks mathematical empirical psychological evaluations tr 
irvine ca university california department information computer science 
aha 

incremental constructive induction instance approach 
proceedings eighth international workshop machine learning pp 

evanston il morgan kaufmann 
aha bankert 

feature selection case classification cloud types empirical comparison 
aha ed 
case reasoning papers workshop tr ws 
menlo park ca aaai press 
aha goldstone 

concept learning flexible weighting 
proceedings fourteenth annual conference cognitive science society pp 

bloomington lawrence erlbaum 
cain pazzani silverstein 

domain knowledge influence similarity judgement 
proceedings case reasoning workshop pp 

washington dc morgan kaufmann 
cardie 

decision trees improve case learning 
proceedings tenth international conference machine learning pp 

amherst ma morgan kaufmann 
masand smith waltz 

trading mips memory knowledge engineering 
communications acm 
daelemans van den bosch 

generalization performance backpropagation learning task 
proceedings connectionism natural language processing pp 

enschede netherlands unpublished 
devijver kittler 

pattern recognition statistical approach 
englewood cliffs nj prentice hall 
domingos context sensitive feature selection lazy learners 
appear artificial intelligence review 
fayyad irani 

multi interval discretization continuous valued attributes classification learning 
proceedings thirteenth international joint conference artificial intelligence pp 

chambery france morgan kaufmann 
kelly jr davis 

hybrid genetic algorithm classification 
proceedings twelfth international joint conference artificial intelligence pp 

sydney australia morgan kaufmann 
kira rendell 

practical approach feature selection 
proceedings ninth international conference machine learning pp 

aberdeen scotland morgan kaufmann 
kohavi langley yun 

heuristic search feature weights instance learning 
unpublished manuscript 
kononenko 

estimating attributes analysis extensions relief 
proceedings european conference machine learning pp 

catania italy springer verlag 
lowe 

similarity metric learning variable classifier 
neural computation 
mohri tanaka 

optimal weighting criterion case indexing numeric symbolic attributes 
aha ed case reasoning papers workshop tr ws 
menlo park ca aaai press 
moore lee 

efficient algorithms minimizing cross validation error 
proceedings eleventh international conference machine learning pp 

new brunswick nj morgan kaufmann 
murphy 

uci repository machine learning databases machine readable data repository ics uci edu 
irvine ca university california department information computer science 
ricci 

learning local similarity metric case reasoning 
appear proceedings international conference casebased reasoning 
portugal springer verlag 
salzberg 

nearest hyperrectangle learning method 
machine learning 
shannon 

mathematical theory communication 
bell systems technology journal 
skalak 

prototype feature selection sampling random mutation hill climbing algorithms 
proceedings eleventh international machine learning conference pp 

new brunswick nj morgan kaufmann 
stanfill waltz 

memory reasoning 
communications acm 
wettschereck 

study distance machine learning algorithms 
doctoral dissertation department computer science oregon state university corvallis 
wettschereck aha mohri 
review comparative evaluation feature weighting methods lazy learning algorithms tr aic 
washington dc naval research laboratory navy center applied research artificial intelligence 
wettschereck dietterich 

experimental comparison nearest neighbor nearest hyperrectangle algorithms 
machine learning 
processed macro package llncs style 
