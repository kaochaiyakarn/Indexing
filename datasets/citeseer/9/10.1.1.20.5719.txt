real time framework natural multimodal interaction large screen displays sharma pennsylvania state univ dept comp 
science eng 
pond lab university park pa usa phone fax presents framework designing natural multimodal modal human computer interaction hci system 
core proposed framework principled method combining information derived audio visual cues 
achieve natural interaction audio visual modalities fused feedback large screen display 
careful design due considerations possible aspects systems interaction cycle integration resulted successful system 
performance proposed framework validated development prototype systems commercial applications retail entertainment industry 
assess impact multimodal systems mms informal studies conducted 
system performed specifications cases users showed ad hoc proficiency indicating natural acceptance systems 
keywords continuous gesture recognition multimodal hci speech gesture analysis visual tracking real time systems 

humans communicate range different modalities modern human machine interfaces rarely reflect 
successful hci system mimic kind effortless expressive natural interaction humans accustomed everyday communication 
known speech gesture compliment create interface powerful modality 
integration speech gesture tangible advantages context hci especially coping complexities spatial representations 
verbal best prospects achieving effortless expressive hci 
requirements natural interactive system include ability understand multiple modalities speech gesture information distributed modalities 
date designs multimodal systems 
bolt put point point paradigm prevails 
advances including speech recognition limited domains gesture recognition limited understanding artificially imposed signs gestures corresponding author :10.1.1.81.4768
cse psu edu advanced interface technologies south allen st suite state college pa usa involve cyber gloves electronic pens 
resulting mms far satisfying naturalness interaction criterion 
instance studies pen voice interface information query manipulation electronic maps indicate linguistic patterns significantly deviated canonical english 
contrast believe fusing remotely sensed natural gestures speech adequate timely feedback large screen display users able achieve natural level interaction 
psycholinguistic studies shown natural gestures string syntactic bindings 
fall category deictic gestures opposed symbolic predefined gestures sign language 
system trained recognize unconstrained natural deictic gestures user exhibiting interacting large display 
system differs related systems gesture recognition learned statistical gesture models trained speech gesture occurrence analysis 
framework evolved number previous multimodal speech gesture interfaces 
campus map direct result weather narration keyword gesture occurrence analysis :10.1.1.26.7667
subsequent crisis management system system running single processing platform 
keyword spotting continuous speech recognition employed 
improvements addressed problems user initialization error recovery removed need user devices ceiling mounted microphone 
systems related mit smart projects microsoft easy living system 

design proposed framework main feature proposed framework speech gesture create natural interface 
system designed accommodate natural gestures speech commands experienced inexperienced user increase usability system domains user training feasible 
important aspect large screen display provide appropriate feedback user 
large screen displays natural choice applications especially interaction spatial geocentric data immersive virtual reality environments collaborative systems allow interaction multiple users simultaneously 

considerations multimodal hci problems designing mms functions outside laboratory range conceptual system design issues 
slow progress addressing conceptual problems hci inherent lack adequate user interaction data called chicken egg problem 
design natural mms statistical techniques needs valid multimodal data 
impossible collect data system exists 
solutions dilemma possible including wizard oz style experiments human scene plays role hypothetical mms 
method guarantee timely accurate system response desirable eliciting adequate user interaction 
bootstrap evolve strategies 
comparative analysis weather channel narration broadcast closely related desired type gestural interaction 
led development statistical training appropriate gesture recognition models bootstrapping stage :10.1.1.26.7667
studies revealed natural hci large display corresponding gestures keywords exhibit temporal pattern alignment helps disambiguating meaning utterances 
weather map analysis campus map 
system design perspective acknowledged resource constraints general severely limit choice sensing technology 
integration single processing unit desired compromises 
addition smooth automatic interaction initialization robust real time visual processing error recovery graceful interaction termination important ensuring user satisfaction 
unexpected system behavior leads sharp drop perceived system quality ultimately acceptance failure 
requires holistic approach multimodal hci system design comes cost system complexity integration complexity 
presents design multimodal framework attempts address issues 

system components capture speech gesture commands imap framework utilizes directional microphone single active camera 
tasks system needs perform vary time empirically evolved state transition model interaction session user system 

interaction session interaction session consists main phases 
initialization phase interaction dialogue new user system established 
followed interaction phase actual communication user system takes place 
termination phase entered user system decides conclude dialogue 
initialization phase absence users sensor range system wait state 
user detection achieved face detection 
detection face indicate presence person front system assures person looking system strong evidence users desire initiate interaction dialogue 
detection leads subsequent head tracking 
person detected system enters attraction state tries establish dialogue currently tracked people 
addition system continues detect track new arrivals 
point people leave sensor area system falls back waiting state 
placement camera waiting reset termination discard initialization attraction bootstrapping palm detection error recovery zoom user interaction interaction state transition model interaction session user system 
microphone sensor require user interact system certain location ensure optimal performance 
system encourages detected people step closer guides proper location interaction 
person stepped proper spot facing system system enters final bootstrapping state initialization phase 
system immediately discards processing available resources invested dialogue main user 
furthermore performs palm detection obtain initial location user active hand initializes algorithm 
adjusts active camera adjust exact location height size user allow optimal sensor utilization interaction phase initiated 
interaction phase interaction phase actual dialogue system user commences 
system utilizes speech recognition motion analysis gesture recognition main interaction modalities 
vision modalities mainly rely robust continuous head hand tracking motion color cues 
hand trajectory data gesture recognition module continuously extracts free hand gestures stochastic gesture models cf 

recognized gestures combined speech recognition results speech gesture modality fusion module 
semantic integration final user commands depends heavily application large screen display text speech text speech engine ibm gesture recognition control tracking control person control face detector control application front vision interface image acquire control active camera control time varying context system constrains set possible user actions increased response reliability 
termination phase hci system may gracefully terminate interaction process example user actively indicated session concluded 
difficult problem sudden termination user example user chooses walk away 
necessary constantly run diagnostic error recovery module decides user 
important decision low possible false alarm rate termination interaction session leads user confusion dissatisfaction 
termination system informs user potential session terminated 
resets internal states positions active camera wide angle initialization mode switches back initialization phase 

visual components ensure smooth progression interaction sessions outlined large number vision face detection palm detection head hand tracking speech command recognition audio feedback related components cooperate tight resource constraints single processing platform 
link system responsiveness user satisfaction mandates strict adherence maximum possible processing rate frames sec possibly fields sec respect motion tracking associated visual feedback 
systems integrated single standard pc allowable complexity motion tracking methods limited especially system latency minimized avoid sluggish interface experience 
face detection important powerful components system face detector robust user detection continuous head track status verification 
implementation neural networks favors low false positive roc speech control speech engine ibm video signal windows message microphone dome camera sensor pan tilt head overview imap system architecture 
bold framed boxes constitutes separate thread execution 
serial ports visual feedback palm detection proper camera placement suitable skin color model extracted face region strong priors placed potential appearance location user active hand view camera 
automatic palm detection rests assumption object detected small skin colored blob region slightly center respect users head 
addition palm detector favors rely occurrence motion location hand integrates evidence sequence frames cf 
details 
head hand tracking algorithms head face tracking similar slightly different approaches 
trackers rectangular tracking windows location continuously adapted kalman filters follow users head hand 
head tracker relies solely skin color image cues hand tracker continuous version palm detector geared skin colored moving objects 
prior knowledge human body utilized avoiding resolving conflicts interference head hand tracks 
tracking methods simple imaging cues extremely efficient require processing time single cpu 
continuous gesture recognition main visual interaction modality continuous gesture recognition 
previous gesture recognition systems user adhere specific predefined gestures 
trained recognize natural gestures gestures person natural tendency perform interacting large screen displays 
approach increases naturalness system tremendously 
gesture recognition component longer able solely carry complete intent user 
semantics command request distributed speech gesture modalities gesture recognition speech recognition tightly coupled extract reliable command request information 
user video stream audio stream gesture recognition context hand tracking modality fusion semantic integration display speech recognition logical flow system 
audio feedback experience examining weather narration broadcast :10.1.1.26.7667
temporally modeled deictic gestures set fundamental gesture primitives pose minimal complete basis large display interaction tasks considered applications 
prep point contour point area ret rest statistical deictic gesture model 
specifically system trained learn pointing gestures selection single item single location area gestures selection number items item extensive size area contour gestures compound point contour point gesture semantically connect selections 
statistical gesture model continuous recognition continuous observation density hidden markov models token passing described detail 
bootstrapping refinement hmm recognition network performed customized application designed pull desired gestures user 
system extracted gesture speech data automatically segmented obtained gesture training data 
training hmms isolated gesture data final embedded training compound network performed 

audio components speech recognition speech recognition improved tremendously years robust incorporation technology multimodal interfaces feasible 
system operating speaker dependent speaker independent recognition engines cf 
details 
speaker dependent systems superior performance speaker independence essential domains potential users unknown speech training infeasible 
set possible utterances defined context free grammar embedded annotations 
allows constraining necessary vocabulary understood system retaining flexibility speech commands formulated 
speech recognition module system reports timestamped annotations application front responsible modality fusion context maintenance 
audio feedback applications developed top proposed framework provide audio feedback user 
audio feedback simple sound effects confirm successful capture user commands selection noise button selected form pre recorded speech narrator text speech synthesis animated character top right 
choice appropriate feedback depends application frontend 
sound effects sufficient interactive game speaking animated avatar appropriate say shopping assistant application 

modality fusion order correctly interpret user intent utterances gestural motions modalities fused appropriately 
due statistical method employed continuous recognition speech recognition gesture recognition systems emit recognition results time delays typically sec 
verbal utterances show region detail taken typical geocentric application see associated cooccurring gestures preparation area gesture stroke retraction 
understanding temporal alignment speech gesture crucial performing association 
pen systems gesture shown occur associated deictic word investigations hci weather narration showed large screen display systems deictic word occurred gesture cases :10.1.1.26.7667
modality fusion reliably triggered occurrence verbal commands 
speech recognition system emits streams time stamped annotation embedded speech grammar case obtain zoom location region annotation location occurring corresponds occurrence time deictic keyword 
similarly gesture recognition report prep area retraction indicating area gesture recognized time interval zoom location region preparation area gesture retraction time speech gesture modality fusion 
time stamp deictic keyword windowed search gesture recognition result history performed 
past gesture stroke checked cooccurrence appropriate annotations 
example time stamps gesture stroke association keyword occurred time ts assumed tse dtb 
dtb dta constants learned training data 
approach allows occurrence keyword short time gesture longer time delay gesture 
successful association physical content area gesture hand trajectory data time interval obtain actual gesture con components compound speech gesture command 
case example area gesture circle fitted obtained gesture data order determine region screen show detail 

system integration framework requires moderate computational resources 
systems run comfortably dual pentium iii mhz correspondingly faster single processing platforms resources required system runs system modules enabled 
detailed description system components see 
systems main tasks separated set separate execution threads shown 
resources consumed vision components system especially face detection algorithm 
components run different time scales especially speech recognition face detector active camera control architecture designed take advantage multi threaded parallel execution 
communication components performed message passing straightforward thread synchronization 

case studies large number successful hci applications build framework 
systems scientific purposes user studies systems led actual commercial products operating unattended public spaces multimodal crisis management system 

geocentric systems information representation access systems geocentric nature 
large screen display systems especially suited interaction spatially geographical data 
system developed penn state imap framework campus map helped visitors find way university park campus 
system queried speech gesture commands name department get library 
multimodal crisis management system dynamic system user takes role emergency center operator speech gesture commands dispatch emergency vehicles rapidly occurring crisis centers virtually generated city 
contrast static systems progression interaction determined user operator react rapidly occurring events time pressure 
system currently conducting cognitive load studies different aspects multimodal interaction measured accurately compared traditional alternative interaction methods variable controlled conditions 
multimodal gis system dave allows interaction multiple users simultaneously 
ongoing project system extended operate multiple users simultaneous interfacing geographical information system gis 
system supports collaborative task planning decision making 

retail entertainment systems number commercial applications developed deployed public environments showing advanced multimodal hci technology reached sufficient maturity robustness extended unattended public retail entertainment environments 
commercial embodiments multimodal hci framework 
left physical structure 
top right retail system virtual avatar helps user navigation 
bottom right interactive multimodal entertainment 
system proved usability expositions interaction sessions unique users 
informal customer observations surveys showed users successful interaction experiences 
addition observations revealed system behaved specifications cases 
furthermore returning users showed dramatic increase interaction proficiency indicating initial novelty barrier overcome acceptance systems high little difficulties understanding mechanics multimodal interaction 
formal user studies currently progress 

discussion describes issues related development robust real time framework exploits natural gestures spoken command input large screen display visual feedback 
framework validated implementing number prototype systems transfer real world interactions novel metaphors bridging gap digital environments user interactions 
careful design integration due considerations possible aspects systems interaction cycle yield successful system 
room improvement exists speech recognition module perform noisy public environments advanced sound acquisition devices microphone arrays 
multiple users introduces additional challenges especially users spatially close 
system resolve ambiguity identifying attaching motion spoken command right user 
model head tracking extracting lip motion gaze tracking localize attention currently investigated improve disambiguation 
modelbased articulated tracking developed extract reliable information visual data 
prosody speech gesture analysis investigation improve continuous gesture recognition 
authors wish contributed development parts providing photos figures respectively 
financial support part national science foundation career iis nsf iis gratefully acknowledged 
bolt put voice gesture graphic interface siggraph computer graphics 
nguyen methods apparatus real time gesture recognition patent 
oviatt multimodal interfaces dynamic interactive maps proc 
conf 
human factors comp 
systems acm press pp 

cassell need know natural gesture intl 
conf 
automatic face gesture recognition keynote address 
starner pentland visual recognition american sign language hidden markov models intl 
workshop automatic face gesture recognition 
visualization space testbed multimodal user interface computer graphics vol 

sharma kim huang natural speech gesture spatial planning virtual map proc 
arl symposium 
sethi sharma natural gesture speech hci case study weather narration proc :10.1.1.26.7667
nd workshop perceptual user interface pp 

sharma map crisis management multimodal interface proc 
th arl symposium 
mit media laboratory media mit edu vismod demos demos html 
brumitt meyers krumm kern shafer easyliving technologies intelligent environments nd intl 
symp 
handheld ubiquitous computing huc bristol uk 
sharma natural gesture speech control large display engineering human computer interaction lecture notes computer science springer verlag 
kuniyoshi detecting tracking human face space varying sensor active head cvpr 
sharma imap real time framework natural multimodal interaction large screen displays dept comp 
science eng 
technical report cse pennsylvania state university may 
agrawal fuhrmann brewer sharma cai maceachren designing human centered multimodal gis interface support emergency management th acm international symposium advances geographic information systems mclean va 
rabiner tutorial hidden markov models selected applications speech recognition proceedings ieee vol 
pp 

young thornton token passing conceptual model connected speech recognition cambridge university engineering dept cued infeng tr 
continuous recognition natural hand gestures human computer interaction ms thesis pennsylvania state university 
sharma experimental evaluation vision speech multimodal interfaces workshop perceptive user interfaces acm digital library 
