line method segmentation identification non stationary time series jens kohlmorgen steven gmd ida german national research center information technology institute computer architecture software technology 
berlin germany mail gmd de web www gmd de 
method analysis non stationary time series dynamical systems switch multiple operating modes 
contrast approaches method processes data incrementally training internal parameters 
performs unsupervised segmentation classification data fly 
cases allows process incoming data real time 
main idea approach track segment changes probability density data sliding window incoming data stream 
application switching dynamical system demonstrates potential usefulness algorithm broad range applications 
alternating dynamics ubiquitous real world systems example speech data physiological recordings eeg meg industrial processes financial markets 
methods analysis time varying dynamical systems aside non stationary possibly non linear important issue application areas 
introduced annealed competition experts ace method time series non linear switching dynamics ensemble neural network predictors specializes different dynamical regimes increasing competition predictors deterministic annealing scheme 
related approaches switching dynamics 
brief review models advantages drawbacks see 
compared aforementioned different approach respects 
segmentation depend predictability system 
merely estimate density distribution data track changes 
particularly improvement systems data hard predict example eeg recordings financial data 
prediction methods possibly fail provide adequate representation underlying system yield reasonable segmentation results cases 
second main motivation line method 
incoming data stream processed incrementally keeping computational effort limited fixed upper bound algorithm able perpetually segment classify data streams fixed amount memory cpu resources 
permits continuously monitor measured data real time long sampling rate high 
main reason achieving high line processing speed fact method contrast approaches involve training iterative adaptation parameters 
optimizes segmentation fly means dynamic programming allowing automatic correction fine tuning previously estimated segmentation bounds 
algorithm consider problem continuously segmenting data stream line simultaneously labeling segments 
data stream supposed temporal structure follows supposed consist consecutive blocks data way data points block exhibit common feature belong common cluster focus stem underlying distribution 
segmentation task performed unsupervised fashion priori labels segmentation bounds 
order allow perpetual processing data potentially real time problem solved fixed amount memory computation time 
feature extraction pdfs incoming data stream analyzed 
sequence passed pre processing step filtering sub sampling 
step processing exploit idea dynamical systems theory embed data higher dimensional reported application process data hz hz including display mhz pentium iii linux expect sufficient large number applications 
long done fly case line scenario 
space principle aims reconstruct state space underlying system gamma gamma gamma parameter called embedding dimension called delay parameter embedding 
dimension vectors idea embedding measured data potentially non linear projection systems state phase space 
case embedding higher dimensional space help resolve structure data property exploited scatter plots 
embedding step perform sub sampling embedded data order reduce amount data real time processing 
step want track density distribution embedded data 
purpose estimate probability density function sliding window length standard density estimator multivariate gaussian kernels centered data points window gammaw gamma gamma oe exp gamma gamma gammaw oe kernel width oe acts smoothing parameter value important obtain representation underlying distribution 
propose choose oe proportional mean distance nearest neighbors averaged sample set measuring similarity pdfs sampled data points compute pdf eq 
compute pdf successive point time soon new data point available 
order quantify difference functions squared norm called integrated squared error ise calculated analytically mixtures gaussians case 
consider case general mixtures ff fi get gamma dx ff ff dx gamma ff fi dx fi fi dx case notation time indices refer subsampled data 
denote specific vector valued point denote vector valued variable 
integral product multivariate spherical gaussian distributions oe oe dx oe oe exp gamma gamma oe oe inserting eq 
yields particular distance function case pdfs estimated data windows oe gamma exp gamma gammaw gamma gammav oe gamma exp gamma gammaw gamma gammav oe exp gamma gammaw gamma gammav oe segmentation algorithm introduce line algorithm necessary discuss respective line algorithm 
line algorithm 
data sequence obtain corresponding sequence pdfs fp fw tg eq 

idea unsupervised segmentation find compact compressed representation sequence pdfs terms small set pdfs fq called prototypes sequence fs ng called segmentation assigns prototype pdf pdf sequence way exhibits number prototype changes gamma gamma ffi 
sequence small residual compression error sum distances pdfs corresponding prototype pdfs eq 

formulate segmentation problem terms objective cost function minimized respect follows weighting factors act regularization constants 
fact eq 
constitutes complex optimization problem 
simplify constrain set prototype pdfs subset definition usual kronecker delta function ffi ffi ffi 
pdfs obtained data fp furthermore just penalize number prototype changes explicitly total number prototypes efficiently compute global optimum simplified cost function dynamic programming 
mean assume fixed number prototypes 
restrict number resulting prototypes indirectly penalizing number segments 
words prototype segment rewarded 
problem minimizing total number prototypes approached subsequent step see labeling section 
cost function minimized simplified case fs denotes sequence pdfs set fp finding optimal segmentation respect corresponds finding optimal path matrix pairwise distances pdfs path runs left right time index selecting pdf prototype time step 
index understood state system 
denote order distinction time state 
finite number possible paths matrix principle obtain optimal path computing gamma gammaw possible paths choosing path global minimum min fo fortunately optimal sequence obtained efficiently single sweep task constitutes typical dynamic programming problem 
time step compute cost optimal path subject constraint ends state time call constrained optimal paths paths unconstrained optimum path 
iteration formulated follows initialization induction min gamma gamma ffi note generally find global minimum cases explicitly designs problem respect 
termination min ffi denotes kronecker delta function 
regularization constant interpreted transition cost switching pdf path 
optimal prototype sequence minimal cost complete series pdfs determined step obtained logging updating paths states iteration choosing minimal costs eq 

line algorithm 
step segmentation algorithm line allow incremental updates new data points arrive 
neglect stage memory cpu resources limited 
suppose processed data gamma 
new data point comes time generate new embedded vector sampled initial data points embedding new pdf sampled embedded vectors pdf window 
easily obtain new pdf distance matrix previous gamma simply computing new row column vector time distance function symmetric vectors identical left computing vector distances new pdf previous pdfs eq 

distance matrix segmentation readily updated incrementally 
newly added pdf affect optimal paths minimal costs gamma obtained far take new pdf potential state account 
updating paths principle require re computation scratch simply line algorithm 
perform simplified update reconsider cost function 
simplified original cost function eq 
order problem efficiently solvable dynamic programming 
restriction simplified cost function eq 
penalize number prototypes choose prototypes pdfs obtained data results optimal segmentations segment typically different pdf prototype taken respective segment 
prototype optimally explains pdf segment distance zero small distance neighboring pdfs partially constructed data 
inherent bias cost function choosing prototypes pdfs segment generally results temporally ascending order prototype pdfs 
vice versa path exhibits switch back previous pdf optimal respect cost function 
neglect consider paths contain switch back obtain optimal path cases 
simply reuse paths gamma new pdf state pdf prototype past segments require switch back respective path optimal 
line update time restricted paths henceforth denote tilde performed follows theta matrix dw 

compute cost gamma new state time gamma gamma compute ae min gamma gamma cg update previous optimal segmentations don need keep complete matrix repeatedly compute minimum states 
store update history optimal segmentations 

update gamma compute states compute min gamma gamma cg get optimal path min note min operation eq 
results switch state optimal gamma path current state switch back succeeding pdf 
limit memory requirements computation time 
line case algorithm shows update equations costs paths 
associated path sequences logged simultaneously computation 
note done just storing sequence switching points path 
far incremental update version segmentation algorithm needs traverse newly added row 
step column 
step distance matrix consider paths prototypes non ascending order exclude due unconstrained min operation eq 

algorithm needs amount memory cpu time increasing new data point 
note need keep full matrices update columns sufficient 
fact rarely finds deviation respect resulting path practice 
order limit resources fixed amount remove old pdfs point 
propose discarding pdfs time indices smaller equal time path associated eq 
exhibits switch back 
algorithm simply done setting case allows discard corresponding old paths 
addition initialization clause eq 
ignored cut gamma path kept compute part 
gamma assume min gamma gamma cg gamma eq 

switch back eq 
indicates new mode established path ends pdf state old distribution starts route path states represent new distribution case lower costs despite incurred additional transition 
vice versa newly obtained pdf properly represent previous mode justifies assumption gamma 
effect proposed cut strategy discard paths pdfs old modes allow find optimal pdf prototype current segment 
cut conditions occur shortly mode changes data 
mode change takes place pdfs discarded 
need set fixed upper limit number candidate paths pdfs simultaneously consideration limited resources available 
limit reached switches detected successively discard oldest path pdf stored result choosing sub optimal prototype segment 
continuous discarding enforces change prototypes time steps switching induced data 
buffer size large possible 
case buffer overflow condition recorded segmentation allows identify artificial 
labeling prototypes need labeling algorithm top segmentation algorithm identify segments stem underlying distribution similar pdf prototypes 
labeling algorithm generates labels segments assigns identical labels segments similar respect 
line clustering prototypes 
simple clustering scheme expect prototypes obtained underlying distribution separated prototypes result segmentation algorithm 
assign new label segment distance associated prototype preceding prototypes exceeds certain threshold assign existing label closest preceding prototype 
developed algorithms compute values hyperparameters oe sample set due limited space forthcoming publication 
application illustrate approach generated time series switching dynamics mackey glass delay differential equation dx dt gamma gamma gamma describes high dimensional chaotic system originally introduced model blood cell regulation 
example stationary operating modes established different delays respectively 
dynamics operates stationary mode certain number time steps chosen random time steps referring sub sampled data step size delta 
switches modes probability choosing mode 
procedure repeated times generating switching chaotic time series stationary segments 
added relatively large amount measurement noise series zero mean gaussian noise standard deviation standard deviation original series 
applied line segmentation algorithm data processing performed line full data set available case 
embedded scalar time series fly sub sampled data pdf window size 
algorithm processed data points second mhz matlab including display ongoing segmentation 
final segmentation depicted fig 

surprisingly bounds segments perfectly recovered noisy data set 
exceptions fourth segment right noticeably shorter original mode segment labeled bit long 
line labeling algorithm assigns labels 
particular segments chaotic mode get different labels 
explained fact segments comprise noisy data points data distributions small sample sets differ extent 
complex mode received labels mode label 
discussion line method unsupervised segmentation identification time series non stationary switching dynamics 
contrast approaches processes data line potentially mode mode mode labels bounds data actual modes line segmentation mackey glass segmentation switching mackey glass time series noise bottom operates different modes top 
resulting line segmentation middle receives data points mode information exhibits perfect segmentation bounds 
line labeling algorithm assigns labels desired presumably due fact segments short noisy 
real time training parameters 
method provides updates segmentation time new data point arrives 
effect past segmentation bounds labels automatically re adapted new incoming data points processed 
number prototypes labels identify segments fixed determined algorithm cases segmentation exactly respective optimal line segmentation 
expect useful applications method fields complex non stationary dynamics plays important role physiology eeg meg climatology industrial applications finance 
bellman 

dynamic programming princeton university press princeton nj 
bengio frasconi 

input output hmm architecture 
nips advances neural information processing systems eds 
tesauro touretzky leen morgan kaufmann 
bishop 

neural networks pattern recognition oxford univ press ny 


learning non stationary conditional probability distributions 
neural networks 
kehagias petridis 

time series segmentation predictive modular neural networks 
neural computation 
kohlmorgen muller pawelzik 

identification nonstationary dynamics physiological recordings biol cybern 
pawelzik kohlmorgen muller 

hidden markov mixtures experts application eeg recordings sleep 
theo biosci 
mackey glass 

oscillation chaos physiological control system 
science 
packard crutchfield farmer shaw 

geometry time series 
phys 
rev letters 
pawelzik kohlmorgen muller 

annealed competition experts segmentation classification switching dynamics 
neural comput 
ghosh 

structurally adaptive modular networks nonstationary environments 
ieee tr 
neural networks 
