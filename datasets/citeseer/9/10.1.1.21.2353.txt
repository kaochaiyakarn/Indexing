bayesian classification theory technical report fia robin hanson sterling software john stutz nasa artificial intelligence research branch nasa ames research center mail field ca usa email name ptolemy arc nasa gov peter cheeseman riacs task inferring set classes class descriptions explain data set placed firm theoretical foundation bayesian statistics 
framework various mathematical algorithmic approximations autoclass system searches probable classifications automatically choosing number classes complexity class descriptions 
simpler version autoclass applied large real data sets discovered new independently verified phenomena released robust software package 
extensions allow attributes selectively correlated particular classes allow classes inherit share model parameters class hierarchy 
summarize mathematical foundations autoclass 
task supervised classification learning predict class memberships test cases labeled training cases familiar machine learning problem 
related problem unsupervised classification training cases unlabeled 
tries predict features new cases best classification surprised new cases 
type classification related clustering useful exploratory data analysis preconceptions structures new data may hold 
previously developed reported autoclass cheeseman cheeseman unsupervised classification system bayesian theory 
just partitioning cases clustering techniques bayesian approach searches model space best class descriptions 
best classification optimally trades predictive accuracy complexity classes overfit data 
classes fuzzy case assigned class case probability member different classes 
research institute advanced computer science autoclass iii released version combines real discrete data allows data missing automatically chooses number classes principles 
extensive testing indicated generally produces significant useful results models uses example inadequate search heuristics 
autoclass iii assumes attributes relevant independent class classes mutually exclusive 
extensions embodied autoclass iv relax assumptions allowing attributes selectively correlated relevance class hierarchy 
summarizes mathematical foundations autoclass bayesian theory learning applying increasingly complex classification problems various single class models hierarchical class mixtures 
problem describe assumptions words mathematics give resulting evaluation estimation functions comparing models making predictions 
derivations results assumptions 
bayesian learning bayesian theory gives mathematical calculus degrees belief describing means beliefs consistent change evidence 
section briefly reviews theory describes approach making tractable comments resulting tradeoffs 
general bayesian agent uses single real number describe degree belief proposition interest 
assumption assumptions evidence affect beliefs leads standard probability axioms 
result originally proved cox cox reformulated ai audience heckerman describe theory 
theory denote evidence known potentially known agent denote hypothesis specifying world particular state sets possible evidence possible states world mutually exclusive exhaustive sets 
example coin headed possible states world ordinary coin headed coin 
toss possible evidence lands heads lands tails 
general denotes real number describing agent degree belief conjunction propositions conditional assumption propositions true 
propositions side conditioning bar arbitrary boolean expressions 
specifically prior describing agent belief absence seeing evidence hje posterior describing agent belief observing particular evidence likelihood embodying agent theory see possible evidence combination possible world consistent beliefs non negative ajb normalized 
agent sure world state evidence observed 
likelihood prior give joint probability eh normalizing joint gives bayes rule tells beliefs change evidence hje eh eh set possible hs continuous prior differential sums replaced integrals 
similarly continuous es differential likelihood dl real evidence deltae finite probability deltal dl deltae de theory agent needs situation choose set states associated likelihood function describing evidence expected observed states set prior expectations states collect relevant evidence 
bayes rule specifies appropriate posterior beliefs state world answer questions interest 
agent combine posterior beliefs utility states says prefers possible state choose action maximizes expected utility eu practice practice theory difficult apply sums integrals involved mathematically intractable 
approximations 
approach 
consider possible states world focus smaller space models analysis conditional assumption world really described models space 
modeling assumption certainly false analysis tractable 
time effort models complex expanding model space order reduce effect simplification 
parameters specify particular model split sets 
set discrete parameters describe general form model usually specifying functional form likelihood function 
example specify variables correlated classes classification 
second free variables general form magnitude correlation relative sizes classes constitute remaining continuous model parameters generally prefer likelihood ts mathematically simple embodies kinds complexity believe relevant 
similarly prefer simple prior distribution js model space allowing resulting integrals described approximated 
prior predicts different parameters independently product terms different parameter helps 
prefer prior broad uninformative possible software different problem contexts principal add specific domain knowledge appropriate prior 
prefer prior gives nearly equal weight different levels model complexity resulting significance test 
adding parameters model induces cost paid significantly better fit data complex model preferred 
integrable priors broad containing meta parameters specify part model space focus prior expectations focus 
cases cheat simple statistics collected evidence going help set priors example see sections 
joint written dj ev js ts js reasonably complex problem usually rugged distribution immense number sharp peaks distributed widely huge high dimensional space 
despair directly normalizing joint required bayes rule communicating detailed shape posterior distribution 
break continuous space regions surrounding sharp peak search tire combinations rt marginal joint ert js dj ev js large possible 
best models rt reported usually certain probable models remain 
model rt reported describing marginal joint ert js discrete parameters estimates typical values region mean estimate dj ev js ert js note variable sits probability expression proposition stands proposition variable particular value 
cheating prior supposed independent evidence 
dj ev js maximum estimates invariant space depend syntax likelihood expressed peak usually sharp differences don matter 
reporting best models usually justified models weaker usually orders magnitude probable best 
main reason reporting models best show range variation models judge different better models 
decision searching better models rt current best principled way estimates longer take find better model better model 
fact data value unknown informative model unknown just possible discrete data value likelihood unknown value just sum possible known values 
predictions resulting models reasonable approximation average answer best peaks weighted relative marginal joints 
weight usually best justifying neglect rest 
tradeoffs bayesian theory offers advantages theoretically founded empirically tested berger offers clear procedure turn crank modulo doing integrals search deal new problem 
machinery automatically trades complexity model fit evidence 
background knowledge included input output flexible mixture different answers clear founded decision theory berger help output 
disadvantages include forced explicit space models searching discipline 
deal difficult integrals sums huge literature help 
search large spaces technique joint probability provides local evaluation function 
clear take computational cost doing bayesian analysis account infinite regress 
perceived disadvantages bayesian analysis really problems practice 
ambiguities choosing prior generally serious various possible convenient priors usually disagree strongly regions interest 
bayesian analysis limited traditionally considered statistical data applied space models world 
general discussion issues see cheeseman illustrate general approach applying problem unsupervised classification 
model spaces overview conceptual overview deal attribute value relational data 
example medical cases described medical forms standard set entries slots 
slot filled elements known set simple values numbers colors blood types 
deal real discrete attributes 
explain data consisting number classes corresponds differing underlying cause symptoms described form 
example different patients fall classes corresponding different diseases suffer 
bayesian analysis need vague notion precise choosing specific mathematical formulas say particular combination evidence 
natural way say certain number classes random patient certain probability come patients distributed independently know underlying classes learning patient doesn help learn patient 
addition need describe class distributed 
need single class model saying evidence know class patient comes 
build multiclass model space pre existing model space arbitrarily complex 
fact spend describing various single class models 
general complex class need invoke multiple classes explain variation data 
simplest way build single class model predict attribute independently build attribute specific models 
class distribution attribute know class case learning values attribute doesn help predict value attributes 
real attributes standard normal distribution characterized specific mean variance mean 
discrete attributes standard multinomial distribution characterized specific probability possible discrete value 
point described model space autoclass iii 
autoclass iv goes introducing correlation inheritance 
correlation introduced removing assumption attributes independent class 
simplest way real attributes discrete attributes 
standard way real attributes multivariate normal basically says set attributes define linear combinations attributes vary independently normal distributions 
simple way discrete attributes define super attribute possible values possible principle prevents bayesian analysis complex model spaces predict relational data 
combinations values attributes 
attributes ways add correlation introduce great parameters models making complex usual priors preferable simpler independent models 
really want simpler models allow partial covariance 
simplest way say class attributes clump blocks inter related attributes 
attributes block attributes blocks 
build block model space covariant model spaces 
simpler form covariance introduces parameters independent case class set parameters multiple classes penalized strongly 
attributes irrelevant classification medical patient favorite color particularly costly 
reduce cost allow classes share specification parameters associated independent blocks 
irrelevant attributes shared classes minimum cost 
allow arbitrary combinations classes share blocks simpler organize classes leaves tree 
block placed node tree shared leaves node 
way different attributes explained different levels abstraction hierarchy 
medical patients tree viral infections near root predicting specific viral disease near leaves predicting disease specific symptoms 
irrelevant attributes favorite color go root 
notation summary models considered evidence consist set cases associated set attributes size case attribute values ik include unknown 
example medical case number described age blood type sections describe applications bayesian learning theory various kinds models explain evidence simple model spaces building complex spaces 
single class 
single attribute considered multiple independent attributes fully covariant attributes selective covariance 
section combine single classes class mixtures 
table gives overview various spaces 
space describe continuous parameters discrete model parameters normalized likelihoods dl ts priors js 
spaces discrete parameters region allowing usually ignore parameters 
approximations resulting marginals ert js estimates derived 
terms general functions may reused 
ap note script letters sets matching ordinary letters denote size 
comments algorithms computational complexity 
likelihood functions considered assume cases independent ts jv ts need give jv ts space fx xik single class models single discrete attribute sd discrete attribute allows finite number possible values unknown usually treated just possible value 
set independent coin tosses example heads tails unknown 
assumption sd discrete attribute parameters continuous parameters consisting likelihoods jv sd possible value coin example say coin unbalanced percent chance coming heads time 
gamma free parameters normalization requires 
likelihood matters data number cases value ffi coin example number heads 
sums called sufficient statistics summarize information relevant model 
choose prior js db jl gamma gamma gamma dq special case beta distribution berger gamma gamma function spiegel 
formula parameterized hyperparameter set different values specify different priors 
set simple problem maximum marginal gamma gamma gamma gamma abstracted function refer 
prior chosen form similar likelihood conjugate prior mean estimate particularly simple abstracted 
note similar classical estimate defined 
hash table results computed order numerical steps independent note ffi uv denotes 
space description subspaces compute time sd single discrete sr single real oe independent attrs sd sr ik sd covariant discrete ik sr covariant real sigma kk block covariance bk sb sd sr nk ik sm flat class mixture ff sc ik sh tree class mixture ff sc ik table model spaces single real attribute sr real attribute values specify small range real line center precision deltax assumed smaller scales interest 
example weight measured sigma 
scalar attributes positive weight best logarithm variable aitchison brown sr real attribute assume standard normal distribution sufficient statistics data mean geometric mean precision deltax deltax standard deviation gamma consists model mean deviation oe likelihood standard normal distribution 
dl jv sr oe gamma gamma oe dx example people weight distributed mean deviation 
real data finite width replace dx deltax approximate likelihood deltal jv sr deltax dl jv sr deltax dx dl jv sr 
usual choose priors treat parameters independently 
js jsr oejs choose prior mean flat range data jsr dr gamma maxx gamma minx general uniform distribution dr gamma dy gamma gamma gamma flat prior preferable non informative note order normalizable cheat information data cut point 
single attribute case similarly choose flat prior log oe 
oejs dr log oe jlog delta log min deltax delta gamma gamma posterior just peak region resulting marginal gamma gamma log delta min deltax deltax gamma delta note joint dimensionless 
estimates simply computation takes order steps compute sufficient statistics 
independent attributes introduce notation collecting sets indexed terms ik single term inside fg denote set indexed terms collected indices fx ik fx ik kg 
collect indices ik fx evidence single case simplest way deal cases having multiple attributes assume independent treating attribute separate problem 
case parameter set partitions parameter sets oe depending discrete real 
likelihood prior joint multiple attributes simple products results attribute sd sr jv ik jv js js ejs js ik evidence associated attribute estimates jes je exactly 
computation takes order ik steps 
fully covariant sd model space sd allows set discrete attributes fully contribute likelihood non trivial combinations obtained treating combinations base attribute values particular values super attribute values large number 
consists terms indexed attributes 
generalizes ffi xik transformation likelihoods look sd ik js db fq fi computation takes order ik steps 
model example single combined hair color attribute allow correlation people blond blue 
fully covariant reals sr assume sr set real valued attributes follow multivariate normal distribution replace oe model covariance matrix sigma kk data covariance matrix kk ik gamma ik gamma sigma kk symmetric sigma kk sigma positive definite satisfying kk sigma kk vector likelihood set attributes dl jv sr dn sigma kk gamma kk xk gamma sigma inv kk gamma sigma kk dx multivariate normal dimensions 
choose prior takes means independent independent covariance js sigma kk js js estimates means remain choose prior sigma kk inverse wishart distribution mardia kent bibby sigma kk js dw inv sigma kk fg kk jg kk gamma sigma kk gammah gammak gamma gamma kk sigma inv kk inv kh gamma gamma gammaa kk sigma kk defined page 
sigma inv ab denotes matrix inverse sigma ab satisfying sigma inv ab sigma bc ffi ac sigma ab denotes components matrix determinant sigma abg 
normalized integrates sigma kk symmetric positive definite 
conjugate prior meaning resulting posterior sigma kk take mathematical form prior 
choice resulting integrals manageable requires choose components kk choose prior broad possible kk cheat choose kk kk ffi kk order avoid overly distorting resulting marginal gamma gammaa gamma gammaa gamma jg kk jis kk kk gamma deltax delta estimates sigma kk kk kk gamma gamma ffi kk gamma kk choose kk large dominates estimates kk small marginal small 
compromise estimate marginal somewhat effect seen previous data agrees data 
note estimates undefined 
computation takes order steps 
lack satisfactory way approximate marginal values unknown 
block covariance sv just having full independence full dependence attributes prefer model space combinations attributes may remain independent 
allows avoid paying cost specifying covariance parameters buy significantly better fit data 
partition attributes blocks full covariance block full independence blocks 
presently lack model allowing different types attributes attributes block type 
real may mutually 
away models partial dependence trees chow liu described pearl choose approach includes limiting cases full dependence full independence 
evidence partitions block wise ik fe sufficient statistics parameters partition parameters fq sigma kk 
block treated different problem discrete parameters specify attributes specifying blocks fk attributes block 
likelihood jv tsv jv sb simple product block terms sb sd sr assuming full covariance block estimates je sb 
choose prior predicts block structure fk independently parameters independent block js fk js js results similarly decomposed marginal js fk js js choose block structure prior fk js kr br kd bd kr set real attributes br number real blocks similarly kd bd 
says equally blocks number blocks possible way group attributes equally 
normalized gamma gamma gamma gamma 
gamma 
gives number ways partition set elements subsets 
prior prefers special cases full covariance full independence fewer ways block combinations 
example comparing hypothesis attribute separate block independent hypothesis particular pair attributes block size prior penalize covariance hypothesis proportion number pairs possible 
prior includes significance test covariance hypothesis chosen added fit data extra covariance overcome penalty 
computation takes order nk ik steps number search trials done quitting gamma 
complete search space 
average search trials attributes block size real attributes unity discrete attributes 
class mixtures flat mixtures sm model spaces sc sv thought describing single class extended considering space sm simple mixtures classes titterington shows model sc fit set artificial realvalued data dimensions 
model space likelihood jv tsm ff jv sc sums products class weights ff give probability case belong class classes class likelihoods describing members class distributed 
limit large model space general able fit distribution arbitrarily closely correct 
autoclass iii finds classes plot attributes vs vs artificial data set 
oe deviation ovals drawn centers classes 
parameters ft fff fv combine parameters class parameters describing mixture 
prior similarly broken js db fff jc js just arbitrary choice broad prior integers 
ff treated choice class discrete attribute 
added classes distinguishable priori 
simple problems resulting joint dj ev js local maxima focus regions space 
find local maxima em algorithm dempster fact maxima class parameters estimated weighted sufficient statistics 
relative likelihood weights ic ff jv sc jv tsm give probability particular case member class weights satisfy ic case really belong classes 
weights break case fractional cases assign respective classes create new class data ik ik ic new weighted class sufficient statistics obtained weighted sums ic sums example ic kc ic ik ic ffi xik deltax kc deltax ik ic substituting statistics previous class likelihood function sc gives weighted likelihood jv sc associated new estimates marginals 
maxima weights ic consistent estimates ff je sc ff 
reach maxima start random seed repeatedly current best estimates compute ic ic re estimate stopping predict 
typically takes gamma iterations 
procedure converge starting point converges slowly near peak second order methods 
integrating joint done directly product sum full likelihood hard decompose fractional cases approximate likelihood jv ff jv sc ff jv sc ic holding ic fixed get approximate joint ert js fi js standard search procedure combines explicit search random search parameters 
trial begins converging classes built random case pairs 
chosen randomly log normal distribution fit cs gamma best trials seen far trying fixed range cs start 
developed alternative search procedures selectively merge split classes various heuristics 
usually better worse 
marginal joints different trials generally follow log normal distribution allowing estimate search longer take average find better peak better 
simpler model space smi sc computation order nick averages search trials 
number possible peaks immense number usually computation examines 
covariant space smv sc sv ik 
class hierarchy inheritance sh class mixture model space sm generalized hierarchical space sh replacing set classes tree classes 
leaves tree corresponding previous classes inherit specifications class parameters higher closer root classes 
purposes parameters specified class classes class pool weight big class 
parameters associated irrelevant attributes specified independently root 
shows class tree time sc sv better fit data 
see hanson stutz cheeseman comparison 
tree classes root class class parent class class child classes cj index ranges children class 
child class weight ff cj relative siblings ff cj absolute weight ff cj ff cj ff ff 
approaches inheritance possible class associated set attributes predicts independently likelihood jv sc class predicts 
avoid having redundant trees autoclass iv finds class tree theta better lists attribute numbers denote covariant blocks class ovals indicate leaf classes 
describe likelihood function empty non leaves 
need ensure attributes predicted leaf class 
call set attributes predicted class start recursively partition attributes kept class predicted directly remaining attributes predicted child ac cj leaves expressed terms leaves likelihood mixture jv tsm ff pp jv sc allowing em procedure find local 
case weights ci wc cj ri sum flat mixture case define class statistics ik ci 
choose similar prior specify js sh db ff cj jj sc sh gamma 
gamma 
ffi rc subsets size range gamma ffi cr gamma replaced ffi note prior recursive prior class depends attributes chosen parent class 
prior says possible number attributes kept equally number kept particular combination equally 
prior prefers simpler cases offers significance test 
comparing hypothesis attributes kept class hypothesis particular attribute kept class prior penalizes hypothesis proportion number attributes kept 
marginal joint ert js sh cj js estimates je sc ff cj cj 
general case sc sv computation takes ik average number classes hierarchy 
usually number leaves model sh typically cheaper compute sm number leaves 
searching complex space challenging 
great search dimensions trade simplicity fit data begun explore possible heuristics 
blocks merged split classes merged split blocks promoted demoted class tree em iterations continued farther try random restart seek new peak 
simplest approaches searching general model space better smarter searches simpler spaces 
bayesian approach unsupervised classification describes class likelihood function free parameters adds parameters describe classes combined 
prior expectations parameters combine evidence produce marginal joint ert js evaluation function classifications region near local maxima continuous parameters choice discrete model parameters evaluation function optimally trades complexity model fit data guide open ended search best classification 
applied theory model spaces varying complexity unsupervised classification 
space provides likelihood prior marginal joint estimates 
provide information allow reproduce autoclass evaluation functions contexts models relevant 
aitchison brown aitchison brown 
lognormal distribution 
university press cambridge 
berger berger 
statistical decision theory bayesian analysis 
springer verlag new york 
cheeseman peter cheeseman james kelly matthew self john stutz taylor don freeman 
autoclass bayesian classification system 
proceedings fifth international conference machine learning 
cheeseman peter cheeseman matthew self james kelly john stutz taylor don freeman 
bayesian classification 
seventh national conference artificial pages saint paul minnesota 
cheeseman peter cheeseman 
finding probable model 
langley eds computational models discovery theory formation pages 
morgan kaufmann palo alto 
cox cox 
probability frequency reasonable expectation 
american journal physics 
dempster dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society series 
hanson stutz cheeseman hanson stutz cheeseman 
bayesian correlation inheritance 
th international joint artificial intelligence pages sydney 
heckerman david heckerman 
probabilistic interpretations mycin certainty factors 
shafer pearl eds readings uncertain reasoning pages 
morgan kaufmann san mateo 
mardia kent bibby mardia kent bibby 
analysis 
academic press new york 
pearl pearl probabilistic reasoning systems 
morgan kaufmann san mateo california 
spiegel murray spiegel 
mathematical handbook formulas tables 
mcgraw hill new york 
titterington titterington smith makov 
statistical analysis finite mixture distributions 
john wiley sons new york 

