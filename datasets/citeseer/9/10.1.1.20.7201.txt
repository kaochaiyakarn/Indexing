aggregating learned probabilistic beliefs maynard reid ii computer science department stanford university stanford ca cs stanford edu computer science department stanford university stanford ca cs stanford edu consider task aggregating beliefs experts 
assume beliefs represented probability distributions 
argue evaluation aggregation technique depends semantic context task 
propose framework assume nature generates samples true distribution different experts form beliefs subsets data chance observe 
naturally optimal aggregate distribution learned combined sample sets 
formulation leads natural way measure accuracy aggregation mechanism 
show known aggregation operator linop ideally suited task 
propose linop learning algorithm inspired techniques developed bayesian learning aggregates experts distributions represented bayesian networks 
show experimentally algorithm performs practice 
belief aggregation subjective probability distributions subject great interest statistics see gz cw artificial intelligence pw machine learning ensemble learning particular especially probabilistic distributions increasingly medicine fields encode knowledge experts 
unfortunately aggregation proposals lacked sufficient semantical underpinnings typically evaluating mechanism satisfies properties justified little intuition 
noted fields belief revision cf 
fh appropriateness properties depends particular context 
take semantic approach aggregation describe realistic framework experts sources learn probability distributions data standard probabilistic learning techniques 
assume decision maker dm traditional name aggregator wants aggregate set learned distributions 
framework suggests natural optimal aggregation mechanism construct distribution learned sources data sets available dm 
original data sets generally available aggregation mechanism come close possible reconstructing data sets learning combined set 
intuition consider task creating expert system specialized medical field 
take advantage expertise doctors working field 
doctors sharpened knowledge patients 
doctors longer recall specifics case formed years fairly accurate models domain represented sets conditional probabilities 
fact expert systems created years eliciting conditional probabilities experts hhn 
course doctor seen patients doctors saw ideal expert system result eliciting model 
isn expert 
system benefit incorporating knowledge experts find 
system account differing levels experience different doctors may practiced longer 
best known aggregation operators linear opinion pool linop aggregates set distributions weighted sum 
shown statistics community intuitive assumptions learning joint distribution combined data set equivalent linop individual joint distributions learned individual data sets 
weights typical uses linop criticized ad hoc framework prescribes semantically justified weights estimated percentages data source saw 
intuitively high weight means believe source seen relatively large amount data reliable 
joint distributions hardly preferred representation probabilistic beliefs real world domains 
bns aka belief networks pea gained popularity structured representations probability distributions 
allow distributions represented compactly avoiding exponential blowup memory size inference complexity 
assume sources beliefs bns learned data 
semantics aggregate bn learn combined sets data 
describe linop bn aggregation algorithm inspired algorithm designed learn bns data 
algorithm uses sources distributions samples search possible bn structures parameter settings 
takes advantage marginalization property linop computation efficient 
explore algorithm behavior running experiments known real life alarm network smaller artificial asia network ls 
formal preliminaries restrict attention domains discrete variables 
consider compute aggregate distribution accuracy computation depends know sources 
formally consider setting sources discrete random variables variable domain dom 
follow convention capital letters denote variables lowercase letters denote values 
symbols bold denote sets 
set possible worlds defined value assignments variables 
true distribution model world 
source data set sampled unknown 
assume finite size corresponding empirical frequency distribution source learns distribution model world 
combined set samples size corresponding empirical distribution dm constructs aggregate distribution optimal aggregate distribution posited distribution dm learn unrealistic expect dm access sources sample sets consider information sources learned distributions approximate specifically consider situation dm knows sources distributions estimate percentage combined set samples source observed learning method 
number assumptions 
assume samples noisy corrupted complete missing values 
second assume individual sample sets disjoint 
implies concatenation equals don concern repeats aggregating 
assumption appropriate 
invalidated multiple sources observe event 
interesting domains property holds 
example motivating medical domain doctors seen disjoint sets patients 
third assume sources believe samples iid independent identically distributed 
machine learning algorithms practice commonly rely assumption 
assume samples combined set sampled iid 
assumption may appear overly restrictive glance 
may preclude common situation sources receive samples different subpopulations 
example doctors different parts world characteristics patients see different 
fact accomodate situation framework assuming distribution domain variables source variable takes different sources values means source observed instantiated domain variables 
generalized distribution sampled iid 
consists subset samples necessary keep values computing give results learning distributions complete samples marginalizing samples iid different subpopulation distributions possible captured different conditional probability distributions domain variables distinct values aggregating learned joint distributions consider case sources learned joint distributions aggregate joint 
learning joint distributions review samples variable goal learner estimate probability occurences value setting domain parameters need learned jw 
distribution parameterized 
standard approaches maximum likelihood estimation mle maximum posteriori estimation map 
implications formulation assumption disjoint implicit approach approaches mle learner chooses member specified family distributions maximizes likelihood data definition random variable dom fx mle distribution data set mle arg max easy show mle distribution empirical distribution samples iid 
map learning hand follows bayesian approach learning directs put prior distribution value parameter wish estimate 
treat parameters random variables define probability distribution 
formally joint probability space includes data parameters 
definition random variable dom fx map distribution data set prior distribution map appropriate conjugate prior variables multinomial distributions dirichlet 
dir hyperparameter 
assume dirichlet distributions assessed method equivalent samples prior distribution estimated sample size simply 
parameterize map definition random variable dom fx probability distribution map denotes distribution map dir 
omit argument mle map notation understood 
linop review turn problem aggregation 
show joint aggregation essentially reduces linop 
linop proposed stone sto generally attributed laplace 
aggregates set joint distributions weighted sum definition probability distributions pl non negative parameters linop operator defined linop pl linop popular practice simplicity 
described gz number attractive properties unanimity linop returns non input followed marginalization property aggregation marginalization commutative operators 
linop dismissed aggregation communities normative aggregation mechanism primarily fails satisfy number properties deemed necessary reasonable aggregator external property aggregation conditioning commute preservation shared independences 
furthermore typical approaches choosing weights criticized ad hoc 
dismissal may overly hasty 
linop proves operator looking framework equivalent having dm learn combined data set intuitive assumptions 
mle aggregation suppose sources dm mle learners 
known statistics time dm need compute linop sources distributions 
proposition win mor mle lg mle linop pl 
straight forward proposition illuminating 
weight corresponding source clear meaning percentage total data seen source 
dm needs provide accurate estimates percentages 
high weight indicates dm believes source seen relatively large amount data reliable 
address common criticism linop weights chosen ad hoc fashion 
known dm compute number samples pl 
linop viewed essentially storing sufficient statistics dm learning problem 
easy see property preservation independence hold semantics 
framework sources strong beliefs independences believed independence depends fits source data 
independence preservation property take account possibility limited data sources may learned independences justified data taken account 
consider example distribution variables ab ab 
obviously independent 
suppose sources received set samples distribution consists ab ab consists ab ab 
suppose mle learn distribution independent distributions 
linop distribution hand effectively takes account evidence seen sources computes variables independent 
map aggregation mle learners known problems overfitting low probability events data materialized 
map learning better job dealing problems especially data sparse 
consequently suppose sources dm map learners dirichlet priors 
optimal aggregate distribution variation linop proposition suppose lg map map 
pl term equation dm map estimation second term accounts sources priors subtracting effect 
corollary suppose lg map map 
lim linop pl large linop distribution approaches surprising known mle learning map learning dirichlet priors asymptotically equivalent 
implication large need know aggregate need know priors sources 
approximate aggregate distribution linop distribution approximation improve samples seen sources 
aggregating learned bayesian networks bayesian networks bns structured representations probability distributions 
bn consists directed omit proofs lack space 
acyclic graph dag nodes random variables 
parents node denoted pa pa denotes particular assignment pa 
structure network encodes marginal conditional independencies distribution 
associated node conditional probability distribution cpd pa 
consider case sources beliefs represented bns learned data 
briefly review techniques learning bns data 
detailed presentation see hec 
learning bayesian networks review structure network known task reduces statistical parameter estimation mle map 
case complete data likelihood function entire bn conveniently decomposes structure network maximize likelihood parameter independently 
structure network known apply bayesian model selection 
precisely define discrete variable states correspond possible models possible network structures encode uncertainty probability distribution 
model define continuous vector valued variable instantiations correspond possible parameters model 
encode uncertainty probability distribution 
score candidate models evaluating marginal likelihood data set model bayesian score practice approximation bayesian score 
commonly mdl score converges bayesian score data set large 
mdl score defined score mdl pa pa log jpa log dim dl dim number independent parameters graph dl description length finding network structure highest score shown np hard general 
resort heuristic search 
search easily get stuck local maximum add random restarts process 
bn learning algorithm 
interested learning bns joint 
pick random dag 
parameterize form 
score 
loop 
dag differing adding removing reversing edge 
parameterize form 
score 
pick highest score replace score score 
change 
return bayesian network learning algorithm 
distributions 
obvious reasons concerning compact representation efficient inference distribution learned bn algorithm may closer original distribution generate data place 
note networks parameterized represent exactly mle map learned joint distributions general fully connected 
intuitively distribution learned finite sample data little noisy true independences look slight dependences mathematically 
result bns interested sources dm exact representations independencies mle map learned distributions account overfitting 
bn learning stretches distribution best fits data match candidate network structures 
structure look best producing highest score parameterization structure 
score balances fit data model complexity 
linop aggregation algorithm suppose source learned bn dag mdl score dm bns semantics aggregate bn close possible dm learn apply bn learning algorithm directly don data sources learn models 
simple solution generate samples source model train dm combined set 
algorithm simple raises new questions 
clear samples generate source 
possibility number estimated number samples source learn model 
number small samples represent generating distribution adequately introducing additional noise process 
generate samples source saw increasing proportionally preserve settings give weight mle component score possibly choosing suboptimal network 
fact experiments described section show algorithm badly practice 
adapt bn learning algorithm sources distributions samples 
main difference way compute mle map parameters structure consider way compute score lines 
algorithm relies observation necessary actual data learn bn sufficient empirical distribution 
demonstrated section come said distribution applying linop operator distributions learned sources 
take advantage marginalization property linop computation efficient 
noted pw parameterize network top fashion computing distribution roots joints second layer variables parents conditional probabilities computed dividing appropriate marginals bayes law 
cases require local computations sources bns 
mdl score requires knowing empirical distribution empirical distribution linop distribution weights chosen correctly sources mle map assuming sufficient data learning possible score candidate networks having actual data 
furthermore marginals mle score family marginals 
previous parameterization step done computing marginals computed 
mdl score requires knowledge dependence may strong especially large case second term dominated likelihood term factor common networks ignored 
rough approximation suffice 
traditional bn learning caching parameterization scoring neighboring networks efficient 
making local changes structure parameters need updating 
arc added removed need recompute new parameters child node arc switched need recompute parameters nodes involved 
linop marginals don change caching computed values may help speed computations 
experiments implemented bn aggregation algorithm matlab kevin murphy bayes net toolbox explored behavior running experiments known reallife alarm network node network part system monitoring intensive care patients smaller node artificial asia network ls 
experiments learned source bns data sampled original bn aggregated results algorithm aggr 
sources dm map parameterize networks 
computing linop weights 
compared proposal accuracy learning combined data sets opt plotting kullback leibler kl divergence distribution true distribution different values jdj 
sensitivity considered situation dm knows priors sources adjusts unduly large number imaginary samples 
sources dms dirichlet prior defined uniform distribution estimated sample size 
varied total number samples having sources see number samples cases different numbers 
conducted multiple runs setting averaged 
plots averages alarm network sources equal due software limitations start structure search fully disconnected graph random restarts larger network 
seen spite limited search algorithm fairly far coming close optimal improving sources 
surprisingly kl divergence drops total number samples increases 
furthermore experiments sources different showed dependence performance algorithm relative difference ran similar experiments asia 
varied number samples runs setting 
run random restarts 
plots average setting 
plot shows able explore search space sufficiently learning aggregation algorithms algorithm consistently improves sources closely approximates optimal 
available www cs berkeley edu bnt html 
kl divergence distribution defined log opt aggr opt aggr sensitivity alarm network results 
asia network results 
sensitivity dm estimation hypothesized earlier actual value dm estimate matter 
demonstrate ran experiments asia network similar leaving fixed varying dm estimate order magnitude figures summarizes results 
approximation orders magnitude provides improvement sources 
estimates complexity penalty sufficiently strong select dags fewer arcs original data 
hand overestimating increase kl distance original danger extreme overestimates causing overfitting 
find increase complexity aggregate networks order magnitude range considered remained arcs average 
summarizing results shows log opt aggr log kl divergence opt aggr kl divergence opt aggr asia network results varying dm estimate 
varying dm estimate 
different subpopulations 
predicted range slack increases samples seen sources important accuracy dm estimate 
subpopulations algorithm performs combining source distributions learned samples different subpopulations 
show modified asia network accomodate sources doctor practicing san francisco practicing cincinnati 
probability distributions root nodes asia network representing patient visited asia significantly different doctors 
patient san francisco smoker cincinnati visited asia 
added source variable described section gave sources equal priors seeing patients source variable parent root variables gave appropriate cpds 
drew samples extended network source learn appropriate subset aggr combine results correct plots kl divergence distribution original distribution source variable marginalized 
sources learning distributions different subpopulations learn relatively far distribution 
dm takes advantage information sources learns bn approximates original closely source 
comparison sampling algorithm experiments compared performance algorithm alternative intuitive algorithm samp described section sample samples source bn learn bn combined data 
samp badly general consistently worse aggr worse sources order magnitude 
related wealth exists statistics aggregating probability distributions 
surveys field include gz cw 
earlier axiomatic approaches suffered lack semantical grounding 
reason community moved modeling approaches 
studied approach introduced win formally established mor mor 
dm prior variables domain possible beliefs sources 
aggregates bayesian conditioning incorporate information receives sources 
fact proposition derives body 
restricted aggregating beliefs represented point probabilities odds joint distributions 
interest particularly ai problem aggregating structured distributions including ma ma pw 
early axiomatic approaches statistics focuses attempting satisfy properties preserving shared independences runs impossibility results consequence 
sense doing viewed ensemble learning bns 
ensemble learning involves combining results different weak learners improve classification accuracy 
simplicity linop justification actual combination 
results justify weak learners mle map bn learning 
new area ai bears similarities line incremental learning bns bun lb fg 
continuous stream samples want maintain bn learned data seen far 
stream long generally possible maintain full set sufficient statistics 
approaches range approximating sufficient statistics restricting network learned 
essentially assuming sufficient statistics data seen source encoded network 
cross fertilization fields may prove profitable 
new approach belief aggregation 
believe formulate problem precisely measure success different techniques answering questions way sources beliefs formulated 
argued framework sources assumed learned distributions data intuitively plausible leads natural formulation optimal dm distribution learned combined data sets natural success measure distance generating true distribution 
observation linop appropriate operator framework sources dm mle learners linop algorithm aggregate beliefs represented bayesian networks 
preliminary results show algorithm performs 
direction involve finding ways relax various assumptions 
example extend framework allow continuous variables allow dependence sources sample sets 
framework dm completely ignores sources priors 
may appropriate priors known unreliable uninformative 
priors real applications informative 
second direction involve finding valid ways advantage sources priors improve quality aggregation 
example sources dirichlet priors dm trusts estimated sample sizes may chose incorporate estimate maynard reid ii partially supported national physical science consortium fellowship 
supported air force contract darpa task program 
beinlich suermondt chavez cooper 
alarm monitoring system 
proc 
european conf 
ai medicine 
bun buntine 
theory refinement bayesian networks 
proc 
uai pages 
cw clemen winkler 
combining probability distributions experts risk analysis 
risk analysis 
fg friedman 
sequential update bayesian network structure 
proc 
uai pages 
fh friedman halpern 
belief revision critique 
proc 
kr pages 
gz genest 
combining probability distributions critique annotated bibliography 
statistical science 
hec heckerman 
tutorial learning bayesian networks 
technical report msr tr microsoft research 
hhn heckerman horvitz 
normative expert systems part pathfinder project 
methods information medicine 
kullback 
information theory statistics 
wiley 
lb lam bacchus 
learning bayesian belief networks approach mdl principle 
computational intelligence 
ls lauritzen spiegelhalter 
local computations probabilities graphical structures application expert systems 
royal statistical society series methodological volume pages 
ma abramson 
topological fusion bayes nets 
proc 
uai pages 
ma abramson 
complexity considerations combination belief networks 
proc 
uai pages 
mor morris 
decision analysis expert 
management science 
mor morris 
combining expert judgements bayesian approach 
management science 
mor morris 
axiomatic approach expert resolution 
management science 
pea pearl 
probabilistic reasoning intelligent systems 
morgan kaufmann 
pennock maynard reid ii giles horvitz 
normative examination ensemble learning algorithms 
proc 
icml pages 
pw pennock wellman 
graphical representations consensus belief 
proc 
uai pages 
sto stone 
opinion pool 
annals mathematical statistics 
win robert winkler 
consensus subjective probability distributions 
management science october 
