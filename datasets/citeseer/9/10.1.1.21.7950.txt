probabilistic analysis rocchio algorithm tfidf text categorization thorsten joachims universitat dortmund fachbereich informatik lehrstuhl str 
dortmund germany thorsten ls informatik uni dortmund de rocchio relevance feedback algorithm popular widely applied learning methods information retrieval 
probabilistic analysis algorithm text categorization framework 
analysis gives theoretical insight heuristics rocchio algorithm particularly word weighting scheme similarity metric 
suggests improvements lead probabilistic variant rocchio classifier 
rocchio classifier probabilistic variant naive bayes classifier compared text categorization tasks 
results show probabilistic algorithms preferable heuristic rocchio classifier founded achieve better performance 
text categorization process grouping documents different categories classes 
amount online information growing rapidly need reliable automatic text categorization increased 
text categorization techniques example build personalized netnews filter learn news reading preferences user lang 
index news stories hayes guide user search world wide web joachims 
widely applied learning algorithms text categorization rocchio relevance feedback method rocchio developed information retrieval 
originally designed optimizing queries relevance feedback algorithm adapted text categorization routing problems 
algorithm intuitive number problems show lead comparably low classification accuracy objective rocchio algorithm maximize particular functional introduced section 
rocchio show maximizing functional lead high classification accuracy 
heuristic components algorithm offer design choices little guidance applying algorithm new domain 
algorithm developed optimized relevance feedback information retrieval clear heuristics best text categorization 
major heuristic component rocchio algorithm tfidf term frequency inverse document frequency salton buckley word weighting scheme 
different flavors heuristic lead multitude different algorithms 
due heuristic class algorithms called tfidf classifiers 
theoretically founded approach text categorization provide naive bayes classifiers 
algorithms probabilistic models classification allow explicit statement simplifying assumptions 
contribution probabilistic analysis tfidf classifier 
analysis implicit assumption tfidf classifier explicit naive bayes classifier 
furthermore provides insight tfidf algorithm improved leading probabilistic version tfidf algorithm called prtfidf 
prtfidf optimizes different design choices tfidf algorithm gives clear recommendations set parameters involved 
empirical results categorization tasks show prtfidf enables better theoretical understanding tfidf algorithm performs better practice conceptually computationally complex 
graphics baseball specs hockey car clinton unix space quicktime computer 
xxx sciences edu newsgroups comp graphics subject need specs apple qt need get specs quicktime 
technical articles nice 
verbose interpretation specs unix ms dos system 
quicktime stuff need specs usable magazines books bag words representation attribute value style 
structured follows 
section introduces definition text categorization 
tfidf classifier naive bayes classifier described section 
section presents probabilistic analysis tfidf classifier states implications 
empirical results sections 
text categorization goal text categorization classification documents fixed number predefined categories 
working definition assumes document assigned exactly category 
put formally set classes set training documents furthermore target concept maps documents class 
known documents training set 
supervised learning information contained training examples find model hypothesis approximates function defining class learned hypothesis assigns document classify new documents 
objective find hypothesis maximizes accuracy percentage times agree 
learning methods text categorization section describes general framework experiments defines particular tfidf classifier naive bayes classifier 
tfidf classifier provides basis analysis section 
representation representation problem strong impact generalization accuracy learning system 
categorization document typically string characters transformed representation suitable learning algorithm classification task 
ir research suggests words representation units ordering document minor importance tasks 
leads representation documents bags words 
bag words representation equivalent attribute value representation machine learning 
distinct word corresponds feature number times word occurs document value 
shows example feature vector particular document 
avoid unnecessarily large feature vectors words considered features occur training data times 
set considered features words called learning algorithms tfidf classifier type classifier relevance feedback algorithm originally proposed rocchio rocchio vector space retrieval model salton 
due heuristic components number similar algorithms corresponding particular choice heuristics 
main design choices ffl word weighting method ffl document length normalization ffl similarity measure 
overview heuristics salton buckley 
popular combination known tf word weights salton buckley document length normalization euclidian vector length cosine similarity 
originally developed information retrieval algorithm returns ranking documents providing threshold define decision rule class membership 
algorithm adapted text categorization 
variant straightforward adaptation rocchio algorithm text categorization domains categories 
algorithm builds representation documents 
document represented vector jf documents similar content similar vectors fixed similarity metric 
element represents distinct word document calculated combination statistics tf df salton 
term frequency tf number times word occurs document document frequency df number documents word occurs 
inverse document frequency idf calculated document frequency 
idf log jdj df jdj total number documents 
intuitively inverse document frequency word low occurs documents highest word occurs 
called weight word document tf delta idf word weighting heuristic says word important indexing term document occurs frequently term frequency high 
hand words occur documents rated important indexing terms due low inverse document frequency 
learning achieved combining document vectors prototype vector class normalized document vectors positive examples class negative examples class summed 
prototype vector calculated weighted difference 
ff jc jj djj gamma fi jd gamma gammac jj djj ff fi parameters adjust relative impact positive negative training examples 
recommended buckley ff fi 
set training documents assigned class jj djj denotes euclidian length vector additionally rocchio requires negative elements vector set 
cosine similarity metric ff fi rocchio shows prototype vector maximizes mean similarity positive training examples prototype vector minus mean similarity negative training examples prototype vector jc cos gamma jd gamma gammac cos unclear maximizing functional connects accuracy resulting classifier 
resulting set prototype vectors vector class represents learned model 
model classify new document document represented vector scheme described 
classify cosines prototype vectors calculated 
assigned class document vector highest cosine 
argmax cos returns argument maximum category algorithm assigns document algorithm summarized decision rule argmax jj jj delta jj jj argmax jf delta jf normalization length document vector left influence argmax 
naive bayes classifier classifier section uses probabilistic model text 
model strong simplification true process text generated hope captures important characteristics 
word unigram models text words assumed occur independently words document 
jcj models category 
documents assigned particular category assumed generated model associated category 
describes approach estimating pr jd probability document class bayes rule says achieve highest classification accuracy assigned class pr jd highest 
argmax pr jd pr jd split considering documents separately length pr jd pr jd delta pr pr equals length document zero 
applying bayes theorem pr jd write pr jd pr jc delta pr jl pr jc delta pr jl pr jc probability observing document class length pr jl prior probability document length class assume category document depend length pr jl pr 
estimate pr pr calculated fraction training documents assigned class pr jc jc jc jdj jc denotes number training documents class jdj total number documents 
estimation pr jc difficult 
pr jc probability observing document class consider documents length simplifying representation huge number different documents impossible collect sufficiently large number training examples estimate probability prior knowledge assumptions 
case estimation possible due way documents assumed generated 
unigram models introduced imply word occurrence dependent class document comes occurs independently words document dependent document length 
pr jc written pr jc jd pr jc ranges sequence words document element feature vector jd number words document estimation pr jc reduced estimating pr jc independently 
bayesian estimate pr jc 
pr jc tf jf jf tf tf number times word occurs documents class estimator called laplace estimator suggested vapnik pages 
assumes observation word priori equally 
bayesian estimator works practice falsely estimate probabilities zero 
weaker assumption linked dependence sufficient cooper considered simplicity 
resulting decision rule equations combined 
argmax pr delta jd pr jc pr delta jd pr jc argmax pr delta pr wjc tf pr delta pr wjc tf pr jd needed measure confidence denominator left change argmax 
prtfidf probabilistic classifier derived tfidf analyze tfidf classifier probabilistic framework 
propose classifier called prtfidf show relationship tfidf algorithm 
terms design choices listed show prtfidf algorithm equivalent tfidf classifier settings ffl word weighting mechanism uses refined idf weight especially adapted text categorization ffl document length normalization done number words ffl similarity measure inner product 
researchers proposed theoretical interpretations vector space retrieval model bookstein wang tfidf word weighting scheme wong yao wu salton 
analyzes parts tfidf algorithm information retrieval text categorization 
prtfidf algorithm naive bayes classifier proposed previous section provided estimate probability pr jd document class making simplifying assumption word independence 
prtfidf algorithm uses different way approximating pr jd inspired retrieval probabilistic indexing rpi approach proposed fuhr 
approach set descriptors represent content documents 
descriptor assigned document certain probability pr xjd 
theorem total probability line bayes theorem line write pr jd pr jx delta pr xjd pr djc pr pr jx delta pr xjd estimation tractable simplifying assumption pr djc pr 
pr jd pr jx delta pr xjd validity assumption depends classification task choice set descriptors states descriptor provides information information document gained category account 
mentioned set descriptors part design 
pragmatic choice consider bags words feature set potential descriptors bags containing words number words parameter controls quality approximation versus complexity estimation 
way looking equation especially suited choice considered 
pr jd approximated expectation pr jx consists sequence words drawn randomly document interpretations underlying assumption text documents highly redundant respect classification task sequence words document equally sufficient classification 
example classifying documents cooking recipes probably equally sufficient know sentences document 
jdj pr jd equals pr jx decreasing simplifying assumption independence assumption naive bayes classifier violated practice 
simplification worth trying starting point 
simplest case lead tfidf classifier introduced section 
line written pr jd pr jw delta pr wjd remains estimate probabilities line 
pr wjd estimated representation document pr wjd tf tf tf jdj jdj denotes number words document pr jw remaining part equation probability correct category know randomly drawn word bayes formula rewrite pr jw pr jw pr wjc delta pr pr wjc delta pr previous section pr estimated fraction training documents assigned class pr jc jc jc jdj pr wjc estimated pr wjc jc pr wjd resulting decision rule prtfidf argmax pr wjc delta pr pr wjc delta pr delta pr wjd connection tfidf prtfidf section show relationship prtfidf classification rule tfidf algorithm section 
start decision rule prtfidf transform shape tfidf classifier 
equation argmax pr delta pr wjc pr delta pr wjc delta pr wjd term pr delta pr wjc equation re expressed modified version inverse document frequency idf 
definition inverse document frequency stated section idf log jdj df df ae contains introduce refined version idf suggested prtfidf algorithm 
idf sqrt jdj df df tf jdj differences definition idf usual 
df argmax jc jdj delta jc delta tf jdj delta idf delta tf jd argmax jc jdj delta jc delta tf delta idf jdj delta tf delta idf jd number documents occurrence word sum relative frequencies document 
idf frequency information just considering binary occurrence information 
dynamics df df similar 
word occurs corpus higher df df 
dynamics different case small fraction documents word occurs frequently 
df rise faster df 
second difference square root dampen effect document frequency logarithm 
functions similar shape reduce impact high document frequencies 
replacing probabilities estimators expression pr delta pr wjc reduced function idf 
pr delta pr wjc jc jdj delta jc delta tf jdj jdj delta tf jdj tf jdj jdj tf jdj jdj idf substituting probabilities estimators decision rule rewritten 
extracting prototype vector component document representation component get decision rule argmax delta jd jc jdj delta jc delta jdj tf delta idf form decision rule previous lines easy see prtfidf decision rule equivalent tfidf decision rule modified inverse document frequency weight idf number words document length normalization inner product measuring similarity 
furthermore suggests set parameters ff fi 
category ff jc jdj fi 
implications analysis analysis shows preconditions tfidf classifier fits probabilistic framework 
prtfidf classifier offers new view vector space model tfidf word weighting heuristic text categorization advances theoretical understanding interactions 
analysis suggests improvements tfidf algorithm changes lead better classifier 
prtfidf implementation tfidf incorporating changes ffl incorporation prior probabilities pr ff 
ffl idf word weighting idf 
ffl number words document normalization euclidian length 
ffl inner product computing similarity 
experiments experiments performed find far implications theoretical analysis lead improved classification algorithm practice 
performances prtfidf tfidf naive bayes classifier bayes compared categorization tasks 
data sets newsgroup data data set consists usenet articles ken lang collected different newsgroups table 
articles taken newsgroups comp graphics sci electronics comp windows sci crypt comp os ms windows misc sci space comp sys mac hardware sci med comp sys ibm pc hardware misc talk politics guns alt atheism talk politics mideast rec sport baseball talk politics misc rec sport hockey talk religion misc rec autos soc religion christian rec motorcycles table usenet newsgroups newsgroup data 
prtfidf bayes tfidf newsgroups acq wheat crude earn table maximum accuracy percentages 
total documents collection 
small fraction articles document belongs exactly newsgroup 
task learn newsgroup article posted results reported dataset averaged number random test training splits binomial sign tests estimate significance 
experiment data testing 
reuters data reuters data collected carnegie group reuters newswire 
averaging categories presents detailed analysis categories frequent categories earn acq categories special properties wheat crude 
wheat crude category narrow definitions 
classifying document contains word wheat yields accuracy wheat category 
category acq corporate acquisitions example obvious definition 
concept number words reasonable predictors 
articles cross posted newsgroups 
cases predicting newsgroups counted correct prediction 
training examples newsgroups prtfidf bayes tfidf accuracy versus number training examples newsgroup data 
training examples reuters acq subsampled prtfidf bayes tfidf accuracy versus number training examples reuters categories acq 
experiments articles appeared april training set 
articles appeared test set 
results corpus training examples test examples 
tfidf classifier principled way dealing uneven class distributions allow fair comparison data subsampled randomly equal number positive negative examples 
results averaged number trials binomial sign tests estimate significance 
experimental results table shows maximum accuracy learning method achieves 
newsgroup data prtfidf performs significantly better bayes bayes significantly better tfidf 
compared tfidf prtfidf leads reduction error 
training examples reuters wheat subsampled prtfidf bayes tfidf accuracy versus number training examples reuters categories wheat 
training examples reuters crude subsampled prtfidf bayes tfidf accuracy versus number training examples reuters categories crude 
prtfidf bayes outperform tfidf reuters categories acq wheat crude 
comparing prtfidf bayes bayes tends better tasks certain single keywords high prediction accuracy tasks wheat crude 
opposite true prtfidf classifier 
achieves comparable performance performance gains bayes categories acq newsgroup data 
behaviour interesting plausible different simplifying assumptions prtfidf bayes 
classifiers perform approximately category earn 
figures show accuracy relation number training examples 
expected accuracy increases number training examples 
holds learning methods categorization tasks 
differences quickly accuracy increases 
contrast bayes training examples reuters earn subsampled prtfidf bayes tfidf accuracy versus number training examples reuters categories earn 
training examples reuters subsampled prtfidf bayes tfidf accuracy versus number training examples reuters categories 
prtfidf particularly newsgroup experiment small numbers training examples 
performance bayes approaches prtfidf high numbers stays tfidf small training sets 
accuracy tfidf classifier increases steeply number training examples compared probabilistic methods 
reuters category acq bayes prtfidf show nearly identical curves 
tfidf significantly probabilistic methods spectrum 
tasks wheat crude classifiers perform similar small training sets difference generally increases increasing number training examples 
shows relationship text classifiers vector space model tfidf word weighting probabilistic classifiers 
presents probabilistic analysis particular tfidf classifier describes algorithm basic techniques statistical pattern recognition probabilistic classifiers bayes 
analysis offers theoretical explanation tfidf word weighting heuristic combination vector space retrieval model text categorization gives insight underlying assumptions 
drawn analysis lead prtfidf classifier eliminates inefficient parameter tuning design choices tfidf method 
prtfidf classifier easy empirical results classification tasks support applicability real world classification problems 
tfidf method showed reasonable accuracy classification tasks probabilistic methods bayes prtfidf showed performance improvements reduction error rate tasks 
empirical results suggest probabilistically founded modelling preferable heuristic tfidf modelling 
probabilistic methods preferable theoretical viewpoint probabilistic framework allows clear statement easier understanding simplifying assumptions 
relaxation combination assumptions provide promising starting points research 
tom mitchell inspiring comments 
sebastian thrun sengers sean slattery ralf klinkenberg peter suggestions regarding ken lang dataset parts code experiments 
research supported arpa number carnegie mellon university 
bookstein bookstein explanation generalization vector models information retrieval salton schneider research development information retrieval berlin 
buckley buckley salton allan effect adding relevance information relevance feedback environment international acm sigir conference pages 
cooper cooper inconsistencies probabilistic information retrieval international acm sigir conference pages 
fuhr fuhr models retrieval probabilistic indexing information processing management pages 
hayes hayes news story categorization system second conference applied natural language processing pages 
joachims joachims freitag mitchell webwatcher tour guide world wide web international joint conference artificial intelligence ijcai 
lang lang newsweeder learning filter netnews international conference machine learning 
rocchio rocchio 
relevance feedback information retrieval salton smart retrieval system experiments automatic document processing chapter pages prentice hall 
salton salton developments automatic text retrieval science vol 
pages 
salton buckley salton buckley term weighting approaches automatic text retrieval information processing management vol 
pages 
vapnik vapnik estimation dependencies empirical data springer 
wang wang wong yao analysis vector space models computational geometry international acm sigir conference 
wong yao wong yao note inverse document frequency weighting scheme technical report department computer science cornell university 
wu salton wu salton comparison search term weighting term relevance vs inverse document frequency technical report department computer science cornell university 
