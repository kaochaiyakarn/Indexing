learning algorithms classification comparison handwritten digit recognition yann lecun jackel bottou corinna john denker harris drucker isabelle guyon urs eduard sackinger patrice simard vladimir vapnik bell labor holmdel 
nj usa gmail yann research att com compare performance se era classifier algorithms standard database handwritten digits 
consider raw accuracy training time recognition time memory requirements 
available report measurements fraction patterns rejected remaining patterns misclassification rates threshold 

great strides achieved pattern recognition years 
particularly striking results attained area handwritten digit recognition 
rapid progress resulted combination number developments including proliferation powerful inexpensive computers invention new algorithms take advantage computers availability large databases characters training testing 
bell laboratories developed suite classifier algorithms 
contrast relative merits algorithms 
addition accuracy look measures affect implementation training time run time memory requirements 

databases describing databases benchmarks past 
began research character recognition assembled database digits 
known database 
database consist ed examples digit different writers 
writers cooperative friends researchers instructed write character box 
database available researchers different institutions europe 
usually data writers comprised training set second writers testing 
quickly discovered data easy recognizers soon achieved better accuracy test set 
abandoned leaning comparison handwritten digit recognition neural eds 
oh cho 
word scientific 
pp bottou 

denker 
gu 
jackel lecun ller 
sackinger simard 
vapnik 
database 
occasionally groups 

zipcode database order obtain database typical real world applications contacted postal service consultants arthur little washington dc 
acquired database training test digits clipped images handwritten 
digits machine segmented zipcode string automatic algorithm 
segmented characters included extraneous ink omitted critical fragments 
segmentation errors resulted characters appeared mislabeled 
example vertical fragment appear mislabeled 
characters comprised test set limited attainable accuracy 
improve accuracy recognizers removing worst offenders training set order maintain objectivity kept characters test set 
database cleaned training data test data served time standard internal benchmarking 
postal service requested distribute database usps arthur little supplied researchers unsegmented database derived 
segmenting done users hand 
common database available meaningful comparisons 
shortcoming database relatively small size training test sets 
recognizers improved soon realized starved training data better results larger training set size 
size test set problem 
test error rates moved range errors uncomfortable large statistical uncertainty caused small sample size 

nist test responding community need better benchmarking national institute standards technology nist provided database handwritten characters cd roms 
nist organized competition data training data known nist special database test data known net test data 
competition completed competitors see achieved error rates validation sets drawn training data performance test data worse 
nist disclosed training set test set representative different distributions training set consisted characters written paid census workers test set collected characters written uncooperative high school students 
examples training test sets shown 
notice test images contain ambiguous patterns 
disparity distributions certainly possible real world application prudent usually possible guard 
general expect best test results recognizers tuned kind data encounter deployed 
fig 

typical images nist training set typical images nist test set 
subtle serious problem arises having training test data belonging different distributions 
machine learning techniques principles structural risk minimization capacity roughly speaking number free parameters classifier adjusted match quantity complexity training data 
difference distributions full machine learning tool set nist data partitioned way 

modified nist mnist training test sets reasons described nist data provide large training test sets share distribution 
describe new database created 
original nist test contains digit images written different writers 
contrast training set blocks data writer appeared sequence data nist test set scrambled 
writer identities test set available information writers 
split nist test set characters written writers went new training set 
remaining writers placed test set 
sets nearly examples 
new training set completed examples old nist training set starting pattern full set training patterns 
similarly new test set completed old training examples starting pattern full set test patterns 
experiments described test images full training samples 
images size normalized fit pixel box preserving output units weights input units fig 

linear classifier 
input unit pixel value contributes weighted sum unit 
output unit largest sum indicates class input digit 
output aspect ratio 
experiments images slanted characters straightened moment inertia methods 
experiments centered larger input field center mass grayscale pixel values reduce effects aliasing 
methods lenet tangent distance subsampled versions images pixels 
training test sets benchmarks described 
call mnist data 

classifiers section briefly describe classifiers study 
complete descriptions readers may consult 

baseline linear classifier possibly simplest classifier consider linear classifier shown 
input pixel value contributes weighted sum output unit 
output unit highest sum including contribution constant indicates class input character 
kind classifier weights lo biases number input pixels 
experiment images mnist ch 
network free parameters 
linear problem weight values determined uniquely 
deficiencies linear classifier documented included simply form basis comparison sophisticated classifiers 
mnist data linear classifier achieved error test set 

baseline nearest neighbor classifier simple classifier nearest neighbor classifier euclidean distance measure input images 
classifier advantage training time nd brain part designer required 
memory requirement recognition time large complete pixel training images megabytes byte pixel megabytes bits pixel available run time 
compact representations devised modest increase recognition time error rate 
previous case images 
mnist test error 
naturally realistic euclidean distance nearest neighbor system operate feature vectors directly pixels systems operate directly pixels result useful baseline comparison 

large fully connected multi layer neural network classifier tested fully connected multi layer neural network layers weights hidden layer 
network implemented music supercomputer purposes comparison numbers quoted figures equivalent times spare lo trained various numbers hidden units 
images input 
best results mnist test set obtained network approximately weights 
remains somewhat mystery networks large number free parameters manage achieve reasonably low error rates test set comparing size number training samples appear grossly parameterized 
classical feed forward multilayer networks possess built self regularization mechanism 
conjecture due nature error surface gradient descent training invariably goes phase weights small 
theoretical analyses confirm sara solla personal communication 
due fact origin weight space weights zero saddle point attractive direction 
small weights cause sigmoids operate quasi linear region making network essentially equivalent low capacity single layer network 
learning proceeds weights grow progressively increases effective capacity network 
better theoretical understanding phenomena empirical evidence definitely needed 
lenet solve dilemma small networks learn training set large networks design specialized network architectures specifically designed recognize dimensional shapes digits eliminating irrelevant distortions variability 
considerations lead idea convolutional known advantages performing shape recognition detecting combining local features 
require network constraining connections layers local unit takes input local receptive field layer 
furthermore salient features distorted character displaced slightly position typical character feature appear different locations different characters 
feature detector useful part image useful parts image 
specifying knowledge performed forcing set units located different places image identical weight vectors 
outputs set units constitute feature map 
sequential implementation scan input image single unit local receptive field store states unit corresponding locations feature map 
operation equivalent convolution small size kernel followed squashing function 
process performed parallel implementing feature map plane share single weight vector 
units feature map constrained perform operation different parts image 
interesting side effect weight sharing technique reduce greatly number free parameters large number units share weights 
addition builds certain level shift invariance system 
practice multiple feature maps extracting different features types image needed 
important stress weights network trained gradient descent 
computing gradient done slightly modified version classical backpropagation procedure 
idea local convolutional feature maps applied subsequent hidden layers extract features increasing complexity abstraction 
interestingly higher level features require precise coding location 
reduced precision position advantageous slight distortion translation input reduced effect representation 
feature extraction layer network followed additional layer performs local averaging subsampling reducing resolution feature map 
subsampling layers introduce certain level invariance distortions translations 
resulting architecture bi pyramid loss spatial resolution input feature maps output fig 

architecture lenet 
plane represents map set units weights constrained identical 
input images sized fit pixel field blank pixels added border field avoid edge effects convolution calculations 
feature maps due subsampling partially compensated increase number feature types 
important stress weights network adaptive 
training process causes convolutional networks automatically synthesize features 
convolutional network architecture lenet shown trained mnist database 
lenet small input field images sampled pixels centered input layer 
multiply add steps required evaluate lenet convolutional nature keeps number free parameters 
lenet developed version usps database size tuned match available data 
mnist lenet achieved error 

lenet experiments lenet clear larger convolutional network needed optimal large size mnist set 
lenet designed address problem 
expanded version lenet input layer mnist images centered center mass includes feature maps additional layer hidden units fully connected layer features maps put units 
lenet requires multiply add steps free parameters 
lenet achieves error mnist test 

lenet lenet architecture similar lenet feature maps larger fully connected layer uses distributed representation encode categories output layer traditional code 
lenet total connections free parameters layers 
non images centered center mass training set augmented distorted versions original characters 
distorted characters automatically generated small randomly chosen affine transformations shift scaling rotation skewing 
achieves error mnist test 

boosted lenet years ago schapire proposed methods called boosting building committee learning machines provide increased accuracy compared single machine 
drucker expanded concept developed practical algorithms increasing performance committee learning machines 
basic method works follows machine trained usual way 
second machine trained patterns filtered machine second machine sees mix patterns machine got right got wrong 
third machine trained new patterns second machines disagree 
testing drucker method machines shown unknown character output scores added highest total score indicating classification 
notice machine version lenet error rate means enormous amount data filtered glean classified patterns train second machine complex lenet 
data required train third machine 
mnist database insufficient data train machines 
lenet unlimited number training patterns generated distorting training data set affine transformations line thickness variations 
choice distortions effect builds knowledge character recognition training process 
trick composite machine consisting versions lenet trained 
attained test error rate best classifiers 
glance boosting appears require times time perform recognition single machine 
fact simple trick additional computation cost factor 
usually machine classifies patterns high confidence outputs machines need evaluated 

tangent distance classifier tdc tdc memory nearest neighbor classifier test patterns compared labeled prototype patterns training set 
class training pattern closest test pattern indicates class test pattern 
key performance determine close means character images 
naive approach nearest neighbor classifiers euclidean distance simply take squares difference values corresponding pixels test image prototype pattern 
flaw approach apparent misalignment identical images lead large distance 
standard way dealing problem hand crafted feature extractor enhance dissimilarity patterns different classes decrease variability class 
simard coworkers modified distance measure making invariant small distortions including line thickness variations translations rotations scale change consider image point high dimensional pixel space dimensionality equals number pixels evolving distortion character traces curve pixel space 
taken distortions define low dimensional manifold pixel space 
small distortions vicinity original image manifold approximated plane known tangent plane 
simard excellent measure closeness character images distance tangent planes 
tangent distance high accuracy classifier crafted postal data 
tangent distance tested mnist images downsampled pixels 
error rate achieved 
prefiltering techniques simple euclidean distance multiple resolutions allowed reduce number necessary tangent distance calculations 
storage requirement assumes patterns represented multiple resolutions byte pixel 

lenet nearest neighbors alternative smart distance measures tdc seek change representation euclidean distance measure pattern similarity 
realized penultimate layer lenet units create feature vector appropriate euclidean distance search 
features test error attained improvement plain lenet 

local learning lenet bottou vapnik employed concept local learning attempt get higher classifier accuracy 
observed lenet family classifiers performs poorly rare atypical patterns interpreted behavior capacity control problem 
modeling capacity network large areas input space patterns rare small areas rx class class fig 

optimal margin classifier transform patterns input space linearly separable new space linearly separable 
shows input space class class linearly separable 
shows transformed space separation possible 
patterns plentiful 
alleviate problem proposed retrain simple linear classifier time new test pattern 
linear classifier trained patterns training set closest current test pattern producing ephemeral local linear model decision surface 
linear classifier operates feature vectors produced penultimate layer lenet 
order control capacity linear classifiers imposed weight decay parameter parameters ij determined cross validation experiments 
previous experiment usps da local learning method improve test error rate original lenet mnist test set 

optimal margin classifier optimal margin classifier method constructing complex decision rules group pattern classification problems 
digit recognition classifiers constructed checking presence particular digit 
way constructing complex decision surfaces transform input patterns higher dimensional vectors simple linear classifier transformed space 
simple example transformation shown 
classical transformation consists computing products input variables 
linear classifier space corresponds polynomial decision surfaces degree input space 
unfortunately impractical input dimension large small idea certain linear decision surfaces transformed space interesting ones maximum distance convex hulls classes 
interestingly planes bk expressed linear ions generalized dot products incoming pattern subset training patterns called support vectors 
support vectors transformed space illustrated class class fig 

support patterns filled squares circles defining decision boundary subset training patterns squares circles 

resulting architecture viewed layer neural network weights layer units support vectors 
units compute dot product input weight pass result non linear transformation ic th degree polynomial surface transformation simply elevation kth power 
products linearly combined produce output 
finding support vectors coefficients amounts solving high dimensional quadratic minimization problem linear constraints 
seen memory technique involves comparing patterns set prototypes expensive pure nearest neighbor subset training set stored prototypes case 
original algorithm developed guyon vapnik succeeds training set linearly separable transformed space 
extended version technique called soft margin classifier proposed vapnik cover non separable case allows labeling errors training set 
smc yields test error mnist data 

discussion summary performance classifiers shown figures 
shows raw error rate classifiers example test set 
classifiers exception simple linear classifier test set boosted lenet clearly best achieving score closely followed lenet 
compa ed estimate human performance 
interestingly substituting layer lenet powerful classifiers change raw accuracy 
illustrates measure accuracy number patterns test error fig 

performance classifiers mnist test set 
uncertainty quoted error rates 
rejected attain fig 

percent test patterns rejected achieve error remaining test examples test set rejected attain error remaining test examples 
applications rejection performance significant raw error rate 
boosted lenet best score 
enhanced versions lenet better original lenet raw accuracy identical 
classification speed prime importance 
shows time required spare method recognize test pattern starting size normalized pixel map image 
see enormous variation speed 
memory method slower neural networks 
times shown represent reasonably optimized code running general purpose hardware 
special purpose hardware higher speeds attained provided hardware matches algorithm 
single board hardware designed lenet mind performs recognition characters set 
cost recognition time seconds fig 

time required spare recognition single character starting pixel map image 
training time days fig 

training time days spare effective hardware implementations memory techniques elusive due enormous memory requirements 
measure practical significance time required train classifiers 
enhanced lenet training time time required train basic lenet produces feature vectors 
algorithms significant variation training time 
shows required training spare measured days 
important note training time interesting measure designers irrelevant customer 
shows measure performance memory requirements various classifiers 
figures bit pixel representations prototypes nearest neighbors byte pixel soft margin tangent distance 
taken upper bounds clever compression data memory requirements megabytes ij fig 

memory requirements classification test patterns 
numbers bit pixel nn byte pixel soft margin tangent distance byte pixel rest 
elimination redundant training examples reduce memory requirements methods 
memory requirements neural networks assume bytes weight bytes prototype component lenet memorybased hybrids experiments show byte weights significant change error rate 
high accuracy classifiers lenet requires memory 
real world applications require multi character recognizer 
implemented single character recognizer score character candidates provided heuristic segmenter 
graph search find best consistent interpretation 
recognizers designed trained find correct character discussed correct segmentation 
achieve recognizer able classify pieces ink resulting erroneous segmentations non characters 
big advantage neural networks trained loop simultaneously recognize characters reject non characters 
addition segmentation errors cause small pieces broken characters sent recognizer scoring 
pieces size normalized individually may turned shapes recognizer may trouble telling apart real characters 
characters individually normalized normalization takes place string level 
causes large variations position size individual characters recognizer 
convolutional neural nets particularly handling variations 

snapshot ongoing 
expect continued changes aspects recognition technology remain valid time 
performance depends factors including high accuracy low run time low memory requirements reasonable training time 
computer technology improves larger capacity recognizers feasible 
larger recognizers turn require larger training sets 
lenet appropriate tao available technology years ago just lenet appropriate 
years ago recognizer complex lenet required months training data available considered 
quite long time lenet considered state art 
local learning classifier optimal margin classifier tangent distance classifier developed improve lenet succeeded 
turn motivated search improved neural network architectures 
search guided part estimates capacity various learning machines derived measurements training test error large mnist database function number training examples 
discovered capacity needed 
series experiments architecture combined analysis characteristics recognition errors lenet lenet crafted 
find boosting gives substantial improvement accuracy relatively modest penalty memory computing expense 
distortion models increase effective size data set data 
optimal margin classifier excellent accuracy remarkable high performance classifiers include knowledge geometry problem 
fact classifier just image pixels encrypted fixed random permutation 
slower memory hungry convolutional nets 
technique relatively new room improvement 
plenty data available methods attain respectable accuracy 
neural net methods require considerable training time trained networks run faster require space memory techniques 
neural nets advantage striking training databases continue increase size 


guyon vapnik training algorithm optimal margin classi krs proceedings fifth annual workshop computational learning theory pittsburgh 

corinna vladimir vapnik soft margin classifier machine learning appear 

john denker christopher burges image segmentation recognition mathematics induction ed addison wesley 


duda hart pattern classification scene analysis chapter john wiley sons 

geman bienenstock doursat neural networks bias variance dilemma neural computation 

drucker schapire simard boosting performance neural networks international journal pattern recognition artificial intelligence 

guyon personnaz dreyfus denker lecun comparing different neural net architectures classifying handwritten digits proc 
ijcnn ii washington dc 
ieee 

lecun denker henderson howard hubbard jackel handwritten digit recognition back propagation network touretzky ed advances neural information processing systems morgan kaufman 

bottou vladimir vapnik local learning algorithms neural computation 

bernhard kohler achieving supercomputer performance neural net simulation array digital signal processors ieee micro magazine october 

minsky papert perceptrons mit press cambridge mass 


eduard sackinger hans peter graf system high speed pattern recognition image analysis proc fourth international conference microelectronics neural networks fuzzy systems ieee 

schapire strength weak learnability machine learning 

patrice simard yann lecun john denker pattern recognition new transformation distance neural information processing systems morgan kaufmann 

vapnik estimation dependencies empirical data springer verlag 
