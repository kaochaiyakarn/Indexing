bayesian learning syntax semantics interface small number examples pairs novel verb language learners learn syntactic semantic features 
syntactic semantic bootstrapping hypotheses rely cross situational observation hone ambiguity single observation 
cast distributional evidence scenes syntax unified bayesian probablistic framework 
previous approaches modeling lexical acquisition framework uniquely models learning small number pairs utilizes integrates syntax semantic evidence reconciling apparent tension syntactic semantic approaches robustly handles noise prior acquired knowledge distinctions explicit specification hypothesis space prior likelihood probability distributions 
learning word syntax semantics small number examples scene utterance pairs novel word child determine range syntactic constructions novel word appear inductively generalize scene instances covered concept represented pinker 
inherent semantic syntactic referential uncertainty single pair established siskind 
contrast multiple scene utterance pairs language learners reduce uncertainty semantic features syntactic features associated novel word 
verbs exemplify core problems referential uncertainty 
verbs selectively participate different alternation patterns cues inherent semantic syntactic features levin 
features words acquired positive evidence scene utterance pairs 
syntactic bootstrapping hypothesis gleitman learners exploit distribution syntactic frames constrain possible semantic features verbs 
learner hears frames form rarely hears learner high confidence infer verb class niyogi niyogi mit edu massachusetts institute technology cambridge ma usa fill sort argument structure 
different distribution informs learner different verb class 
considerable evidence mounted support hypothesis fisher 
contrast semantic bootstrapping hypothesis pinker learners common scenes constrain possible word argument structures 
learner sees liquid undergoing location change uttered verb class pour sort meaning 
hypotheses require distribution observations 
prior accounts model word learning ignored essential role syntax word learning siskind tenenbaum xu require thousands training observations regier enable learning 
bayesian model learning syntax semantics verbs overcomes barriers demonstrating word concept mappings achieved little evidence evidence information scenes syntax 
bayesian learning features illustrate approach bayesian analysis single feature 
accounts verbs possess cause feature may valued harley depending value cause feature verb may appear frame externally caused ex touch load touched glass 
glass touched 
externally ex break fill broke glass 
glass broke 
internally caused ex laugh glow children 
children 
assuming analysis learners hear utterances containing novel verb knowing value cause feature choose distinct hypotheses 
clearly utterance uniquely determine value feature learners hear ved feature sup ports similarly learners hear ved feature may 
utterances determine feature uniquely 
learners receive supporting uniquely 
may accidentally receive utterances form resolving ambiguity 
learners received utterances form overwhelming support respectively far 
bayesian analysis renders analysis precise quantitative 
knowledge encoded core components structure hypothesis space prior probability hi hypothesis hi learners provided evidence likelihood observing evidence particular hi hi 
evidence 
xn independent observations bayes rule posterior probability particular hypothesis hi hi xj hi hi 
xn signaling support particular hypothesis hi evidence case xj observation syntactic frame distribution syntactic frames 
simple prior probability model hi hypotheses equally encoding verb equally touch laugh break class likelihood model xj hi encoding observe frames different feature values cause xj xj xj xj xj xj likelihood model says verb cause expect frames form ved time verb cause expect ved time verb cause expect syntactic frames 
prior probability model likelihood model stipulated encoding learner prior knowledge grammar 
probability models allows explicit computation support hypothesis 
suppose learner receives 
support hypotheses may computed number situations may analyzed evidence evidence situation consistent observation nearly twice 
observations situation observations situation increasingly correct hypothesis 
evidence situation contrast far evidence situation 
frame noise frame followed representative frames situation 
framework just observations sufficient informed judgement 
note additional observation increases certainty noise handled gracefully 
modeling semantic bootstrapping section extend single feature analysis multiple features feature represents information scenes modality perceptual mental 
setting aside verbal aspect may model possible verb meanings set features feature represents predicate arguments verb 
example set single argument predicates include moving rotating supported liquid container specifying perceived situation argument verb moving moving particular manner second set predicates specify relationships arguments externally caused cause event contact support attach predicates idealized partial lexicon contain word concept mappings cause arg arg lower raise rise fall specifying linear order value argument predicates lower cause moving rotate concept covers externally cased motion events agent moves theme downwards supported contact 
verb raise nearly identical fall rise involve internally caused motion cause specify argument predicates 
values rotating liquid container attach predicates signal features irrelevant verb concept 
perception scene amounts evaluating predicates scenes may may fall verb concept conditioned values predicates 
presence irrelevant features valued implies possible scenes consistent concept 
hypothesis space possible verb concepts formed sorts predicates task learning verb meaning observations 
xn scenes determine possible concepts 
just bayesian model computing posterior probability distribution hi concepts prior distribution hypotheses hi likelihood distribution generating particular xj example hi xj hi xj hi hi bayes rule eq compute likelihood hypothesis independent examples 
intuitively likelihood model says possible scenes fall concept hi equally likewise prior probability model holds concepts equally 
consider reduced hypothesis space concepts distribution scenes directly compute posterior probability hi different concepts 
shown increasing generality specific concept covering scene general concept covering possible scenes observation single scene observation explained hypotheses situation graded fashion 
repeated observations situation mass concentrated 
scene observations require abstracting away irrelevant features specific concepts discarded favor general concepts situation vs 
example consistent general concept reduces ambiguity possible concepts situation vs 
modeling syntactic bootstrapping section demonstrate bayesian model distribution syntactic frames envisioned gleitman may determine semantic features verb 
introduce new notion semantic agreement features lexical head agree complement 
consider idealized lexicon fill fig con pour fig load fig fig fig glass con water lexical head fill agrees complement glass water water glass lexical head complement value fig dimension 
likewise lexical head pour agrees complement water glass glass water opposite value fig 
lexical head load agrees accepts complements 
load hay load hay valid derivations 
large number verb classes seen pattern classes different feature dimensions way 
number feature dimensions may hypothesized may include selectional features fill requiring container con pour requiring liquid complement 
suppose learner hears glass water 
features novel verb unknown features complement glass water known 
fig feature dimension possible values corresponding hypotheses 
observation insufficient infer possible 
likelihood model unknown verb feature value feature value complement agreeing feature dimension fig loc con compute probability distribution hi intuitively says high probability agree low probability agree 
joint distribution encodes prior distribution conditional distribution assumption perfect knowledge feature values complement multiple observations distributional evidence support hypotheses readily evaluated 
test different distributions syntactic frames correctly yield different probability distributions verbs syntactic semantic features bayesian model gleitman syntactic bootstrapping 
suppose learner gets syntactic frames form 
equivalent having perfect observations fig annotate 
likelihood posterior probability possible hypotheses evaluated directly bayes rule likelihood posterior shown distributions syntactic frames sit utterances ved ved ved ved ved ved ved ved ved ved ved examples uncertainty value feature rapidly reduced situations 
number examples increases situation vs evidence supports behavior significant number noisy frames situation vs 
modeling integrated syntactic semantic bootstrapping integrate forms bootstrapping described distribution scenes syntactic frames probability distribution concepts consistent sources evidence determined 
consider possible syntactic frames utterance attention water glass water glass water glass water glass perceptually derived semantic features scenes scene description semantic features pour fill person pouring water glass filling glass manner state full water manner pouring state splash fill person water glass filling glass manner state full water manner state spray fill person water glass filling manner state full manner state pour empty person pouring water glass emptying manner state empty manner pouring state splash empty person water glass emptying manner state empty manner state pour person pouring water glass manner state manner pouring state spray person water glass manner state manner state features ordered fig manner motion change state utterance scene possibility subscripts annotate observation argument dimensions 
may describe just distributional evidence independent scene utterance pairs 
sn un yields different word concept mappings hi independent combination sources evidence hi sj hi uj hi hi expository purposes consider learner rank precise hypotheses assume entertain english verb hypothesis feature pour spray splash fill empty move likelihood sj hi independent dimensions sj 
sd hi sk hi model scene observations kth dimension dk sk hi hk hk dk hk hk hk annotate value kth dimension hypothesis hi hk 
lines model feature valued hk scenes typically kth dimension match probability 
observing pouring manners observing filling emptying breaking change states situation scene utterance pour fill water glass pour fill glass water pour fill water glass glass water pour fill pour empty water glass pour water pour fill splash fill glass water spray fill glass pour fill splash empty water spray water word concept mapping hi scene utterance evidence novel verb far observing manner motion change state 
observing different value sj occurred accident may important feature concept 
second lines model feature valued scenes typically match feature value match probability 
example hypothesis scenes contain pouring 
examples qualitatively results sensitive changes values 
output model shown 
suppose situation learner single scene utterance pair pour fill water glass wish compute hi hi assume learner attend argument extract relevant features scene 
scene pour fill paired utterance water glass bayesian model places high weight 
situation scene syntax glass water provides learner information attend water manner motion glass change state 
model weights heavily 
situation scene syntax gives learner information argument scene speaker may referring unknown arguments scene speaker particular argument mind 
learner condition possibilities sj hi sj hi za za learners consider arguments equally salient effectively models zi equivalent probability probability 
simplicity assume water glass referential uncertainty modeled higher conditioning possibilities yields certain word concept mapping 
situation syntactic frames provided situations scene information 
syntactic information provided frame situation water glass manner motion locative verbs preferred change state locative verbs differentiation possible scenes 
likewise frame provides opposite cue situation glass water opposite preference achieved differentiation possible change state verb concepts 
zero syntactic information available situation hypotheses prove equally 
situation verb concept mapping ambiguous primarily situation learners provided additional examples disambiguate 
scenes syntactic frames situation support situation scenes syntactic frames support 
situation different scene utterance pairs primarily support superordinate concept subordinate manner concept 
discussion reason analysis able infer little evidence embedded knowledge sources structure hypothesis space examples contained small number feature dimensions possible values may specified interfaces perceptual motor memory theory representations 
innate acquired conditional source 
priors hi hypotheses equal priors updating hi language input natural 
verbal domain commonly observed manner vs path tight loose fit biases 
likelihood scenes word concept sj hi 
stipulated static values acquired observation 
perfect knowledge features complement 
simplifying assumption illustrate essential elements model learners acquire features parallel 
likelihood agreement feature novel verb complement speculate sufficient structure partially learned words acquire structure joint distribution feature values 
richness knowledge contrast models employed regier desai train connectionist neural networks learn word scene associations adjectives nouns verbs respectively 
high dimensionality models forces need thousands training trials interpretation weights notoriously difficult 
assumptions models justified authors 
contrast bayesian approach hypotheses priors likelihoods explicit holding structure central 
siskind views lexical acquisition constraint satisfaction offers robust algorithm mapping input hypothesis space accomplished pruning hypotheses occur cross 
provided idealized tokenization world algorithm need large number examples 
siskind model yield form preference different concepts especially important concepts may equally constrained data 
shown bayesian analysis explicitly yields preferences concepts posterior probability distribution hi 
tenenbaum xu take important step putting word learning bayesian framework adopt showing noun learning occur small number examples continuous variable input space 
crucially models ignore constraining role syntax despite considerable evidence children syntax guide hypothesis space gleitman fisher gleitman 
qualitatively models performance matches preferences child learners modeling acquisition little example 
statistics imply commitment radical empiricism 
prior knowledge stipulated structure hypothesis space priors hypotheses likelihood pairs hypotheses 
specified innate learnable 
linguistics lexical semantics provide detailed theories larger syntactic semantic hypothesis space little prevents inclusion framework 
robert berwick motivating supporting 
jesse josh tenenbaum provided stimulating discussions 
funded provost prof joel moses 
desai 

bootstrapping miniature language acquisition 
proceedings fourth international conference cognitive modeling pp 

hillsdale nj erlbaum 
harley 
licensing lexicon 
bert peeters ed 
lexicon encyclopedia interface amsterdam elsevier press 
fisher hall gleitman 

better receive give syntactic conceptual constraints vocabulary growth 
lingua 
gleitman 
structural sources verb meanings 
language acquisition 
levin 
english verb classes alternations preliminary investigation university chicago press chicago il 


children syntax learn verb meanings 
journal child language 
jones berwick 
architecture universal lexicon case study shared syntactic information japanese hindi greek english 
coling 
pinker 
learnability cognition 
mit press cambridge ma 
regier 
emergence words 
proceedings rd annual conference cognitive science society 
siskind 
computational study cross situational techniques learning word meaning mappings 
cognition gleitman 
hard label concepts 
hall waxman eds weaving lexicon cambridge ma mit press 
tenenbaum xu 
word learning bayesian inference 
proceedings nd annual conference cognitive science society pp 

