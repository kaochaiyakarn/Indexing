text categorization regularized linear classi cation methods tong zhang mathematical sciences department ibm watson research center yorktown heights ny watson ibm com frank oles mathematical sciences department ibm watson research center yorktown heights ny oles watson ibm com number linear classi cation methods linear squares llsf logistic regression support vector machines svm applied text categorization problems :10.1.1.39.3129
methods share similarity nding hyperplanes approximately separate class document vectors complement 
support vector machines far considered special demonstrated achieve state art performance 
worthwhile understand performance unique svm design achieved linear classi cation methods 
compare number known linear classi cation methods variants framework regularized linear systems 
discuss statistical numerical properties algorithms focus text categorization 
provide numerical experiments illustrate algorithms number datasets 
background text categorization problem determine prede ned categories incoming unlabeled message document containing text information extracted training set labeled messages documents :10.1.1.39.3129
text categorization important practical problem companies wish computers categorize incoming email enabling automatic machine response email simply assuring email reaches correct human recipient 
email text items categorized may come sources including output voice recognition software collections documents news stories patents case summaries contents web pages :10.1.1.39.3129
text categorization problem reduced set binary classi cation problems category wishes determine method predicting versus class membership 
supervised learning problems widely studied past 
methods developed classi cation problems applied text categorization 
example apte damerau weiss applied inductive rule learning algorithm swap text categorization problem :10.1.1.39.3129
yang chute proposed linear squares algorithm train linear classi ers 
yang compared number statistical methods text categorization 
best performances previously reported literature weighted resampled decision trees linear support vector machines :10.1.1.11.6124:10.1.1.39.3129:10.1.1.161.6020
integral parts approaches tokenization feature selection creating numeric vector representations documents 
rst step tokenization laid detail :10.1.1.39.3129
functionality common methods text categorization 
tokenization procedure depicted steps may omitted keeping may improve performance :10.1.1.39.3129
steps retained elimination stopwords step may done stemming step 
elimination stopwords step may instances subsumed subsequent feature selection discussed 
consistency tokenization procedure documents training build categorization rules incoming documents categorized system employing classi ers obtained training phase :10.1.1.39.3129
tokenization document represented list word occurrences 
program carry feature selection 
feature selection skipped entirely tokens taken sole features interest 
take speci cs feature selection number methods varying degrees sophistication studied :10.1.1.32.9956
feature selection done entire data set experience indicates better results obtained doing feature selection separately category re ecting intuition features indicative category membership di er moves category category 
feature selection assumption separate set features selected category laid 
output feature selection normally speci cation separate list selected features words category intend generate classi er 
speci cation necessarily detailed permit computer identify occurrence feature tokenized representation document 
feature selection document represented vector word occurrences category vector component corresponds word feature selected category previous step 
shows steps necessary list features selected relevance particular category convert tokenized representation document numeric vector representing document 
vast numbers di erent words may appear text generally numerical vectors world occurrences gets sparse vectors high dimensionality 
text categorization necessitates techniques supervised class categorization problem suited highdimensional sparse data 
formally class categorization problem determine label associated vector input variables :10.1.1.39.3129
useful method solving problem linear discriminant functions consist linear combinations input variables 
various techniques proposed determining weight values linear discriminant classi ers training set labeled data :10.1.1.39.3129
document number items training set 
speci cally seek weight vector threshold label label :10.1.1.39.3129
hyperplane consisting approximately separate class vectors class vectors 
problem just described may readily converted threshold taken zero 
embedding data space dimension translating original space chosen nonzero distance original position 
normally takes :10.1.1.39.3129
conversion vector traded :10.1.1.39.3129
hyperplane original space unique hyperplane larger space passes origin larger space 
searching dimensional weight vector threshold search dimensional weight vector anticipated threshold zero :10.1.1.39.3129
assumption vectors input variables suitably transformed may take training error rate linear classi er weight vector step function number approaches solving categorization problems nding linear discriminant functions advanced years :10.1.1.39.3129
early statistical literature weight obtained linear discriminant analysis assumption class gaussian distribution cf 
chapter :10.1.1.39.3129
similar linear discriminant analysis approach widely statistics usually regression classi cation squares algorithm 
squares applied text categorization problems 
assumption underlying distribution linear separator obtained perceptron scheme minimizes training error :10.1.1.39.3129
commonly approach statistics obtaining linear classi er logistic regression 
logistic regression closely related support vector machines gained popularity 
long history logistic regression information retrieval seen partial list :10.1.1.11.9519:10.1.1.39.3129
number reasons method ective way text categorization 
result comparison suggested negative opinions performance logistic regression 
combination factors led negative results 
reason lack regularization formulation 
keep mind regularization important success svm 
come back issue regularization section 
reason choice newton raphson method numerical optimization experience unstable especially regularization 
introduce stable numerical optimization procedure 
third reason threshold adjustment problem explain section problem ect break performance measurement 
furthermore previous studies logistic regression typically employed relatively small set features 
suggested helpful large set features order tens thousands :10.1.1.39.3129
experiments section con rm observation 
organized follows 
section describe various linear classi cation algorithms 
discuss issues related ectiveness algorithms statistical point view 
analysis introduce method variant linear squares method svm 
presence large feature sets introduced numerical challenges 
issues discussed section investigate computational aspects algorithms including logistic regression support vector machines 
experiments section compare performance di erent methods 
particular illustrate logistic regression achieves results comparable support vector machines 
summarize section 
linear classi ers start discussion squares algorithm formulation compute linear separator arg inf solution problem formulation matrix may singular ill conditioned :10.1.1.39.3129
occurs example dimension note case exists nitely solutions implies nitely possible solutions remedy problem pseudo inverse :10.1.1.39.3129
problem pseudo inverse approach computational complexity 
order handle large sparse systems need iterative algorithms rely matrix factorization techniques 
standard ridge regression method adds regularization term arg inf appropriately chosen regularization parameter :10.1.1.39.3129
solution ni denotes identity matrix :10.1.1.39.3129
note ni non singular solves ill condition problem :10.1.1.39.3129
statistically squares formulation associated regression model assumed iid noise 
assume noise comes zero mean gaussian distribution formulation corresponds penalized maximum likelihood estimate gaussian prior 
condition squares method optimal estimator sense corresponds optimal bayesian estimator bayesian framework 
situations method typically sub optimal 
unfortunately gaussian noise assumption continuous satis ed approximately classi cation problem discrete :10.1.1.39.3129
statistics logistic regression cf 
section employed remedy problem 
pointed considerable interest applying logistic regression text categorization ectiveness fully explored 
despite close relationship logistic regression support vector machines results text categorization direct large scale logistic regression reported literature standard reuters dataset 
shall brie describe logistic regression method connection linear svm completeness 
logistic regression model conditional probability jw exp :10.1.1.39.3129
follows likelihood exp xy note jw jw :10.1.1.39.3129
standard procedure obtain estimate maximum likelihood estimate minimizes arg inf ln exp similar formulation may ill conditioned numerically :10.1.1.39.3129
standard method solve problem adding scaling identity matrix hessian equivalent regularized logistic regression formulation arg inf ln exp appropriately chosen regularization parameter :10.1.1.11.9519:10.1.1.39.3129
regularized formulation belongs class methods called penalized likelihood cf 
section bayesian framework interpreted map estimator prior exp :10.1.1.11.9519
furthermore employ slightly general family ln exp cz log likelihood function ln exp absorbed regularization parameter :10.1.1.11.9519:10.1.1.39.3129
support vector machine method originally proposed vapnik nice properties sample complexity theory :10.1.1.39.3129
designed modi cation perceptron algorithm 
slightly di erent approach forcing threshold compensating appending constant component usually take document vector standard linear support vector machine cf :10.1.1.39.3129
chapter explicitly includes quadratic formulation follows arg inf eliminating formula equivalent formulation arg inf comparison plot svm loss function logistic loss function ln exp modi ed logistic loss function ln exp :10.1.1.39.3129
shapes functions similar 
di erences shapes absorbed scaling regularization parameter remaining di erences insigni cant text categorization purposes demonstrate experiments shall note lim ln exp support vector machine formulation regarded approximation limiting case generalized family logistic regression formulations :10.1.1.39.3129
standard support vector machine solved directly quadratic programming problem dual formulation describe corresponding loss function introduces diculties direct numerical optimization 
loss function replaced smooth function case logistic regression squares algorithm easier solved directly primal formulation 
shall intentionally replace smoother function consider alternative formulation arg inf formulation suciently smooth direct numerical optimization performed eciently see section :10.1.1.11.9519:10.1.1.39.3129
compared logistic regression advantage expensive exponential function evaluation required numerical optimization 
particular choice equivalent instance general form support vector machine see chapter interesting note formulation intentionally excluded standard svm described chapter chapter :10.1.1.39.3129
argument comes impression looks better approximation step function 
argument relies statistical learning theory basis svm regards approximation classi cation error function 
reasoning partially correct 
statistical point view regard logistic model exp xy approximation true conditional probability yjx known asymptotic optimality property maximum likelihood estimate maximum likelihood estimate asymptotically ecient estimator distribution dependent parameter asymptotically unbiased estimators estimator asymptotically ecient estimator support vector machine :10.1.1.39.3129
suggests achieve better eciency need better distribution model need incorporate model regularized maximum likelihood estimate regularization control numerical stability 
point view better distribution model captures heavy tail probability histogram xy reasonable logistic model conditional likelihood :10.1.1.39.3129
explained earlier gaussian noise assumption satis ed approximately 
compares histograms xy training set estimated projection parameter squares method modi ed squares method :10.1.1.39.3129
experiment done earn category largest category reuters news story dataset see section 
surprisingly squares method computes ts gaussian histogram modi ed squares method accommodates heavy tailed distribution appropriate classi cation problems 
squares method close state art performance reasonable expect state art performance achieved better distribution model :10.1.1.39.3129
hand statistical learning theory current stage lacks concept eciency concept distribution modeling fully explain superior performance compared standard support vector machine formulation text categorization problems reported section :10.1.1.39.3129
due mentioned di erent philosophies avoid unnecessary confusion standard svm modi ed svm introduce appropriate regard modi ed regularized squares method incorporates heavy tail distribution aspect logistic model gaussian model squares method :10.1.1.39.3129
compared logistic model modi ed squares formula advantage numerical eciency need evaluate time consuming exponential functions computation 
probability interpretation logistic model desirable applications 
possible derive kernel methods see algorithms mentioned section dual formulation see section :10.1.1.39.3129
text categorization problems sucient evidence show kernel methods help 
example results plain linear classi ers slightly better kernels :10.1.1.39.3129
note ectiveness kernels relies assumption high order word correlations word pairs word triples convey information single words 
correlation captured linear combination word occurrences 
conducted preliminary experiments failed demonstrate statistically signi cant evidence high order methods word pairs kernels helpful 
addition kernel classi ers complex computationally 
avoid methods 
numerical algorithms text categorization problems input vector usually sparse large dimensionality 
traditional numerical techniques small dense problems may suitable especially require kind matrix factorization :10.1.1.11.9519
section study iterative methods solving large sparse systems associated schemes discussed 
denote index training samples denote feature index 
example ij th component th training sample th component weight vector 
column relaxation algorithm consider general formulation arg inf relatively smooth function continuous rst order derivative non negative piecewise continuous second order derivative :10.1.1.39.3129
case formulation convex unique local minimum global minimum :10.1.1.39.3129
methods investigated section generic relaxation algorithm example see algorithm gauss seidel nd :10.1.1.39.3129
approximately minimizing :10.1.1.39.3129
ij 
update 
ij update :10.1.1.39.3129
specialize algorithm produce computational routines solving regularized linear squares formulation ridge regression modi ed squares formulation regularized logistic regression formulation :10.1.1.11.9519:10.1.1.39.3129
regularized linear squares formulation obtain algorithm solved exactly 
note linear systems conjugate gradient cg procedure preferred gauss seidel method 
addition preconditioned cg symmetrized gauss seidel method preconditioner usually performs better 
fact advantage possible special structures combination method choice general purpose symmetric large sparse linear system solver details see relevant chapters 
cg nonlinear optimization problems due number reasons suitable systems considered 
simplicity consistent methods shall algorithm solve experiments 
algorithm cls :10.1.1.39.3129
ij nw ij update :10.1.1.39.3129
ij update :10.1.1.39.3129
algorithm may apply newton method directly suggested :10.1.1.39.3129
resulting linear system newton method solved iterative solver gauss seidel iteration cg 
properly implemented line search method complicated required guarantee convergence 
line search method result small step sizes slows convergence 
comparison algorithm advantage simplicity guaranteed convergence long update :10.1.1.39.3129
objective function decreases time :10.1.1.39.3129
purpose rewrite taylor expansion :10.1.1.39.3129
ij 
ij :10.1.1.39.3129
:10.1.1.39.3129
ij ij 
equality implies 

ij :10.1.1.39.3129
jx ij ij arbitrary function sup 












objective function decreases step 
property ensures convergence algorithm :10.1.1.39.3129
apply idea :10.1.1.39.3129
helpful enhance smoothness introducing continuation parameter smoothness decreases decreases algorithm modi ed step di erent chosen :10.1.1.39.3129
function shall replaced note continuation parameter required order algorithm converge :10.1.1.39.3129
experience simple modi cation accelerates rate convergence algorithm pick decreasing sequence :10.1.1.39.3129
pick 
de ne function de ne function jyj :10.1.1.39.3129
ij nw 
ij ij 
min max 


update 
ij update :10.1.1.39.3129
update 


logistic regression obtain algorithm speci choice algorithm clg pick :10.1.1.11.9519:10.1.1.39.3129
de ne function min jyj :10.1.1.39.3129
exp ij nw :10.1.1.39.3129
ij ij 
min max 


update 
ij update :10.1.1.39.3129
update 


algorithm formulations long computation organized eciently 
update formula 
sure suciently large step taken 
tolerance parameter set text categorization problems :10.1.1.39.3129
initial choice 
unimportant set :10.1.1.39.3129
standard newton procedure initially obtain estimates 
update formulae 

example formula 
max 

gives method similar standard trust region update 
theoretically trust region update appealing 
inner column wise update asymptotically behaves newton method :10.1.1.39.3129
algorithms written form xed iteration algorithms picked usually sucient text categorization purposes :10.1.1.39.3129
dicult impose explicit stopping criterion checks convergence parameter measure iteration 
example text categorization applications change 
percent jr terminate algorithm :10.1.1.39.3129
row relaxation algorithm introduce dual form suggest class numerical algorithms solve dual formulation :10.1.1.39.3129
dual formulation obtain representation socalled reproducing kernel hilbert space rkhs 
representation allows linear classi er learn non linear functions original input space considered major advantage kernel methods including popularized support vector machines gaussian processes dual ridge regression 
mentioned section unable observe statistically signi cant evidence kernel methods superior general text categorization tasks helpful special problems 
useful know methods logistic regression modi ed squares share advantage svm terms kernel representation 
proceed introduce dual form considering auxiliary variable data arg inf sup :10.1.1.39.3129
legendre transform 
sup uv 
known convex 
switching order inf sup valid minimax convex programming problem proof appendix obtain arg sup minimized substituting equation obtain arg inf similar algorithm solves primal problem generic relaxation algorithm solves dual problem algorithm dual gauss seidel nd :10.1.1.11.9519:10.1.1.39.3129
approximately minimizing 
:10.1.1.39.3129

update 
ij update :10.1.1.39.3129
dual formulation leads kernel methods embed feature vector hilbert space :10.1.1.11.9519:10.1.1.39.3129
computing associated hilbert space computationally infeasible may choose keep dual variable 
order compute replace inner product kernel function just kernel svm :10.1.1.39.3129
important implementation issue algorithm data ordering signi cant ect rate convergence feature ordering appear noticeable ect algorithm 
speci cally order data points members class grouped may experience slow convergence :10.1.1.39.3129
hand dual algorithm appears random data ordering 
intuitively phenomenon surprising class members grouped go data class tend portion data encounter data class 
deep understanding data ordering issue 
study may potentially lead systematic method data re ordering accelerate convergence 
attractive property dual formulation computation pre computed stored memory update step small system eciently solved constant time approximately predetermined numerical accuracy line search algorithm 
rate convergence dual row wise algorithm di erent primal column wise algorithm 
general 
suciently smooth sucient evidence dual algorithm ecient :10.1.1.39.3129
hand dual formulation computationally useful svm due non smooth loss function primal form 
standard svm method text categorization smo dual formulation described chapter :10.1.1.39.3129
complication dual svm employs explicit threshold associated equality constraint dual variable specially treated numerical optimization routine 
example smo done modifying pair dual variable components simultaneously step enforce constraint 
heuristics select pairs dual variables optimize 
experience smo indicates non negligible amount time spent heuristic dual pair selection procedure numerical optimization 
simple remedy problem set standard svm append constant feature algorithms 
modi cation leads variant svm dual equality constraint 
speci cally dual formulation de ned value ectively simple constraint dual variable :10.1.1.11.9519:10.1.1.39.3129
algorithm applied formulation update 
max min procedure mentioned platt chapter dismissed inferior standard svm :10.1.1.39.3129
shall experimental evidence contrary platt opinion 
fact method successfully employed :10.1.1.39.3129
distinguish procedure standard svm shall call modi ed svm 
update corresponds exact minimization dual objective functional :10.1.1.39.3129
practice observe useful smaller update step :10.1.1.39.3129
modi ed squares formulation de ned obtain instance algorithm replaced closed form solution :10.1.1.39.3129
min logistic regression legendre dual ln ln :10.1.1.11.9519:10.1.1.39.3129
de ned :10.1.1.39.3129
solution obtained solving equation ln :10.1.1.39.3129

:10.1.1.39.3129
experimental results section compare methods discussed regularized linear squares method denoted lin reg modi ed squares formulation denoted mod squares regularized logistic regression denoted logistic reg linear support vector machine denoted svm modi ed svm corresponding denoted mod svm :10.1.1.11.9519:10.1.1.39.3129
comparison purposes include results naive bayes baseline method :10.1.1.39.3129
implementation issues number feature selection methods text categorization compared :10.1.1.32.9956:10.1.1.39.3129
experiments employ method similar information gain ig approach described :10.1.1.32.9956
replace entropy scoring ig criterion gini index 
reason choice remove stopwords preprocessing step 
experience gini index remove stopwords ectively entropy function 
di erence small 
didn experiment methods described :10.1.1.32.9956
experiments select features input algorithms ignoring features occurring times suggested :10.1.1.39.3129
feature binary word occurrence indicating word occurs document :10.1.1.39.3129
word title regarded di erent feature word body 
complicated representations tfidf weighting schemes employed :10.1.1.39.3129
didn compare ectiveness di erent approaches 
implementation naive bayes multinomial model formulation described :10.1.1.39.3129
replace laplacian smoothing probability count increased smoothing probability count increased :10.1.1.39.3129
formulation considered regularization method avoid zero denominator problem 
experience replacement laplacian smoothing smoothing signi cant di erence practice :10.1.1.39.3129
choice :10.1.1.39.3129
certain data see description laplacian smoothing gives classi cation accuracy signi cantly worse average reported table methods described implementation algorithms described section :10.1.1.11.9519:10.1.1.39.3129
default number iterations chosen :10.1.1.39.3129
algorithms accelerated feature selection inside iterations algorithms truncate small column weight variables zero ignore corresponding column iteration 
addition computation speed may enhance classi cation performance 
lin reg solved algorithm logistic reg solved algorithm :10.1.1.11.9519
mod squares choose max algorithm :10.1.1.39.3129
standard svm solved implementation smo algorithm chapter :10.1.1.39.3129
modi ed svm solved algorithm choose :10.1.1.39.3129
reuters dataset see regularization parameter algorithm determined fold cross validation training set 
parameter identical categories 
try values :10.1.1.11.9519:10.1.1.39.3129
particular optimal choices obtain summarized table :10.1.1.39.3129
datasets simply computed reuters dataset 
experience indicates di erence choice optimal value speci dataset relatively small 
naive bayes lin reg mod squares logistic reg svm mod svm table values regularization parameter due di erent formulations termination criteria parameter choices dicult rigorous timing comparisons di erent algorithms :10.1.1.39.3129
rough idea implementation naive bayes typically order magnitude faster algorithms 
lin reg slower nb faster due simplicity 
mod squares mod svm similar speed minutes reuters pentium mhz linux pc features :10.1.1.39.3129
timing includes feature selection corresponds approximately seconds category 
logistic regression standard svm smo noticeably slower particular implementation 
classi cation performance text categorization frequently happens category contains small percentage documents 
happens standard classi cation error measurement indicative get low error rate simply saying item class result useful categorizer 
text categorization standard performance measures classi cation method precision recall classi cation error precision true positive true positive false positive recall true positive true positive false negative classi cation algorithm contains parameter adjusted facilitate tradeo precision recall compute break point bep precision equals recall evaluation criterion performance classi cation algorithm :10.1.1.11.9519:10.1.1.39.3129
widely single number metric metric de ned harmonic mean precision recall :10.1.1.39.3129
standard dataset comparing text categorization algorithms reuters set news stories publicly available www research att com lewis reuters html :10.1.1.39.3129
reuters modapte split partition documents training validation sets :10.1.1.39.3129
dataset contains non empty classes addition documents categorized documents training set documents test set :10.1.1.11.9519:10.1.1.39.3129
micro averaged precision recall computed statistics summed confusion matrices classes performances algorithms classes reported table :10.1.1.39.3129
bep computed rst adjusting linear decision threshold individually category obtain bep category micro averaged classes :10.1.1.39.3129
svm result consistent previously reported results :10.1.1.39.3129
naive bayes result consistent 
interestingly note particular experimental setup obtained performance squares method 
partially explained statistical learning theory argument svm :10.1.1.39.3129
learning theory point view svm perform exists linear separator small norm separates class data class data relative large margin xy shall valid data :10.1.1.39.3129
hand svm learning bounds require kxk small means jw xj kwk kxk small 
ect clustered positive point class data clustered negative point class data 
squares method computes weight clustered class data clustered negative data perform reasonably long svm suitable problem :10.1.1.39.3129
experiments noticed naive bayes methods nd hyperplanes comparable quality 
re ected similar break points 
numbers unmodi ed computed thresholds usually suboptimal methods :10.1.1.39.3129
small categories reuters dataset computed thresholds biased causing fewer errors dominant class data ect compensated lowering computed thresholds 
experiments adjust threshold hyperplane computed compensate problem nding value minimizes training error 
phenomenon observed platt tting sigmoid output svm enhance performance reuters ect sigmoid tting equivalent di erent threshold :10.1.1.11.9519:10.1.1.39.3129
unclear best procedure adjust threshold hyperplane computed 
topic requires investigation 
bep metric independent di erent threshold choices regarded pure measure quality computed hyperplane 
regarded lower bound estimate best metric obtain nd ideal threshold :10.1.1.11.9519:10.1.1.39.3129
include break points table 
results modi ed squares naive bayes indicate choice threshold potentially lead better value implying smaller classi cation error bep value :10.1.1.39.3129
hand logistic regression suboptimal threshold re ected better bep measure measure :10.1.1.39.3129
addition micro average results argued useful report macro averaged values categories correspond un weighted averages values class :10.1.1.11.9519:10.1.1.39.3129
shows macro averaged values top categories :10.1.1.11.9519:10.1.1.39.3129
include values largest categories reuters dataset table best number category highlighted :10.1.1.39.3129
naive bayes lin reg mod squares logistic reg svm mod svm precision recall bep table binary classi cation performance reuters classes naive bayes lin reg mod squares logistic reg svm mod svm earn acq money fx grain crude trade interest ship wheat corn table performance reuters top classes shows break performance logistic regression di erent feature sizes :10.1.1.11.9519:10.1.1.39.3129
see performance improves number features increases 
suggests excessive feature selection bene cial regularized linear classi cation methods 
eliminate feature selection step words noisy features occurring times performance degrades breakeven point 
note study bep metric avoid complications threshold adjustment discussed earlier 
shows break performance logistic regression di erent values regularization parameter 
theory performance logistic regression unpredictable :10.1.1.39.3129
problem linearly separable solution logistic regression ill de ned linearly separate data positive scaling parameter easy verify logistic objective function ln exp decreases zero :10.1.1.39.3129
implies optimal solution non regularized logistic regression nite ill de ned data linearly separable situation happens reuters 
implies performance degradation logistic regression severe 
de nitive explanation phenomenon 
conjecture due fact relatively small number iterations iteration update step size small consequence weight vector get large 
svm behaved linearly separable data linearly non separable data 
interestingly dual algorithms relatively behaved xed number iterations 
mod svm mod squares micro averaged reduced number iterations dual algorithms :10.1.1.39.3129
reason behavior due slow convergence dual algorithms 
example :10.1.1.39.3129
proportionally small large reasonable number iterations 
shown non separable data optimal value nite 
case slow convergence dual algorithm avoids bad scenario nite 
consequently primal algorithm solving leads poorer result bep corresponding dual algorithm bep primal algorithm converges faster veri ed comparing objective function values computed weights :10.1.1.39.3129
experience squares method degrades severely limit micro averaged break point 
features features squares method achieves break point limit :10.1.1.39.3129
table give comparative results data set consists records call center inquiries ibm customers 
data categorized classes partitioned training set items test set items :10.1.1.11.9519:10.1.1.39.3129
characteristics data di erent reuters 
bep performances algorithms consistent 
metric demonstrates choice linear threshold signi cantly ect performance algorithm algorithm break point :10.1.1.39.3129
naive bayes lin reg mod squares logistic reg svm mod svm precision recall bep table binary classi cation performance text categorization problems reuters document multiply classi ed applications documents singly classi ed :10.1.1.11.9519:10.1.1.39.3129
case usually interested treating problem directly single multi class problem separate binary class problems 
classi cation accuracy de ned percentage correctly predicted category useful performance measure type problems 
compare algorithms datasets 
rst datasets available www cs cmu edu datasets html 
newsgroup collection messages di erent newsgroups messages newsgroup 
collection web pages companies various economic sectors :10.1.1.39.3129
categories :10.1.1.39.3129
webkb contains web pages collected computer science departments various universities january :10.1.1.39.3129
version categories student faculty sta department course project 
contains proposals 
categories corresponding areas proposals 
contains web pages collected www ibm com categorized classes 
choices regularization parameter listed table various algorithms :10.1.1.39.3129
observe performance di erence default choice close optimal parameter datasets 
implies parameters reasonably variety datasets partially eliminate necessity cross validation parameter selection procedure undesirable due extra computational cost 
binary classi cation case perform threshold adjustment experiments 
obtain multi class categorizer compute linear classi er weight category treating binary problem predict class data point label largest value naive bayes gives method call naive bayes 
comparison purposes implemented direct multi class naive bayes method described call naive bayes :10.1.1.39.3129
studies suggesting certain coding schemes error correcting output code convert binary classi er multi class categorizer statistically signi cant improvement coding schemes implementation 
select features algorithms naive bayes :10.1.1.39.3129
feature selection performed naive bayes :10.1.1.39.3129
consistent improvement naive bayes naive bayes surprising :10.1.1.39.3129
observe improvement entirely due feature selection 
explanation phenomenon 
dataset generate random training test splits training set size times test set size 
table reports classi cation accuracies algorithms random splits dataset 
result format mean standard deviation 
relative performances algorithms consistent logistic regression worse linear regression :10.1.1.39.3129
naive bayes consistently worse methods 
statistically signi cant di erence indicate linear regression worse remaining algorithms naive bayes 
di erences algorithms signi cant draw rm 
observe di erences algorithms signi cant datasets 
unclear characterize dataset order predict signi cant performances di erent algorithms 
method newsgroup webkb naive bayes naive bayes lin reg mod squares logistic reg svm mod svm table multi class classi cation accuracy summary applied logistic regression model modi cation squares method text categorization :10.1.1.11.9519:10.1.1.39.3129
compared linear classi cation methods support vector machines 
statistical properties di erent methods discussed section providing useful insights various methods 
argued text categorization application requires numerical iterative algorithms suitable solving large sparse systems associated proposed formulations 
numerical issues discussed section 
speci cally introduced column wise relaxation algorithm row wise relaxation algorithm solve large sparse systems 
experience proposed algorithms ecient 
may possible enhance algorithms example quasi newton acceleration top iterations 
insucient smoothness formulations described may cause trouble quasi newton enhancement 
open question signi cant improvement obtain 
experimental comparisons algorithms number datasets reported 
naive bayes consistently worse algorithms 
regularized linear squares method performance close state art relatively small di erence appears statistically signi cant 
demonstrated appropriately implemented regularized logistic regression perform svm regarded state art text categorization 
furthermore logistic regression advantage yielding probability model useful applications 
number open issues revealed study 
interesting problem nd threshold computed hyperplane 
interesting issue ect changing regularization parameter simple method select value dataset 
brie touched issue 
behaviors di erent algorithms examined section 
addition experience suggests xing default value leads reasonable performance variety datasets compared baseline naive bayes 
useful carefully investigate related issues impact certain dataset characteristics training sample size average length document choice 
feature selection issue investigated experiments 
showed moderate amount feature selection useful 
issue discussed previous works studies ect feature selection regularization methods discussed valuable 
observed performance di erences algorithms data dependent 
useful nd dataset characteristics explain variation 
characterization provide valuable insights di erent algorithms behave perform poorly particular data 
validity dual formulation show valid interchange order inf sup :10.1.1.39.3129
need show exists inf sup sup inf known example see duality gap non negative inf sup sup inf furthermore equation solution :10.1.1.39.3129
need nd inf sup case called saddle point 
construct saddle point consider minimizes primal problem satis es estimation equation obtain implies achieves minimum inf de nition derivative :10.1.1.39.3129
know achieves maximum sup combining obtain implies interchangeability order inf sup :10.1.1.39.3129
apte damerau weiss :10.1.1.39.3129
automated learning decision rules text categorization 
acm transactions information systems :10.1.1.39.3129
cooper 
probabilistic retrieval staged logistic regression 
pages :10.1.1.39.3129
cortes vapnik 
support vector networks 
machine learning :10.1.1.39.3129
dumais platt heckerman sahami 
inductive learning algorithms representations text categorization 
proceedings acm th international conference information knowledge management pages :10.1.1.39.3129
fuhr pfeifer 
combining model oriented description oriented approaches probabilistic indexing 
sigir pages :10.1.1.11.9519:10.1.1.39.3129
:10.1.1.11.9519
inferring probability relevance method logistic regression 
sigir pages :10.1.1.39.3129
golub van loan 
matrix computations 
johns hopkins university press baltimore md third edition :10.1.1.11.9519:10.1.1.39.3129
hastie tibshirani 
generalized additive models 
chapman hall london :10.1.1.39.3129

ridge regression biased estimation nonorthogonal problems 
technometrics :10.1.1.11.9519:10.1.1.39.3129
ittner lewis ahn :10.1.1.39.3129
text categorization low quality images 
symposium document analysis information retrieval pages :10.1.1.39.3129
jaakkola haussler :10.1.1.39.3129
discriminative framework detecting remote protein homologies 
journal computational biology :10.1.1.39.3129
joachims :10.1.1.39.3129
text categorization support vector machines learning relevant features 
european conference machine ecml pages :10.1.1.39.3129
lewis gale :10.1.1.39.3129
sequential algorithm training text classi ers 
sigir pages :10.1.1.39.3129
mccallum nigam :10.1.1.39.3129
comparison event models naive bayes text classi cation 
aaai icml workshop learning text categorization pages :10.1.1.39.3129
minsky papert :10.1.1.39.3129
perceptrons 
mit press cambridge ma expanded edition edition :10.1.1.39.3129
platt :10.1.1.11.9519:10.1.1.39.3129
probabilistic outputs support vector machines comparisons regularized likelihood methods 
smola bartlett scholkopf schuurmans editors advances large margin classi ers 
mit press :10.1.1.39.3129
ripley :10.1.1.39.3129
pattern recognition neural networks 
cambridge university press :10.1.1.11.9519:10.1.1.39.3129
rockafellar :10.1.1.39.3129
convex analysis 
princeton university press princeton nj :10.1.1.39.3129
sch olkopf burges smola editors :10.1.1.39.3129
advances kernel methods support vector learning 
mit press :10.1.1.39.3129
sch hull pedersen 
comparison classi ers document representations routing problem 
sigir pages :10.1.1.39.3129
vapnik :10.1.1.39.3129
statistical learning theory 
john wiley sons new york :10.1.1.39.3129
wahba 
support vector machines reproducing kernel hilbert spaces randomized 
sch olkopf burges smola editors advances kernel methods support vector learning chapter :10.1.1.11.9519
mit press :10.1.1.39.3129
weiss apte damerau johnson oles goetz 
maximizing text mining performance 
ieee intelligent systems :10.1.1.11.9519:10.1.1.39.3129
yang 
evaluation statistical approaches text categorization 
information retrieval journal :10.1.1.11.9519:10.1.1.39.3129
yang chute 
example mapping method text categorization retrieval 
acm transactions information systems :10.1.1.39.3129
yang liu :10.1.1.11.9519
re examination text categorization methods 
sigir pages :10.1.1.39.3129
yang pedersen :10.1.1.32.9956
comparative study feature selection text categorization 
proceedings fourteenth international conference machine learning :10.1.1.39.3129
:10.1.1.39.3129
read document 
segment document sections separate identity significant categorization 

tokenize section contains text 

optional convert tokens canonical forms stemming 

optional delete stopwords common words useful categorization 
:10.1.1.11.9519
output tokenized representation document list tokens section determined 
document tokenization :10.1.1.39.3129
:10.1.1.39.3129
assemble set tokenized representations training documents 

assembled set select set features relevant document membership category 

output list selected features training set category 
feature selection 
:10.1.1.11.9519:10.1.1.39.3129
read tokenized representation document list selected features training data category 

document create vector counts occurrences features list 
optional transform vector step normalization rounding component fixed maximum value 

augment vector step adding final component set fixed nonzero value producing vector 
output vector numerical representation document creating vector represent document selected features 
loss functions solid ln exp dash ln exp dotted :10.1.1.39.3129
squares method modi ed squares method projected histogram xy number categories macro averaged naive bayes lin reg mod square logistic reg svm mod svm macro averaged measure number features break performance logistic regression feature size lambda bep break performance logistic regression :10.1.1.11.9519:10.1.1.39.3129
