ieee transactions signal processing vol 
march bayesian curve fitting mcmc applications signal segmentation elena christophe andrieu arnaud doucet william fitzgerald propose bayesian methods address problem fitting signal modeled sequence piecewise constant linear parameters regression models example autoregressive volterra models 
joint prior distribution set number changepoints knots positions orders linear regression models segment unknown 
hierarchical priors developed resulting posterior probability distributions bayesian estimators admit closed form analytical expressions reversible jump markov chain monte carlo mcmc methods derived estimate quantities 
results obtained standard denoising segmentation speech data problems examined literature 
results demonstrate performance methods 
index terms bayesian model curve fitting markov chain monte carlo methods signal segmentation 
problem statement regression problems common problems signal processing 
aim estimate assumed functional relationship response explanatory variables noisy measurements 
parametric semi parametric methods proposed literature order solve problems including smoothing splines kernel methods 
adopt standard model regression function assumed function low order pieces standard linear regression models segments number position segments parameters estimate 
formally denote generic sequence vector real observations 
elements may represented models corresponding case signal form linear regression model piecewise constant parameters changepoints 
manuscript received january revised october 
associate editor coordinating review approving publication dr 
fitzgerald signal processing laboratory department engineering university cambridge cambridge 
mail op eng cam ac uk eng cam ac uk 
andrieu department mathematics statistics group university bristol bristol 
mail andrieu bristol ac uk 
doucet department electrical engineering university melbourne victoria australia mail doucet ee mu oz au 
publisher item identifier 
ieee vector model parameters th segment vector gaussian noise samples variance associated th model 
changepoints model arranged vector adopt convention notational convenience 
denote matrix matrix basis functions th segment example piecewise polynomial model piecewise constant autoregressive ar process form typically orders different linear regression models assumed equal known practice numerous applications speech processing example different model orders considered different segments estimated data 
general case number changepoints associated parameters unknown 
aim estimate background model allows wide range applications curve fitting noisy data changepoint detection signal segmentation 
example general piecewise linear model extension study multiple changepoints non gaussian impulsive noise environments studied 
shown piecewise constant autoregressive ar processes excited white gaussian noise proved useful processing real signals speech data 
general class models flexible large number parameters estimated needs prevent overfitting way 
adopt bayesian approach notational simplicity index suppressed 
ieee transactions signal processing vol 
march set prior works penalty overfitting unknown parameters 
bayesian curve fitting signal segmentation related models studied authors including 
gustafsson djuri proposed perform map maximum posteriori changepoint estimation deterministic algorithms 
methods fast give results compute confidence intervals perform bayesian model averaging 
difficult generalize case model order segment unknown 
resolution favor full bayesian approach complete posterior distribution posterior feature interest estimated mcmc 
bayesian approaches multiple changepoint detection mcmc different models proposed example 
closest technique followed see 
methodology different respects 
model general allows unknown number segments unknown model order segment necessary face double model selection problem 
adopt hierarchical prior distributions hyperparameters assumed random vague prior distribution similar approach adopted 
effect increasing robustness bayesian models comparison standard approach parameters fixed demonstrated simulation study 
propose efficient algorithms order sample posteriors reversible jump mcmc 
plan rest organized follows 
sake clarity double selection problem quite complex chosen section ii case orders different linear regression models equal known section iii case different unknown treated 
section iv apply methods standard denoising problems speech segmentation 
ii 
bayesian inference fixed model dimensions assume model order segment fixed known priori notational convenience denote unknown parameters case bayesian model estimation objectives follow bayesian approach regarded random known prior reflects degree belief different values quantities 
order increase robustness prior hyperparameters assumed random vague distribution adopt hierarchical bayesian model 
bayesian hierarchical model case natural introduce binomial distribution prior distribution number changepoints positions indicator function set 
assign normal distribution parameters models hyperparameter segments model order assumed known conjugate inverse gamma distribution noise variances choice prior see gaussian noise model allows marginalization parameters algorithm requires specification itis clear parameters play important role model selection 
bayes factors dependent 
order increase robustness prior propose estimate data done example consider random 
assign vague conjugate inverse gamma distribution scale hyperparameter set choose uniform prior distribution noninformative improper jeffreys prior hierarchical structure assumed prior parameters 
visualized directed acyclic graph dag shown fig 
convenience show fixed parameters 
problem parameter space written finite union subspaces denotes space parameters th segment denotes hyperparameter space defined section ii 
worth noticing algorithmic point view model allows faster updating markov chain due conditional independence coefficient regression variance parameters hyperparameters 
bayesian curve fitting mcmc applications signal segmentation fig 

dag prior distribution 
natural hierarchical structure setup formalize modeling joint distribution variables noise assumed gaussian section likelihood takes form euclidean norm 
bayesian detection estimation bayesian inference posterior obtained bayes theorem aim estimate posterior distribution specifically features marginal distributions 
case possible obtain quantities analytically requires evaluation high dimensional integrals nonlinear functions parameters 
apply mcmc methods reversible jump mcmc method particular see section ii details 
key idea build ergodic markov chain equilibrium distribution desired posterior distribution 
weak additional assumptions samples generated markov chain asymptotically distributed posterior distribution easily evaluate posterior features interest 
proposed bayesian model allows integration nuisance parameters hyperparameter resultant expression normalizing constant fixed model order case 
pointed posterior distribution complex parameters posterior model probability determined analytically 
section develop method estimate needed approximation obtained number changepoints positions easily estimated map criterion corresponding estimates 
alternatively compute minimum mean square error mmse estimate regression function bayesian model averaging 
mcmc algorithm problem addressed fact model uncertainty problem variable dimensionality terms number changepoints 
treated efficiently reversible jump mcmc method 
method extends traditional metropolis hastings algorithm case moves ieee transactions signal processing vol 
march dimension proposed accepted probability 
probability designed special way order preserve reversibility ensure invariant distribution markov chain mc 
general propose move model parameters model parameters proposal distribution acceptance probability proposal directly new parameter space dimensional matching random variables jacobian term equal see detailed 
fact particular choice moves affect convergence rate algorithm 
ensure low level rejection want proposed jumps small moves selected birth changepoint proposing new changepoint random death changepoint removing changepoint chosen randomly update changepoint positions proposing new position existing changepoints 
iteration moves described randomly chosen probabilities death changepoint impossible birth impossible choose move update hyperparameters performed 
describe main steps algorithm 
reversible jump mcmc algorithm main procedure initialize set birth new changepoint section ii death changepoint section ii update changepoints positions section ii 
update hyperparameters section ii 
goto 
detail steps algorithm 
simplify notation drop superscript variables iteration death birth changepoints current state mc consider death move implies modification dimension model respectively proposal begins choosing changepoint removed existing ones 
move accepted segments th fig 

death left birth right moves 
th merged reducing new segment created see fig 

algorithm death move choose changepoint removed evaluate see 
new mc state accepted 
birth move position new changepoint proposed means th segment split move accepted 
assuming current state mc 
algorithm birth move propose new changepoint position evaluate see 
new state mc accepted 
acceptance ratio birth death changepoint moves deduced general expression corresponding acceptance probabilities birth changepoint obtain bayesian curve fitting mcmc applications signal segmentation convenience denote segment changepoints update changepoint positions update changepoint positions involve change dimension somewhat complicated birth death moves 
fact updating position changepoint means removing th changepoint proposing new approach facilitates extension algorithm complex case unknown model orders segment 
determine worth noticing update move may described combination birth changepoint death changepoint see fig 

just update position segment 
process repeated existing changepoints described detail 
algorithm update changepoint positions propose new position th changepoint determine evaluate see see 
new state mc accepted 
update positions changepoints combines birth th changepoint death th changepoint time acceptance ratio proposed move fig 

update changepoint positions 
defined 
metropolis update random walk proposal perform local exploration space 
update hyperparameters algorithm developed requires simulation hyperparameters done standard gibbs moves sampled inverse gamma gamma distributions respectively 
probability distribution allowing update requires simulation nuisance parameters turn sampled considering fixed model order case 
assuming current state mc update hyperparameters performed algorithm 
algorithm update hyperparameters update sample see 
sample see 
sample see 
ieee transactions signal processing vol 
march iii 
bayesian inference unknown model dimensions previous section addressed problem segmentation assumption known applications different model orders considered different segments model orders estimated data available 
address difficult problem 
extended bayesian model section ii original bayesian model proposed case fixed model orders segment 
steps analogous taken section yield extended bayesian model unknown parameters including orders models different segments regarded drawn appropriate prior distributions 
hierarchical structure prior adopt truncated poisson distribution model order mean interpreted expected approximate number model parameters 
parameters assign priors similar ones introduced section ii see exception hyperparameter different different segments assumed drawn inverse gamma distribution particular case bayes factors depends hyperparameter see assume randomly distributed conjugate prior gamma distribution prior robust fixed 
similarly assign conjugate prior gamma density 
uniform prior distribution noninformative jeffreys priors chosen correspondingly 
result extended hierarchical structure assumed prior parameters 
fact discrete probability distribution may adopted prior addition priors dependent number changepoints introduced 
fig 

dag prior distribution 
visualized dag shown fig 
convenience show fixed parameters 
bayesian inference mentioned section ii bayesian inference unknown parameters posterior probability distribution fixed model order case parameters hyperparameter integrated giving marginalized expression see 
resulting posterior distribution appears highly nonlinear parameters precluding analytical calculations bayesian curve fitting mcmc applications signal segmentation mcmc methods employed order evaluate posterior features interest 
mcmc algorithm case orders models segment unknown bayesian computation estimation joint posterior distribution complex 
double model selection terms number changepoints model orders performed 
mcmc sampler capable jumping subspaces variable dimensionality terms constructed 
order able sample directly joint distribution denotes hyperparameter space propose reversible jump mcmc method 
procedure similar described section ii 
moves model parameters model parameters generated proposal distribution randomly accepted acceptance probability particular moves relative birth death changepoints update positions randomly chosen probabilities section ii 
addition due double variable dimensionality update model order segments performed changepoint moves 
algorithm proceeds follows 
reversible jump mcmc algorithm main procedure initialize set birth new changepoint section iii death changepoint section ii update changepoints positions section iii 
update model orders hyperparameters section ii 
goto 
integrate parameter increases significantly computational complexity resulting mcmc sampler 
different steps algorithm described superscript variables iteration dropped 
changepoint moves algorithms birth death changepoints update positions section ii easily extended case unknown 
main difficulty choose proposal new model orders 
employ approach 
segments th th merged model order new segment sum model orders original segments model orders existing th th segments 
th segment split new model orders selected randomly set equal order original th model see fig 

ensures birth death moves reversible 
update move mentioned performed combination birth death changepoints see fig 

update position changepoint respect changepoints change stays changepoints update order models 
addition sample hyperparameter new segments created removing adding changepoint recall different segment segment 
select proposal distribution means distributions matrices corresponding value hyperparameter mean distribution see details acceptance probabilities birth death moves birth changepoint obtain ieee transactions signal processing vol 
march acceptance probability update changepoint positions algorithms moves detail 
model orders update update model orders segment involve changing number changepoints positions 
perform jumps subspaces different dimensions continue reversible jump mcmc method formulated complicated form 
similarly moves chosen birth model parameter death model parameter update hyperparameter probabilities choosing moves defined exactly way changepoint moves procedure performed segment main steps described follows 
algorithm update model orders hyperparameters propose propose see new state mc accepted 
sample see sampled 
propose see see sample hyperparameters see 
acceptance probability different types moves terms model orders birth move acceptance ratio assuming current model order obtains acceptance ratio death move birth death moves reversible 
account series representation exponential function adopt proposal distribution parameter sample metropolis hastings step acceptance probability equal hyperparameters sampled standard gibbs move analogy 
similarly sample iv 
simulations set simulations address standard problem denoising smooth test functions :10.1.1.124.5288
compare results fixed model order subsequently apply algorithm unknown model orders segmentation signals modeled piecewise constant ar processes speech signal 
curve fitting fixed model order smooth function assessed performance algorithm proposed section ii applying synthetic piecewise polynomials model parameters noise variances table 
estimates number changepoints positions obtained map criterion see section ii iterations algorithm burn period iterations yielded appreciable difference 
estimated number changepoints equal table ii gives estimated changepoint positions 
fig 
show original noisy estimated curves 
fig 
estimation marginal posterior distributions number changepoints 
mmse estimate regression function obtained making bayesian model averaging 
bayesian curve fitting mcmc applications signal segmentation table parameters polynomial model noise variance segment table ii real estimated values changepoint positions fig 

piecewise polynomials 
top original curve 
middle curve noise added 
bottom estimate 
fig 

estimation marginal posterior distribution number changepoints piecewise polynomial model 
algorithm coded matlab simulations performed mhz intel pentium iii pc 
processing iterations required average table iii estimated number changepoints average mmse fig 

blocks test curve 
top true function noise added 
bottom estimate function 
fig 

heavisine test curve 
top true function noise added 
bottom estimate function 
functions second example applied algorithm common curves blocks heavisine previously literature test :10.1.1.124.5288:10.1.1.124.5288
number grid points taken standard noise deviation set equal segments model order polynomial set results number changepoints average mmse compared obtained table iii 
figs 
show original functions noise added estimates obtained 
worth noticing polynomial order equal adopted reconstructions blocks heavisine curve perfect 
estimated number changepoints average mmse bumps ieee transactions signal processing vol 
march table iv parameters ar model noise variance segment table real estimated values changepoint model order doppler obtained additional experiments shown table iii 
results wavelet methods method performs significantly better 
hierarchical model allows automatic determination hyperparameter values contrary 
effect providing sparse representation regression function reduced mmse prevent overfitting 
signal segmentation unknown model orders piecewise constant autoregressive processes illustrate performance segmentation method proposed applying synthetic data described piecewise constant autoregressive ar process changepoints 
parameters ar models noise variances drawn random table iv 
interpret samples initial conditions proceed analysis remaining data points 
number iterations algorithm results higher number iterations indistinguishable described section ii adopt map detection criterion finds changepoints 
fixed model order segment estimated map results table fig 
shows segmented signal estimation marginal posterior distributions changepoint positions estimated mean associated standard deviation marginal posterior distributions realizations experiment fixed model parameters changepoint positions 
results fig 
worth noticing stable respect fluctuations excitation noise realization 
speech segmentation implemented proposed algorithm processing real speech signal previously examined literature 
recorded inside car french national agency fig 

top segmented signal original changepoints shown solid line estimated changepoints shown dotted line 
bottom estimation marginal posterior distribution changepoint positions 
fig 

mean standard deviation realizations posterior distribution 
telecommunications testing evaluating speech recognition algorithms described 
sampling frequency khz highpass filtered version signal cut frequency hz resolution bits fig 

different segmentation methods applied signal summary results 
show results table vi order compare ones obtained proposed method see figs 

estimated orders ar models table vii see quite different segment segment 
resulted different positions changepoints especially crucial case third fourth changepoint 
position changed significantly due estimated model orders second third segments 
illustrated fig 
changepoints obtained proposed method visually preferable 
bayesian curve fitting mcmc applications signal segmentation fig 

segmented speech signal changepoints estimated gustafsson shown dotted line ones estimated proposed method shown solid line 
table vi changepoint positions different methods fig 

changepoint positions changepoints estimated gustafsson shown dotted line ones estimated proposed method shown solid line 
table vii estimated model orders bayesian methods variety challenging functions modeled piecewise constant linear regression 
model algorithms appear quite flexible applications denoising signal segmentation 
prediction straightforwardly evaluate predictive distribution mcmc samples 
possible extensions 
extend algorithm multivariate signals statistical assumptions noise distribution relaxed line version algorithm developed particle filtering methods 
denison mallick smith automatic bayesian curve fitting stat 
soc 
vol 
pp 

gustafsson adaptive filtering change detection 
new york wiley 
fitzgerald numerical bayesian methods applied signal processing 
new york springer verlag 
andr new statistical approach automatic segmentation continuous speech signals ieee trans 
acoust speech signal processing vol 
pp 
jan 
basseville detection abrupt changes theory application 
englewood cliffs nj prentice hall 
djuri map solution line segmentation signals proc 
conf 
ieee icassp pp 

barry hartigan bayesian analysis changepoint problems amer 
stat 
assoc vol 
pp 

stephens bayesian retrospective multiple changepoint identification appl 
stat vol 
pp 

mallick bayesian curve estimation polynomials random order statist 
plan 
inform vol 
pp 

richardson green bayesian analysis mixtures unknown number components roy 
stat 
soc 
vol 
pp 

green reversible jump mcmc computation bayesian model determination biometrika vol 
pp 

robert bayesian choice 
decision theoretic motivation nd ed 
new york springer verlag 
andrieu djuri doucet mcmc computation bayesian model selection signal process vol 
pp 

godsill relationship mcmc model uncertainty methods comput 
graph 
stat published 
robert casella monte carlo statistical methods 
new york springer verlag 
andrieu doucet fitzgerald bayesian segmentation piecewise constant autoregressive processes mcmc cambridge univ eng 
dept tech 
rep cued feng tr 
donoho johnstone ideal spatial adaptation wavelet shrinkage biometrika vol 
pp 

basseville benveniste design comparative study sequential jump detection algorithms digital signals ieee trans 
acoust speech signal processing vol 
assp pp 

appel brandt adaptive sequential segmentation piecewise stationary time series inform 
sci vol 
pp 

elena received sc 
degree mechanical engineering moscow state technical university moscow russia phil 
degree information engineering university cambridge cambridge 
currently pursuing ph degree signal processing laboratory department engineering university cambridge 
research interests focus bayesian inference markov chain monte carlo methods sequential monte carlo methods application wireless communication problems 
christophe andrieu born france 
received degree institut national des communications paris france degree ph degree university paris xv 
september research associate signal processing group cambridge university cambridge lecturer department mathematics bristol university bristol research interests include spectral analysis source separation markov chain monte carlo methods stochastic optimization 
ieee transactions signal processing vol 
march arnaud doucet born france november 
received degree institut national des telecommunications paris france ph degree university paris xi orsay france 
visiting scholar signal processing group cambridge university cambridge research associate group 
march senior lecturer department electrical engineering melbourne university victoria australia 
research interests include markov chain monte carlo methods sequential monte carlo methods bayesian statistics hidden markov models 
edited de freitas gordon sequential monte carlo methods practice new york springer verlag 
william fitzgerald received sc sc ph degrees physics university birmingham birmingham head statistical modeling group signal processing laboratory department engineering university cambridge cambridge university reader statistics signal processing 
prior joining cambridge worked institut langevin grenoble france years professor physics swiss federal institute technology eth rich years 
fellow christ college cambridge director studies engineering 
works bayesian inference applied signal data modeling 
interested nonlinear signal processing image restoration extreme value statistics 
