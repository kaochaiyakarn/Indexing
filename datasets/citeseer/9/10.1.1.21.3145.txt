temporally dependent plasticity information theoretic account gal chechik naftali tishby school computer science engineering interdisciplinary center neural computation hebrew university jerusalem israel cs huji ac il paradigm hebbian learning received novel interpretation discovery synaptic plasticity depends relative timing pre post synaptic spikes 
derives temporally dependent learning rule basic principle mutual information maximization studies relation experimentally observed plasticity 
nd supervised spike dependent learning rule sharing similar structure experimentally observed plasticity increases mutual information stable near optimal level 
analysis reveals temporal structure time dependent learning rules determined temporal lter applied neurons inputs 
results suggest experimental prediction dependency learning rule neuronal biophysical parameters hebbian plasticity major paradigm learning computational neuroscience years ago interpreted learning correlated neuronal activity 
series studies shown changes synaptic highly depend relative timing pre postsynaptic spikes ecacy synapse excitatory neurons increases presynaptic spike precedes postsynaptic decreases 
magnitude synaptic changes decays roughly exponentially function time difference pre post synaptic spikes time constant tens milliseconds results vary studies especially regard synaptic depression component compare 
computational role delicate type plasticity termed spike timing dependent plasticity authors suggested answers question modeling studying ects synaptic neural network dynamics 
importantly embodies inherent competition incoming inputs shown result normalization total incoming synaptic strength maintain irregularity neuronal ring supported part human frontier science project rg 
lead emergence synchronous subpopulation ring recurrent networks 
may play important role sequence learning 
dynamics synaptic operation strongly depends implemented additively independent baseline synaptic value multiplicatively change proportional synaptic ecacy 
takes di erent approach study spike dependent learning rules studies model study model properties start deriving spike dependent learning rule rst principles simple rate model compare experimentally observed 
derive learning rule consider principle mutual information maximization 
idea known infomax principle states goal neural network learning procedure maximize mutual information output input 
current applies infomax leaky integrator neuron spiking inputs 
derivation suggests computational insights dependence temporal characteristics biophysical parameters shows may serve maximize mutual information network spiking neurons 
model study network input neurons ring spike trains single output target neuron point time target neuron accumulates inputs temporal lter due voltage attenuation synaptic transfer function dt synaptic ecacy ith input neuron target neuron spike spike th spike train membrane time constant 
lter may consider general synaptic transfer function voltage decay ects set example exponential lter exp 
learning goal set synaptic weights uncorrelated patterns input activity may discriminated output 
pattern determines ring rates input neurons noisy realization due stochasticity point process 
input patterns periods length order tens milliseconds 
period pattern randomly chosen presentation probability patterns rare abundant may thought background noisy pattern 
stressed model information coded non stationary rates underlie input spike trains 
rates observable learning depends observable input spikes realize underlying rates 
mutual information maximization focus single presentation period omitting notation look value period dt denoting input output mutual information network de ned log dx di erential entropy distribution di erential entropy network known input pattern 
mutual information measures easy decide input pattern network observing network output calculate conditional entropy assumption input neurons re independently number large input target neuron network pattern normally distributed mean variance wx wx brackets denote averaging possible realizations inputs network pattern calculate entropy note mixture gaussians resulting presentation input pattern assumption approximate entropy 
details derivation omitted due space considerations 
di erentiating mutual information regard obtain cov cov expected value averaged presentation pattern general form complex gradient simpli ed sections discussion biological learning 
derived gradient may gradient ascent learning rule repeatedly calculating distribution moments depend updating weights 

learning rule climbs gradient bound converge local maximum mutual information 
plots mutual information operation learning rule showing network reaches possibly local mutual information maximum 
depicts changes output distribution learning showing splits segregated bumps corresponds pattern corresponds rest patterns 
learning biological system aiming obtain spike dependent biologically feasible learning rule maximizes mutual information turn approximate analytical rule derived rule implemented biology 
steps taken step corresponds biological constraint solution 
biological synapses limited excitatory inhibitory regimes 
information believed coded activity excitatory neurons limit weights positive values 
secondly terms global functions weights input distributions depend distribution moments avoid problem approximate learning rule replacing fk constants constants set optimal values remain xed set 
numerically high performance demonstrated section may obtained wide regime constants 
mi time steps output value mutual information output distribution learning gradient ascent learning rule eq 

patterns constructed setting input neurons re poisson spike trains hz rest re hz 
poisson spike trains simulated discretizing time millisecond bins 
simulation parameters msec 
input output mutual information output distribution learning steps 
outputs segregate distinct bumps corresponds presentation pattern corresponds rest patterns 
thirdly summation patterns embodies batch mode learning requiring large memory average multiple presentations 
implement online learning rule replace summation patterns pattern triggered learning 
note analytical derivation yielded summation performed rare patterns eq 
pattern triggered learning naturally implemented restricting learning presentations rare patterns fourthly learning rule explicitly depends cov observables model 
replace performing stochastic weighted averaging spikes yield spike dependent learning rule 
case inhomogeneous poisson spike trains input neurons re independently covariance terms obeys cov dt expectations may simply estimated weighted averaging observed spikes precede learning moment 
estimating dicult stated learning triggered rare patterns 
spikes ect rare pattern 
possible solution fact highly frequent spikes vicinity presentation high probability spikes average spikes presentation background activity estimation 
spikes temporally weighted ways computational point view bene cial weigh spikes uniformly time may require long memory biologically improbable 
refrain suggesting speci weighting background spikes obtain rule fact learning rules learning triggered presentation background pattern explicitly depend prior probabilities robust uctuations uctuations strongly reduce mutual information obtained rules conclude pattern triggered learning triggered rare pattern 
activated rare patterns mem 
dt dt dt dt denote temporal weighting spikes 
noted learning rule uses rare pattern presentations external supervised learning signal 
general form learning rule performance discussed section 
analyzing biologically feasible rule comparing performance obtained new spike dependent learning rule may implemented biological system approximates information maximization learning rule 
approximations 
learning biologically feasible learning rule increase mutual information 
level curves gure compare mutual information learning rule eq 
eq 
traced simulation learning process 
apparently approximated learning rule achieves fairly performance compared optimal rule reduction performance due limiting weights positive values 
interpreting learning rule structure general form learning rule eq 
pictorially gure allow inspect main features structure 
synaptic potentiation temporally weighted manner determined lter neuron applies inputs learning apply average dt dt 
relative weighting components numerically estimated simulating optimal rule eq 
order magnitude 
second model synaptic depression targeted learning underlying structure background activity 
analysis restrict temporal weighting depression curve 
major di erence obtained rule experimentally observed learning rule rule learning triggered external learning signal corresponds presentation rare patterns experimentally observed rule learning triggered postsynaptic spike 
possible role postsynaptic spike discussed section 
unsupervised learning considered learning scenario external information telling pattern background pattern decide learning take place 
learning signal missing tempting postsynaptic spike signaling presence interesting input pattern learning signal 
yields learning procedure eq 
time learning triggered postsynaptic spikes external signal 
resulting learning rule similar previous models experimentally observed 
mechanism ectively serve learning postsynaptic spikes occur presentation rare pattern 
occurrence may achieved supplying short learning signals presence interesting patterns attentional mechanisms increasing neuronal 
induce learning postsynaptic spikes triggered rare pattern presentation 
issues await investigation 
time steps gradient ascent rule eq positive weights limitation spike dependent rule eq pre learning dw weight dependent weight possible spike weighting sum weight dependent independent components comparing optimal eq 
approximated eq 
learning rules 
input neurons set re hz rest re hz 
neurons re hz yielding similar average input patterns 
learning rates ratio eq 
numerically searched optimal value yielding arbitrary choice 
rest parameters fig 
pictorial representation eq 
plotting 
function time di erence learning signal time input spike time pike 
potentiation curve solid line sum exponents constants dashed lines 
depression curve constrained derivation examples brought dot dashed lines 
discussion framework information maximization derived spike dependent learning rule leaky integrator neuron 
learning rule achieves near optimal mutual information principle implemented biological neurons 
analytical derivation rule allows obtain insight learning rules observed experimentally various preparations 
fundamental result time dependent learning stems neuronal output inputs 
model embodied lter neuron applies input spike trains 
lter determined biophysical parameters neuron membrane leak synaptic transfer functions dendritic arbor structure 
model yields direct experimental predictions way temporal characteristics potentiation learning curve determined neuronal biophysical parameters 
cells larger membrane constants exhibit longer synaptic potentiation time windows 
interestingly time window observed potentiation ts time windows channel agreement cortical membrane time constants predicted current analysis 
features theoretically derived rule may similar functions experimentally observed rule model synaptic weakening targeted learn structure background activity 
synaptic depression potentiation model triggered rare pattern presentation allow near optimal mutual information 
addition synaptic changes depend synaptic baseline value sub linear manner 
experimental results regard unclear theoretical investigations show weight dependency large ect networks dynamics 
learning rule equation assumes independent ring input neurons derivation holds wider class inputs 
case correlated inputs learning rule involves cross synaptic terms may dicult compute biological neurons 
highly sensitive synchronous inputs remains interesting question investigate approximations infomax rule time structured synchronous inputs 
levy steward 
temporal contiguity requirements long term associative depression hippocampus 
neuroscience 
thompson 
asynchronous pre postsynaptic activity induces associative long term depression area ca rat hippocampus vitro 
proc 
natl 
acad 
sci 

regulation synaptic ecacy coincidence postsynaptic aps 
science 
zhang tao holt harris poo 
critical window cooperation competition developing synapses 
nature 
bi poo 
precise spike timing determines direction extent synaptic modi cations hippocampal neurons 
neurosci 
feldman 
timing ltp vertical inputs layer ii iii cells rat barrel cortex 
neuron 
kempter gerstner van hemmen 
hebbian learning spiking neurons 
phys 
rev 
abbot song 
temporally hebbian learning spike timing neural variability 
solla cohen editors advances neural information processing systems pages 
mit press 
song miller abbot 
competitive hebbian learning dependent synaptic plasticity 
nature neuroscience pages 
horn levy ruppin 
distributed synchrony spiking neurons hebbian cell assembly 
solla leen muller editors advances neural information processing systems pages 
mehta quirk wilson 
hippocampus ect ltp spatio temporal dynamics receptive elds 
bower editor computational neuroscience trends research 
elsevier 
rao sejnowski 
predictive sequence learning recurrent neocortical circuits 
solla leen muller editors advances neural information processing systems pages 
mit press 
rubin lee 
equilibrium properties temporally asymmetric hebbian plasticity 
phys 
rev press 
linsker 
self organization perceptual network 
computer 
shannon 
mathematical theory communication 
bell syst 
tech 

kempter gerstner van hemmen 
intrinsic stabilization output rates spike time dependent hebbian learning 
submitted 
