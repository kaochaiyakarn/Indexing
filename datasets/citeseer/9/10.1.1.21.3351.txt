link ping electronic articles computer information science vol 
nr automated classification web sites john pierre interwoven nd street th floor san francisco california usa interwoven com link ping university electronic press link ping sweden www ep liu se ea cis published february link ping university electronic press link ping sweden link ping electronic articles computer information science issn series editor erik sandewall john pierre typeset author atex formatted style recommended citation author 
title 
link ping electronic articles computer information science vol 
nr 
www ep liu se ea cis 
february 
url contain link author home page 
publishers keep article line internet possible replacement network period years date publication barring exceptional circumstances described separately 
line availability article implies permanent permission read article line print single copies unchanged non commercial research educational purpose including making copies classroom 
permission revoked subsequent transfers copyright 
uses article conditional consent copyright owner 
publication article date stated included production limited number copies archived swedish university libraries written works published sweden 
publisher taken technical administrative measures assure line version article permanently accessible url stated unchanged permanently equal archived printed copies expiration publication period 
additional information link ping university electronic press procedures publication assurance document integrity please refer www home page www ep liu se conventional mail address stated 
discuss issues related automated text classification web sites 
analyze nature web content metadata relation requirements text features 
find html metatags source text features wide despite role search engine rankings 
approach targeted spidering including metadata extraction opportunistic crawling specific semantic hyperlinks 
describe system automatically classifying web sites industry categories performance results different combinations text features training data 
system serve basis generalized framework automated metadata creation 
estimated pages accessible world wide web pages added daily 
describing organizing vast amount content essential realizing web full potential information resource 
accomplishing meaningful way require consistent metadata descriptive data structures semantic linking 
categorization important ingredient evident popularity web directories yahoo open directory project 
resources created large teams human editors represent type classification scheme widely useful suitable applications 
classification fundamental intellectual task take axiom important essential organizing understanding web content 
automated classification needed important reasons 
sheer scale resources available web nature 
simply feasible keep fast pace growth change web manual classification effort expending immense time effort 
second reason classification subjective activity 
different classification schemes needed different applications 
single classification scheme suitable applications 
different types classification schemes representing different facets knowledge may need applied ongoing fashion new applications demand 
domain specific classification schemes quickly applied large amounts content automated methods hold great promise generating effective metadata 
classification considered larger context metadata 
specific fields metadata records correspond different classification schemes 
effective rich metadata important establishing leveraging power semantic web 
web content shifts primarily text primarily multimedia oriented metadata important 
structured metadata serve driver applications knowledge search retrieval reasoning engines intelligent agents multi faceted organization information 
metadata creation tedious time consuming 
automated methods described useful facilitating metadata creation 
discuss practical issues applying methods automated classification web content 
take size fits approach advocate targeted specific classification tasks relevant solving specific problems 
section discuss nature web content implications automated categorization 
extracting features accurately different categories important part text categorization system 
possible desirable exploit metadata current web environment find far widespread 
section describe specialized system automatically classifying web sites industry categories 
system serve generalized framework efficient automated categorization web content includes targeted spidering domain specific classification trainable general purpose text categorization engine 
section results controlled experiments 
show text features extracted different parts web pages effect classification accuracy demonstrate metatags provide best results 
compare training data obtained different domain versus training data drawn target domain 
find training examples taken content classified give better results training data different domain suffice cases assembling new data scratch feasible 
related discussed section 
section state suggestions research 
text categorization web content current state web differs markedly vision semantic web outlined tim berners lee 
web content machine readable part far machine understandable 
furthermore ability computers understand written human language quite limited point time 
adopted text categorization approach relies heavily word indexing statistical classification sophisticated natural language processing knowledge inferencing 
approach capable giving results way robust assumptions content analyzed 
important consideration heterogenous nature web content 
main challenges classifying web pages wide variation content quality 
text categorization methods rely existence quality texts especially training 
known collections typically studied automated text classification experiments trec reuters comparison web lacks homogeneity 
matters worse existing web page content images plug applications non text media 
usage metadata inconsistent non existent 
section survey landscape web content relation requirements text categorization systems 
analysis web content attempt characterize nature content classified performed rudimentary quantitative analysis 
results obtained analyzing collection web domains obtained random dump database known domain name registration 
course results reflect biases small samples don necessarily generalize web reflective issues hand 
classification method text important know amount quality text features typically appear web sites 
existing standards web content tend de facto loosely enforced 
convention holds vast majority web sites top level entry point html web page take primary source text features 
body text generally free form typical html page common include title possibly set keywords description metatags 
promising sources text features web page metadata 
trend multimedia assets puts assumption doubt dealing problem non text information scope 
table show percentage web sites certain number words type 
analyzed sample domains live web sites counted number words content attribute meta name keywords meta name description tags title tags 
counted free text body tag excluding html tags 
table percentage web pages words html tags tag type words words words words title meta description meta keywords body text obvious source text body web page 
noticed top level web pages usable body text 
cases include pages contain frame sets images plug ins user agent followed redirects possible 
quarter web pages contained words majority web pages contained words 
title tags common amount text relatively small titles containing words 
titles contain names terms home page particularly helpful subject classification 
metatags keywords descriptions major search engines play important role ranking display search results 
despite third web sites contain tags 
turns metatags useful exist contain text specifically intended aid identification web site subject areas time metatags contained words smaller percentage containing words contrast number words body text tended contain words 
lack widespread metatags despite apparent incentive improve search engine rankings instructive 
metadata usually part presentation content benefit somewhat intangible tends neglected 
creating metadata tedious task 
methods facilitate creation quality metadata especially automated methods greatly needed 
text features feature selection important part building automated classification system 
proper set features classifier able accurately discriminate different categories 
feature set sufficiently broad wide variations occur instances class 
hand number possibilities misuse abuse tags improve search engine rankings known practices widespread sample little consequence 
features needs constrained reduce noise limit burden system resources 
argued purposes automated text categorization features 
relatively number 
moderate frequency assignment 
low redundancy 
low noise 
related semantic scope classes assigned 
relatively unambiguous meaning due wide variety purpose scope current web content items difficult requirements meet classification tasks 
subject classification metatags meet requirements better sources text titles body text 
lack widespread metatags problem coverage majority web content desired 
long term automated categorization really benefit greater attention paid creation usage rich metadata explicit semantic structures especially requirements taken consideration 
short term implement strategy obtaining text features existing html natural language cues takes requirements goals classification task consideration 
techniques shallow parsing information extraction useful regard 
experimental setup constructed full scale automated classification system performed experiments real world data order gauge system performance test ideas 
goal targeted domain specific task rapidly classify web sites domain names broad industry categories 
section describe main ingredients classification experiments including data architecture evaluation measures 
classification scheme categorization scheme top level north american industrial classification system naics consists broad industry categories shown table 
resources previously classified older standard industrial classification sic system 
cases published mappings convert assigned sic categories naics equivalents 
full naics levels hierarchy contains subcategories 
experiments lower level naics subcategories generalized appropriate top level category entire classification scheme utilized system finer grained categorization desired 
naics sic examples authoritative controlled vocabularies 
published standardized classification scheme idea order take advantage person hours time takes construct 
addition may possible take advantage table top level naics categories naics code naics description agriculture forestry fishing hunting mining utilities construction manufacturing wholesale trade retail trade transportation warehousing information finance insurance real estate rental leasing professional scientific technical services management companies enterprises administrative support waste management remediation services educational services health care social assistance arts entertainment recreation accommodation food services services public administration public administration unclassified establishments existing content classified scheme source training data 
targeted spidering results section obvious selection adequate text features important issue certainly taken granted 
balance needs text classifier speed storage limitations large scale crawling effort took approach spidering web sites gathering text targeted classification task hand 
preliminary tests best classifier accuracy obtained contents keywords description metatags source text features 
adding body text decreased classification accuracy 
due lack widespread usage metatags limiting features practical sources text titles body text needed provide adequate coverage web sites 
targeted spidering approach attempted gather higher quality text features metatags resorted lower quality texts needed 
opportunistic spider began top level page web site attempted extract useful text metatags titles exist followed links frame sets existed 
followed hyperlinks contained key substrings anchor text product services info press news looked content pages 
substrings chosen ad hoc frequency analysis assumption tend point content useful deducing industry classification 
content spider gather actual body text web page 
extracted text concatenated single representative document site classification engine 
efficiency ran spiders parallel working different lists individual domain names 
attempting restricted set hyperlinks take advantage current web implicit semantic structure 
advantages moving explicit semantic structure hypertext documents opportunistic spidering approach really benefit formalized description semantic relationships linked web pages 
allow spiders easily find relevant resources having crawl entire network web 
test data initial list domain names targeted spider determine sites live extracted text features approach outlined section 
domain names usable text content pre classified industry categories set data drew samples training testing validation 
training data took approaches constructing training sets classifiers 
approach combination naics category labels including subcategories securities exchange commission sec public companies training examples 
second approach set pre classified domain names text domain obtained spider 
approach considered prior knowledge obtained different domain 
interesting see knowledge different domain generalizes problem classifying web sites 
furthermore case training examples difficult obtain need automated solution place 
second approach conventional classification example 
case possible fact database domain names pre classified industry categories 
classifier architecture text classifier consisted modules targeted spider extracting text features associated web site information retrieval engine comparing queries training examples decision algorithm assigning categories 
industry classifications domain names provided dunn 
sec annual reports required public companies describe business activities year 
public assigned sic category 
spider designed quickly process large database top level web domain names domain com domain net 
described section implemented opportunistic spider targeted finding high quality text pages described business area products services commercial web site 
accumulating text features query submitted text classifier 
domain name automatically assigned categories logged central database 
spiders run parallel efficient system resources 
information retrieval engine latent indexing lsi 
lsi variation vector space model information retrieval uses technique singular value decomposition svd reduce dimensionality vector space 
words tend occur document share large projections directions reduced space 
theoretically reduces noise due redundant spurious word usage automatically derives relationships words inherent concepts 
cosine similarity computed reduced vector space amounts concept matching word 
example queries containing word car match documents containing word automobile provided relationship words concept established corpus 
previous shown lsi provided better accuracy fewer training set documents category standard tf idf weighting 
queries compared training set documents cosine similarity ranked list matching documents scores forwarded decision module 
decision module nearest neighbor algorithm ranking categories assigned top ranking category web site 
type classifier tends perform compared methods robust tolerant noisy data important qualities dealing web content 
addition algorithm capable producing results amount training data limited 
decision module responsible thresholding presenting final set automatically assigned categories 
evaluation measures system evaluation carried standard precision recall measures 
precision number correct categories assigned divided total number categories assigned serves measure classification accuracy 
higher precision smaller amount false positives 
recall number correct categories assigned divided total number known correct categories 
higher recall means smaller amount missed categories 
theory scores desirable precision recall 
practice human assigned classifications may achieve scores depending classification task 
extent classification subjective task usually grey areas classification scheme 
measure combines precision recall equal importance single parameter optimization defined precision recall 
computed global estimates system performance microaveraging results computed global sums decisions macro averaging results computed category basis averaged categories 
micro averaged scores tend dominated commonly categories macro averaged scores tend dominated performance rarely categories 
distinction relevant problem turned vast majority commercial web sites associated manufacturing category 
results experiment varied sources text features preclassified web domains 
constructed separate test sets text extracted body text metatags keywords descriptions combination 
training set consisted sec documents naics category descriptions 
results shown table 
table performance vs text features sources text micro micro micro body body metatags metatags metatags source text features resulted accurate classifications 
precision decreases noticeably body text 
interesting including body text metatags resulted accurate classifications 
results influenced design spider extracted metatags foremost grabbing body text resort 
usefulness metadata source high quality text features meets criteria listed 
second experiment compared classifiers constructed different training sets described section 
results shown table 
table performance vs training set classifier micro micro micro macro macro macro sec naics web pages sec naics training set achieved respectable micro averaged scores macro averaged scores low 
reason classifier generalizes categories common business web domains trouble recall categories represented business domain poor precision categories common web domain 
training set constructed web site text performed better 
macro averaged recall lower micro averaged recall 
partially explained example 
categories wholesale trade retail trade subtle difference especially comes web page text tends focus products services delivered retail vs wholesale distinction 
training set category common tended assigned place resulting low recall 
rare categories tended low recall 
related automatically constructed large scale web directories deployed commercial services northern light inktomi directory engine web site catalog 
details systems generally unavailable proprietary nature 
interesting directories tend popular manually constructed counterparts 
system automated discovery classification domain specific web resources described part desire ii project 
classification algorithm weights terms metatags higher titles headings weighted higher plain body text 
describe classification software topic filter harvesting subject specific web index 
system pharos part alexandria digital library project scalable architecture searching heterogeneous information sources leverages metadata automated classification 
hyperlink structure web exploited automated classification anchor text context linking documents source text features 
approaches efficient web spidering investigated especially important large scale crawling efforts 
complete system automatically building searchable databases domain specific web resources combination techniques automated classification targeted spidering information extraction described 
automated methods knowledge discovery including classification important establishing semantic web 
classification basic intellectual task challenging automate due somewhat subjective nature 
possible achieve results automated methods meet exceed manual results 
single classification scheme adequate applications 
advocate pragmatic approach including targeted techniques specialized domain knowledge applied specific classification tasks 
result efficient optimized system task hand 
described practical system automatically classifying web sites industry categories gives results 
type system applied domain specific classification scheme 
needed define categories assemble training data configure spider extract appropriate features 
spider may constructed follow specific types links extract sections web page content useful domain 
results table concluded metatags best source quality text features compared body text 
limiting metatags able classify large majority web sites 
opted targeted spider extracted text looked pages described business activities degraded text necessary 
clear text contained structured metadata fields results better automated categorization 
web moves formal semantic structure outlined tim berners lee automated methods benefit 
different kinds automated classification tasks accomplished accurately web useful usable 
rich metadata web content key better searching better organization managment content improved intelligent agents capable discovering acting knowledge embedded vast online resources 
shown creation metadata remains bottleneck despite strong incentives better rankings search engine results 
way ensure widespread quality metadata process metadata creation painless possible 
automated methods reliably accurately generate metadata existing content hold promise regard 
furthermore metadata needs multi faceted current extensible 
automated systems keep pace rate generation new web content see today 
outline basic approach building targeted automated categorization solution web content knowledge gathering important clear understanding domain classified quality content involved 
web heterogenous environment domains patterns commonalities emerge 
advantage specialized knowledge improve classification results 
targeted spidering classification task different features important 
due lack homogeneity web content existence key features quite inconsistent 
targeted spidering approach tries gather key features possible little effort possible 
type approach benefit greatly web structure encourages metadata semantically typed links 
interesting detailed analysis semantic spidering effect system performance 
training best training data comes domain classified gives best chance identifying key features 
cases feasible assemble training data target domain may possible achieve acceptable results training data gathered different domain 
true web content unstructured uncontrolled immense difficult assemble quality training data 
controlled collections pre electronic documents obtained important domains legal medical applied automated categorization web content 
classification addition accurate possible classification method needs efficient scalable robust tolerant noisy data 
classification algorithms utilize link structure web including formalized semantic linking structures investigated 
non text content images applets plugins music video prevalent web 
devising automated methods deal kind content important area investigation 
effective metadata way help manage types non text assets 
better acceptance metadata key semantic web 
creation quality metadata tedious prime candidate automated methods 
preliminary method outlined serve basis bootstrapping sophisticated classifier takes full advantage semantic web 
bill collaboration system design software implementation roger mark butler ron daniel useful discussions 
special network solutions providing classified domain names 
berners lee 
semantic web road map 
www org designissues semantic html 
yahoo www yahoo com www com open directory project www dmoz org lewis 
text representation intelligent text retrieval classification oriented view 
jacobs editor text intelligent systems chapter 
lawrence erlbaum 
north american industrial classification system naics united states 
www census gov www naics html pierre butler 
practical evaluation ir automated classification systems 
eighth international conference information knowledge management 
deerwester dumais furnas landauer harshman 
indexing latent semantic analysis 
journal american society information science 
van rijsbergen 
information retrieval 
butterworths london 
lewis 
evaluating text categorization 
proceedings speech natural language workshop morgan kaufmann 
yang liu 
re examination text categorization methods 
proceedings nd annual acm sigir conference research development information retrieval 
northern light www northernlight com inktomi directory engine www inktomi com products portal directory web site catalog search com html koch 
construction subject index 
eu project desire ii working 
www lub lu se desire desire wp html kock 
automatic classification full text specific subject area 
eu project desire ii working 
www lub lu se desire desire wp html agrawal dillon el abbadi 
pharos scalable distributed architecture locating heterogeneous information sources version 
proceedings th international conference information knowledge management 
agrawal el abbadi pearlman 
automated classification summarizing selecting heterogeneous information sources 
lib magazine january 
attardi sebastiani 
automatic web page categorization link context analysis 
chris hutchison gaetano eds proceedings thai european symposium telematics hypermedia artificial intelligence 
cho garcia molina page 
efficient crawling url ordering 
computer networks isdn systems www vol 

rennie mccallum 
reinforcement learning spider web efficiently 
proceedings sixteenth international conference machine learning 
mccallum nigam rennie seymore 
machine learning approach building domain specific search engines 
sixteenth international joint conference artificial intelligence 
jones mccallum nigam riloff 
bootstrapping text learning tasks 
ijcai workshop text mining foundations techniques applications 

