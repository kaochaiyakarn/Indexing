hierarchically classifying documents words daphne koller gates building computer science department stanford university stanford ca koller cs stanford edu mehran sahami gates building computer science department stanford university stanford ca sahami cs stanford edu proliferation topic hierarchies text documents resulted need tools automatically classify new documents hierarchies 
existing classification schemes ignore hierarchical structure treat topics separate classes inadequate text classification large number classes huge number relevant features needed distinguish 
propose approach utilizes hierarchical topic structure decompose classification task set simpler problems node classification tree 
show smaller problems solved accurately focusing small set features relevant task hand 
set relevant features varies widely hierarchy relevant feature set may large classifier examines small subset 
reduced feature sets allows utilize complex probabilistic models encountering standard computational robustness difficulties 
past decade witnessed explosion availability online information millions documents topic easily accessible internet 
available information increases inability people assimilate profitably utilize large amounts information apparent 
successful paradigm organizing mass information making comprehensible people categorizing different documents topic topics organized hierarchy increasing specificity 
hierarchical classifications type long special purpose collections documents medline hersh collections patent documents self 
internet search engines yahoo yahoo 
infoseek infoseek categorize contents world wide web 
bottleneck classification tasks need person read document decide appropriate place hierarchy 
clearly avoid bottleneck automatically classifying new documents 
infoseek attempted overcome difficulty neural network technology automatically categorize webpages infoseek 
ways task ideally suited application machine learning techniques 
specified set classes topics hierarchy large training set consisting documents classified 
exceptions notably almuallim focused hierarchically structured attributes classes classification ignored problem supervised learning presence hierarchically structured classes 
unsupervised hierarchical clustering fisher 
course standard classification techniques applied problem directly 
simply construct flattened class space class leaf hierarchy 
presence absence different words features 
train single classifier document classified belonging precisely possible basic classes 
unfortunately simplistic approach breaks context text classification 
resulting classification problem huge large corpus may hundreds classes thousands features 
computational cost training classifier problem size prohibitive 
furthermore variance resulting classifier typically large model parameters need estimated easily lead overfitting training data 
result typically able simple classifiers naive bayes 
previous schutze hull pedersen shown feature selection useful tool dealing issues 
eliminate words appear corpus topic 
previous koller sahami showed obtain significant increase accuracy reducing number words classification :10.1.1.155.2293
features computational cost robustness pose significant limitations 
propose new approach classification structured hierarchy topics 
ignoring topical structure building single huge classifier entire task structure break problem manageable size pieces 
basic insight supporting approach topics close hierarchy typically lot common topics far apart 
difficult find precise topic document color printers may easy decide agriculture computers 
building intuition approach divides classification task set smaller classification problems corresponding splits classification hierarchy 
example may classifier distinguishes articles agriculture articles computers applied documents agriculture distinguishes animal crop farming 
subtasks significantly simpler original task classifier node hierarchy need distinguish small number categories 
possible determination small set features 
example appears fairly small number words computer farm plant software presence absence document clearly differentiates documents agriculture documents computers 
ability restrict small feature set avoids difficulties describe 
resulting models robust subject overfitting 
important note key merely feature selection integration hierarchical structure 
understand integration observe set features required subtasks varies widely 
example words help differentiate agriculture computers useful distinguishing animal crop farming word farm helpful fairly appear documents types word computer helpful appear virtually documents reach classifier 
classifier uses small set features set features classification process large 
single flattened classifier consider features order reasonable job classifying documents 
document features irrelevant serve confuse classifier 
hierarchical approach document percolating hierarchy classifiers encounters questions concerning small fraction features process document computers probably meet classifier utilizing word cow 
features classifier utilize divided focus attention classifier features relevant classification subtask hand 
reduction feature space allows go simple classifiers naive bayes restricted tasks involving large number features 
example train probabilistic classifier tan friedman goldszmidt kdb sahami takes account correlation different features fact microsoft windows tend cooccur classifiers provide realistic model text data potentially leading higher classification accuracy 
search complex hypothesis space significantly expensive quadratic exponential number features versus linear time behavior naive bayes 
furthermore willing spend time complex models usually unsuitable large feature spaces due problem overfitting 
case feature selection tion idea hierarchical structure key success 
results show flat classifier virtually incapable advantage richer models regardless size feature space 
contrast expressive classifiers nodes hierarchy classification accuracy increases substantially 
conjecture choice feature set dependency models appropriate different nodes hierarchy radically different 
flat classification case dependency model aggregate various lower level models 
consequence dependencies wash complex represent limited dependency model 
hierarchical structure allows focus feature space dependency model relevant distinctions 
show combination techniques provides significant accuracy gains standard flat approach 
note techniques designed dealing huge classification tasks arising context text classification may useful domains 
example medical applications want classify patient disease symptoms test results 
classes diseases organized taxonomic hierarchy small number features needed distinguish neighboring classes 
rest structured follows 
section discuss specific techniques feature selection classification 
focus probabilistic techniques provide coherent underlying framework feature selection construction classifiers various complexities 
emphasize basic paradigm way depends particular techniques 
section section provide experimental methodology variety results supporting approach 
show technique allows restrict set features significantly resulting classifier addition smaller easier train provides better accuracy flat classifier 
conclude section discussion directions 
probabilistic framework general approach described consists constructing hierarchical set classifiers set relevant features 
uses main subroutines feature selection algorithm deciding appropriate feature set decision point supervised learning algorithm constructing classifier decision 
general approach instantiated variety ways depending choice subroutines 
chosen focus probabilistic methods feature selection classification 
probabilistic framework provides efficient principled techniques pruning large feature sets koller sahami range classifiers varying complexities accuracies pazzani friedman goldszmidt sahami singh provan :10.1.1.155.2293
provide brief overview probabilistic framework application classification feature selection 
bayesian classifiers heart probabilistic framework idea model world represented probability distribution space possible states world 
typically state world described set random variables state assignment values variables 
bayesian network pearl allows provide compact descriptions complex distributions large number random variables 
uses directed acyclic graph encode conditional independence assumptions domain independence assumptions allow distribution described product small local interaction models 
variable feature represented node network 
arc nodes denotes existence direct probabilistic dependency variables 
essentially structure network denotes assumption node network conditionally independent parents pi 
describe probability distribution satisfying assumptions associate node network conditional probability table specifies distribution possible assignment values parents pi 
parents simply contains prior probability distribution values 
network structure associated parameters uniquely define probability distribution variables network 
bayesian classifier simply bayesian network applied classification domain 
contains node unobservable class variable node features 
specific instance assignment values xn feature variables bayesian network allows compute probability possible class bayes optimal classification achieved simply selecting class probability maximized 
possible bayesian network variables bayesian classifier singh provan empirical evidence friedman goldszmidt suggests networks feature variables directly connected class variable better classification task 
simplest earliest classifier naive bayesian classifier 
classifier significantly predates development bayesian networks widely employed today 
naive bayesian classifier simplifying restrictive assumption domain features conditionally independent class variable 
words xjc jc assumption corresponds bayesian network structure 
assumption features conditionally independent class variable clearly unrealistic text domain 
approaches proposed augmenting naive bayesian classifier limited interactions feature variables allow node parents class variable illustrated 
unfortunately problem inducing optimal bayesian classifier np hard restrict node additional parents chickering 
optimal algorithm constructing classifier exponential number features worst case 
main solutions proposed problem tan algorithm friedman goldszmidt restricts node additional parent case optimal classifier quadratic time number features kdb algorithm sahami hand compromises heuristically searching potentially suboptimal structure 
finding classifiers node parents arbitrary values essentially chooses parents node features dependent metric class conditional mutual information jc cover thomas 
part goal experiment classifiers varying complexity chose kdb basis experiments 
structure selection phase done greedy fashion requiring time linear quadratic total number features 
course size conditional probability table node parents exponential requires corresponding amount time parameter estimation phase 
feature selection recall text domains feature word appears document corpus 
algorithms kdb tan merely quadratic opposed exponential total number features cost prohibitive 
wish employ optimal models exponential complexity size feature set feature selection absolute 
text domains particular number priori features overwhelming feature selection imperative ignore improved classification benefits feature selection outlined addressing issue continue probabilistic framework applying feature selection method koller sahami 
essentially algorithm greedily eliminates features disrupt original conditional class distribution 
remaining feature algorithm finds feature set mb approximates markov blanket set features render conditionally independent remaining domain features 
algorithm determines expected cross entropy cover thomas ffi mb cjx mb oe omega log oe cross entropy distributions oe probability space omega gamma algorithm eliminates feature ffi minimized 
process iterated eliminate features desired 
compute cjx mb feature selection algorithm simply uses naive bayes model assuming mb run quickly thousands features 
respect algorithm applicable text domains features 
previously demonstrated experiments text method selected 
bayesian networks corresponding naive bayesian classifier complex bayesian classifier allowing limited dependencies features 
experimental methodology order test scheme hierarchical classification needed obtain hierarchically classified text data 
basis reuters dataset 
reuters collection predetermined hierarchical classification scheme document multiple labels 
identified labels tended subsume labels higher level topics hierarchy 
hierarchical subsets reuters collection call hier hier hier extracted datasets described 
initially applied single pass zipf law feature selection method eliminates words appear fewer times corpus 
done get unrealistic improvements accuracy simply eliminating features rarely appear testing frequent bearing classification 
previous information retrieval van rijsbergen supports contention words generally able improve classification accuracy 
document represented boolean vector feature denotes presence absence word appeared corpus survived initial zipf law feature selection 
number features reported dataset application initial feature selection 
datasets experiments detailed employing fold cross validation produce multiple training testing sets dataset 
experimental seek show hierarchical approach compares favorably simple approach constructing single large classifier collection obtained anonymous ftp pub reuters ciir ftp cs umass edu 
arrangements access david lewis 
flattened topic space 
cases feature selection phase plays crucial role performance resulting classifier 
hierarchical classification scheme begins applying probabilistic feature selection entire training dataset just topics hierarchy associated document classes 
resulting reduced feature set build probabilistic classifier tier hierarchy 
currently employ naive bayes kdb classification methods 
training documents tier topics second tier topics class labels 
tier topic separate round probabilistic feature selection employed 
note feature selection done starting original feature set pruned zipf law observed indicative features level hierarchy particularly useful lower levels 
construct separate classifier subtopics tier topic appropriate reduced feature set 
note node hierarchy subset total class labels nodes second tier hierarchy fewer instances additional cost feature selection induction substantially flat classification scheme 
test documents classified hierarchy filtering level classifier sending document chosen second level final class assignment leaf node 
note errors level hierarchy unrecoverable second level 
method needs correct classifications order test document considered properly classified 
conducted initial experiments documents sent multiple paths hierarchy classifiers way addressing issue scope 
flat classification scheme simply treat grain money effects crude oil wheat dollar interest natural gas shipping corn hier 
oil carcass hog hier food commodities metals money barley rice cocoa copper tin dollar bond hier hier hierarchy features documents 
hier hierarchy features documents 
hier hierarchy features documents 
low level topic leaf node separate class 
apply feature selection induce probabilistic model reduced feature set 
results belief small set features suffices accurately distinguishing topics furthermore helps avoid overfitting employed aggressive feature selection policy 
hier hier domains considered reducing feature space features 
hier considered features dataset contains largest number final classes need distinguish 
chose particular pattern feature space sizes order facilitate fair comparison flat method hierarchical method 
recall hierarchical case potentially different set features selected node hierarchy 
hierarchical method examines larger set features 
allow fair comparison compare hierarchical method number features flat method times features hierarchies contain classifiers 
considered running method full feature set number features large prohibitive consider models assume conditional independence features kdb results naive bayes cases 
accuracies standard deviations fold cross validation table 
noting substantial improvements accuracy feature selection aggressively employed versus case domain features 
see improvement hierarchical case flat case single dataset 
hier dataset gains particularly visible statistically significant difference test obtained naive bayes flat classifier features versus original hierarchical classifier features 
cases features features zipf law pruning eliminated 
course number features reduced zero 
general observe initial significant improvement number features reduced decrease accuracy number features reduced 
phenomenon instance familiar bias variance tradeoff 
selection features say helps reduce error associated variance model strongly biases model 
conversely features say cause probability estimates classification models inaccurate lead poorer performance 
believe combining hierarchical method relatively aggressive feature selection help address issue allows larger space features considered provides variance control classifier hierarchy having focus just relevant features 
turn attention main question difference hierarchical flat classification methods 
comparing approaches equivalent number features classifier 
compare hierarchical classifier features node flat classifier features total 
case naive bayes comparison inconclusive 
cases hier features hierarchical approach significantly better hier features flat approach wins 
alternatively compare cases classification schemes see roughly comparable number features hierarchical features node versus flat features total 
case comparison naive bayes predominantly inconclusive exception hier flat classifier wins 
hierarchical flat dataset features nb kdb kdb nb kdb kdb hier sigma sigma sigma sigma sigma sigma hier sigma sigma sigma sigma sigma sigma hier sigma sigma sigma sigma sigma sigma hier sigma sigma hier sigma sigma sigma sigma sigma sigma hier sigma sigma sigma sigma sigma sigma hier sigma sigma sigma sigma sigma sigma hier sigma sigma hier sigma sigma sigma sigma sigma sigma hier sigma sigma sigma sigma sigma sigma hier sigma sigma sigma sigma sigma sigma hier sigma sigma table accuracy percentages hierarchical flat learning employing feature selection 
hierarchical approach appears benefits restrict attention simple classifiers naive bayes 
explained primary benefits hierarchical approach ability train complex classifiers richer dependency models 
seen complexity algorithms learning expressive classifiers grows rapidly number features 
growth quadratic kdb algorithm significantly expensive learn single classifier features learn classifiers features 
furthermore wish construct accurate models optimal bayesian network learning algorithm task may achievable case features clearly infeasible case 
richer dependency models hierarchical approach see significant accuracy gains naive bayes 
hier features example kdb provides accuracy improvement statistically significant 
hier features gains dramatic kdb providing reduction error significance 
contrast flat approach virtually incapable advantage richer model space 
cases observe improvement accuracy kdb classifiers case improvement statistically significant 
mentioned previously conjecture reason shortcoming arises fact dependency models different classes quite different 
phenomenon form context specific independence friedman goldszmidt 
topic discriminating words top level dollar dealer agriculture oil grain wheat corn gas grain london taiwan wheat gulf corn eep enhancement winter money effects dollar japan yen money england stg shortage system crude oil production ship gas natural iran cubic barrel iranian attack table discriminating words fold hierarchical method hier dataset 
flat classifier required capture single model complex dependency structure resulting aggregating disparate dependency structures 
case dependencies washed noise complex impossible capture restricted dependency structure consider 
see key success approach combination techniques hierarchical classification structured topic hierarchy aggressive feature selection node hierarchy richer dependency models 
hierarchy serves focus distributions uniform characteristics allowing target selected features dependency model local classification task 
illustrate phenomenon table shows set features words level hierarchy learned run fold hier dataset 
top level hierarchy see selection high level terms various major topics 
yen coupon bond bank denomination fee list issuing callable luxembourg cocoa copper steel mine rice barley crop word dependencies kdb features selected top level hier hierarchy 
longer indicative lower levels 
example terms agriculture useful identifying documents grain topic useful distinguishing subtopics 
see specific words corn wheat help distinguish subtopics corn wheat grain topic 
similarly money effects topic contains terms help distinguish documents dollar relate japan yen vs articles relate interest rates 
feature selection crude oil topic autonomously homed terms appearing names various subtopics natural gas ship 
note features selected different crossvalidation folds contain words node indicating method generally robust terms selecting meaningful features different partitions dataset 
localization phenomenon allows dependency model tailored relevant distribution 
illustrate point examined actual dependencies constructed automatically kdb algorithm top level node folds hier dataset obtained biggest gains richer dependency models 
recall task node distinguish topics food commodities metals money 
shows dependencies omitting universal dependence class variable 
see word dependencies form small clusters reflecting correlations words domain 
cluster associated word intriguing word metals food commodities 
model constructs subclusters connected related metals food commodities 
dependency helps classifier interpret word appropriate context 
approach allows utilize synergy tools resulting significantly improved classification accuracy 
see compare best hierarchical classifier versus best flat classifier number features marked boldface table 
hier hier typically see small improvements significant ones 
note hierarchical approach detracts accuracy hierarchical method requires data fragmented lower levels hierarchy 
examine robustness hierarchical approach fragmentation included data impoverished hier dataset 
see case hierarchical method performed significantly worse flat method 
fact achieved best results dataset kdb features node high variance precluded significant accuracy results 
hier dataset provided largest hierarchical structure classification information leverage see true power approach 
comparing cases equivalent numbers features classifier see hierarchical method significantly outperforms flat classification scheme comparison 
compare cases equivalent number total features 
hierarchical method features internal kdb performs equivalently flat method features far faster train 
compelling fact hierarchical method utilizing features classifier significantly outperforms flat method features 
note hierarchical method kdb features achieves highest classification accuracy hier dataset significantly outperforming run flat method regardless classifier number features 
proliferation systems hierarchically organize massive amounts text documents calls algorithms hierarchically categorize new documents come 
describe approach utilizes existing rich hierarchical structure order facilitate process 
building single massive classifier approach generates hierarchy classifiers utilizing feature selection tailor feature set classifier task 
shown resulting reduction size classifier allows obtain significantly higher accuracy reduction due increased robustness ability richer complex classifiers 
hope pursue expressive computationally expensive classifiers nodes hierarchy 
way hope able obtain better classification results able handle text collections wider variety statistical characteristics 
investigate problems specific hierarchy classifiers 
particular mentioned problem recovering classification errors early hierarchy 
investigate problem discovering new classes hierarchy multiple documents don fit nicely 
importantly intend investigate issue scalability applying method wider variety text datasets 
conducted preliminary applying method small hierarchies topics extracted yahoo 
web directory 
results inconclusive web data tendency extremely varied incorporating media aside text encouraged initial results seeking ways improve 
particular hope integrate classification method larger information retrieval system making existing subject hierarchies commercial web directories 
authors benefitted discussions adam grove marti hearst tom mitchell nils nilsson 
supported nsf arpa nasa stanford digital libraries contract 
almuallim 
efficient algorithm finding optimal gain ratio multiple split tests hierarchical decision tree learning 
proc 
aaai 
chickering 
learning bayesian networks np complete 
lecture notes statistics 
cover thomas 
elements information theory 
wiley 
fisher 
knowledge acquisition incremental conceptual clustering 
machine learning 
friedman goldszmidt 
building classifiers bayesian networks 
proc 
aaai 
friedman goldszmidt 
learning bayesian networks local structure 
proc 
uai 

estimation probabilities essay modern bayesian methods 
mit press 
hersh buckley leone 
ohsumed interactive retrieval evaluation new large test collection research 
proc 
sigir 
infoseek 

internet directory query service 
www infoseek com 
infoseek 

categorizes web sites infoseek 
info infoseek com doc hnc html 
koller sahami 
optimal feature selection 
proc 
icml 
pazzani 
searching dependencies bayesian classifiers 
proc 
ai stats 
pearl 
probabilistic reasoning intelligent systems networks plausible inference 

sahami 
learning limited dependence bayesian 
proc 
kdd 
schutze hull pedersen 
comparison document representations classifiers routing problem 
proc 
sigir 
self 
personal communication 
singh provan 
efficient learning selective bayesian network classifiers 
icml 
van rijsbergen 
information retrieval 
butterworths 
yahoo 

line guide internet 
www yahoo com 
