hmms segment models unified view stochastic modeling speech recognition ostendorf digalakis kimball boston university sri international bbn systems boston ma menlo park ca cambridge ma october years alternative models proposed address shortcomings hidden markov model currently popular approach speech recognition 
particular variety models broadly classified segment models described representing variable length sequence observation vectors speech recognition applications 
aspects common approaches including general recognition training problems useful consider unified framework 
goal describe general stochastic model encompasses models proposed literature pointing similarities models terms correlation parameter tying assumptions drawing analogies segment models hidden markov models 
addition summarize experimental results assessing different modeling assumptions point remaining open questions 
date successful speech recognition systems hidden markov model hmm hmms acoustic modeling dominates continuous speech recognition field 
hmms continue play role recognition systems long time come alternative models proposed years address shortcomings hmms 
new higher order models tend require computation hmms increase computational power broad progressive search techniques viable interest current systems 
unfortunately research new models tended proceed isolated pockets proliferation terms describe different modeling assumptions difficult appreciate common themes various proposed methods 
goal bring variety common framework order easier different researchers benefit successes developing robust parameter estimation techniques making appropriate assumptions variable dependence parameter tying 
particular describe general stochastic model representing variable length sequence observations show different proposed models correspond different distribution assumptions framework draw analogies assumptions general model different hmms raise questions unresolved current studies 
broadly speaking hmm limitations various models tried address weak duration modeling assumption conditional independence observations state sequence restrictions feature extraction imposed frame observations 
limitation hmm state duration model implicitly geometric distribution addressed introducing models explicit state duration distributions 
relaxation assumption conditional independence observations widely recognized practically useful unrealistic subject studies 
simple mechanism capturing time dependence augment observation space feature derivatives 
addition variations hmms proposed explicitly model correlation including conditionally gaussian hmms segmental hmms 
goal segmental frame features probably initial motivating factor development segmental acoustic models led bush zue colleagues 
stochastic modeling problem difficult segmental fixed length features requiring heuristic weightings posterior distributions 
excluding posterior distribution models shall show proposed models special cases general segment model sm looking problem generally easier interpret pose research questions different modeling assumptions 
remainder organized follows 
section address problem modeling frame features introducing segment model generalization hmm 
describe stochastic segment models general terms giving recognition training algorithms showing differences respect standard hmm algorithms problems 
section discuss specific distribution assumptions model dynamics feature vectors show different models seen special cases dynamical system model show relate hmms 
treating frame features section move problem modeling fixed length segmental features discuss issues posterior distributions segment modeling 
section concludes discussion open questions segmental acoustic modeling 
segmental hidden markov models general problem recognizing word sequence equivalently phone sequence constrained pronunciations lexicon typically framed statistical perspective goal find sequence labels fa phones sequence avoided term stochastic segment model ssm clear term sm includes modifier stochastic apply 
bs 
hmm sm hmm sm illustrated generative processes frame generated hmm state variable length sequence frames generated sm state associated random length dimensional feature vectors fy argmax jy argmax ja note need phone label triphone phone unit conditioned surrounding phonetic context sub phonetic unit automatically learned unit provide mapping word sequence 
class conditional distributions equation need language model acoustic model ja 
focus options acoustic model ja 
hidden markov modeling fundamental observation distribution model frame level set hmm discrete states phone typically represented sequence states 
segment modeling fundamental distribution model ja represents segment random variable set segment labels 
assume loss generality segment starts frame 
illustrates difference hmm sm perspective generative models 
acoustic modeling segment correspond phone sized unit segment models represent sub phone units syllables 
term segment general sense typical linguistic association segment phonetic units 
unit size affect probabilistic formalism impact computational costs model greater length variability accounted longer units 
hmms sms discrete state sequence respectively typically modeled markov chain 
hmm options modeling distribution including discrete distributions full diagonal covariance gaussian densities gaussian mixtures laplacian terminology class conditional distribution refers probability observations class label yja posterior distribution refers probability class label observation ajy 
distributions speech recognition 
similarly possible distribution assumptions segment models fact options large number degrees freedom model 
cases general recognition training algorithms described distribution assumptions 
section describe model general level particular case class conditional distributions 
general modeling framework general segment model provides joint model random length sequence observations generated unit density jl letting set possible observation lengths frames segment model label characterized duration distribution gives likelihood segment length likelihood particular segmentation utterance family output densities fb lg describes observation sequences different lengths 
addition markov assumption sequences implicitly explicitly embedding phone segments word pronunciation network probabilistic finite state network 
elaborating definition consider simple extensions hmms fit model 
simplest distribution assumption segment model uses single output distribution assumes successive observed frames independent identically distributed segment boundaries 
case probability segment label length product probability observation ja segment model reduces state hmm explicit duration model opposed typical implicit geometric hmm duration model 
simple segment model known hidden semi markov model continuously variable duration hmm segment model 
introducing explicit state duration distribution models added complexity hypothesizing segmentations recognition training 
accept additional cost natural move simple single region segment model complex segment models benefit explicit length distribution small relative gains possible restrictive observation distribution assumptions 
model slightly complex multiple distribution regions assume observations conditionally independent segment length 
case probability segment label duration ja specific distribution vector corresponds region sequence regions constrained length dependent mapping particular segment model thought hidden markov model complex topology parallel paths different lengths state parameter tying specified mapping distribution regions 
segment model generalized variety ways 
give framework represent broad class segment models leaving specific examples section 
segment duration distribution fp lg parametric non parametric 
parametric models investigated included poisson distribution gamma distribution speaking rate normalized gamma distribution context dependent clustered gamma models 
boston university non parametric model smoothed relative frequencies 
phone sized units reasonable assumption works empirically probably contribution duration model small relative segment observation probability higher dimensional space 
family output densities fb lg represents length trajectories vector space series distributions thought dividing segment regions time 
observations may correlated regions distribution parameters time invariant region 
sense segment distribution region similar hmm state 
collection distribution mappings time warping transformations ft lg associate frame variable length observation model regions 
mapping region dependent distributions provide means specifying large range small number parameters 
mapping key component needed specify distribution family 
deterministic dynamic 
variations deterministic mapping possible fixed number distributions table look continuum models determined sampling segment trajectory shown 
trajectory sampling appealing units smooth trajectories assumption piecewise constant locally stationary regions required 
hand constraint fixed number distributions allows automatic mapping estimation discussed 
dynamic mapping implemented dynamic programming find maximum likelihood mapping fixed number regions 
distribution family equation segment model unconstrained dynamic mapping equivalent hmm network explicit duration distribution 
deterministic mappings advantage reduced computation relative dynamic programming phone sized units smaller quite practice 
addition experimental evidence systematic intra segmental timing patterns speech supports deterministic mapping 
explain segment model illustrate relationship segment models hidden markov models consider problem computing probability phone sequence continuous speech 
phone represented sequence hmm states segment model 
hidden markov model length observation sequence connected length phone sequence state sequence ja ja js ja distribution mapping fixed number model regions vs 
continuum distributions trajectory sampling illustrated cases linear time warping frame arrows twelve frame arrows observations 
line represents distribution mean function time dimensional observations 
js js js ja js gamma indicator function equals state sequence permissible phone sequence zero 
equations require usual hmm assumptions observation vector conditionally independent observations states current state equation state sequence markov equation 
feature variability captured frame level observation distributions associated state time variability represented state sequence model puts geometric distribution time spent state 
alternatively segment model map segmentation 
segmentation uniquely specified sequence segment lengths fl ja ja jl ja jl gamma jl gamma ja ja gamma gamma time ith segment gamma gamma segment length 
assumptions required get equations equations respectively definition state expanded include label duration pair 
segment model feature variability represented general probability distribution conditioned segment label time variability represented segment duration probabilities length dependent mapping specifies distribution sequence segment 
comparison continuous word recognition word represented network sub word units 
case hmm state sequence serves intermediary observations words hmm states map uniquely word sequence 
sm segmentation segment labels form intermediate stage words observations 
training recognition problems hmm state sequence sm segmentation hidden impact difference modeling assumptions larger state space sm see remainder section 
recognition algorithms recognition algorithm segment model similar hidden markov models dynamic programming find state sequence viterbi decoding 
section provide details algorithm techniques reducing computation serve counter high cost search segment models 
viterbi decoding standard recognition solution hmms large state space large vocabulary continuous speech recognition involves finding state sequence argmax js viterbi decoding dynamic programming mapping state sequence appropriate word sequence 
assuming state network decoding defined word pronunciation network mapping gives unique word sequence 
segment models solution analogous case state includes segment label duration 
words notation state general segment state theta hmm state segment recognition involves finding argmax fmax jl ja dynamic programming algorithm mapping segment label sequence appropriate word sequence appropriate segment network decoding 
general model definition stronger assumption ja gamma gamma ja simplify presentation reduce number free parameters 
key difference sm hmm search algorithms explicit evaluation different segmentations adds extra dimension dynamic programming search described 
algorithm described similar described includes duration distribution markov label assumption pruning notation include heuristic length penalty introduced handle fixed length features 
define set segment models active time determined word grammar phoneme pronunciation networks optional search pruning 
define ae set allowable segment boundaries segment label time determined utterance length constraints allowable phone duration time 
define ffi log probability segmentation label sequence segment label observations fy ffi max gamma log gamma contains traceback information duration label pair led ffi segment label gamma best preceding state segment length corresponding equations time gamma 
dynamic programming recognition algorithm segment model involves initialize ffi log jl iterate ffi max ae ffi log jl ji ijj gamma argmax ae ffi log jl ji ijj gamma traceback argmax ffi iterate note indexed recognized segment labels time total number recognized segments known traceback finished information stored traceback array 
ignoring effect pruning implemented hmms sms state space segment model determined product model set size jaj number allowable segment duration start times roughly maximum allowable segment duration max max read speech ms frame rate 
comparable state space hmms jqj typically times jaj 
top difference possibly higher cost sm probability evaluations models section 
consequence sm cost reduction techniques considered 
case segment model uses assumption conditional independence possible distribution score caching sm eliminate redundant gaussian computations 
resulting sm computational requirements comparable analogous hmm plus additional non trivial cost associated overhead tracking segment structure 
segment level score caching useful especially general distribution assumptions word recognition applications 
second approach reducing computation segment pruning eliminating phone candidates partial segment likelihoods 
experiment candidates pruned threshold contributions likelihood successive frames reduction computation obtained loss recognition accuracy 
reduced search spaces useful reduce cost segment evaluations important reduce number segment evaluations reducing search space 
basic strategies shrinking search space reducing set segmentations considered sm rescoring multi pass search framework 
utterance length approximately segmentations considered optimal search 
reducing size set significantly reduce segment modeling search costs 
initial area involved hierarchical clustering frames similarity measure resulted dendrogram representation set possible segmentations 
reduced set segmentations searched dynamic programming algorithm dendrogram effectively specifies segmentation constraints fae different strategy local search algorithm proposed 
local search algorithm starts initial segmentation iteratively adjusts segmentation times segment labels improve likelihood step 
possible adjustments splits merges combination moves define local neighborhood searched possible segmentation set reduced determined dynamically part recognition 
computational cost local search algorithm determined empirically estimates experiments timit corpus suggest computation scale model complexity reduction substantial high order models 
eliminating segmentations search space may introduce errors early search process ultimate goal word recognition higher level unit alternative approaches investigated 
particular hmm simpler model provide set sentence hypotheses subsequently segment model 
sentence hypotheses described best list word lattice 
case recognition algorithm section set segment labels evaluate time reduced 
rescoring time reduced factor loss performance hmm segmentation times available case fae window times corresponding hmm segmentation time phone starting window rescoring segmentation time constraints large vocabulary recognition segment models feasible computation reduced rescoring lattice time constraints equivalent best list factor depending size best list 
course hmm pruning search space introduces errors empirical question approach effective 
experience rescoring hmm pruned search space effective word recognition terms accuracy computation reduction reducing segmentation space segment model 
additional advantage rescoring framework provides simple mechanism combining knowledge sources 
best rescoring knowledge source separately scores hypothesis scores linearly combined hypotheses reranked combined score 
weights score combination estimated automatically minimize optimization criteria word error rate top ranking sentence generalized mean rank correct answer 
estimation weights unconstrained multi dimensional minimization problem solved powell method grid search chooses different local optima 
examples scores combined include hmm sm acoustic log probabilities language model log probabilities prosody parse scores see phone word silence counts automatic estimation insertion penalties 
markov knowledge sources hmm sm trigram language model scores straightforward estimate weights best rescoring framework weights dynamic programming search word lattice 
knowledge sources combined long distance knowledge sources dynamic programming search option best rescoring lattice local search 
parameter estimation algorithms hidden state component common acoustic models section complicates parameter estimation algorithms requiring form iterative algorithm maximum likelihood ml estimation 
section generalizations common iterative schemes speech recognition applicable segmental hidden markov models 
estimates conditional probability distribution hidden state sequence instance expectation maximization em algorithm 
finds hidden state sequence iteration called viterbi training segmental means algorithm hmm case example 
solution hidden state problem provided second step iterative algorithm estimation parameters associated state transitions output distributions segmental frame typically straightforward exception complex models segmental dynamics 
discuss problem estimating distribution mapping part section discuss parameter estimation issues section introduce specific distribution assumptions reserve detailed parameter update equations appendix discrete state estimation generalized forward backward algorithm em algorithm applied hmms baum colleagues standard estimation tool speech recognition 
new variations hmms proposed extensions baum welch algorithm derived including continuously variable duration hmms segmental hmms conditionally gaussian hmms general models conditional independence assumptions represented chordal graph 
give solution general notion discrete hidden state handles models shall refer generalized forward backward algorithm expectation step extension called hmm forward backward algorithm compute posterior probability state observed data 
approach general unobserved state maximum likelihood estimate model parameters obtained maximizing marginal distribution gamma summation admissible discrete state sequences gamma transcription training data length term indicate state sequence necessarily time scale observation sequence allowing variable length observations 
solution maximization problem obtained em algorithm treating missing data maximizing iteration ae log fi fi fi fi oe jy log respect parameters parameters previous iteration 
discrete state sequence markov property terms needed maximizing equation aligned time time state preceding 
note state sequences state aligned time case null value 
probabilities computed efficiently forward backward algorithm shown subscript dropped simplify notation 
recursions second probability derived similarly 
standard forward backward recursion extended general states follows 
gamma state sequences state aligned time delta represent contiguous observation sequence corresponds particular state sequence states 
represents observation sequence associated state non overlapping state sequences assumed non overlapping observation sequences 
gamma gamma gamma jq gamma ff fi ff fi general calculated recursive generalized forward backward algorithms 
label sequence associated time scale observations denote previous states respectively corresponding times observation level 
note markov sequence fq tg markov semi markov terminology 
ff gamma gamma jq gamma gamma gamma jq ff fi jq gamma jq gamma jq gamma deltap jq gamma jq gamma fi jq jq step derivations assumptions general observations conditionally markov current state label sequence markov 
equations easily simplify standard hmm results letting gamma forth 
segment model theta gammal gamma state likelihood interpreted probability segment label length ends time term ff corresponds probability partial observation sequence segment length label time similarly fi corresponds probability partial observation sequence segment length label ends time preceding observations terms ff fi segment model calculated recursive forward backward algorithms ff gammal ja ff gammal fi ja fi ja ja additional unnecessary assumption segment observations conditionally independent segment label sequence 
addition equations simplify include earlier assumptions segment lengths depend current segment label segment labels markov aja equivalent 
summary general forward backward recursions compute probabilities jy gamma jy step em algorithm 
mentioned earlier step depends form delta treated separately 
equations allow markov dependence observations states needed handle conditionally gaussian models easily simplified assuming jq jq equations 
discrete state estimation state sequence alternative approach jointly estimate state sequence model parameters maximizing likelihood max gamma 
maximization performed alternating steps 
find state sequence current parameter estimates 
re estimate model parameters newly obtained state sequence 
find state sequence case segment models find segment label sequence jointly segmentation sequence segment lengths 
recognition dynamic programming algorithm differs described section possible label sequence constrained known word sequence effectively reducing sets active segment labels fa describing algorithm terms dynamic programming algorithm section represents slight generalization described constrain segmentation specific phone sequence allow alternate pronunciations words available recognition lexicon 
parameter re estimation step unify treatment em algorithms degenerate conditional distributions discrete state sequence current model parameters observations 
re estimation procedure applied hmms segmental models 
hmms shown certain conditions procedure yield asymptotically identical results baum welch algorithm 
debated practice procedure provides satisfactory estimates initial estimates hmms training data 
particular provides practical alternative sm training generalized forward backward algorithm costly 
starting initial segmentation hmm segment model training requires iterations training 
robust parameter estimation main difficulty modeling context segment recognition sm large number free parameters requires significant amount training 
representing context number models increases effective amount training model reduced 
addition interesting sm distribution assumptions amenable simple smoothing techniques occurrence smoothing discrete distribution hmms variance clipping bayesian smoothing continuous distribution hmms 
alternative solution parameter tying assuming model parameters shared models regions 
parameters tied heuristic rules knowledge application determined automatically distribution clustering 
parameter tying distribution clustering successfully segment modeling extensively hidden markov modeling 
general approach recognition systems divisive clustering maximize likelihood training data represented clustered models alternatively minimize entropy 
algorithm uses greedy search successively add models binary splits subsets data decision tree design objective function maximum likelihood data assuming parametric representation 
possible split evaluated terms likelihood ratio vs distributions representing data node tree 
distribution clustering works training algorithm described parameter re estimation step algorithm intermediate step uses segmentation information design model topology followed em iterations 
divisive clustering algorithm described node evaluation functions different cases parameter tying gaussian distributions useful region level observations assumed conditionally independent 
principle algorithm extended distribution assumption assumes conditional independence necessarily regions node evaluation functions may costly complex distribution assumptions 
consequence combination heuristics clustering experimentation probably needed solve parameter tying problem segment models 
distribution mapping estimation deterministic function provides distribution regions long observation typically frames lg defined 
mapping chosen heuristically fixed region trajectory sampling approaches automatically fixed region approach rg 
heuristic algorithms successfully including linear time warping phones linear sampling cepstral vector trajectory phones functions consonant vowel structure syllables 
evidence intra phone timing systematic non linear better performance may obtained deriving mapping automatically 
approaches outlined principle combined divisive distribution clustering temporal domain trajectory estimation maximum likelihood criterion 
approach ml distribution clustering techniques 
case start region segment successively split distributions add regions segment model increase training likelihood decision tree design techniques questions fl fl 
fl threshold learned tree design 
resulting mapping model independent constant number regions model clustering statistics models 
alternatively clustering phone dependent case different phones assigned different numbers regions 
temporal clustering algorithm combined distribution clustering define parameter sharing different triphones re estimation step training algorithm resulting algorithm similar successive state splitting 
second approach assumes known number regions model general model dependent 
algorithm finds separately length optionally group models ae argmax log ja argmax log ja ith observed segment length label jth feature vector segment equation maximized dynamic programming see appendix assuming mapping constrained monotonic model indices successive frames conditionally independent markov length warping estimate refine obtained divisive clustering algorithm greedy sub optimal simply iteratively model parameter re estimation embedded step algorithm 
disadvantage algorithm heuristics finding mapping infrequently observed lengths models feature dynamics hmm special case segment model segment model capable achieving level performance hmm experiments shown performance similar equivalent distribution assumptions numbers free parameters 
segment model allows general families distributions hmm particularly distributions implicitly explicitly model feature dynamics 
course possible distribution assumptions represent feature dynamics advantages disadvantages weighed experimentally 
section outline different alternatives including constrained mean gauss markov general linear models viewed family models segmental mixture models 
case describe analogous hmm assumptions 
insights experimental trade offs provided space parameterizations explored draw strong relative advantages different assumptions 
constrained mean trajectory simplest distribution assumption equation segment model characterized distribution regions frame observations assumed conditionally independent region sequence state sequence hmm 
hmm region sequence segment constrained deterministic distribution mapping term constrained mean 
issues determine particular type trajectory model distribution mapping trajectory sampling function indexing function fixed set regions mean trajectory parametric non parametric parametric mean parameters specifying constant linear higher order polynomial trajectory estimated distributions specific regions determined points trajectory 
non parametric trajectory models hand distribution parameters separately estimated model region 
parametric non parametric models type deterministic distribution mapping common parametric trajectory trajectory sampling non parametric trajectory fixed set region dependent models 
frame segment models introduced non parametric fixed length observations variable length observations 
boston university continued vein exploring different frame level distribution parameter tying assumptions 
time correlation captured implicitly derivative features robust parameter estimation easier frame level model difficult improve terms performance 
distributions segment constrained parameter estimation explicit model feature dynamics 
parametric segment models introduced separately gish ng segment model deng non stationary hmm 
cases mean trajectory parameterized polynomial dimensional vector space frame level observations assumed conditionally independent segment length 
specifically sequence parametric vs non parametric terminology borrowed glass classify model described section fixed length features 
parametric trajectory model proposed krishnan rao represent probabilities regression terms observations model fits section 
distributions computing likelihood length segment described sequence means bz theta matrix coefficients polynomial order gamma theta time sampling matrix 
row vector vector row vector normalized times second row 
example value th component mean vector normalized time quadratic case ti ij element coefficient matrix approaches differ representation time 
gish ng define observed segment linear sampling complete trajectory 
deng absolute time jth frame segment trajectory varies segment length 
absolute time advantage efficient recognition segmentation algorithms markov assumption holds segments reasonable sub phonetic units phone length generally correspond half phone length 
approaches reduce training costs advantage assumption covariance identical frames segment assumption may associated trade offs speech recognition performance 
non parametric trajectory modeling find covariance determinants vary function region phone variation phone middle 
results sets researchers constant linear quadratic mean functions context independent phone modeling suggest little gained going linear models possible exception modeling 
parametric non parametric approaches respective advantages parameter estimation equations appendix parametric approach motivated smooth trajectories speech units 
units vary smoothly time consonants better non parametric model parametric model smaller units 
non parametric approach computational storage advantages distribution means stored small table score caching reducing computation 
parametric models potentially fewer parameters non parametric models non parametric models may better suited parameter tying successful sub segment level distribution mapping estimation 
clearly research needed assess relative benefits 
conditionally gaussian models assumption conditional independence observations simplest assumption markov assumption 
gaussian distributions corresponds assumption optionally regions segment model 
researchers long observed hmm assumption conditional independence valid investigated alternative assumptions 
early markov assumptions referred conditionally gaussian hmms due described extensions viterbi baum explicitly term conditionally gaussian hmm distinguish models autoregressive hidden filter hmms 
autoregressive model represents conditional dependence fixed dimensional vector waveform samples effectively scalar linear prediction 
conditionally gaussian hmm represents conditional dependence vectors variable length sequence effectively vector linear sampled time warped time illustration possible correlation assumptions time warping correlation successive observations vs time sampling correlation hidden regions 
welch algorithms case brown explored models experimentally 
conditionally gaussian models rediscovered kenny brown benefit conditionally gaussian model simple cepstral features features augmented derivatives 
analogous gauss markov assumption segment models explored digalakis similar 
deng extended parametric trajectory model gauss markov case assess performance recognition experiments 
hmms explicit time correlation modeling generated encouraging results 
woodland achieves improved performance higher order vector predictors discriminant output distributions 
takahashi obtained results conditioning quantized version previous observation modeling conditional dependence mixture framework 
segment modeling research hand took different approach solving problem adding observation noise term described section 
training gauss markov parameters segment model analogous hmm solution 
segment modeling convenient training associate observations model regions deal added complexity hidden segmentation update equations appendix addition segment modeling question correlation represented observations sequence regions hidden trajectory illustrated time warping vs time sampling respectively time sampling requires complex parameter estimation process 
dynamical system model stochastic linear dynamical system ds general described equations prediction 
classes models similar fixed vs variable length sequence distinction 
addition covering discrete observation markov assumptions explored 
classical hmm hmm explicit duration model conditionally gaussian hmm cond 
indep 
sm constrained mean trajectory gauss markov sm dynamical system sm noisy gauss markov hidden constant gaussian mean geometric known family tree stochastic models variable length frame observation sequence 
arrows indicate model simplifications representing hmm state sm region 
unobserved state vector observed feature vector uncorrelated gaussian vector processes mean autocovariance functions cw qw ffi ffi ffi tu kronecker delta 
initial state gaussian mean covariance sigma 
dynamical system model widely estimation control problems non stationary signals introduced speech recognition model digalakis 
order segmental modeling framework parameter set theta fh qw sigma defined region segment model 
words assume system parameters locally time invariant region probability segment product region probabilities 
probability sequence observations region computed innovation sequence fe forward kalman recursions jr jr jr gaussian zero mean covariance sigma sigma equations appendix part parameter estimation 
region ds model viewed continuous state hmm hidden trajectory vectors continuous valued 
taken ds segment model combines continuous unobserved state discrete state index model region view hidden trajectory filtered series targets sense ds model similar proposed exception approach uses minimum error maximum likelihood training recognition 
perspective stochastic process generated model thought scaled noisy observation specified observation equation gauss markov process described state equation 
includes gauss markov process special case 
modeling assumptions described viewed special cases ds model see 
example unobserved state taken zero terms provide distributions regions ds model corresponds constrained mean trajectory assumption 
section shall describe additional special cases 
training equivalent maximum likelihood identification stochastic dynamical system 
classical method obtain maximum likelihood estimates requires integration adjoint equations involved certain distribution assumptions large number models typically speech 
alternatively em algorithm provides simpler solution viewing state variables hidden continuous state 
noise process assumed gaussian em algorithm simply involves iteratively computing expected second order sufficient statistics state observation vectors current parameter estimates 
computation sufficient statistics done recursively fixed interval smoothing form kalman filter additional recursions computation cross covariance 
case correlation defined locally trajectory sampling trajectory invariance assumption recursions take form fixed interval smoother blackouts 
new estimates system parameters obtained statistics simple multivariate regression coefficients 
details provided appendix note iterative algorithm embedded parameter re estimation step general iterative training algorithm 
options structure model parameter tying important difficult 
far parameter tying experimentation heuristic assumptions knowledge units modeled 
assumed tied regions phones arguing observation noise independent phone label 
structure compared experimentally variations included non parametric constrained mean gauss markov assumptions phone classification experiments ds model time warping correlation assumption gave best results 
presumably benefit ds model greater context dependent modeling case reduced variance initial state 
addition results suggest segmental models better suited representing detailed contexts 
contrast chose parameters equivalent making model dependent tying phones arguing represented rate movement articulators phone independent letting represent hidden targets 
ross ostendorf defined parameter tying intonation modeling linguistic studies different factors affecting intonation 
clearly untested options framework experimental needed assessing broad topology assumptions parameter tying 
non linear models dynamical system model described previous section generalized allowing non linear models proposed 
possibilities include predictive neural network models generalized segment framework 
course nonlinear models introduces additional computational burden particularly automatic training 
cepstral features speech recognition applications non linear models probably justified 
studies compared performance linear vs non linear regression explaining variance particular observation segment nonlinear regression alternating conditional expectation ace algorithm 
addition validating widely held belief observations segment highly correlated results showed percentage variance explained linear regression method cases close ace method 
linear models gaussian assumptions probably adequate modeling dependencies cepstral parameters phone sub phone units 
linear regression predictions phones non linear models may useful diphone units features sets 
segment level mixtures mixture distributions successfully hidden markov models natural extension models described far segmental mixtures 
direct analogy hmm general gaussian mixtures called continuous density hmms discrete mixture finite collection segmental distributions mixture mode specifying mixture components generated segment observations 
alternatively envision continuous mixture mode defining prior parametric trajectory 
cases discussed relation dynamical system model 
case correlation sequence random variables segment represented mixture mode 
advantage frame level mixture distributions stems systematic variation speech segmental mixtures may able represent systematic component introducing constraint keeps mixture mode constant segment 
contrast frame level mixture model allows mixture modes change randomly time step 
course advantage frame level mixtures simply gaussian models fit data frame mixtures efficient representation segmental mixtures 
question answered empirically remains open point intuition preliminary experiments favor systematic variation interpretation 
discrete mixture modes constrained mean sm formulation described earlier assumption model single time frequency trajectory corresponding single sequence observation densities model regions 
unit representing exhibits distinct trajectory model may inaccurate 
discrete mode segmental mixture model attempts address deficiency generalizing trajectory modeling capability basic sm permit collection trajectories segment 
non parametric trajectory segmental mixtures introduced parametric trajectory mixtures described 
mixture model proposed case fixed length segment observations model strictly follow framework described 
discrete mode segmental mixture distribution segment mixture finite set sm distributions mixture weights correspond probability observing particular trajectory segment 
segment general mixture probability unit length nc ja jc nc mixture components jc gives probability complete segment conditioned component probability ja mixture weight 
assuming jc constrained mean model section may parametric non parametric probability length segment nc ja jc mapping frame distribution region 
principle segmental mixture model distribution assumptions number parameters increases substantially 
discrete segmental mixture model viewed special case dynamical system model component distributions delta gaussian concatenate frame level components mixture mode form high dimensional state vector constrain matrices qw block diagonal mixture weights scale blocks interpretation allows general trajectory models component mixtures segmental mixture re estimation formulae easier understand ds framework 
problem segmental mixture models greater number free parameters lead training difficulties estimating robust context dependent models described 
number free parameters reduced necessary parametric constrained mean trajectories mixtures subsegment level mixtures explored 
training limitations big problem segmental mixture model generalized include frame level mixtures 
case equation nc ja nd jd jc inner summation frame level mixture nd gaussian components jd region jc frame level mixture weight 
model allows single constrained mean previous segmental mixture models special cases setting nc nd respectively parameter estimation equations appendix correspond general case 
full set parameters estimated training include segment region level mixture weights ja jc respectively means covariances sigma gaussians different mixture modes regions 
dynamical system model iterative em algorithm required estimate parameters mixture mode hidden 
case step requires computation posterior probabilities mixture component observation time simplified somewhat segment label region sequence algorithm step uses probabilities weight observations updating parameters 
steps repeated adequate convergence observed typically iterations 
em algorithm estimating frame level mixture distributions sensitive issues initialization unbounded likelihoods problems segmental mixtures severe higher dimensional space 
consequence techniques variance clipping important obtaining results 
experiments segmental mixtures non parametric constrained trajectory models give performance context independent phone modeling resource management task outperforming single constrained mean frame level mixture models 
context dependent modeling case training data parameter tying needed approach outperforms frame level mixture model 
continuous mixture modes alternative approach modeling multiple trajectories segment represent continuum possibilities putting prior parameter trajectory simplest implement parametric constrained mean trajectory model 
simplest model assumes conditionally independent frame level observations constant mean segment sigma mean modeled gaussian prior sigma 
model proposed russell gales young segmental hmm ostendorf digalakis target state sm 
constant gaussian mean model special case dynamical system model state model sigma 
addition viewed sophisticated version variable frame rate analysis shown segmentation incorporated variable frame analysis segment mean observed value 
viewing model continuous mixture trajectories constant hidden state allows general interpretation parametric trajectories holmes russell developed simple representation putting priors linear potentially higher order trajectories 
different views constant random mean model resulted approaches computing probability segment 
russell proposed approximation segment probability trajectory gales young give formula exact probability log jl ff gamma log sigmaj log sigma gamma log sigma sigma gamma sigma gamma gamma sigma gamma includes terms indicates transpose vs observation length sigma gamma sigma gamma sigma gamma sigma sigma gamma sigma gamma dynamical system interpretation exact solution obtained recursively product innovation probabilities section save computation allows segment pruning 
recursive solution efficient terms sigma pre stored lengths practical short segments 
initial experimental results sites context independent modeling discouraging constant mean covariance assumptions 
context dependent phone modeling led improved performance frame models sub phone units higher order trajectories lead improvements 
segmental features posterior distributions mentioned earlier initial motivations considering segmental models potential incorporating segmental features 
section return problem representing segmental features show problematic stochastic modeling general 
promising approach incorporating segmental features valid statistical model posterior distributions highlight modeling issues critical approach 
segmental feature transformation variable length segment observation sequence vector average formant frequencies 
approaches conditional distribution models feature transformations equation jl gamma jl jl gamma represents observations th segment 
section indicate modeling assumption 
early stochastic segment modeling segmental features segment represented fixed length sampled version observation sequence 
unfortunately fixed length feature mapping changes segment probability distribution dimensionality probability space sequence proportional number hypothesized segments fewer segments favored smaller number probability terms 
dimensionality problem addressed heuristically length dependent weighting factor reflects general problem conditioning different events different segmentations illustrated posterior distribution equations 
context dependent models non parametric constrained mean distribution kimball finds fixed length feature assumption hurts performance appropriate heuristic length weighting factor 
dimensionality difference obviously problem posterior distribution models 
looking problem phone classification reasonable assume piecewise constant mapping space segment observations phone likelihoods ajy problem phone recognition segment boundaries known priori modeling assumption requires feature processing depend segmentation 
posterior distributions segment recognition problem equation argmax fmax jl jy argmax fmax jf jy ff feature processing assumption necessarily depends segmentation features differ function segmentation conditioning event unique 
case foundation statistical detection theory lost theory holds comparing jz comparison jw 
segmental features problematic general joint segmentation recognition problems useful restricted case rescoring hypotheses segmentation 
posterior distribution models need restricted fixed length features interest models provide broader potentially powerful class discriminant functions 
particular decision trees neural networks estimate posterior distributions providing general non parametric models mapping observations class labels 
neural networks see connection hmms general discussion 
posterior distributions successfully hmms certain assumptions shown mathematically consistent 
segmental posterior distributions additional difficulties encountered segmentation dependence 
fundamentally problems segmentation likelihood modeling appropriate independence assumptions context dependent models 
various methods computing segmentation likelihood jy equation explored success experiments suggest needed 
addition appropriate choice segmentation probability model address conditioning event mismatch problem segmental features zjy compared jw recognition problem joint maximization labels features 
context dependent modeling difficult reasons 
context modeling general requires methods robust estimation handle practical problem large number free parameters issue explored posterior distributions 
bigger problem relates conditional independence assumptions theoretically inconsistent currently proposed segmental posterior distribution models 
theoretically valid model equation simplified assuming jl fl jl fl jl fl gamma fl triphone label 
successive triphones necessarily depend appropriate drop fl gamma conditioning 
reasonable approximation simplify equation fl jl fl gamma fl jfl gamma gamma kg assuming observations sufficiently distant time affect current state 
reasonable assume fl independent shown experimentally seen intuitively considering analogous problem hmms likelihood hmm state observation sequence predicted likelihood single frame 
say posterior distribution modeling segmental features completely unsuccessful 
despite theoretical problems fixed length segmental features practical success variety systems 
facilitate segmental neural networks allow joint correlation modeling entire segment 
question raised results successful slightly revised framework 
area posterior distribution modeling received attention models conditional distributions observations phonetic units questions interest fully answered problems raised undoubtedly addressed 
discussion summary segment models thought simply higher dimensional version hidden markov model markov states generate random sequences single random vector observation 
basic segment model includes explicit segment level duration distribution model representing family length dependent joint distributions specified region dependent distributions mapping regions 
segment models generalization hmms standard hmm training recognition algorithms easily extended handle segment models higher computational cost due expanded state space 
advantage segment models possible modeling alternatives representing family distributions allows explicit trajectory correlation modeling 
distribution assumptions proposed literature described 
looking group options key aspects modeling assumptions include trajectory model hidden dynamical system various segmental mixture models vs observed constrained mean trajectory gauss markov models correlation modeled explicitly gauss markov assumptions mixture mode vs implicitly distribution mapping constraints trajectory hidden represented parametrically non parametrically 
addition deterministic mapping distribution regions raises questions best model intra segmental timing 
different aspects modeling assumptions explored isolated experiments needed assess relative benefits different modeling assumptions 
particular large number free parameters possible general distribution assumptions described important assess trade offs parametric vs non parametric approaches hidden vs observable trajectories determine particular model parameters useful general models 
course answer questions depend particular feature vectors units represented raises questions problems segmental models best suited 
addition better understanding behavior different segmental models empirical studies algorithmic theoretical development needed fronts 
example distribution clustering proved useful hidden markov models successfully applied segment models clustering region dependent distributions 
sub phonetic distribution clustering successful hmms phone level clustering new approaches clustering may needed robust estimation context dependent parametric trajectory models segmental mixture models 
related area needing adaptation 
speaker incremental adaptation proved powerful tools improving hmm performance parameters segment model hmm difficult substantial changes distribution parameters small amount data new adaptation techniques 
posterior distributions segment modeling early stages done advance models 
theoretical framework segment model applied problems outside acoustic modeling speech recognition 
example phrase structured language modeling formulated variable length state segment process segment corresponds phrase observations words 
case observation distribution assumptions need reflect discrete nature word observations 
sms synthesis applications trajectory modeling structural constraints ds model useful hmms 
theory segment models applied time series recognition problems fields 
acknowledgments authors gratefully acknowledge people provided comments manuscript particularly kannan sagisaka 
credit kannan rohlicek significant contributions segmental acoustic modeling described 
segment modeling boston university jointly funded nsf arpa nsf number iri arpa onr number onr 
additional support ostendorf preparing manuscript provided atr 
bahl jelinek mercer maximum likelihood approach continuous speech recognition ieee trans 
pattern analysis machine intelligence vol 
pami pp 

rabiner tutorial hidden markov models selected applications speech recognition proc 
ieee vol 
pp 

russell moore explicit modeling state occupancy hidden markov models automatic speech recognition proc 
int 
conf 
acoust speech signal proc pp 

levinson continuously variable duration hidden markov models automatic speech recognition computer speech language vol 
pp 

explicit time correlation hidden markov models speech recognition proc 
int 
conf 
acoust speech signal proc pp 

brown acoustic modeling problem automatic speech recognition ph thesis computer science department cmu may 
kenny linear predictive hmm vector valued observations applications speech recognition ieee trans 
acoust speech signal proc vol 
assp pp 

russell segmental hmm speech pattern matching proc 
int 
conf 
acoust speech signal proc vol 
ii pp 

gales young segmental hmms speech recognition proc 
european conf 
speech commun 
technology pp 

bush network connected digit recognition ieee trans 
acoust speech signal proc vol 
assp pp 

zue glass philips seneff acoustic segmentation phonetic classification summit system proc 
int 
conf 
acoust speech signal proc pp 

meng zue signal representation comparison phonetic classification proc 
int 
conf 
acoust speech signal proc may pp 

digalakis segment stochastic models spectral dynamics continuous speech recognition ph thesis department boston university january 
kannan ostendorf comparison trajectory mixture modeling word recognition proc 
int 
conf 
acoust speech signal proc vol 
ii april pp 

deng sun wu speech recognition hidden markov models polynomial regression functions nonstationary states ieee trans 
speech audio proc vol 
pp 

sondhi hidden markov models templates non stationary states application speech recognition computer speech language vol 
pp 

ostendorf ross recognition intonation labels dynamical system model proc 
atr int workshop computational modeling prosody spontaneous speech processing pp 

levinson development acoustic phonetic hidden markov model continuous speech recognition ieee trans 
signal proc vol 
pp 

gish ng rohlicek secondary processing speech segments hmm word spotting system proc 
int 
conf 
spoken language proc vol 
pp 

ostendorf gish stochastic segment modeling estimate maximize algorithm proc 
int 
conf 
acoust speech signal proc pp 

ostendorf kannan kimball rohlicek continuous word recognition stochastic segment model proc 
darpa workshop csr 
ostendorf richardson iyer kimball rohlicek stochastic segment modeling csr bu wsj benchmark system proc 
arpa workshop spoken language technology pp 

fong statistical models duration synthesis recognition thesis department boston university 

lee 
juang segment model approach speech recognition proc 
int 
conf 
acoust speech signal proc pp 

van santen template speech timing proc 
atr int workshop computational modeling prosody spontaneous speech processing pp 

ostendorf roukos stochastic segment model phoneme continuous speech recognition ieee trans 
acoust speech signal proc vol 
pp 

digalakis ostendorf rohlicek fast search algorithms phone classification recognition segment models ieee trans 
signal proc vol 
pp 

richardson ostendorf rohlicek lattice search strategies large vocabulary speech recognition proc 
int 
conf 
acoust speech signal proc 
ostendorf kannan austin kimball schwartz rohlicek integration diverse recognition methodologies reevaluation best sentence hypotheses proc 
darpa workshop speech natural language pp 

nguyen schwartz zhao best dead proc 
arpa workshop human language technology pp 

rayner carter digalakis price combining knowledge sources reorder best speech hypothesis lists proc 
arpa workshop human language technology pp 

kannan ostendorf rohlicek weight estimation best rescoring proc 
darpa workshop speech natural language pp 

ostendorf prosody parse scoring application atis proc 
arpa workshop human language technology pp 

dempster laird rubin maximum likelihood estimation incomplete data journal royal statistical society vol 
pp 

rabiner 
juang segmental means training procedure connected word recognition technical journal vol 
pp 

baum petrie soules weiss maximization technique statistical analysis probabilistic functions finite state markov chains ann 
math 
stat vol 
pp 

gales young theory segmental hidden markov models cambridge university engineering department technical report cued infeng tr 
stochastic models allow baum welch training manuscript 
merhav hidden markov modeling state sequence proc 
int 
conf 
acoust speech signal proc pp 

schwartz kimball kubala feng chow barry makhoul robust smoothing methods discrete hidden markov models proc 
int 
conf 
acoust speech signal proc pp 

lee rabiner pieraccini acoustic modeling large vocabulary speech recognition computer speech language vol 
pp 

gauvain lee map estimation continuous density hmm theory applications proc 
darpa workshop speech natural language pp 

kimball ostendorf context modeling stochastic segment model ieee trans 
signal proc vol 
pp 

ross computational models intonation speech synthesis ph thesis department boston university april 
kannan ostendorf rohlicek maximum likelihood clustering gaussians speech recognition ieee trans 
speech audio proc vol 
pp 

bahl de souza gopalakrishnan nahamoo decision trees phonological rules continuous speech proc 
int 
conf 
acoust speech signal proc pp 


hwang huang modeling speech recognition proc 
darpa workshop speech natural language pp 

woodland young htk tied state continuous speech recogniser proc 
european conf 
speech commun 
technology pp 

digalakis optimizing degree tying large vocabulary hmm speech recognizer proc 
int 
conf 
acoust speech signal proc vol 
pp 

breiman friedman olshen stone classification regression trees wadsworth brooks 
successive state splitting algorithm efficient modeling proc 
int 
conf 
acoust speech signal proc vol 
pp 

gong 
non linear time alignment stochastic trajectory models speech recognition proc 
int 
conf 
spoken language proc pp 

digalakis ostendorf rohlicek improvements stochastic segment model phoneme recognition proc 
darpa workshop speech natural language pp 

glass modeling spectral dynamics vowel classification proc 
european conf 
speech commun 
technology pp 

kimball ostendorf tied mixture distributions proc 
darpa workshop speech natural language pp 

gish ng segmental speech model applications word spotting proc 
int 
conf 
acoust speech signal proc pp 
ii 
krishnan rao segmental phoneme recognition piecewise linear regression proc 
int 
conf 
acoust speech signal proc vol 
pp 

linear predictive hidden markov models speech signal proc 
int 
conf 
acoust speech signal proc pp 


juang rabiner mixture autoregressive hidden markov models speech signals ieee trans 
acoust speech signal proc vol 
assp pp 

paliwal temporal correlation successive frames hidden markov model speech recognizer proc 
int 
conf 
acoust speech signal proc vol 
ii pp 

deng stochastic model speech incorporating hierarchical nonstationarity ieee trans 
speech audio proc vol 
pp 

woodland hidden markov models vector linear prediction discriminative output distributions proc 
int 
conf 
acoust speech signal proc pp 

takahashi matsuoka shikano phoneme hmms constrained frame correlations proc 
int 
conf 
acoust speech signal proc vol 
ii pp 

digalakis rohlicek ostendorf dynamical system approach continuous speech recognition ieee trans 
speech audio proc vol 
pp 

articulatory speech production model controlled prior knowledge notes frontiers speech processing robust speech recognition cd rom 
holmes russell experimental evaluation segmental hmms proc 
int 
conf 
acoust speech signal proc 
ross ostendorf dynamical system model generating synthesis proc 
esca ieee workshop speech synthesis pp 

bourlard linear nonlinear prediction speech recognition hidden markov models proc 
european conf 
speech commun 
technology pp 

levin word recognition hidden control neural architecture proc 
int 
conf 
acoust speech signal proc pp 

waibel large vocabulary recognition linked predictive neural networks proc 
int 
conf 
acoust speech signal proc pp 

iso watanabe speaker independent word recognition neural prediction model proc 
int 
conf 
acoust speech signal proc pp 

neural predictive hidden markov model proc 
int 
conf 
spoken language proc pp 

breiman friedman estimating optimal transformations multiple regression correlation amer 
stat 
assoc vol 
pp 

young odell woodland tree state tying high accuracy acoustic modeling proc 
arpa workshop human language technology pp 

kimball segment modeling alternatives continuous speech recognition ph thesis department boston university september 
gong 
stochastic trajectory modeling speech recognition proc 
int 
conf 
acoust speech signal proc vol 
pp 

ostendorf digalakis stochastic segment model continuous speech recognition proc 
th asilomar conference signals systems computers pp 

holmes russell speech recognition linear dynamic segmental hmm proc 
european conf 
speech commun 
technology 
bourlard links markov models multilayer perceptrons ieee trans 
pattern analysis machine intelligence vol 
pp 

richard lippman neural network classifiers estimate bayesian posteriori probabilities neural computation vol 
pp 

morgan bourlard continuous speech recognition multilayer perceptrons hidden markov models proc 
int 
conf 
acoust speech signal proc pp 

renals morgan bourlard cohen franco connectionist probability estimators hmm speech recognition ieee trans 
speech audio proc vol 
part ii pp 

ostendorf automatic labeling prosodic patterns ieee trans 
speech audio proc vol 
pp 

hochberg cook renals robinson abbot hybrid connectionist hmm large vocabulary recognition system proc 
arpa workshop spoken language technology pp 

ostendorf rohlicek joint quantizer design parameter estimation discrete hidden markov models proc 
int 
conf 
acoust speech signal proc pp 

leung zue speech recognition stochastic modeling proc 
european conf 
speech commun 
technology vol 
pp 

kimball ostendorf rohlicek recognition classification segmentation scoring proc 
darpa workshop speech natural language pp 

leung zue speech recognition stochastic segmental neural networks proc 
int 
conf 
acoust speech signal proc vol 
pp 

bourlard morgan wooters renals context dependent neural network continuous speech recognition proc 
int 
conf 
acoust speech signal proc pp 

zhao schwartz makhoul hybrid segmental neural net hidden markov model system continuous speech recognition ieee trans 
speech audio proc vol 
part ii pp 

glass statistical trajectory models phonetic recognition proc 
int 
conf 
spoken language proc pp 

glass empirical acquisition language models speech recognition proc 
int 
conf 
spoken language proc pp 

waibel better language models spontaneous speech proc 
int 
conf 
spoken language proc pp 

moore combining linguistic statistical knowledge sources natural language processing atis proc 
arpa workshop spoken language technology pp 

kalman new approach linear filtering prediction problems trans 
asme series basic eng vol 
pp 

distribution mapping estimation goal distribution mapping estimation find vector region indices rg allowable segment length models set ae maximum likelihood approach solution argmax log ja argmax log ja ith observed segment model label jth feature vector segment similar solution proposed solution somewhat efficient sufficient statistics 
describing mapping estimation algorithm introduce notation sufficient statistics training data fixed segment boundaries 
vector described dimensional multivariate gaussian sufficient statistics set independent identically distributed observations superscript indicates vector transpose 
log likelihood data parameter vector log log flog gamma tr gamma vector mean inverse covariance matrix 
length dependent distribution mapping segments length ith frame map jth distribution model 
sufficient statistics parameter estimation unknown warping models observed lengths frames order nl max statistics models maximum phoneme length max 
statistics likelihood length observed segments written log ja log parameters distribution characterizing region model 
note similar formula defined models represent time correlation sufficient statistics somewhat different 
order describe dynamic programming algorithm maximizing equation define ll mapping observations segment length regions segment model log max gamma gamma log maximum likelihood mapping specific fixed distribution parameters 
compute log 
compute log max gamma argmax gamma 
find region trace back find remaining argmax gamma parameter estimates different distribution assumptions appendix provide re estimation formulae different distribution assumptions described section specifically step training algorithm 
analogous solutions general forward backward algorithm simple extensions sums observations weighted likelihood region region corresponding mixture component mixture distribution case 
full derivations results omitted due space limitations included case 
cases simplify notation index indicate segment model region model label combined index shared distribution 
accordingly assume training model set segment observations fy ag training region model frame observations fy rg 
define jaj number observations set non parametric constrained gaussian mean trajectory 
non parametric constrained trajectory model parameter estimates simply standard mean covariance estimates jy ya sigma jy ya gamma gamma parametric constrained gaussian mean trajectory 
parametric trajectory model characterized theta matrix describing variable length sequences dimensional vectors th order polynomial single theta covariance matrix sigma frames 
particular parameter estimation solution depends trajectory sampling assumption different solutions vs 
gish ng solution model compatible distribution mapping assumptions examples included appendix 
equations differ slightly transposed version consistent notation included segmental mixture terms 
define ml estimates trajectory parameter matrix single segment observation fi gamma theta time sampling matrix described section 
statistic segments map specific model interest new model parameters ya fi ya gamma sigma ya ya gamma bz gamma bz th column bz estimated mean sample length segment 
dependence region case 
dynamical system gauss markov models 
em algorithm estimating parameters dynamical system model proposed involves computing conditional expectations sufficient statistics hidden state step reestimate parameters step iterating convergence 
discussion assume jy iteration assuming observations complete step involves computation expected second order statistics observation set current model parameters fh qw sigma fx jy fx jy sigma fx gamma jy gamma jn sigma gamma jn simplify notation discussion drop specific parameters dependence continues hold cases 
statistics calculated standard forward recursions kalman filter followed backward pass shown 
forward backward equations augmented cross covariance recursions get second order statistics 
forward backward passes analogous hmm forward backward algorithm state expectations computed state likelihoods state continuous 
step forward recursions tjt tjt gamma jt tjt gamma gamma tjt gamma sigma tjt gamma sigma gamma sigma sigma tjt gamma sigma tjt sigma tjt gamma gamma sigma sigma gamma jt gamma sigma gamma jt gamma sigma jt sigma tjt qw step backward recursions gamma jn gamma jt gamma gamma tjt gamma sigma gamma jn sigma gamma jt gamma sigma gamma sigma tjt gamma sigma gamma jt gamma gamma sigma gamma tjt gamma sigma gamma jn sigma gamma jt sigma gamma sigma tjt sigma gamma tjt sigma gamma jt step new parameter estimates estimated state statistics 
simplify equations section define operators ffi ae jt ffi ffi ae ts jt ts ffi ft includes observations mapping ft excludes observation 
reestimation formulae fully observable efx jy ae efx jy efx jy efx jy ae gamma ae gamma efx jy ae efx jy efx jy ae ts efx jy efx jy efx jy ae ts gamma qw efx jy ae ts gamma efx jy efx jy ae ts means assumed zero equations simplify 
efx jy ae efx jy ae gamma ae gamma efx jy gy ae estimates sigma simply standard gaussian mean covariance equations estimated second order moments fx jy fx jy instances observation mapping region gauss markov model corresponds special case observation noise solution case simply equations substituting omitting step 
solution derivations assumes fully observable observation sequence include solutions case completely partially missing 
addition addresses problem tying parameters different subsets models 
discrete mode segmental mixture model 
sets parameters estimated training level mixture model weights components segmental mixtures weights gaussian mixtures segment stationary regions jc means covariances sigma gaussians dropping term simplify notation nc nd training involves em algorithm embedded step algorithm involves iteratively computing likelihood counts different mixture modes re estimating parameters counts 
iteration repeated adequate convergence observed 
step em algorithm accumulate sets counts segmental mixture distribution segmental components region dependent frame densities segmental components 
iteration em algorithm segmental count ij gives estimate probability segmental component segment observation ij jy jc jc frame level count ij trk gives probability jc th component region level gaussian mixture distribution region segment level mixture component observation ij trk jc jd jc jc equations allow mixture mode vary region 
slightly different solution obtained mixture mode fixed region case dk counts region level ij rk dk jc 
step requires estimation mixture weights component means covariances counts step 
update formulae mixture weights ya ij ya ij jc ya ij ij trk ya ij ij trk represents subset frames map region consideration 
mean covariance estimates ya ij ij trk ya ij ij trk sigma ya ij ij trk gamma gamma ya ij ij trk complete derivation equations 
tying assumed practice may situations parameter tying advantageous 
case equations change slightly essentially summing counts tied parameters 
solution revised accommodate mixtures parametric trajectories eliminating frame level mixture terms replacing mean covariance estimates weighted variation equations 
continuous mode segmental mixture 
assumption constant hidden mean gaussian prior views model led proposed parameter estimation algorithms 
gales young find closed form solution mean training algorithm ya sigma sigma gamma ya sigma sigma gamma simple solution covariance terms 
address problem approximation sigma ae sigmaj get parameters estimates step avoiding embedded iterations maximization step 
russell gives solution mean deals problem covariance estimates covariance values previous iteration update equations giving approximate step solution 
digalakis ostendorf treat segment means hidden variables em algorithm get maximum likelihood solution embedded iteration required 
em solution dynamical system model recursive step equations needed single hidden state estimate 
constant gaussian mean expected statistics hidden state segment computed step efx jy sigma sigma sigma sigma gamma efx jy sigma gamma sigma gamma gamma map estimate 
step update equations jy ya sigma jy ya efx jy gamma sigma ya ya gamma embedded estimation solution give better results training approach step solutions better suited training generalized forward backward algorithm 

