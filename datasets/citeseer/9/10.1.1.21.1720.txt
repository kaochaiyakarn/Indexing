incremental decremental support vector machine learning gert ece dept johns hopkins university baltimore md gert jhu edu tomaso poggio cbcl bcs dept massachusetts institute technology cambridge ma tp ai mit edu line recursive algorithm training support vector machines vector time 
adiabatic increments retain conditions previously seen training data number steps computed analytically 
incremental procedure reversible decremental offers efficient method exactly evaluate leave generalization performance 
interpretation decremental feature space sheds light relationship generalization geometry data 
training support vector machine svm requires solving quadratic programming qp problem number coefficients equal number training examples 
large datasets standard numeric techniques qp infeasible 
practical techniques decompose problem manageable subproblems part data limit perform iterative pairwise component wise optimization 
disadvantage techniques may give approximate solution may require passes dataset reach reasonable level convergence 
line alternative formulates exact solution training data terms data new data point 
incremental procedure reversible decremental training sample produces exact leave estimate generalization performance training set 
incremental svm learning training svm incrementally new data discarding previous data support vectors gives approximate results 
follows consider incremental learning exact line method construct solution recursively point time 
key retain kuhn tucker kt conditions previously seen data adding new data point solution 
kuhn tucker conditions svm classification optimal separating function reduces linear combination kernels training data ff training vectors corresponding labels sigma 
dual formulation training problem sabbatical leave cbcl mit performed 
support vector error vector soft margin classification svm training 
coefficients ff obtained minimizing convex quadratic objective function constraints min ff ff ij ff gamma ff ff lagrange multiplier offset symmetric positive definite kernel matrix ij 
order conditions reduce kuhn tucker kt conditions ff ij ff gamma gamma ff ff ff ff partition training data corresponding coefficients fff bg categories illustrated set margin support vectors strictly margin set error support vectors exceeding margin necessarily misclassified remaining set ignored vectors margin 
adiabatic increments margin vector coefficients change value incremental step keep elements equilibrium keep kt conditions satisfied 
particular kt conditions expressed differentially deltag ic deltaff ij deltaff deltab fcg deltaff deltaff ff coefficient incremented initially zero candidate vector outside margin vector working set fs changes coefficients satisfy delta deltab deltaff 
deltaff gamma 
deltaff symmetric positive definite jacobian delta delta delta delta delta delta 
delta delta delta equilibrium deltab fi deltaff deltaff fi deltaff coefficient sensitivities fi fi 
fi gammar delta 
gamma fi outside substituted margins change deltag fl deltaff fcg margin sensitivities fl ic ij fi fi fl bookkeeping upper limit increment deltaff tacitly assumed deltaff small element moves process 
ff change ff bookkeeping required check conditions determine largest possible increment deltaff accordingly 
gc equality joins 
ff equality joins 
ff equality transfers equality transfers 
equality transfers 
equality transfers recursive magic updates add candidate working margin vector set expanded 
delta delta delta fl fi fi 
fi delta fi fi delta delta delta fi formula applies add vector necessarily candidate parameters fi fi fl calculated 
expansion incremental learning reversible 
remove margin vector contracted ij ij gamma kk gamma ik kj index refers term 
update rules similar line recursive estimation covariance gaussian processes 
support vector error vector incremental learning 
new vector initially ff classified negative margin new margin error vector 
incremental procedure adding point candidate margin error vector fcg 
new solution fff expressed terms solution fff jacobian inverse candidate algorithm incremental learning 
initialize ff zero 
gc terminate margin error vector 
gc apply largest possible increment ff conditions occurs gc add margin set update accordingly terminate ff add error set terminate elements migrate bookkeeping section update membership elements changes update accordingly 
repeat necessary 
incremental procedure illustrated 
old vectors previously seen training data may change status way process adding training data solution converges finite number steps 
practical considerations trajectory example incremental training session shown 
algorithm yields results identical convergence qp approaches comparable speeds various datasets ranging thousands training points practical line variant larger datasets obtained keeping track limited set reserve vectors fi fflg discarding data ffl 
small ffl implies small overhead memory larger ffl smaller probability missing margin error vector previous data 
resulting storage requirements dominated inverse jacobian scale number margin support vectors decremental leave loo standard procedure predicting generalization power trained classifier theoretical empirical perspective 
naturally implemented decremental adiabatic reversal incremental learning training data full trained solution 
similar different bookkeeping elements migrating applies incremental case 
matlab code data available bach ece jhu edu pub gert svm incremental 
iteration coefficients trajectory coefficients ff function iteration step training non separable points dimensions gaussian kernel oe 
data sequence shown left 
leave loo decremental ff estimating generalization performance directly training data 
nc gamma reveals loo classification error 
leave procedure gamma removing point margin error vector nc fcg 
solution fff nc nc expressed terms fff bg removed point solution yields nc determines leaving training set generates classification error nc gamma 
starting full point solution algorithm decremental gamma loo classification 
margin error vector terminate correct left correctly classified 
margin error vector gc gamma terminate incorrect default training error 
margin error vector gc gamma apply largest possible decrement ff conditions occurs gc gamma terminate incorrect ff terminate correct elements migrate update membership elements changes update accordingly 
repeat necessary 
leave procedure illustrated 
trajectory loo margin function leave coefficient ff data parameters 
leave considerations exact loo estimate requested passes data required 
loo pass similar run time complexity memory requirements incremental learning procedure 
significantly better conventional approach empirical loo evaluation requires partial possibly extensive training sessions 
clear correspondence generalization performance loo margin sensitivity fl shown value loo margin nc obtained sequence vs ff segments decrement steps determined slopes fl incidentally loo approximation linear response theory corresponds segment loo procedure effectively extrapolating value nc initial value fl simple loo approximation gives satisfactory results cases illustrated example loo session 
statistical learning theory sought improved generalization performance considering non uniformity distributions feature space non uniformity kernel matrix 
geometrical interpretation decremental sheds light dependence generalization performance fl geometry data 
geometric interpretation feature space differential kuhn tucker conditions translate directly terms sensitivities fl fi fl ic ij fi fi fcg fi nonlinear map feature space kernel matrix elements reduce linear inner products ij delta kt sensitivity conditions feature space fl delta fi fi fcg fi fl equivalent minimizing functional min fi fi subject equality constraint lagrange parameter fi 
furthermore optimal value immediately yields sensitivity fl fl fi words distance feature space sample projection determines extent leaving affects classification note margin support vectors relevant error vectors contribute decision boundary 
concluding remarks incremental learning particular decremental offer simple computationally efficient scheme line svm training exact leave evaluation generalization performance training data 
procedures directly extended broader class kernel learning machines convex quadratic cost functional linear constraints including sv regression 
algorithm intrinsically line extends query learning methods 
geometric interpretation decremental feature space elucidates connection similar generalization performance distance data subspace spanned margin vectors 
campbell cristianini smola query learning large margin classifiers proc 
th int 
conf 
machine learning icml morgan kaufman 
opper sparse representation gaussian process models adv 
neural information processing systems nips vol 


cristianini campbell kernel algorithm fast simple learning procedure support vector machines th int 
conf 
machine learning morgan kaufman 
jaakkola haussler probabilistic kernel methods proc 
th int 
workshop artificial intelligence statistics 
joachims making large scale support vector machine learning practical scholkopf burges smola eds advances kernel methods support vector learning cambridge ma mit press pp 
opper winther gaussian processes svm mean field results leave oneout adv 
large margin classifiers smola bartlett scholkopf schuurmans eds cambridge ma mit press pp 
osuna freund girosi improved training algorithm support vector machines proc 
ieee workshop neural networks signal processing pp 
platt fast training support vector machines sequential minimum optimization scholkopf burges smola eds advances kernel methods support vector learning cambridge ma mit press pp 
pontil verri properties support vector machines neural computation vol 
pp 
scholkopf shawe taylor smola williamson generalization bounds eigenvalues gram matrix neurocolt technical report 
liu sung incremental learning support vector machines proc 
int 
joint conf 
artificial intelligence ijcai 
vapnik nature statistical learning theory new york springer verlag 
vapnik chapelle bounds error expectation svm smola bartlett scholkopf schuurmans eds advances large margin classifiers cambridge ma mit press 
