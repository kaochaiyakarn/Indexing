fl kluwer academic publishers boston 
manufactured netherlands 
winnow approach context sensitive spelling correction andrew golding golding merl com merl mitsubishi electric research laboratory broadway cambridge ma dan roth cs uiuc edu department computer science university illinois urbana champaign springfield avenue urbana il editor raymond mooney claire cardie 
large class machine learning problems natural language require characterization linguistic context 
characteristic properties problems feature space high dimensionality target concepts depend small subset features space 
conditions multiplicative weight update algorithms winnow shown exceptionally theoretical properties 
reported algorithm combining variants winnow weighted majority voting apply problem aforementioned class context sensitive spelling correction 
task fixing spelling errors happen result valid words substituting casual causal 
evaluate algorithm winspell comparing bayspell statistics method representing state art task 
find run full unpruned set features winspell achieves accuracies significantly higher bayspell able achieve pruned unpruned condition compared systems literature winspell exhibits highest performance aspects winspell architecture contribute superiority bayspell primary factor able learn better linear separator bayspell learns run test set drawn different corpus training set drawn winspell better able bayspell adapt strategy combines supervised learning training set unsupervised learning noisy test set 
keywords winnow multiplicative weight update algorithms context sensitive spelling correction bayesian classifiers 
large class machine learning problems natural language require characterization linguistic context 
problems include lexical disambiguation tasks part speech tagging word sense disambiguation grammatical disambiguation tasks prepositional phrase attachment document processing tasks text classification context usually document 
problems distinctive properties 
richness linguistic structures represented results extremely high dimensional feature spaces problems 
second target concept depends small subset features leaving huge balance features irrelevant earlier version appeared icml 
particular concept 
learning algorithm architecture properties suitable class problems 
algorithm builds introduced theories multiplicative algorithms 
combines variants winnow littlestone weighted majority littlestone warmuth 
extensive analysis algorithms colt literature shown exceptionally behavior presence irrelevant attributes noise target function changing time littlestone littlestone warmuth herbster warmuth 
properties particularly suited class problems studied 
theoretical properties winnow family algorithms known people started test claimed abilities algorithms applications 
address claims empirically applying winnow algorithm large scale real world task aforementioned class problems context sensitive spelling correction 
context sensitive spelling correction task fixing spelling errors result valid words peace cake peace typed piece intended 
errors account observed spelling errors kukich go undetected conventional spell checkers unix spell flag words word list 
context sensitive spelling correction involves learning characterize linguistic contexts different words piece peace tend occur 
problem multitude features characterize contexts features test presence particular word nearby target word features test pattern parts speech target word 
tasks consider number features ranges 
feature space large target concepts context piece occur depend small subset features vast majority irrelevant concept hand 
context sensitive spelling correction fits characterization provides excellent testbed studying performance multiplicative weight update algorithms real world task 
evaluate proposed winnow algorithm winspell compare bayspell golding statistics method successful tried problem 
compare winspell bayspell heavily pruned feature set bayspell normally uses typically features 
winspell perform comparably bayspell condition 
full unpruned feature set winspell comes achieving substantially higher accuracy achieved pruned condition better accuracy bayspell achieved condition 
calibrate observed performance bayspell winspell compare methods reported literature 
winspell significantly outperform methods tried comparable feature set 
core winspell bayspell linear separators 
fundamental similarity algorithms ran series experiments understand winspell able outperform bayspell 
aspects winspell architecture contribute superiority principal factor winspell simply learned better linear separator bayspell 
attribute fact bayesian linear separator idealized assumptions domain winnow able adapt mistake driven update rule conditions held practice 
address issue dealing test set dissimilar training set 
arises context sensitive spelling correction related disambiguation tasks patterns word usage vary widely documents training test documents may quite different 
confirming experimentally performance degrade unfamiliar test sets strategy dealing situation 
strategy called sup unsup combines supervised learning training set unsupervised learning noisy test set 
find strategy bayspell winspell able improve performance unfamiliar test set 
winspell particularly achieving comparable performance strategy unfamiliar test set achieved familiar test set 
rest organized follows section describes task context sensitive spelling correction 
bayesian method 
winnow approach problem introduced 
experiments winspell bayspell 
final section concludes 

context sensitive spelling correction widespread availability spell checkers fix errors result nonwords teh predominant type spelling error kind results real unintended word example typing intended 
fixing kind error requires completely different technology conventional spell checkers requires analyzing context infer word intended 
call task context sensitive spelling correction 
task includes fixing classic types spelling mistakes errors peace piece typographic errors form mistakes commonly regarded grammatical errors errors cross word boundaries may 
problem started receiving attention literature half dozen years 
number methods proposed context sensitive spelling correction directly related lexical disambiguation tasks 
methods include word trigrams bayesian classifiers gale decision lists yarowsky bayesian hybrids golding combination part speech trigrams bayesian hybrids golding schabes transformation learning brill latent semantic analysis jones martin differential grammars powers 
research systems gradually achieving higher levels accuracy believe winnow approach particularly promising problem due problem need large number features characterize context word occurs winnow theoretically demonstrated ability handle large numbers features 

problem formulation cast context sensitive spelling correction word disambiguation task 
ambiguity words modelled confusion sets 
confusion set fw wn means word set ambiguous word 
see occurrence hear target document take ambiguous hear task decide context intended 
acquiring confusion sets interesting problem right reported take confusion sets largely list words commonly confused back random house dictionary includes mainly errors 
confusion sets random house added representing grammatical typographic errors 
bayesian winnow methods context sensitive spelling correction described terms operation single confusion set say disambiguate occurrences words wn methods handle multiple confusion sets applying technique confusion set independently 

representation target problem context sensitive spelling correction consists sentence ii target word sentence correct 
bayesian algorithms studied represent problem list active features active feature indicates presence particular linguistic pattern context target word 
types features context words collocations 
context word features test presence particular word words target word collocations test pattern contiguous words part speech tags target word 
experiments reported set 
examples useful features confusion set include cloudy sigma words verb feature context word feature tends imply weather 
feature collocation checks pattern verb immediately target word tends imply don know laugh cry 
intuition types features capture important complementary aspects context 
context words tell kind words tend appear vicinity target word lexical atmosphere 
capture aspects context wide scope semantic flavor discourse topic tense 
collocations contrast capture local syntax target word 
similar combinations features related tasks accent restoration yarowsky word sense disambiguation ng lee 
feature extractor convert initial text representation sentence list active features 
feature extractor preprocessing phase learns set features task 
convert sentence list active features simply matching set learned features sentence 
preprocessing phase feature extractor learns set features characterize contexts word confusion set tends occur 
involves going training corpus time word confusion set occurs generating possible features context context word feature distinct word words collocation way expressing pattern contiguous elements 
gives exhaustive list features training set 
statistics occurrence features collected process 
point pruning criteria may applied eliminate unreliable uninformative features 
criteria aforementioned statistics occurrence feature occurred practically training instances specifically fewer occurrences fewer non occurrences presence feature significantly correlated identity target word determined chi square test significance level 

bayesian approach various approaches tried context sensitive spelling correction bayesian hybrid method call bayspell successful method adopt benchmark comparison winspell 
bayspell described golding briefly reviewed version uses improved smoothing technique described 
sentence target word correct bayspell starts invoking feature extractor section convert sentence set active features 
bayspell normally runs feature extractor pruning enabled 
approximation bayspell acts naive bayesian classifier 
suppose moment really applying naive bayes 
calculate probability word confusion set correct identity target word features observed bayes rule conditional independence assumption jf jw probability right hand side calculated maximum likelihood estimate mle training set 
pick answer highest jf 
bayspell differs naive approach respects assume conditional independence features heuristics detecting strong dependencies resolving deleting features left reduced set relatively independent features place equation 
procedure called dependency resolution 
second estimate jw terms bayspell simple mle tends give likelihoods rare features abundant task hand yielding useless answer posterior probability 
bayspell performs smoothing interpolating mle jw mle unigram probability 
means incorporating model way generally regarded essential performance chen goodman 
interp jw gamma pml jw pml set probability presence feature independent presence word extent independence holds accurate robust estimate jw 
calculate chi square probability observed association due chance 
enhancement smoothing minor extent dependency resolution greatly improve performance bayspell naive bayesian approach 
effect enhancements seen empirically section 
winnow approach various ways learning algorithm winnow littlestone task context sensitive spelling correction 
straightforward approach learn confusion set discriminator distinguishes specifically words set 
drawback approach learning applicable particular discrimination task 
pursue alternative approach learning contextual characteristics word individually 
learning distinguish word word perform broad spectrum natural language tasks roth 
briefly general approach concentrate task hand context sensitive spelling correction 
approach developed influenced system suggested valiant 
system consists large number items range correspond high level concepts humans words lower level predicates high level ones composed 
lowerlevel predicates encode aspects current state world input architecture outside 
high level concepts learned functions lower level predicates particular high level concept learned cloud ensemble classifiers 
classifiers cloud learn cloud high level concept autonomously function lower level predicates different values learning parameters 
outputs classifiers combined output cloud variant weighted majority algorithm littlestone warmuth 
classifier variant winnow algorithm littlestone 
training occurs architecture interacts world example reading sentence text architecture receives new values lower level predicates turn serve example training high level ensembles classifiers 
learning line process done continuous basis valiant 
shows instantiation architecture context sensitive spelling correction particular correcting words 
bottom tier network consists nodes lower level predicates application correspond features domain 
clarity nodes shown thousands typically occur practice 
high level concepts application correspond words confusion set desert dessert 
high level concept appears cloud nodes shown set overlapping bubbles suspended box 
output clouds activation level word confusion set comparator selects word highest activation final result context sensitive spelling correction 
sections elaborate winnow weighted majority winspell followed discussion properties architecture 

winnow job classifier cloud winspell decide particular word confusion set belongs target sentence 
classifier runs winnow algorithm 
takes input representation target sentence set active features returns binary decision word belongs sentence 
set active features feature weight arc connecting classifier hand 
winnow algorithm returns classification positive iff threshold parameter 
experiments reported set 
initially classifier connection feature network 
training establishes appropriate connections learns weights connections 
training example consists sentence represented set active features word confusion set correct sentence 
example treated positive example classifiers negative example classifiers words confusion set 
training proceeds line fashion example system representation classifiers updated example discarded 
step training classifier example establish appropriate desert desert weighted majority words hot arid words desert activation desert comparator cake words desert dessert decision dessert dessert weighted majority dessert activation dessert 
example winspell network 
nodes bottom tier network correspond features 
higher level clouds nodes shown overlapping bubbles suspended box correspond words confusion set 
nodes cloud run winnow algorithm parallel different setting demotion parameter fi copy input arcs weights arcs 
activation level word confusion set obtained applying weighted majority algorithm nodes word cloud 
word highest activation level selected 
connections classifier active features example 
active feature connected classifier sentence positive example classifier classifier corresponds target word occurs sentence add connection feature classifier default weight 
policy building connections needed basis results sparse network connections demonstrated occur real examples 
note build new connections sentence negative example classifier consequence different words confusion set may links different subsets possible features seen 
second step training update weights connections 
done winnow update rule updates weights mistake 
classifier predicts positive example correct classification weights promoted ff delta ff promotion parameter 
classifier predicts negative example correct classification weights demoted fi delta fi demotion parameter 
experiments reported ff set fi varied see section 
way weights non active features remain unchanged update time algorithm depends number active features current example total number features domain 
sparse architecture described coupled representation example list active features reminiscent infinite attribute models winnow blum 

weighted majority evaluating evidence word single classifier winspell combines evidence multiple classifiers motivation doing discussed 
weighted majority littlestone warmuth combination 
basic approach run classifiers parallel cloud try predict belongs sentence 
classifier uses different values learning parameters slightly different predictions 
performance classifier monitored weight derived reflecting observed prediction accuracy 
final activation level output cloud sum predictions member classifiers weighted abovementioned weights 
specifically clouds composed classifiers differing values winnow demotion parameter fi values 
weighting scheme assigns jth classifier weight fl fl constant total number mistakes classifier far 
essential property weight classifier mistakes rapidly disappears 
start fl decrease value number examples seen avoid weighing mistakes initial hypotheses heavily 
total activation returned cloud fl fl classification returned jth classifier cloud denominator normalization term 
rationale combining evidence multiple classifiers twofold 
running mistake driven algorithm known behavior asymptotically guarantee current hypothesis point time better previous 
common practice predict average hypotheses weighting hypothesis example length mistake free stretch littlestone 
second layer winspell weighted majority layer partly serves function line fashion 
second motivation weighted majority layer comes desire algorithm tunes parameters 
task context sensitive spelling correction self tuning automatically accommodate differences confusion sets particular differences degree words confusion set overlapping usages 
example words occur essentially disjoint contexts training example gives word classifier predicts surely wrong 
hand numerous contexts words acceptable disagreement training example necessarily mean classifier wrong 
mistake want demote weights case 
updating weights various demotion parameters parallel allows algorithm select best setting parameters confusion set 
addition layer strictly increases expressivity architecture 
plausible cases linear separator unable achieve prediction layer architecture able 

discussion multiplicative learning algorithms studied extensively learning theory community years littlestone kivinen warmuth 
winnow shown learn efficiently linear threshold function littlestone mistake bound depends margin positive negative examples 
functions exist real weights wn real threshold xn iff 
particular functions include boolean disjunctions conjunctions variables threshold functions 
key feature winnow mistake bound grows linearly number relevant attributes logarithmically total number attributes sparse architecture keep variables add variables necessary number mistakes disjunctions conjunctions logarithmic size largest example seen linear number relevant attributes independent total number attributes domain blum 
winnow analyzed presence various kinds noise cases linear threshold function perfect classifications littlestone 
proved assumptions type noise winnow learns correctly retaining abovementioned dependence number total relevant attributes 
see kivinen warmuth thorough analysis multiplicative update algorithms versus additive update algorithms exact bounds depend sparsity target function number active features examples 
algorithm independence assumptions attributes contrast bayesian predictors commonly statistical nlp 
condition investigated experimentally simulated data littlestone 
shown redundant attributes dramatically affect bayesian predictor superfluous independent attributes dramatic effect number attributes large order 
winnow mistake driven algorithm updates hypothesis mistake 
intuitively algorithm sensitive relationships attributes relationships may go unnoticed algorithm counts accumulated separately attribute 
crucial analysis algorithm shown crucial empirically littlestone 
advantages multiplicative update algorithms logarithmic dependence number domain features 
property allows learn higher linear discrimination functions increasing dimensionality feature space 
learning discriminator original feature space generate new features conjunctions original features learn linear separator new space exist 
modest dependency winnow dimensionality worthwhile increase dimensionality simplify functional form resulting discriminator 
reported regarded path define collocations patterns words part speech tags restricting tests singleton elements 
increases dimensionality adds redundancy features time simplifies functional form discriminator point classes linearly separable new space 
similar philosophy albeit different technically followed support vector machines cortes vapnik 
theoretical analysis shown winnow able adapt quickly changing target concept herbster warmuth 
investigate issue experimentally section 
feature winspell prune poorly performing attributes weight falls low relative highest weight attribute classifier 
pruning way greatly reduce number features need retained representation 
important observe tension compacting representation aggressively discarding features maintaining ability adapt new test environment 
focus adaptation study discarding techniques 
tradeoff currently investigation 

experimental results understand performance winspell task context sensitive spelling correction start comparing bayspell pruned set features feature extractor bayspell normally uses 
evaluates winspell purely method combining evidence multiple features 
important claimed strength winnow approach ability handle large numbers features 
tested essentially disabling pruning resulting tasks features seeing winspell bayspell scale 
experiment showed winspell bayspell perform relative outside 
calibrate performance compared algorithms methods reported literature baseline method 
success winspell previous experiments brought question able outperform bayspell methods 
investigated ablation study stripped winspell simple non learning algorithm gave initial set weights allowed emulate bayspell behavior exactly 
restored missing aspects winspell time observing contributed improving performance bayesian level 
preceding experiments drew training test sets population traditional pac learning assumption 
assumption may unrealistic task hand system may encounter target document quite seen training 
check fact problem tested corpus performance methods 
significantly worse corpus performance 
address problem tried strategy combining learning training set unsupervised learning noisy test set 
tested winspell bayspell able perform unfamiliar test set strategy 
sections describe experimental methodology experiments interleaved discussion 

methodology experiments follow training test sets drawn corpora word brown corpus kucera francis word corpus articles wall street journal wsj marcus 
note particular annotations needed corpora task context sensitive spelling correction simply assume texts contain context sensitive spelling errors observed spellings may taken gold standard 
algorithms run confusion sets taken largely list words commonly confused back random house dictionary 
confusion sets selected basis brown wsj include mainly confusions 
confusion sets random house added representing grammatical errors typographic errors may beg 
results reported percentage correct classifications confusion set score gives percentage correct confusion sets pooled 
comparing scores tested significance mcnemar test dietterich possible data individual trials available system comparison comparison different test sets study test difference proportions 
tests reported significance level 

pruned versus unpruned step evaluation test winspell conditions bayspell normally runs pruned set features feature extractor 
random split sentence brown training test sets 
results running algorithm confusion sets appear pruned columns table 
confusion sets algorithm better winspell performs comparably bayspell 
preceding comparison shows winspell credible method task test claimed strength winnow ability deal large numbers features 
test modified feature extractor minimal pruning features features pruned occurred exactly training set features extremely afford generalizations extremely numerous 
hope considering full set features pick minor cases holte 
called small disjuncts normally filtered pruning process 
results shown unpruned columns table 
algorithms better unpruned condition winspell improves confusion set markedly result outperforms bayspell unpruned condition confusion set 
results focus behavior algorithms unpruned case 
table 
pruned versus unpruned performance bayspell winspell 
pruned condition algorithms pruned set features feature extractor unpruned condition full set excluding features occurring just training set 
algorithms trained brown tested 
columns give number features conditions 
bar graphs show differences adjacent columns shading indicating significant differences mcnemar test level 
confusion set pruned unpruned pruned unpruned features features bayspell winspell bayspell winspell accept affect effect amount number cite sight site country county fewer lead led may passed past peace piece principal principle quiet quite raise rise re weather re 
system comparison previous section shows winspell bayspell perform relative evaluate respect external standard compared methods reported literature 
methods test sets readily compared rules transformation learner brill method latent semantic analysis lsa jones martin 
compare baseline canonical straw man task simply identifies common member confusion set training guesses time testing 
results appear table 
scores lsa taken jones martin different breakdown brown systems 
scores rules version system uses feature set 
comparison shows winspell significantly higher performance systems 
interestingly brill able improve rules score level winspell making various clever enhancements feature set table 
system comparison 
algorithms trained brown tested lsa breakdown 
version rules uses feature set 
bayspell winspell run unpruned condition 
column gives number test cases 
bar graphs show differences adjacent columns shading indicating significant differences test difference proportions level 
ended bars indicate difference percentage points 
lines pool results different sets confusion sets 
confusion set test baseline lsa rules bayspell winspell cases accept affect effect amount number cite sight site country county fewer lead led may passed past peace piece principal principle quiet quite raise rise re weather re sets sets including tagger assign word possible tags context merely word complete tag set 
suggests winspell get similar boost adopting enhanced set features 
note lsa system lsa reported best confusion sets words part speech 
hold confusion sets lsa score adversely affected 

ablation study previous sections demonstrate superiority winspell bayspell task hand explain winnow algorithm better 
core winspell bayspell linear separators roth winnow multiplicative update rule able learn better linear separator bayesian probability theory 
non winnow enhancements winspell particularly weighted majority voting provide leverage 
address questions ran ablation study isolate contributions different aspects winspell 
study observation core computations winnow bayesian classifiers essentially isomorphic winnow decisions weighted sum observed features 
bayesian classifiers decisions sum product likelihoods prior probability logarithm functional form yields linear function 
understanding start full bayspell system strip bayesian essence map log simplified non learning version winspell performs identical computation add back removed aspects winspell time understand contributes eliminating performance difference equivalent bayesian essence full winspell system 
experiment proceeds series steps morph bayspell winspell bayspell full bayspell method includes dependency resolution interpolative smoothing 
simplified bayspell bayspell dependency resolution 
means matching features highly interdependent ones bayesian calculation 
strip bayspell way naive bayes mle likelihoods performance poor unrepresentative bayspell undermine experiment seeks investigate winspell improves bayspell pale imitation thereof 
simplified winspell minimalist winspell set emulate computation simplified bayspell 
layer architecture weighted majority layer uses full network sparse initialized bayesian weights explained momentarily learning update bayesian weights 
bayesian weights simply log simplified bayspell likelihoods plus constant non negative required winnow 
occasionally likelihood case smooth log likelihood gamma large negative constant gamma 
addition add pseudo feature winnow representation active example corresponds prior 
weight feature log prior 
layer winspell simplified winspell adds learning 
lets see winnow multiplicative update rule able improve bayesian feature weights 
ran learning cycles training set 
layer winspell layer winspell adds weighted majority voting layer architecture 
bayesian winspell replaces full network layer winspell sparse network 
yields complete winspell algorithm performance affected fact started bayesian uniform weights 
table 
ablation study 
training brown testing 
algorithms run unpruned condition 
bar graphs show differences adjacent columns shading indicating significant differences mcnemar test level 
confusion set bayspell simplified layer layer bayesian bayspell winspell winspell winspell accept affect effect amount number cite sight site country county fewer lead led may passed past peace piece principal principle quiet quite raise rise re weather re ablation study breakdown brown previous section unpruned feature set 
results appear table 
simplified winspell omitted table results identical simplified bayspell 
primary finding measured aspects winspell contribute positively improvement bayspell ranking strongest weakest benefit update rule weighted majority layer sparse networks 
large benefit afforded update rule indicates winnow able improve considerably bayesian weights 
reason bayesian weights optimal bayesian assumptions conditional feature independence adequate data estimating likelihoods hold fully practice 
winnow update rule difficulties tuning likelihoods feedback fit situation holds imperfect world 
feedback obtained training set set bayesian likelihoods 
incidentally interesting note sparse network improves accuracy fairly consistently confusion sets 
reason improves accuracy omitting links features occurred target word training effectively sets weight features apparently better accuracy setting weight log bayesian likelihood case smoothed version mle probability 
second observation concerns performance winspell starting bayesian weights score compared winspell starting uniform weights see table 
suggests performance winnow improved moving hybrid approach bayes initialize network weights 
hybrid approach improvement bayes experiment pure bayesian approach scored updates performed bayesian weights score increased 
final observation experiment intended primarily ablation study winspell serves mini ablation study bayspell 
difference bayspell simplified bayspell columns measures contribution dependency resolution 
turns negligible glance surprising considering level redundancy unpruned set features 
instance features include collocation treaty include collocations det treaty noun sing 
reasons dependency resolution little benefit 
features generated systematically feature extractor tend evidence equally words 
second naive bayes known sensitive conditional independence assumption ask predict probable class opposed asking predict exact probabilities classes duda hart domingos pazzani 
contribution interpolative smoothing enhancement bayspell naive bayes addressed table 
investigated briefly comparing performance bayspell interpolative smoothing performance mle likelihoods naive method number alternative smoothing methods 
table gives scores 
score bayspell interpolative smoothing dropped mle likelihoods lower alternative smoothing methods tried 
shows dependency resolution improve bayspell naive bayes interpolative smoothing sizable benefit 

corpus performance preceding experiments assumed training set representative test set 
context sensitive spelling correction assumption may overly strong word usage patterns vary widely author document 
instance algorithm may trained corpus discriminate desert dessert tested article persian gulf war unable detect misspelling desert operation dessert storm 
check fact problem tested corpus performance algorithms trained brown tested randomly chosen sentences table 
score bayspell different smoothing methods 
method interpolative smoothing 
training brown testing 
mle likelihoods broke ties choosing word largest prior ties arose words probability 
katz smoothing absolute discounting ney turing discounting resulted invalid discounts task 
kneser ney smoothing absolute discounting backoff distribution marginal constraint 
interpolation fixed katz kneser ney set necessary parameters separately word deleted estimation 
smoothing method mle likelihoods interpolation fixed ney 
laplace kohavi 
matches kohavi 
katz smoothing katz kneser ney smoothing kneser ney interpolative smoothing section wsj 
results appear table 
algorithms degrade significantly 
glance magnitude degradation small score bayspell winspell 
viewed increase error rate fairly serious bayspell error rate goes increase winspell increase 
section strategy dealing problem unfamiliar test sets evaluate effectiveness winspell bayspell 
strategy observation test document imperfect provides valuable source information word usages 
returning desert storm example suppose system asked correct article containing instances phrase operation desert storm instance erroneous phrase operation dessert storm 
treat test corpus training document start running feature extractor generate collocation operation storm 
algorithm bayspell winspell able learn training test qua training corpus feature typically occurs desert evidence favor word 
algorithm feature fix erroneous spelling phrase test set 
important recognize system cheating looking test set cheating answer key test set 
system really doing enforcing consistency test set 
detect sporadic errors systematic ones writing operation dessert storm time 
possible pick systematic errors doing regular supervised learning training set 
leads strategy call sup unsup combining supervised learning training set unsupervised learning noisy test set 
table 
versus corpus performance bayspell winspell 
training brown cases 
testing corpus case brown corpus case wsj 
algorithms run unpruned condition 
bar graphs show differences adjacent columns shading indicating significant differences test difference proportions level 
ended bars indicate difference percentage points 
confusion set test cases test cases bayspell winspell accept affect effect amount number cite sight site country county fewer lead led may passed past peace piece principal principle quiet quite raise rise re weather re learning training set supervised benevolent teacher ensures spellings correct establish simply assumption 
learning test set unsupervised teacher tells system spellings observes right wrong 
ran winspell bayspell sup unsup see effect corpus performance 
needed test corpus containing errors generated corrupting correct corpus 
varied amount corruption corruption means altered randomly chosen occurrences confusion set different word confusion set 
sup unsup strategy calls training training corpus corrupted test corpus testing uncorrupted test corpus 
purposes experiment split test corpus parts avoid confusion training testing data 
trained brown plus corrupted version wsj tested uncorrupted version wsj 
results level corruption shown table level corruption corresponds typical typing error rates 
table compares table 
corpus performance bayspell winspell sup unsup strategy 
performance compared doing supervised learning 
training sup unsup case brown plus wsj corrupted supervised case brown 
testing cases wsj 
algorithms run unpruned condition 
bar graphs show differences adjacent columns shading indicating significant differences mcnemar test level 
ended bars indicate difference percentage points 
confusion set test cases bayspell winspell sup sup unsup sup sup unsup accept affect effect amount number cite sight site country county fewer lead led may passed past peace piece principal principle quiet quite raise rise re weather re corpus performance algorithm additional boost unsupervised learning part test corpus 
bayspell winspell benefit unsupervised learning amount difference winspell suffered considerably bayspell moving corpus condition 
result winspell bayspell able recover corpus performance level sup unsup strategy corpus condition 
borne mind results table depend factors 
size test set larger test set information provide unsupervised learning 
second factor percentage corruption test set 
shows performance function percentage corruption representative confusion set 
expect improvement unsupervised learning tends decrease percentage corruption increases 
bayspell performance corruption negate benefit unsupervised learning 
winspell sup unsup winspell sup bayspell sup unsup bayspell sup percentage corruption performance 
corpus performance bayspell dotted lines winspell solid lines sup unsup strategy supervised learning 
curves show performance function percentage corruption test set 
training sup unsup case brown plus wsj corrupted supervised case brown 
testing cases wsj 
algorithms run confusion set unpruned condition 

theoretical analyses winnow family algorithms predicted excellent ability deal large numbers features adapt new trends seen training properties remained largely 
reported architecture winnow weighted majority applied real world task context sensitive spelling correction potentially huge number features experiments 
showed algorithm winspell performs significantly better methods tested task comparable feature set 
comparing winspell bayspell bayesian statistics algorithm representing state art task winspell mistake driven update rule weighted majority voting sparse architecture contributed significantly superior performance 
winspell exhibit striking advantages bayesian approach 
winspell substantially accurate bayspell running full unpruned feature sets bayspell confusion sets achieving score 
second winspell better bayspell adapting unfamiliar test corpus strategy pre sented combines supervised learning training set unsupervised learning test set 
represents application techniques developed theoretical learning community years touches important issues active research 
demonstrates ability winnow algorithm successfully utilize strategy expanding space features order simplify functional form discriminator done generating collocations patterns words part speech tags 
strategy winnow shares philosophy technical underpinnings support vector machines cortes vapnik 
second layer architecture related various voting boosting techniques studied years learning community freund schapire breiman littlestone warmuth 
goal learn combine simple learners way improves performance system 
focus reported doing learning line fashion 
issues investigate order develop complete understanding multiplicative update algorithms real world tasks 
important issues raises need understand improve ability algorithms adapt unfamiliar test sets 
clearly crucial issue algorithms real systems 
related issue size comprehensibility output representation 
brill similar set features demonstrate massive feature pruning lead highly compact classifiers surprisingly little loss accuracy 
clear tension achieving compact representation retaining ability adapt unfamiliar test sets 
analysis tradeoff investigation 
winnow approach developed part research program trying understand networks simple slow neuron elements encode large body knowledge perform wide range interesting inferences instantaneously 
investigate question context learning knowledge representations support language understanding tasks 
light encouraging results contextsensitive spelling correction results dagan reddy tadepalli roth extending approach tasks 
acknowledgments neal lesh grace stan chen reviewers editors helpful comments 
second author research supported feldman foundation israeli ministry science arts done partly harvard university supported nsf ccr darpa contract afosr 
notes 
tested successfully features results reported 

word sentence tagged set possible part speech tags obtained dictionary 
tag match word tag member word tag set 

maximum likelihood estimate jw number occurrences presence divided number occurrences 
purpose experimental studies update knowledge representation testing 
done provide fair comparison bayesian method batch approach 

interfere subsequent updating weights conceptually treat non connection link weight remain multiplicative update 

exact form decreasing function unimportant interpolate quadratically decreasing function number examples 


example consider error rates task 
blum 

learning boolean functions infinite attribute space 
machine learning 
breiman 

bagging predictors 
technical report university california berkeley 
cesa bianchi freund helmbold warmuth 

line prediction conversion strategies 
computational learning theory eurocolt pages 
oxford university press 
chen goodman 

empirical study smoothing techniques language modeling 
proc 
th annual meeting association computational linguistics santa cruz ca 
cortes vapnik 

support vector networks 
machine learning 
dagan karov roth 

mistake driven learning text categorization 
emnlp second conference empirical methods natural language processing pages 
dietterich 

approximate statistical tests comparing supervised classification learning algorithms 
neural computation 
appear 
domingos pazzani 

optimality simple bayesian classifier zero loss 
machine learning 
duda hart 

pattern classification scene analysis 
wiley 


statistical methods rates proportions 
john wiley sons 
editor 
random house dictionary 
random house new york 
second edition 
freund schapire 

decision theoretic generalization line learning application boosting 
computational learning theory eurocolt pages 
springer verlag 
gale church yarowsky 

method disambiguating word senses large corpus 
computers humanities 
golding 

bayesian hybrid method context sensitive spelling correction 
proc 
rd workshop large corpora boston ma 
golding schabes 

combining trigram feature methods context sensitive spelling correction 
proc 
th annual meeting association computational linguistics santa cruz ca 
herbster warmuth 

tracking best expert 
proc 
th international conference machine learning pages 
morgan kaufmann 
holte acker porter 

concept learning problem small disjuncts 
proc 
international joint conference artificial intelligence detroit 
jones martin 

contextual spelling correction latent semantic analysis 
proc 
th conference applied natural language processing washington dc 
katz 

estimation probabilities sparse data language model component speech recognizer 
ieee trans 
acoustics speech signal processing assp 
kivinen warmuth 

exponentiated gradient versus gradient descent linear predictors 
acm symp 
theory computing 
kneser ney 

improved backing gram language modeling 
proc 
international conf 
acoustics speech signal processing pages 
vol 

kohavi becker sommerfield 

improving simple bayes 
proc 
european conference machine learning 
kukich 

techniques automatically correcting words text 
acm computing surveys 
kucera francis 

computational analysis day american english 
brown university press providence ri 
littlestone 

learning quickly irrelevant attributes abound new linear threshold algorithm 
machine learning 
littlestone 

redundant noisy attributes attribute errors linear threshold learning winnow 
proc 
th annual workshop computational learning theory pages 
morgan kaufmann 
littlestone 

comparing linear threshold learning algorithms tasks involving superfluous attributes 
proc 
th international conference machine learning pages 
morgan kaufmann 
littlestone warmuth 

weighted majority algorithm 
information computation 
brill 

automatic rule acquisition spelling correction 
proc 
th international conference machine learning 
morgan kaufmann 
marcus santorini marcinkiewicz 

building large annotated corpus english penn treebank 
computational linguistics 
damerau mercer 

context spelling correction 
information processing management 
ney essen kneser 

structuring probabilistic dependences stochastic language modelling 
computer speech language 
ng lee 

integrating multiple knowledge sources disambiguate word sense exemplar approach 
proc 
th annual meeting association computational linguistics santa cruz ca 
powers 

learning application differential grammars 
proc 
meeting acl special interest group natural language learning madrid 
reddy tadepalli 

active learning committees text categorization 
proc 
national conference artificial intelligence pages 
roth 

learning resolve natural language ambiguities unified approach 
proc 
national conference artificial intelligence pages 
roth 

part speech tagging network linear separators 
coling acl th international conference computational linguistics pages 
valiant 

circuits mind 
oxford university press 
valiant 

rationality 
workshop computational learning theory pages 
yarowsky 

decision lists lexical ambiguity resolution application accent restoration spanish french 
proc 
nd annual meeting association computational linguistics las cruces nm 
