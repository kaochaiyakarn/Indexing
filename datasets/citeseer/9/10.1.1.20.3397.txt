adaptive scaling feature selection svms yves umr cnrs universite de technologie de france yves utc fr st ephane psi de st etienne du france stephane fr introduces algorithm automatic relevance determination input variables kernelized support vector machines 
relevance measured scale factors defining input space metric feature selection performed assigning zero weights irrelevant variables 
metric automatically tuned minimization standard svm empirical risk scale factors added usual set parameters defining classifier 
feature selection achieved constraints encouraging sparsity scale factors 
resulting algorithm compares favorably state art feature selection procedures demonstrates effectiveness demanding facial expression recognition problem 
pattern recognition problem selecting relevant variables difficult 
optimal subset selection attractive yields simple interpretable models combinatorial procedure acknowledged unstable 
problems may better resort stable procedures penalizing irrelevant variables 
introduces procedure applied support vector machines svm 
relevance input features may measured continuous weights scale factors define diagonal metric input space 
feature selection results determining sparse diagonal metric sparsity encouraged constraining appropriate norm scale factors 
approach summarized setting global optimization problem pertaining parameters svm classifier parameters feature space mapping defining metric input space 
standard svms tunable hyper parameters set penalization training errors magnitude kernel 
formalism derive efficient algorithm monitor slack variables optimizing metric 
resulting algorithm fast stable 
organized follows 
presenting previous approaches hard soft feature selection procedures context svms algorithm 
exposure followed experimental section illustrating performances conclusive remarks 
feature selection adaptive scaling scaling usual preprocessing step important outcomes classification methods including svm classifiers 
defined linear transformation input space diag diagonal matrix kk kk scale factors 
adaptive scaling consists letting adapted estimation process explicit aim achieving better recognition rate 
kernel classifiers set hyperparameters learning process 
structural risk minimization principle tuned ways 
estimate parameters classifier empirical risk minimization values produce structure classifiers multi indexed select element structure finding set minimizing estimate generalization error 

estimate parameters classifier hyper parameters empirical risk minimization second level hyper parameter say constrains order avoid overfitting 
procedure produces structure classifiers indexed value computed minimizing estimate generalization error 
usual paradigm consists computing estimate generalization error regularly spaced hyper parameter values picking best solution trials 
approach requires intensive computation trials completed dimensional grid values 
authors suggested address problem optimizing estimate generalization error respect hyper parameters 
svm classifiers cristianini proposed apply iterative optimization scheme estimate single kernel width hyper parameter 
weston chapelle generalized approach multiple hyper parameters order perform adaptive scaling variable selection :10.1.1.102.7476
experimental results show benefits optimization 
relying optimization generalization error estimates hyper parameters hazardous 
optimized unbiased estimates biased bounds provided vc theory usually hold kernels defined priori see proviso radius margin bound 
optimizing criteria may result overfitting 
second solution considered estimate generalization error minimized respect single second level hyper parameter constrains role constraint twofold control complexity classifier encourage variable selection input space 
approach related successful soft selection procedures lasso bridge frequentist framework automatic relevance determination ard bayesian framework 
note type optimization procedure proposed linear svm frequentist bayesian frameworks 
method generalizes approach nonlinear svm 
algorithm support vector machines decision function provided svm sign function defined parameters obtained solving optimization problem min subject defined 
problem setting parameters feature space mapping typically kernel bandwidth tunable hyper parameters need determined user 
global optimization problem adaptive scaling performed iteratively finding parameters svm classifier fixed value minimizing bound estimate generalization error respect hyper parameters 
algorithm minimizes svm empirical criterion respect parameters estimate generalization error respect hyper parameters 
approach avoid enlargement set hyper parameters letting standard parameters classifier 
complexity controlled constraining magnitude 
defines single hyper parameter learning process related scaling variables 
learning criterion defined follows min min subject standard svm classification minimization estimate generalization error postponed step consists picking best solution trials dimensional grid hyper parameters 
constraint chosen constraint allows sparse solutions 
allowed go zero positive 
encourage sparsity zeroing small allow high increase small 
limit constraint counts number non zero scale parameters resulting hard selection procedure 
choice appropriate purpose amounts attempt solve highly non convex optimization problem number local minima grows exponentially input dimension avoid problem suggest smallest value problem convex linear mapping linear kernels constraint amounts minimize standard svm criterion penalization norm replaced penalization norm 
setting provides solution svm classifier described 
non linear kernels solutions differ notably algorithm modifies metric input space svm classifier modifies metric feature space 
note unicity guaranteed kernel gaussian kernels large bandwidths setting ensures uniqueness 
alternated optimization scheme problem complex propose solve iteratively series problems 
function optimized respect parameters fixed mapping standard svm problem 
parameters feature space mapping optimized characteristics kept fixed step starting value optimal computed 
determined descent algorithm 
scheme computed solving standard quadratic optimization problem 
implementation interior point method detailed 
svm retraining necessary faster usual training algorithm initialized appropriately solutions preceding round 
solving minimization problem respect reduced conjugate gradient technique 
optimization problem simplified assuming variables fixed 
tried versions fixed lagrange multipliers fixed set support vectors fixed 
versions optimal value optimal value slack variables obtained solving linear program optimum computed directly single iteration 
detail version ones performed better 
main steps versions sketched 
parameters update starting initial solution goal update solving simple intermediate problem providing improved solution global problem 
assume lagrange multipliers defining affected updates defined 
regarding problem sub optimal varies guaranteed admissible solution 
minimize upper bound original primal cost guarantees admissible update providing decrease cost intermediate problem provide decrease cost original problem 
intermediate optimization problem stated follows min subject solving problem difficult cost complex non linear function scale factors 
stated updated descent algorithm 
requires evaluation cost gradient respect 
particular means able compute value 
values solution problem min subject dual formulation max subject linear problem solved directly algorithm sort descending order positive examples side negative examples side compute pairwise sum sorted values set positive negative examples sum positive 
derivative respect easily computed 
parameters updated conjugate reduced gradient technique conjugate gradient algorithm ensuring set constraints verified 
updating lagrange multipliers assume support vectors remain fixed optimizing 
assumption derive rule update reasonable computing cost lagrange multipliers computing 
holds 
support vectors category 
support vectors second category equations assumption support vectors remain support vectors category change derives system linear equations defining derivatives respect 
support vectors category 
support vectors second category 
system completed stating lagrange multipliers obey constraint value updated equations step size limited ensure support vectors category 
version admissible sub optimal solution regarding problem 
experiments experiments reported constraint 
scale parameters optimized version set support vectors assumed fixed 
hyper parameters chosen span bound 
value bound faithful estimate test error average loss induced minimizer bounds quite small 
toy experiment weston compared versions feature selection algorithm standard svms filter methods preprocessing methods selecting features pearson correlation coefficients fisher criterion score kolmogorov smirnov statistic artificial data benchmarks provide basis comparing approach minimization error bounds :10.1.1.102.7476
types distributions provided detailed characteristics 
linear problem dimensions relevant 
nonlinear problem features relevant 
distribution experiments conducted average test recognition rate measures performance method 
problems standard svm achieve error rate considered range training set sizes 
results shown 
results obtained benchmarks :10.1.1.102.7476
left linear problem right nonlinear problem 
number training examples represented axis average test error rate axis 
test performances qualitatively similar ones obtained gradient descent radius margin bound improved forward selection algorithm minimizing span bound :10.1.1.102.7476
note weston provide results correct number features provided user results obtained fully automatically 
knowing number features selected algorithm somewhat similar select optimal value parameter non linear problem training examples average features selected average features selected 
figures show feature selection scheme effective stringent smaller value appropriate type problem 
relevant variables selected cases 
values ranked second 
regarding training times optimization required average times computing time standard svm fitting linear problem times nonlinear problem 
increases scale linearly number variables certainly improved 
expression recognition tested algorithm demanding task test ability handle large number features 
considered problem consists recognizing happiness expression facial expressions corresponding universal emotions disgust sadness fear anger surprise 
data sets gray level images frontal faces standardized positions eyes nose mouth 
training set comprises positive images negative ones 
test set positive images negative ones 
raw pixel representation images resulting highly correlated features 
task accuracy standard svms test errors 
recognition rate significantly affected feature selection scheme errors pixels considered completely irrelevant iterative procedure estimating required times computing time standard svm 
selection brings important clues building relevant attributes facial recognition expression task 
represents scaling factors black zero white represents highest value 
see classifier relevant areas recognizing happiness expression mainly mouth area especially mouth wrinkles lesser extent white eyes detects open eyes outer eyebrows 
right hand side displayed masked support faces support faces scaled expression mask 
lost important features regarding identity people expression visible faces 
areas irrelevant recognition task forehead nose upper cheeks erased expression mask 
introduced method perform automatic relevance determination feature selection nonlinear svms 
approach considers metric input space defines set parameters svm classifier 
update scale factor performed iteratively minimizing approximation svm cost 
efficiently minimized respect slack variable metric varies 
approximation cost function tight allow large update metric necessary 
furthermore step algorithm global cost decrease stable 
left expression mask happiness provided scaling factors right top row positive masked support face right bottom row negative masked support faces 
preliminary experimental results show method provides sensible results reasonable time high dimensional spaces illustrated facial expression recognition task 
terms test recognition rates method comparable 
comparisons needed demonstrate practical merits paradigm 
may beneficial mix approaches method cristianini determine resulting algorithm differ relative relevance feature measured estimated empirical risk minimization driven estimate generalization error 
bradley mangasarian 
feature selection concave minimization support vector machines 
proc 
th international conf 
machine learning pages 
morgan kaufmann san francisco ca 
breiman 
heuristics instability stabilization model selection 
annals statistics 
chapelle vapnik bousquet mukherjee 
choosing multiple parameters support vector machines 
machine learning 
cristianini campbell shawe taylor 
dynamically adapting kernels support vector machines 
kearns solla cohn editors advances neural information processing systems 
mit press 
hastie tibshirani friedman 
elements statistical learning data mining inference prediction 
springer series statistics 
springer 
jebara jaakkola 
feature selection dualities maximum entropy discrimination 
artificial 
neal 
bayesian learning neural networks volume lecture notes statistics 
springer 
vapnik 
nature statistical learning theory 
springer series statistics 
springer 
weston mukherjee chapelle pontil poggio vapnik :10.1.1.102.7476
feature selection svms 
advances neural information processing systems 
mit press 
