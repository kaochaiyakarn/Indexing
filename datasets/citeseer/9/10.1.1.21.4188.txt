mining biomolecular data background knowledge artificial neural networks ma jason wang james biomolecular data mining activity finding significant information protein dna rna molecules 
significant information may refer motifs clusters genes protein signatures classification rules 
chapter presents example biomolecular data mining recognition promoters dna 
propose level ensemble classifiers recognize coli promoter sequences 
level classifiers include bayesian neural networks learn different feature sets 
outputs level classifiers combined second level give final result 
enhance recognition rate background knowledge characteristics promoter sequences employ new techniques extract high level features sequences 
expectation maximization em algorithm locate binding sites promoter sequences 
empirical study shows precision rate achieved indicating excellent performance proposed approach 
result ongoing human genome project dna rna protein data accumulated speed growing exponential rate 
mining biomolecular data extract significant information extremely important accelerating genome processing 
classification supervised learning major data mining processes 
classification classify set data categories 
categories called binary classification 
chapter focus binary classification dna sequences 
department computer information science new jersey institute technology newark nj homer njit edu 
department computer information science new jersey institute technology newark nj jason cis njit edu 
corresponding author phone fax 
los alamos national laboratory mail los alamos nm lanl gov 
binary classification training data including positive negative examples 
positive data belong target class negative data belong non target class 
goal assign unlabeled test data target class nontarget class 
case test data unlabeled dna sequences positive data promoters negative data non promoters 
goal identify promoters unlabeled dna sequences terms classification recognition interchangeably chapter 
related table summarizes biomolecular data mining 
column indicates knowledge mined biomolecular data second column shows techniques third column provides 
knowledge mined includes dna sequence signals splice sites promoters protein sequence classification rules protein sequence motifs 
knowledge mined techniques genes dna hidden markov model neural networks xu splice sites dna pattern matching wang markov chain salzberg promoters dna neural networks opitz decision tree hirsh protein classification rules hidden markov model krogh neural networks wu protein motifs minimum description length table summary performed biomolecular data mining 
chapter propose level approach recognizing coli promoters dna sequences 
level classifiers include bayesian neural networks trained different feature sets 
outputs level classifiers combined second level give final classification result 
dietterich indicated ensemble classifiers achieve better recognition rate single classifier recognition rate individual classifier ensemble greater ii errors individual classifier uncorrelated 
experimental results show proposed combined classifiers outperform individual classifiers solely bayesian neural networks 
ensemble classifiers process biomolecular data studied brunak wang zhang 
brunak exploited complementary relation exon splice sites build joint recognition system allowing exon signal control threshold assign splice sites 
specifically higher threshold required avoid false positives regions small changes exon activity 
lower threshold detect donor site regions exon activity decreases significantly 
similarly lower threshold detect acceptor site regions exon activity increases significantly 
zhang developed hybrid system included neural network statistical classifier memory reasoning classifier predict secondary structures proteins 
initially classifiers trained separately 
neural network combiner trained combine outputs classifiers learning weights classifier training data 
result classification combiner 
wang studied complementarity classifiers protein sequence recognition proposed ensemble classifiers outperformed individual classifier 
contrast previous apply bayesian neural networks recognizing promoters dna 
bayesian neural networks predictions marginalization weight distribution 
furthermore bayesian neural networks control model complexity avoid overfitting problem :10.1.1.27.9072
rest chapter organized follows 
section describes characteristics coli promoters feature extraction methods 
section section level classification approach 
section presents experimental results 
section concludes chapter 
promoter recognition encoding methods important issue applying neural networks analysis regarding encode represent input neural networks 
input representations easier neural networks recognize underlying regularities 
input representations crucial success neural network learning 
encoding methods orthogonal encoding 
orthogonal encoding nucleotides amino acids viewed unordered categorical values represented dimensional orthogonal binary vectors cardinality letter dna 
sliding window example orthogonal encoding dna sequence 
alphabet fa cg cardinality letter amino acid alphabet fa yg 
binary variables binary variable set represent possible categorical values rest set 
instance represent nucleotide amino acid 
orthogonal encoding frequently early 
shows example orthogonal encoding dna sequence 
orthogonal encoding requires equal length sample variable lengths window fixed size 
disadvantage wastes lot input units input layer neural network 
instance protein sequence amino acids input units required represent protein sequence 
requires neural network weight parameters training data making difficult train neural network 
alternative encoding method proposed chapter high level features extracted 
high level features relevant biologically meaningful 
relevant mean high mutual information features region region region box box region region transcriptional start site example promoter sequence 
regions highlighted upper case letters 
region region box region region box region ca respectively 
output neural network mutual information measures average reduction uncertainty output neural network values features 
biologically meaningful mean features reflect biological characteristics sequences 
characteristics coli promoters coli promoter located immediately coli gene 
successfully locating coli promoter identifying coli gene 
uncertain characteristics coli promoters contribute difficulty promoter recognition 
coli promoters contain binding sites coli rna polymerase kind protein binds 
binding sites box box respectively 
binding site consists bases nucleotides 
central nucleotides binding sites roughly bases bases respectively upstream transcriptional start site 
transcriptional start site nucleotide codon transcription begins serves point position 
consensus sequences prototype sequences composed frequently occurring nucleotide position binding site binding site respectively 
promoters exactly match consensus sequences 
average conservation nucleotides meaning promoter sequence match average nucleotides consensus sequences 
shows example promoter sequence binding site binding site 
conservation includes nucleotides 
binding sites separated spacer 
length spacer effect relative orientation region region 
spacer nucleotides probable 
promoter sequence spacer nucleotides 
spacer box transcriptional start site variable length 
probable length spacer nucleotides 
promoter sequence spacer nucleotides 
variable spacing appropriate orthogonal encoding encode view promoter sequence attribute tuple length promoter sequence 
promoter sequences position nucleotide upstream transcriptional start site transcriptional start site position 
region includes nucleotides position transcriptional start site 
promoter sequence nucleotide position nucleotide transcriptional start site 
addition salient characteristics binding sites transcriptional start site non salient characteristics regions 
pattern matching method applied characterization coli promoters 
weak motifs regions 
weak motif subsequence occurs frequently region 
term weak frequency base motif significant frequency base consensus sequences occurring binding sites 
nucleotides weak motifs spacer region binding sites contributions specificity promoter sequences 
pedersen engelbrecht adopted neural network characterize coli promoters 
significance weak motif measured decrease maximum correlation coefficient motifs weak motif fed neural network 
method authors weak motifs regions 
interesting observe weak motifs spaced regularly period nucleotides corresponding helical turn 
phenomenon suggests rna polymerase contact promoter face dna 
subsequently characterization coli promoter sequences carried hidden markov model 
observed position box relative transcriptional start site flexible 
clustering analysis carried larger set coli promoter sequences containing promoters 
weak motifs region 
weak motifs revealed sequence logos described schneider stephens 
displays sequence logos coli promoters aligned transcriptional start site 
set aligned sequences sequence logos measure general distance binding site transcriptional start site varies bases 
distance binding site binding site varies bases 
varying distances render promoter recognition difficult contents positions binding sites uncertain 
subsection expectation maximization em algorithm locate binding sites promoter sequences 
sequence logos produced software available 
gov toms html 
sequence logos bits sequence logos coli promoter sequences 
position transcriptional start site equivalent position described text 
negative positions consistent described text 
non randomness position independently shannon entropy position log jdj gamma gamma log jdj cardinality letter dna alphabet log jdj maximum uncertainty position gamma log shannon entropy position frequency base position height position represents information content position 
higher information content random position size base position logos proportional frequency base 
recall weak motif frequently occurring subsequence region 
sequence logos weak motif consists positions bases non zero information content 
seen weak motifs exist regions 
characteristics coli promoter sequences reported literature explore methods extracting high level features regions promoters region nucleotides long region nucleotides long region nucleotides long region nucleotides long region nucleotides long region nucleotides long see 
method maximal dependence decomposition mdd technique second motif method 
region nucleotides long nucleotide long motif region statistically significant apply mdd method extracting features region 
order calculate feature values know precisely possible regions 
subsection expectation maximization em algorithm locating binding sites promoter sequences 
describe feature extraction methods detail 
locating binding sites em algorithm align subsequences region region region region need locate binding sites coli promoters 
locating box box may done em algorithm 
general em algorithm applied maximum likelihood estimation data incomplete 
locating binding sites em algorithm proposed lawrence 
generalized allow different binding sites 
published methods assumed location binding sites uniformly distributed attempted locate continuous region included box box spacer variable length 
contrast method assumption uniform distribution considers binding sites separately spacer 
represent set promoter sequences training set contains positive training sequences 
denote cardinality promoter sequence length spacer region transcriptional start site denoted sp length spacer region region denoted sp unobserved observed 
specifically refer positive training sequences observed data 
observed data incomplete lengths lengths referred unobserved missing data 
proposed em algorithm estimates model parameters defined incomplete data 
estimates model parameters algorithm determines locations binding sites dna sequence 
general sp varies sp varies 
assume nucleotides positions independent 
position weight matrix pwm described model nucleotides position 
binding site consists bases 
denote probability occurring position region 
denote 
denote probability occurring position region 
denote 
denote probability occurring regions outside binding sites 
sp sp denote probability sp sp sp sp respectively random variable denoting distance transcriptional start site region respectively region 

em algorithm proceeds iteratively converge 
iteration consists steps expectation step step maximization step step 
step calculates expected complete data log likelihood expectation distribution missing data observed data current estimates 
assume promoter sequences training set independent 
step calculates sp sp jt logp sp sp sp sp jt log jsp sp sp sp sp sp log jsp sp sp sp suppose promoter sequences training set nucleotides long aligned respect transcriptional start site position 
denote nucleotide position promoter sequence define denote number occurrences nucleotide outside binding sites promoter sequence sp sp jsp sp pi gammam pi gammam gamman pi bayes rule sp sp jsp sp ps sp sp jsp sp ps sp sp jsp sp ps sp sp pi gammam pi gammam gamman pi po ps sp sp pi gammam pi gammam gamman pi po ps sp sp previous applications em algorithm assumed sp sp uniformly distributed 
term sp sp deleted 
assume sp sp uniformly distributed probable values probable values 
substituting logp logp gamma logp logp gammam sp sp gammam gamman sp sp gamma sp sp sp sp denote value iteration 
initialized randomly step proceed 
iteration current estimate calculate expected complete data log likelihood 
step maximizes respect 
maximum likelihood estimates complete data known just sample frequencies 
new value iteration 
process iterates convergence 
model parameters calculated positive training sequences promoter sequences training data set determine locations binding sites dna sequence training sequence test sequence positive sequence negative sequence choosing spacer lengths sp sp maximize sp sp jsp sp sp sp 
align binding sites training promoter sequences extract features different regions mdd technique motif method described 
mdd technique mdd technique proposed detect splice site human genomic dna gene prediction software 
adopted latest version gene prediction software morgan 
mdd derived pwm model overcome limitation consensus sequence modeling nucleotide distribution position 
disadvantage pwm assumes positions independent 
disadvantage removed weight array model wam generalizes pwm allowing dependencies adjacent positions 
wam essentially order markov chain conditional probability upstream adjacent nucleotide generalized second order markov chain third order markov chain dependencies tries model free parameters model requiring training data appropriately estimate parameters model 
general danger tries complex models free parameters training data estimate free parameters 
instance suppose promoter sequences available estimate parameters pwm represents probability nucleotide occurring position sequences 
equivalently roughly promoter sequences available estimate parameters jx gamma wam jx gamma represents conditional probability position gamma position gamma upstream neighbor position gamma promoter sequences reliably estimate free parameters 
mdd provides flexible solution problem iteratively clustering dataset significant adjacent non adjacent dependencies 
essentially models order second order third order higher order dependencies depending amount training data available 
specifically mdd works follows 
set aligned sequences chooses consensus nucleotide position case set includes subsequences region positive training sequences promoter sequences 
statistic ij calculated measure dependencies nucleotides position 
significant dependencies detected simple pwm 
significant dependencies detected dependencies exist adjacent positions wam 
mdd procedure carried 
mdd procedure iterative process calculate sum ij select position sm maximal decompose dataset disjoint subsets um containing sequences consensus nucleotide cm position gamma um containing sequences consensus nucleotide cm position 
mdd procedure applied recursively um gammau respectively conditions holds decomposition possible significant dependencies positions exist resulting subsets number sequences resulting subsets threshold reliable estimation parameters possible decomposition 
apply mdd method region region region region region region region respectively training promoter sequences 
result region region modeled pwm level decomposition carried regions 
set sequences mdd feature values sequence calculated follows 
mdd technique applied positive training data coli promoter sequences 
results probability matrices region region conditional probability matrices region region region region region 
secondly positive negative sequences matrices calculate mdd feature values sequence 
particular feature value subsequence region region sequence calculated probability position example suppose probability matrix region positive training sequences gamma example subsequence tg tg theta 
feature value subsequence regions sequence calculated cm cp cp gamma gamma cp cp cm xm cp cp gamma gamma cp cp cm respectively represents probability cm respectively position cp cp respectively gamma represents conditional probability position xm cm cm respectively 
motif method calculate motif feature values sequence apply pattern matching tool positive training data coli promoter sequences find weak motifs region region region region region respectively sequences 
set sequences 
define occurrence number motif subsequence number sequences contain motif 
tool find motifs allowed mut mutations occur sequences set jm length jm represents number nucleotides mut occur length user specified parameters 
case set includes subsequences region region positive training sequences 
required length motifs fixed region 
study length regions length regions 
minimum occurrence number required allowed mutation number 
occurrence number motif assigned weight motif 
intuitively frequently motif occurs region positive training sequences higher weight 
set sequences training sequences test sequences positive sequences negative sequences motif feature values regions training sequence calculated follows 
motif method described applied regions positive training data 
result sets motifs regions 
secondly region sequence subsequence region matched motifs region 
matched motifs feature value region maximum weight matched motifs feature value assigned 
motif feature value region sequence assigned nucleotide position nucleotide transcriptional start site assigned 
basic classifiers developed basic classifiers classifier classifier classifier 
classifiers bayesian neural network 
training sequence test sequence encoded high level features described previous section feature values input neural networks 
classifier classifier bayesian neural network hidden units input units including mdd features distance features distance number nucleotides box box distance box transcriptional start site 
second classifier classifier bayesian neural network hidden units input units including motif features distance features 
third classifier classifier bayesian neural network hidden units input units including mdd features motif features distance features 
number hidden units determined experimentally evidence model 
bayesian neural network hidden layer sigmoid activation functions 
output layer neural network output unit 
output value bounded logistic activation function gammaa neural network fully connected adjacent layers 
shows bayesian neural network architecture classifier 
bayesian neural networks bayesian neural network integration bayesian inference neural network 
bayesian inference model neural network consists set free parameters viewed random variables 
prior model represented 
likelihood probability data model specified djm 
posterior probability model quantified jd 
bayes rule jd djm djm dm normalizing constant 
case fx denotes training dataset including positive negative training data total number training sequences input feature vector contains respectively input values classifier classifier classifier respectively binary target value output unit 
represents promoter sequence 
denote mdd feature region mdd feature region mdd feature region mdd feature region mdd feature region mdd feature region mdd feature region spacer spacer motif feature region motif feature region motif feature region motif feature region motif feature region motif feature region bayesian neural network architecture classifier 
spacer distance box box 
spacer distance region transcriptional start site 
input feature vector dna sequence training sequence test sequence 
architecture weights neural network output value uniquely determined input vector output value interpreted jx probability represents promoter sequence likelihood probability data model calculated djw pi gamma gammat exp gammag djw djw cross entropy error function djw gamma gamma log gamma djw objective function minimized non bayesian neural network training process maximum likelihood estimation method assumes possible weights equally 
non bayesian neural network weight decay avoid overfitting training data poor generalization test data adding term ff objective function ff weight decay parameter hyperparameter sum square weights neural network number weights 
objective function minimized penalize neural network weights large magnitudes penalizing complex model favoring simple model 
precise way specify appropriate value ff tuned offline 
bayesian neural network hyperparameter ff interpreted parameter model optimized online bayesian learning process 
weight decay term ff scaled ff exp gamma ff interpreted prior probability weight vector gaussian distribution zero mean variance ff larger neural network weights probable 
bayesian neural network generalizes previous weight decay term associating weights different layers different variances 
hyperparameter vector ff 
ff ff ff represent vector ff ff associated group weights number weights associated ff denote qc prior exp gamma ff zw zw exp gamma ff dw gaussian integral 
get posterior probability wjd ff exp gamma ff gamma djw zm zm exp gamma ff gamma djw dw normalizing constant 
training phase bayesian training neural networks iterative procedure 
implementation bayesian neural network adopt iteration involves level inferences 
illustrates training process 
level value hyperparameter ff initialized random value iteration infer probable value weight vector mp corresponding maximum wjd ff neural network training minimizes ff djw 
level inference bayes rule wjd ff djw djff software available wol ra phy cam ac uk pub mackay readme html 
try model converge hyperparameter find probable training data 
output chosen model parameters 
choose number hidden units model 
initialize hyperparameter weight parameters 
training process level inference 
reestimate hyperparameter second level inference 
calculate evidence model third level inference 
training process bayesian neural network 
djw wjd ff respectively 
wjd ff approximated gaussian centered mp wjd ff mp jd ff exp gamma gamma mp gamma mp gamma logp wjd ff mp hessian matrix evaluated mp second level hyperparameter ff optimized :10.1.1.27.9072
second level inference bayes rule ffjd djff dja lack prior knowledge assume constant ignore 
normalizing factor dja constant value ff maximizing posterior ffjd inferred maximizing evidence ff djff normalizing factor 
djff djw dw 
evidence djff maximized differentiation respect ff 
find new hyperparameter value ff new setting differentiation zero ff new fl mp fl number determined parameters group :10.1.1.27.9072
new hyperparameter value ff new iteration 
process iterates convergence reached 
third level inference model comparison 
level carried manually 
third level inference bayes rule jd dja prior probability assumed constant constant 
posterior probability jd determined evidence dja normalizing factor second level inference 
dja djff dff 
different models different number hidden units tested 
model largest evidence value chosen 
testing phase testing phase basic classifiers developed output bayesian neural network marginalization network weight distribution 
jx jx wjd dw wjd dw classifier classifier classifier combiner combiner region region box region region box region spacer spacer mdd feature set motif feature set basic classifiers combiners classification scheme 
output bayesian neural network jx probability unlabeled test sequence promoter 
greater decision boundary test sequence classified promoter decision boundary test sequence classified non promoter test sequence gets opinion verdict 
combination basic classifiers basic classifiers described previous section combined classifier second level see 
explore methods combining basic classifiers combiner combiner 
combiner employs unweighted voter 
output output value classifier basic classifiers agree classification results promoter opinion final result results classifiers classifiers agree classification results final result results classifiers classifiers agrees classification results final result result classifier min gamma output output minimal final result opinion 
combiner employs weighted voter 
output weighted sum outputs basic classifiers 
represent weight classifier weight precision rate classifier training phase 
output combiner theta output note assign equal weights basic classifiers combiner reduced combiner 
experiments results data study adopted coli promoter sequences taken latest coli promoter compilation 
coli promoters aligned transcriptional start site 
trimmed promoter sequence sequence nucleotides including nucleotides nucleotides upstream transcriptional start site nucleotides downstream transcriptional start site 
gave promoter sequences 
negative data non promoter sequences retrieved genbank 
negative data coli genes preceding promoter region deleted 
negative sequence nucleotides long 
negative sequences 
results table gives fold cross validation results basic classifiers table gives results combined classifiers 
fold cross validation dataset containing positive data promoters negative data non promoters randomly split mutually exclusive folds approximately equal size 
bayesian neural network trained tested times 
ith run neural network trained gamma tested allocated data way training dataset gamma test dataset respectively approximately respectively positive data respectively negative data 
average tests taken 
machine conduct experiments mhz pentium ii pc running linux operating system 
time spent extracting features basic classifier run seconds average 
time spent training basic classifier run seconds average 
time spent testing phase run seconds average 
available ftp ncbi nlm nih gov repository eco 
classifier classifier classifier precision rate specificity sensitivity table performance basic classifiers 
precision rate measure performance studied classifiers 
precision rate defined theta number test sequences classified correctly total number test sequences 
false positive non promoter test sequence misclassified promoter sequence 
true positive promoter test sequence classified promoter sequence 
specificity defined gamma fp ng theta fp number false positives ng total number negative test sequences 
sensitivity defined tp po theta tp number true positives po total number positive test sequences 
combiner combiner precision rate specificity sensitivity table performance combined classifiers 
table see combiner combiner outperform basic classifiers 
combiner gives best precision rate 
reason combiner higher precision rate basic classifiers explained bernoulli model 
instance assume basic classifiers precision rate classification errors completely independently 
combiner classification error classifier errors time 
precision rate unweighted voter basic classifiers gamma gamma gamma practical precision rate bit lower 
reason classifier classifier classifier errors completely independently 
classification results percentage test sequences combiner combiner agreed correct combiner combiner agreed wrong combiner combiner disagreed combiner correct combiner combiner disagreed combiner correct combiner combiner disagreed wrong table complementarity combiner combiner 
table illustrates complementarity combiner combiner 
combiner combiner agree classification higher likelihood correct 
agree probability classification correct 
table see combiner combiner disagree probability correct 
chapter proposed level ensemble classifiers recognize coli promoter sequences 
level classifiers include bayesian neural networks trained different feature sets 
outputs level classifiers combined second level give final result 
recognition rate achieved 
currently extending approach classify protein sequences recognize full gene structure 
acknowledgments sequence logos software developed dr thomas schneider 
dr david mackay sharing bayesian neural network software 
dr providing promoter sequences chapter 
anonymous reviewers gave useful comments helped improve presentation quality chapter 
bailey unsupervised learning multiple motifs biopolymers expectation maximization 
machine learning 
ukkonen discovering patterns subfamilies 
proceedings fourth international conference intelligent systems molecular biology pages 
brunak engelbrecht knudsen prediction human mrna donor acceptor sites dna sequence 
journal molecular biology 
burge karlin prediction complete gene structures human genomic dna 
journal molecular biology 
stormo expectation maximization algorithm identifying protein binding sites variable lengths unaligned dna fragments 
journal molecular biology 
craven shavlik machine learning approaches gene recognition 
ieee expert 
dempster laird rubin maximum likelihood incomplete data em algorithm 
journal royal statistical society series 
dietterich machine learning research current directions 
ai magazine 
human genome project informatics 
communications acm 
eggert waterman rigorous pattern recognition methods dna sequences analysis promoter sequences escherichia coli 
journal molecular biology 
hirsh noordewier background knowledge improve inductive learning dna sequences 
proceedings tenth conference artificial intelligence applications pages 
hirst sternberg prediction structural functional features protein nucleic acid sequences artificial neural networks 
biochemistry 
krogh brown mian haussler hidden markov models computational biology applications protein modeling 
journal molecular biology 
haussler reese generalized hidden markov model recognition human genes dna 
proceedings fourth international conference intelligent systems molecular biology pages 
lawrence reilly expectation maximization em algorithm identification characterization common sites unaligned biopolymer sequences 
proteins structure function genetics 
margalit compilation coli mrna promoter sequences 
nucleic acids research 
mackay bayesian interpolation :10.1.1.27.9072
neural computation 
mackay practical bayesian framework backprop networks 
neural computation 
smith recognition characteristic patterns sets functionally equivalent dna sequences 
computer applications biosciences 
neal bayesian learning neural networks 
lecture notes statistics volume springer verlag new york 
opitz shavlik connectionist theory refinement genetically searching space network topologies 
journal artificial intelligence research 
non canonical sequence elements promoter structure 
cluster analysis promoters recognized escherichia coli rna polymerase 
nucleic acids research 
pedersen baldi brunak chauvin characterization eukaryotic promoters hidden markov models 
proceedings fourth international conference intelligent systems molecular biology pages 
pedersen engelbrecht investigations escherichia coli promoter sequences artificial neural networks new signals discovered upstream transcriptional start point 
proceedings third international conference intelligent systems molecular biology pages 
salzberg decision tree system finding genes dna 
technical report cs johns hopkins university 
salzberg method identifying splice sites translational start sites eukaryotic mrna 
computer applications biosciences 
schneider stephens sequence logos new way display consensus sequences 
nucleic acids research 
computer methods locate signals nucleic acid sequences 
nucleic acids research 
wang marr shasha shapiro chirn discovering active motifs sets related protein sequences classification 
nucleic acids research 
wang marr shasha shapiro chirn lee complementary classification approaches protein sequences 
protein engineering 
wang shapiro shasha wang yin new techniques dna sequence classification 
journal computational biology 
wang shapiro shasha 
eds 
pattern discovery biomolecular data tools techniques applications 
oxford university press new york 
wu berry fung neural networks full scale protein sequence classification sequence encoding singular value decomposition 
machine learning 
xu mural einstein shah grail multiagent neural network system gene identification 
proceedings ieee 
zhang marr weight array method splicing signal analysis 
computer applications biosciences 
zhang mesirov waltz hybrid system protein secondary structure prediction 
journal molecular biology 

