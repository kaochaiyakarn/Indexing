hierarchical gaussian process latent variable models neil lawrence neill cs man ac uk school computer science university manchester building oxford road manchester pl andrew moore moore dcs shef ac uk dept computer science university sheffield regent court portobello street sheffield dp gaussian process latent variable model gp lvm powerful approach probabilistic modelling high dimensional data dimensional reduction 
extend gp lvm hierarchies 
hierarchical model tree allows express conditional independencies data manifold structure 
introduce gaussian process hierarchies simple dynamical model extend approach complex hierarchy applied visualisation human motion data sets 

gaussian process latent variable model lawrence lawrence proven highly effective approach probabilistic modelling high dimensional data lies non linear manifold ferris 
curse dimensionality assuming high dimensional data intrinsically low dimensional nature 
reduces effective number parameters model enabling generalisation small data sets non linear models dimensionality features larger number data points 
alternative manifold representations modelling high dimensional data develop latent variable model sparse connectivity explain data 
example tree structured models suggested modelling images williams feng feng preliminary 
review international conference machine learning icml 
distribute 
object recognition felzenszwalb huttenlocher ioffe forsyth human pose estimation forsyth sigal lan huttenlocher 
probabilistic perspective pearl tree structures sparse probabilistic models offer convenient way specify conditional independencies model 
general clear conditional independencies specified context dimensional reduction 
show construct dimensionality reduction hierarchical way allowing concurrently exploit advantages expressing conditional independencies low dimensional non linear manifolds 

gp gaussian process latent variable model gp lvm fully probabilistic non linear latent variable model generalises principal component analysis 
model inspired observation particular probabilistic interpretation pca product gaussian process models linear covariance function 
consideration nonlinear covariance functions non linear latent variable model constructed lawrence lawrence 
important characteristic gp lvm ease accuracy probabilistic reconstructions data possibly new point latent space 
characteristic exploited successful applications gp lvm learning style motion capture data learning prior model tracking robot simultaneous localisation mapping ferris 
smooth mappings latent space data space 
probabilistic approach non linear dimensionality reduction mackay bishop formulate latent variable model la hierarchical gaussian process latent variable models tent dimension lower data dimension latent space governed prior distribution 
latent variable related observation space probabilistic mapping fi xn ith feature nth data point noise term typically taken gaussian matrix mapping parameters 
prior taken independent data points marginal likelihood data written yn xn xn yn xn yin fin xn mapping chosen linear fi xn xn prior latent variables taken gaussian maximum likelihood solution model spans principal subspace data tipping bishop 
mapping non linear unclear general propagate prior distribution uncertainty nonlinearity 
alternative approach taken gp lvm place prior distribution mappings latent variables 
mappings may marginalised marginal likelihood optimised respect latent variables yin fin 
turns prior taken gaussian process independent data dimensions linear covariance function restricting mappings linearity maximum likelihood solution respect embeddings principal component analysis 
covariance function allows non linear functions rbf kernel model provides probabilistic non linear latent variable model 
advantages mapping latent variable 
particular non linear latent variable model require approximations recovered 
additionally probabilistic model data expressed form usual form 
model non parametric size denote gaussian distribution mean covariance 
row xn associated data observation yn 
easier augment model additional constraints prior information data 
interesting examples include adding dynamical priors latent space wang constraining points latent space intuitively reasonable visualisation criteria lawrence qui candela 
exploit characteristic proposing hierarchical gaussian process latent variable models 
section illustrate nature simple layered hierarchical model considering novel approach incorporating dynamics gp lvm section consider complex hierarchies focussing models human body motion 

dynamics simple hierarchy standard latent variable model setting dynamical system modelled constructing dynamical prior distribution tractability typically takes form markov chain xt xt 
latent variable marginalised inducing correlations neighbouring time points 
gp lvm marginalise respect mapping marginalisation performed integrating latent space associated dynamical prior analytically intractable 
may choose combine dynamical prior gp lvm likelihood seek maximum posteriori map solution 

gaussian process dynamics seeking map solution approach taken wang autoregressive gaussian process prior augment gp lvm dynamics 
utility approach nicely demonstrated context tracking show dynamics track sustained subject fully occluded frames 
autoregressive approach works predicting temporal location latent space previous models xt xt 
prediction gaussian process unimodal prediction xt xt 
problems consider example case subject walking breaking run 
expect walking steps broadly periodic point cycle projecting similar point latent space 
point sub hierarchical gaussian process latent variable models ject begins break run bifurcation dynamics 
bifurcation captured correctly unimodal autoregressive dynamics 
additionally autoregressive approach assumes samples taken uniform intervals occasional drop outs may case shall see section 

simple hierarchical model illustration hierarchical gp lvm consider alternative implementation dynamics 
just wang implement dynamics gaussian process prior seek map solution 
contrast approach model autoregressive 
simply place gaussian process prior latent space inputs time points approach alleviates requirement uniform intervals time samples prior latent space longer function location latent space allows path latent space points subject example breaks run 
set dimensional observations motion capture sequence 
yt seek model gaussian process latent variable model kx column design matrix element different point time sequence kx covariance matrix kernel depends dimensional latent variables 
xt element example kx xi xj rbf exp xi xj white ij radial basis function rbf covariance matrix noise term 
parameters covariance variances different terms rbf white length scale rbf term lx 
dropped dependence parameters avoid cluttering notation 
construct simple hierarchy placing prior elements wish prior temporally smooth ensuring points temporally close xi xj nearby space 
suitable prior gaussian process 
typical sample paths rbf covariance function temporal prior latent space 
input gaussian process time kt vector times observed sequence jth column kt covariance matrix form kt ti tj rbf exp ti tj white dimensional latent space typical sample paths covariance function shown 
temporal prior combined gp lvm likelihood form new model dx unfortunately marginalisation intractable 
seek progress seeking maximum posteriori map solution maximising log log log const 
respect term equation standard objective function gp lvm second term form log jk const jth column gradient additional term may log dx combined gradient log find map solution 
easily gradient methods 
interaction subject subject hierarchical gaussian process latent variable models data data 
simple hierarchy capturing interaction subjects data associated subject subject 
variable sets controlled latent variables 
latent variables turn controlled 

complex hierarchies turn slightly complex hierarchy dynamical model described previous section 
consider motion capture example multiple subjects interacting 
form interaction possible model subject independently 
form conditional independence captured hierarchical model shown 
joint probability distribution represented graph 

dx dx dx conditional distribution gaussian process 
required tractable 
turn map solutions finding values latent variables 
model means maximisation log log log log sum gaussian process log likelihoods 
terms associated subjects 
third term provides ordination subjects 

interacting subjects demonstrate hierarchical model considered motion capture data set consisting interacting subjects 
data taken cmu data base consists subjects approach high 
algorithm optimisation latent variables proceeded follows 
initialise leaf node latent variable set principal component analysis corresponding data set 

initialise root node latent variable set principal component analysis concatenated latent variables dependents 

optimise jointly parameters kernel matrices gaussian process model latent variable positions 
original data sampled frames second 
extracted frames sub sampling frames second frames full sample rate frames sub sampling frames second 
gives data set variable sample rate 
context data variable sample rate important section higher sample rate contains subjects hands 
motion rapid accurately reconstructed sample rate frames second 
variable sample rate presents problems autoregressive dynamics reviewed section 
regressive dynamics introduced section variable sample rate simply reflected vector dynamics adding layer hierarchy 

dx dx dx 
cs cmu edu 
subjects numbered data base 
motion number 
hierarchical gaussian process latent variable models 
decomposition skeleton hierarchical modelling 
separating component parts skeleton manner model space natural motions component part express independently jointly 
optimise parameters dynamics wish latent space constrained dynamics 
effect dynamics descend hierarchy 
constrained noise parameter white gaussian process associated allow variance free effect dynamics diluted drop hierarchy 
constraining variance force temporal correlations data respected 
show results mapping motions hierarchical model 

subject decomposition decomposing interactions subjects hierarchy consider decomposition single subject parts 
discussed different approaches modelling motion capture data tree models models typically assume nodes tree observed tree rigidly reflects skeletal structure 
effort model additional correlations motion data augmenting tree additional common latent variable lan huttenlocher 
hierarchical model closer structure tree models williams feng tree structure refers hierarchy latent variables hierarchy observed variables 
considered data set composed walking mo tion running motion taken cmu data base 
run taken motion subject walk taken motion subject 
data sub sampled frames second cycle motion 
location motion root position set zero subject running walking place 
modelled subject decomposition shown reflect fact different motions data constructed hierarchy roots 
root associated run second root associated walk 
prior induced run root applied run data points layer hierarchy abdomen legs upper body 
similarly prior induced walk root applied data points walk data 
upper body legs leaf nodes applied entire data set 
construction enables express motion sequences separately whilst retaining information required jointly model component parts skeleton 
aim nodes lower levels hierarchy span range motions whilst upper layer specifies particular motion type 
motion broadly periodic periodic kernel mackay regressive dynamics latent space see pg 
rasmussen williams details 
resulting visualisation shown 
discussion hierarchical version gaussian process latent variable model 
gaussian process latent variable model involves paradigm shift probabilistic latent variable models latent variables optimising mappings marginalise mappings optimise latent variables 
far easier construct hierarchies models 
philosophy optimising versus carried hierarchical gp lvm maximise respect latent variables different levels hierarchy 

overfitting modelling gp lvm characterised large numbers parameters form latent points 
standard case num ber parameters increases linearly fraction number data 
long depends data set problems overfitting hierarchical gaussian process latent variable models 
high example 
subjects modelled walk high 
plot shows simple hierarchy model data 
regressive dynamical prior type described section placed latent space root node 
root node controls individual subjects 
illustrate model taken points time input values dynamical model frames 
points mapped hierarchy data space 
plots subjects subject right subject left 
normally occur 
adding additional latent variables run risk overfitting data hierarchy deep 
point note upper levels hierarchy serve leaf nodes leaf nodes independently overfit entire model 
words ensure leaf nodes qi di qi number columns xi di dimensionality yi 
modifying locations latent variables nodes higher hierarchy changing nature regularisation leaf nodes 
unconstrained model simply act way remove regularisation 
implementation attempted counter potential problem ways 
firstly provided fixed dynamical prior top level 
parameters prior optimised top level node regularised 
possibility fixed regularisation di goal achieved back constraints lawrence qui candela explore approach 
noise descend hierarchy 
prevent happening constrained noise variance gaussian process leaf node close zero high prevent numerical instabilities kernel matrix inverses 
strategy proved effective experiments 

hierarchical models apparent similarities model names natural ask relationship hierarchical gp lvm hierarchical probabilistic pca bishop tipping 
models philosophically distinct 
hierarchical pca related hierarchical gtm model tino node hierarchy associated probabilistic model data space 
hierarchy hierarchy latent variables hierarchical clustering mixture components discrete mixture probabilistic pca models gtm models 
similar approach taken gp lvm approach described 
hierarchical gaussian process latent variable models 
combined model run walk 
skeleton decomposed shown 
plots crosses latent positions associated run circles associated walk 
mapped points motion hierarchy 
periodic dynamics latent spaces 

applications see hierarchical gp lvm important tool application areas 
application areas believe algorithm particular promise 
firstly gp lvm proposed prior model tracking 
key problem constructing prior models difficult cover space natural human motions 
hierarchical model expect inspired language modelling able perform variant back 
depending motion different models swapped top level hierarchy actions modelled 
case suggest backing context translate dropping hierarchy applying models layer hierarchy independently data 
application area see great promise model animation 
hierarchical gp lvm model different portions character animated separately jointly circumstances demand 
animator time dominating cost games film entertainment industries computer special effect techniques combination hierarchical gp lvm appropriate inverse kinematic techniques seek ameliorate costs 
funded eu fp pascal network excellence pump priming 
motion capture data project obtained cs cmu edu 
database created funding nsf eia 
helpful discussions 
recreating experiments source code re running experiments detailed available www cs man ac 
uk neill release 
ravindran 

image modelling tree structured conditional random fields 
proceedings th international joint conference artificial intelligence ijcai pp 

bishop svens williams 

gtm generative topographic mapping 
neural computation 
bishop tipping 

hierarchical latent variable model data visualisation 
ieee transactions pattern analysis machine intelligence 
hierarchical gaussian process latent variable models felzenszwalb huttenlocher 

efficient matching pictorial structures 
proceedings conference computer vision pattern recognition pp 

hilton head island south carolina ieee computer society press 
feng williams 

combining belief networks neural networks scene segmentation 
ieee transactions pattern analysis machine intelligence 
ferris fox lawrence 

wifi slam gaussian process latent variable models 
proceedings th international joint conference artificial intelligence ijcai pp 

martin hertzmann popovic 

style inverse kinematics 
acm transactions graphics siggraph 
ioffe forsyth 

mixtures trees object recognition 
proceedings conference computer vision pattern recognition pp 

hawaii ieee computer society press 
lan huttenlocher 

trees common factor models human pose recovery 
ieee international conference computer vision iccv pp 

china ieee computer society press 
lawrence 

gaussian process models visualisation high dimensional data 
advances neural information processing systems pp 

cambridge ma mit press 
lawrence 

probabilistic non linear principal component analysis gaussian process latent variable models 
journal machine learning research 
lawrence qui candela 

local distance preservation gp lvm back constraints 
proceedings international conference machine learning pp 


mackay 

bayesian neural networks density networks 
nuclear instruments methods physics research 
mackay 

gaussian processes 
bishop ed neural networks machine learning vol 
series computer systems sciences 
berlin springer verlag 
pearl 

probabilistic reasoning intelligent systems 
san francisco morgan kaufmann 
forsyth 

finding tracking people bottom 
proceedings conference computer vision pattern recognition pp 

madison wisconsin ieee computer society press 
rasmussen williams 

gaussian processes machine learning 
cambridge ma mit press 
sigal bhatia roth black isard 

tracking loose people 
proceedings conference computer vision pattern recognition pp 

washington dc ieee computer society press 
tino 

hierarchical gtm constructing localized nonlinear projection manifolds principled way 
ieee transactions pattern analysis machine intelligence 
tipping bishop 

probabilistic principal component analysis 
journal royal statistical society 
fleet fua 

people tracking gaussian process dynamical models 
proceedings conference computer vision pattern recognition pp 

new york ieee computer society press 
fleet hertzmann fua 

priors people tracking small training sets 
ieee international conference computer vision iccv pp 

china ieee computer society press 
wang fleet hertzmann 

gaussian process dynamical models 
advances neural information processing systems 
cambridge ma mit press 
williams feng 

tree structured belief networks models images 
proceedings ninth international conference artificial neural networks icann 
