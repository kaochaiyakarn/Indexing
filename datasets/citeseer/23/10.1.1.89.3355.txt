psychological review copyright american psychological association vol 
doi representing word meaning order information composite holographic lexicon michael jones university colorado boulder douglas mewhort queen university authors computational model builds holographic lexicon representing word meaning word order unsupervised experience natural language 
model uses simple convolution superposition mechanisms cf 
murdock learn distributed holographic representations words 
structure resulting lexicon account empirical data classic experiments studying semantic typicality categorization priming semantic constraint sentence completions 
furthermore order information retrieved holographic representations allowing model account limited word transitions need built transition rules 
model demonstrates broad range psychological data accounted directly structure lexical representations learned way need complexity built processing mechanisms representations 
holographic representations appropriate knowledge representation higher order models language comprehension relieving complexity required higher level 
keywords semantic memory mental lexicon latent semantic analysis statistical learning holographic models language immensely complex behavior 
minimum requires knowledge words language typically thought stored mental lexicon knowledge grammatical application words sentences 
higher order models language comprehension kintsch require realistic representation word meaning order information successful particularly appealing representations learned statistical redundancies natural language 
mental lexicon rich history psychology 
traditionally lexicon viewed dictionary database entry containing word meaning cases syntactic rules phonological characteristics see elman pustejovsky reviews 
precise representation information greatly debated 
article presents model signal processing associative mem michael jones institute cognitive science university colorado boulder douglas mewhort department psychology queen university kingston ontario canada 
article part doctoral dissertation submitted queen university michael jones 
research supported douglas mewhort natural sciences engineering council canada nserc sun microsystems 
michael jones supported graduate scholarship postdoctoral fellowship nserc 
simulations conducted supercomputers high performance computing virtual laboratory 
grateful walter kintsch tom landauer ben murdock comments draft article 
correspondence concerning article addressed michael jones department psychological brain sciences indiana university bloomington 
mail indiana edu ory theory builds distributed representations words containing semantic order information unsupervised learning natural language 
word meaning order information stored single pattern elements distributed holographic lexicon 
word representation gradually stabilizes statistical sampling experience 
furthermore transition information retrieved lexicon holographic decoding resonance 
charles osgood george miller worked independently seemingly unrelated problems 
osgood tackled problem representing word meaning 
appeared outset representing word meaning unobtainable best approach possible approximate semantic relationships subjective ratings 
miller hand interested problem sequential dependencies words 
sequential dependencies involve direct statistical transitions words appeared outset comprehensive model sequential dependencies reach comprehensive model semantics unobtainable 
sequential dependency problem turned considerably complicated originally anticipated 
models learn semantic representations words automatically experience text landauer dumais lund burgess 
models neglect information inherent word transitions 
theoretical word meaning largely developed independently theoretical word transitions complete model lexicon needs incorporate sources information account broad range semantic categorization typicality priming data data relating lexical class word usage word transitions sentences 
representing word meaning pioneering representation knowledge focused feature lists semantic networks 
feature lists represent words lists descriptive binary features smith shoben rips 
example birds wings dogs 
feature lists typically hand coded theorist basis intuition 
models word recognition bypass problem determining semantic features employing random binary vectors masson plaut build lists empirical feature reports generated subjects de sa seidenberg 
feature list models share common theme semantic representation word list binary features describe 
contrast semantic network theories collins quillian collins loftus assume words represented localist nodes interconnected concepts 
words connected direct pathways similar meaning 
semantic networks learn new concepts connections unsupervised large scale learning structure parallels human learning ways steyvers tenenbaum 
third method semantic representation built osgood multidimensional representation scheme see salton salton wong yang 
modern hyperspace models build semantic representations directly statistical occurrences words text typically representing high dimensional semantic space landauer dumais lund burgess 
semantic space approach minimizes representation processing assumptions model complexity learned environment hardwired model 
semantic space models differ semantic networks generate distributed representations words localist representations semantic nets 
semantic space models feature lists distributed word representations principled difference semantic space model feature list model nature word features 
semantic space model features represent word values identifiable meaning isolation features 
particular feature bird feature list wings presence meaning meaning bird semantic space model aggregate distributed pattern dimensions interpretable meaning 
latent semantic analysis lsa deerwester dumais furnas landauer harshman landauer dumais received attention semantic space models 
lsa begins computing word document frequency matrix large corpus text words documents obviously matrix quite sparse 
raw entries represent frequency word appears particular document 
entries converted log frequency values divided word entropy log documents 
dimensionality word document matrix optimized singular value decomposition svd word represented vector approximately dimensions dimensions particular meaning jones mewhort direct correspondence text 
svd effect bringing latent semantic relationships words occurred document 
basic premise lsa aggregate contexts word appear provide set mutual constraints induce word meaning landauer foltz laham firth put shall know word keeps 
lsa successful simulating wide range psycholinguistic phenomena judgments semantic similarity landauer dumais word categorization laham discourse comprehension kintsch judgments essay quality landauer laham schreiner 
lsa earned college entrance level grades test english foreign language toefl shown acquire vocabulary rate comparable standard developmental trends landauer dumais 
spite successes occurrence models criticized number grounds 
importantly lsa criticized bag words model ignores statistical information inherent word transitions documents di eugenio wiemer hastings 
word meaning defined context temporal position context relative words 
noted lack syntax architectural failure lsa 
lsa learn document meanings benefit word order landauer order information certainly necessary comprehensive model word meaning usage 
constraints word order solving bag words problem major step development semantic space models 
providing additional information meaning transition information defines word lexical class grammatical behavior 
amend firth comment shall know word keeps keeps 
models sequential dependencies language miller notion gram see miller selfridge 
classic gram model records frequency occurrence possible word sequence chunk encounters usually window words target considered grams 
predict word new sentence position model looks frequency target word encountered bigram trigram training 
useful gram models usually need trained massive amounts text require extensive storage space relatively little information 
focus identify simulate generative rules word transitions inspired chomsky 
probabilistic methods hidden markov models infer generative rules produce text corpus observing transition statistics modern example see horn ruppin edelman 
similarly sto clear gram article refers specifically sequence chunks words encountered learning term refer sequences letters word 
context free grammars booth learn probabilities production rules training data jelinek lafferty 
models typically require supervised training tagged corpora part speech syntactic role word sentence hand coded human rater marcus santorini marcinkiewicz palmer gildea kingsbury 
open questions regarding exact nature representation word meaning consensus representation word grammatical usage form rules production systems 
follows knowledge word meaning knowledge word usage represented different forms stored separately 
early simple recurrent networks srns elman see servan schreiber cleeremans mcclelland promising learning sequential dependency information artificial languages 
srn connectionist network learns predict temporal sequences recurrent context layer represents network previous initial state hidden layer input state 
hidden layer state affected hidden layer state previous time step turn affected state srns retain sequential dependency information time steps elman 
srn representation word meaning pattern activation hidden layer 
information predicting transitions word 
srns afford possibility word meaning word usage type knowledge may stored 
srns predict word sequence elman identify embedded structure elman recursion christiansen chater track limited long range dependencies elman 
unfortunately srns questionable scaling properties 
function quite small finite state grammars date scaled unconstrained natural language see burgess lund howard kahana similar large scale models 
addition srns difficulty retaining information time steps questionable feedback necessary srn plate 
srns scaled real word language problems provide appealing architecture 
random distributed representation words simple composite memory system learn particular words appeared sentence document composite vector formed superposition adding averaging vectors words sentence document 
lexicon built composite vector word theoretically learn occurrence information learn sequential information 
composite vector longer determine features elements go objects original vectors referred binding problem 
furthermore way determine original ordering vectors composite representation 
section presents binding operation adapted signal processing associative memory theory preserves order potential solution binding problem 
comprehensive model lexicon requires information word meaning word order account sources information learned represented retrieved memory 
merging semantic space model holographic lexicon sequential dependency model trivial task 
precedents exist fusing types information domains memory theory 
assuming language similar types associative stimuli possible small scale srns suggested knowledge word transition represented stored manner knowledge word meaning 
human data accounted directly structure knowledge represented manner 
operationalizing language merely complex type sequential dependency stimulus allows ideas borrowed wellestablished theory associative memory 
lessons associative memory theory model paired associate learning murdock successfully convolution mechanism form associations pairs random vectors representing words objects 
convolution basically method compressing outer product matrix vectors avoids binding problem 
correlation called deconvolution approximate inverse decode convolution association back parent representations parent representation environment 
example associate item vectors convolve third vector storage association vector may contain superposition pairwise associations minimal interference 
retrieval memory vector probed item pair item successfully reconstructed noisy form correlation convolution associative commutative distributes addition 
convolution correlation memory models referred holographic models mathematical principles light see plate review 
convolution code associative information vectors losing track features belong parent vector associative information stored composite memory representation item vectors murdock convolution appears particularly appealing encoding sequential information language 
section describe model uses convolution encode word order language apply model large scale statistical structure text corpus 
convolution adapted encoding mechanism appropriate structure language 
memory models predominantly aperiodic linear convolution bind pairs vectors 
aperiodic convolution diagonals outer product matrix summed procedure illustrated 
convolution dimensional vectors produces vector containing association dimensions 
association directly compared added item vectors larger dimensionality 
finesse problem models pad item vectors zeros balance dimensionality murdock simply truncate association vector trimming outside elements match dimensionality item vectors metcalfe 
expanding dimensionality convolution serious problem sequential dependency model applied language binding successive pairs proceeds recursively triplets 
padding truncating solutions limit model learning pairwise associations target word 
associate words sentence recursively bind successive grams produces serious problem expanding dimensionality successive bindings 
generally convolving vectors having dimensions produces associative vector kn dimensions 
dimensionality convolved vectors rapidly expands recursive aperiodic convolution 
systems machine learning tensor product algebra bind multiple vectors smolensky addition expanding dimensionality higher order grams expanding rank problem 
association dog bit represented second order tensor product square matrix association dog bit represented third order tensor product cube matrix 
grams represented higher dimensional rank structures directly compared aperiodic convolution tensor algebra 
expanding dimensionality difficult compare different order grams words sentence fragments 
addition known priori large dimensional structure required encode sentence 
avoid expanding dimensionality problem recursive convolution employ circular convolution technique extensively image signal processing roberts see plate examples cognitive modeling 
circular convolution dimensional vectors produces associative vector dimensionality modulo subscripts 
operation circular convolution illustrated 
recursive circular convolution vectors dimensionality original item vectors waste information truncating elements 
jones mewhort 
collapsing outer product matrix aperiodic convolution argument vectors represents resulting compressed vector collapsing outer product matrix 
values represent row column indices respectively element outer product matrix 
aperiodic convolution circular convolution commutative associative distributes addition 
produces unique associations circular convolution vector unique vectors produces unique association vectors 
unique associations contain kick ball kick table unique assuming ball table unique 
follows circular convolution uniqueness grams property 
bigram unique trigram associations directly compared dimensionality higher order grams different sources information lower order ones 
commutative property circular convolution create problem coding transitions language change order successively bound items distinguished 
example dog bite bite dog different ideas produce associative vector convolution 
take advantage asymmetric temporal structure language binding operation noncommutative required unique 
computational efficiency noncommutative permutation convolution proposed plate 
plate suggested circular convolution noncommutative permuting elements argument vectors differently prior convolving 
details directional operation outlined appendix result circular convolution commutative associative distributes practice fast fourier transform fft convolution computationally efficient 
convolution fft takes log time compute technique modulo subscripts takes time compute may result run time difference days trained large corpus 
fft lends parallelization easily 
denotes operation circular convolution 
addition preserves similarity 
unique associations 
statistical structure language sequential data stream sample text sources statistical information learn words 
occurrence information word information word context words tend appear context unit phrase sentence document 
transition information hand information position word relative words context unit 
occurrence transition information come direct indirect statistical sources 
direct statistical information comes words directly appeared context unit particular word 
example sentence robin flew tree ate worm direct occurrence relationship formed robin flew worm appeared 
indirect latent occurrence relationship formed robin feathers bird hawk may directly occurred 
robin feathers appeared flew robin feathers appeared 
furthermore robin bird hawk frequently appear words fly feathers indirect information forms relationship words directly occurred 
sample sentence direct transition relationships formed robin surrounding words precedes robin flew succeeds robin appears trigram flew 
indirect transition information learned robin similar hawk bee words may flew 
early holographic lexicon 
collapsing outer product matrix circular convolution argument vectors represents resulting compressed vector collapsing outer product matrix 
values represent row column indices respectively element outer product matrix 
learning direct sources information occurrence transition important learning word meanings words learned progressively information learned indirect sources occurrence transition 
beagle model section describe bound encoding aggregate language environment beagle computational model builds semantic space representation meaning word order directly statistical redundancies language 
document model develops distributed representation meaning words contextual similarity model develop representation word order forming associations words vector convolution binding 
demonstrate types information word meaning word order learned single pattern vector elements composite lexical representation sequential dependencies retrieved lexicon 
assume word meaning usage single pattern vector elements 
lexical representation initially random pattern gradually representation noise reduced experience word sampled different contexts positions contexts 
sufficient samples stable central limit vector element emerges stabilizing lexical pattern 
depending variability word meaning usage language fewer observations word required converge stable lexical representation 
statistical noise reduction simple powerful mechanism learning word meaning order information 
borrowing terms associative memory murdock information word occurrence referred context information information word usage position relative words sentence order information 
similar murdock models context information comes vector addition order information comes vector convolution 
beagle processes text sentence time learning context order information word proceeding sentence 
front routines parse text discrete units meaning unit usually sentence types punctuation semicolons parentheses interpreted marking complete unit meaning 
sentences representation word similar representation words frequently occur sentences words frequently occur similar contexts 
example representation robin similar representation fly frequently cooccur sentence direct relationship 
robin similar hawk may occurred sentence 
robin hawk tend occur contexts similar words fly feathers indirect latent relationship 
addition robin hawk share order similarity similar positions relative words respective sentences 
robin hawk shared order information fly sing robin sing shared order information 
representation words represented high dimensional holographic vectors 
time word encountered assigned random environmental vector environment vector elements sampled random gaussian distribution vector dimensionality 
subsequent time word encountered environmental vector coding context order information word environmental representation change 
word memory lexical vector changes time word encountered new context unit 
time word encountered environmental vector coding new context order information new information added environmental vectors intended represent physical characteristics words environment orthography phonology memory vectors represent internal memory contexts positions environmental vectors encountered 
point agnostic actual environmental features words assume structural similarities words represent different random representation 
environment vectors generated random time word encountered word physically unique words uniqueness specifically mean expected cosine vectors zero 
add structural similarities words matrix generated reflect known structural similarities words see heiser van der van den focus 
jones mewhort learning context information context information word sentence sum environmental vectors words sentence ci ej 
context vectors computed word sentence 
word lexical representation updated adding new context information 
sentence word encountered new context information constructed environmental vectors words sentence lexical representation updated adding new context vector 
word lexical representation superposition vectors reflects word history occurrence words sentences 
high frequency function words pollute lexical representation formed way frequently occur sentences words 
example lexical vector dog similar lexical vector simply occur sentence 
large dimensionality may near neighbor word 
continuous entropy functions allow model degrade contribution high frequency function words gracefully learns tend appear different sentences 
functions require priori knowledge word frequency 
large text samples entropy function learn words sentence contribute useful semantic information approximate outcome observed word frequencies known priori 
computational efficiency simulations reported employ standard list function words calculation context information 
word appears list omitted calculation context information 
lists sort commonly corpus models semantics smith humphreys steyvers griffiths press 
example context information consider coding word dog sentence dog bit context information sum vectors words sentence list words dog bit 
context dog sentence superposition words bit 
new sentence information added dog may contain past superimposed vectors tail wag fetch 
direct relationship formed dog bit random information summed lexical representations 
example context information bit sentence table examples nearest neighbors various target words context space computer hospital heart cognitive lexical beer data doctor artery development syntactic wine computers medical vessels child semantic processing nurse pumps intellectual prefixes processed patients arteries learning derivational drinks storage patient blood stages context alcohol program emergency research meaning drink programmer care circulation personality meanings alcoholic microcomputer clinic pumping piaget suffixes keyboard doctors piaget inflectional memory nursing psychological synonym rum input physician aorta psychologists word beverage user ward behavior words bottle cpu call skills convey electronic hospitals vessel theory dictionary ethanol information arterial understanding grammatical software surgery ventricles concept connotation cola disk office beat communication denotation computing birth liver education digital ambulance heartbeat motivation analyze sick hypertension knowledge phrase pint note 
numbers neighbor words vector cosines 
bit dog dog bit 
obvious latent relationship lexicon formed dog wolf dog biting region corpus wolf biting chasing attacking deer hunter 
dog wolf separate sentences doing similar things similar lexical representations 
frequently words share context directly indirectly common random vectors summed lexical representations similar lexical representations 
describe coding order information beagle provide examples semantic structure emerges simple vector superposition algorithm applied real world language 
simulations reported vector dimensionality set words learned lexicon 
model trained tasa corpus compiled touchstone applied science associates see landauer contains collection english text approximately equivalent average college level student read lifetime 
tasa corpus web version lsa trained 
table illustrates nearest neighbors similarities various target words target words capitalized lexical space trained tasa corpus context information 
words learned total table displays sample structure lexicon 
table shows vectors representing semantically similar words developed similar patterns elements function shared context 
occurrence information comes direct relationships indirect relationships 
holographic lexicon lexicon thought multidimensional space similar vectors proximal space dissimilar vectors distant 
shows dimensional scaling plot various financial science sports terms learned 
initially chaotic structure space words positioned randomly origin cross hair 
experience structure forms accumulation common random vectors lexical representation 
example financial terms similar distinct science terms sports terms 
soccer baseball football may directly occurred random information summed lexical representations player sport ball 
sentences documents plotted terms proximity concepts 
random vector accumulation simple powerful method acquiring semantic representations words contextual occurrence 
representations gradually form structure initial randomness noise reduction experience 
mentioned previously occurrence information source statistical information word meaning induced 
transition information word defines lexical class grammatical behavior contributing meaning 
transition information absent semantic space models addition information may benefit semantic space accounts variety tasks 
random vector accumulation allows model learn occurrence fixed dimensional representation representation dimensionality semantic space models depends largely input matrix grateful tom landauer providing tasa corpus 
similarity article refers specifically cosine angle vectors normalized dot product 
zation criterion 
circular convolution operation described previously represent word order history fixed dimensionality merge types information unified representation 
learning order information word order information calculated binding gram chunks sentence include 
binding beagle accomplished directional circular convolution 
position word coded represented constant random placeholder vector sampled element distribution environment vectors constructed 
phi fixed learning 
method uniquely codes associations bigram trigram neighbors word 
gram convolutions unique contain words 
order information word sentence sum gram convolutions belongs 
circular convolution grams represented dimensionality summed single order vector jones mewhort 
example word clustering subset context space financial science sports concepts 
represents word associative position relative words sentence oi position word sentence bind jth convolution binding word coded 
word order vector added lexical vector context vector pattern elements representing word position relative words sentences learned 
lambda chunking parameter sets maximum neighbors word bound number possible grams increases nonlinear function position sentence length 
lambda theoretical limit practical 
number possible convolutions large long sentences 
computing possible convolutions drastically increase compute times order grams contribute generalizable order table examples nearest neighbors various target words order space mike went minutes dan ran seconds tom came beneath moments pete rushed hours ben weeks returned days jeff goes near months ted walked inches jim twelve moved milliseconds tim eighteen continued charlie began yards pam proceeded decades dad tommy refused joe fifteen fell cents dave rode bobby eleven drove inside years jack seventeen appealed centimeters danny sought woody liked bites andy fourteen listened note 
numbers neighbor words vector cosines 
structure 
simulations reported lambda set miller selfridge classic experiments order approximation english 
example consider coding order information word dog sentence dog bit function words important syntactic structure list entropy weighting calculation order information 
order information dog gram bindings 
bind dog bind dog bit bigrams bind dog bit bind dog bit trigrams bind dog bit bind dog bit bind dog bit gram convolutions produced unique pattern stored vector superposition 
sentence trigram convolution dog bit unique bigram convolution dog higher order gram associations different lower order constituents 
example bind dog codes bigram association immediately preceding word coded dog 
furthermore bind dog records bit immediately succeeding word coded bind dog codes trigram association word coded appeared left bit right 
trigram dog bit contains bigrams dog holographic lexicon dog bit trigram association vector unique compared bigram constituents 
convolutions unique circular convolution coding operation compress dimensionality summed single order vector represents dog position sentence relative words 
vector added dog lexical representation contains position dog relative words sentences learned 
lexical representation acquires pattern elements reflects word history position association words sentences 
text experienced common associations stronger superposition common associations 
produces natural weighting lexicon lower order grams important word order history higher order grams simply lower order grams consistently encountered stable chunks experience 
demonstrate structure learned convolution order algorithm table shows nearest neighbors various target words target words capitalized lexicon trained tasa order information dimensions words 
clear table different pattern structure emerged order encoding algorithm lexical classes 
action verb went similar action verbs gradually types verbs 
locative similar temporal prepositions 
near neighbor noun verb space possible context space 
information long distance dependencies sentences bootstrapped context information 
shows dimensional scaling plot parts speech nouns verbs determiners clockwise top group 
shaded regions show expansions dense areas 
similar associations learning words commonly similar positions relative words developed similar patterns vector elements common random vectors convolved lexical representations 
transition information comes direct relationships comes indirect relationships 
note relationships learned order information different learned context information comprehensive model able consider types information 
contextual order information demonstrative purposes described structure learned context order information separately 
practice beagle codes context order information jones mewhort 
example word clustering subset order space nouns prepositions verbs determiners clockwise top 
gray regions show expansions dense neighborhoods 
lexical vector word time encountered new context unit 
mi mi ej 
composite lexical vector contains superposition word context order information sentences experienced 
composite representation powers context order information contains semantic associative information word vectors representing context order information normalized length prior averaging type information may depending sentence length 
position information retrieved order information demonstrated section context surrounding words 
shows scaling plots demonstrate organization words learned context see order see mechanisms words 
context holographic lexicon information see verbs proximal nouns operate 
example food related eat car related drive book related read eat drive read highly related food car book 
context information accounts primarily semantic associations 

subset nouns verbs adjectives context space context space verbs proximal nouns operate similar contextual experience 
words order space order space parts speech form proximal clusters similar experience 
contrast shows structure words learned order algorithm convolution 
order space words appear similar positions relative words sentences developed similar patterns accumulation common association vectors 
drive eat read proximal cluster distinctly nouns car food book similar 
context space adverbs close verbs quickly slowly proximal drive 
order space adverbs formed cluster 
context algorithm learned semantic relationships order algorithm learned information lexical classes 
semantic class information represented order space nouns distinct verbs nouns animates tend distinct 
animates example fish cluster birds cluster 
context space fish cluster swim birds cluster wings fly 
contextual order information complement 
table shows top neighbors cosines eat car reading slowly full versions space words lexicon 
verbs eat reading neighbors context space predominantly semantic associates neighbors order space similar verbs 
interesting note nearest neighbors composite space may necessarily close target context order space 
organization composite space emphasizes higher order semantic relationships example idea eat related grow reading related understanding grow understanding list top neighbors context order space 
retrieving order information lexicon beagle order information incorporated context information produce higher fidelity representation word meaning 
product convolution order binding mechanism represent associative position word sentences order information retrieved lexical representations allowing model predict word transitions sentences learned history explicitly coded rules grammaticality transition 
coded word lexical representation preferred position relative words occurred sentences 
information may retrieved lexicon ways order information may decoded lexical representations determine words precede succeed order context order pattern may compared lexicon determine lexical entries resonate words learned order context high frequency 
decoding retrieves instance learned associative infor table twelve nearest neighbors eat car reading slowly context order composite spaces eat car context order composite context order composite eaten buy feed driver boat truck eating get grow drive ship road food sell produce driving truck driver hunt move die road house bus digest save digest drove bus train ate sleep kill wheels computer garage grow keep chew truck fire highway need swallow survive seat train house foods avoid hunt drivers bank street plants win provide parked camera fire insects catch cook cars ball horse produce preserve street dog reading slowly context order composite context order composite read writing writing quickly quickly quickly book making learning turned carefully rapidly books studying studying walked rapidly suddenly reading teaching understanding moved freely moved facts planning books moving easily turned readers describing teachers left quietly running authors mathematics saw swiftly walked write cutting speech move clearly reached words seeing comprehension suddenly suddenly stopped comprehension skills reached costly carefully learn selecting information close softly rose library watching language street efficiently left note 
numbers neighbor words vector cosines 
jones mewhort mation holographic vector inverse convolution compares retrieved instance environment determine goodness fit 
resonance hand compares associative instance environment memory determine memory vectors highly excited pattern 
specific details encoding decoding beagle detailed appendix retrieving order information decoding 
learned items may decoded order information lexical representation inverse circular convolution circular correlation modn 
example predict word sequence lexicon information decoded lexical representations words sequence far 
retrieved vector noisy version environmental vector transition sequence preceding transitions 
partially convolution throws away information partially composite compression introduces noise partially correlation approximate inverse convolution decoded vector noisy version original form 
retrieved vector higher cosine original environmental form environment vectors see murdock characteristics convolution correlation retrieval 
example consider martin luther king jr frequently occurring fairly unique chunk tasa corpus 
lexical vectors martin luther king jr relatively little associative information coded frequently appear stable chunk exception king appears name chunks 
decoding luther left retrieves facsimile martin decoding luther right retrieves facsimile king 
illustrates similarity vectors decoded luther right left environmental vectors learning 
shows cosine environmental vectors vector retrieved decoding luther right words succeeded luther 
retrieved vector clearly resembles environmental vector king learned succeed luther randomly similar environmental vectors 
contrast shows cosine environmental vectors vector retrieved decoding luther left words preceded luther 
retrieved vector clearly resembles environmental vector martin learned precede luther shows random similarity 
lexical representation luther different directional associations decoded 
grams included probe decoded response constrained 
example fidelity vector retrieved similarity king increases consistent words added sources decode 
luther luther decode 
martin luther blank position decoded luther martin luther intervening position martin position predicted 
holographic lexicon 
cosine decoded vectors environmental vectors learning 
similarity vector decoded luther succeeding position 
clearly decoded vector similar environmental vector representing king 
similarity vector decoded luther preceding position 
decoded vector similar environmental vector representing martin 
forward backward associations decoded lexical representation 
martin luther jr blank position estimated decoding jr backwards 
adding context word modify retrieval changes order constraint emphasize word consistent information retrieved memory vectors 
words agree word retrieved luther martin luther agree king retrieved fidelity retrieved vector king simply increases 
example luther martin luther martin luther jr similarity retrieved vector king increases respectively additional words added position decoded 
specifics decoding described detail appendix retrieving order information resonance 
demonstrated previous section order information may decoded specific lexical trace 
order information distributed lexicon 
example learning sequence dog bit transition information dog bit stored lexical entry dog dog dog bit 
information transition stored correlation stable system noise retrieval exact inverse plate 
tasa contains information king luther 
lexical vector bit bit bit dog 
furthermore bigram transition simultaneously stored part larger chunk dog bit 
transition information distributed lexical entries various lexical vectors may assist retrieving order information particular sequential pattern 
determine lexical entries highly activated position sentence fragment gram convolutions position built summed probe vector learning position replaced placeholder vector 
lexical vectors frequently vacant position learning resonate order vector position gram tokens vacant position fit order histories 
resonance tendency object absorb energy probed frequency matches object natural frequency frequencies 
example tone sounded piano strings resonate depending match natural frequencies pitch tone 
resonance metaphor describe lexicon response probe similar lexical vectors resonate order probe depending amount shared variance 
holographic lexical vectors may resonant frequencies jones mewhort 
state lexicon probed vector succeeding abraham succeeding new preceding lincoln preceding york 
coded responding possible input patterns depending word learned history 
example vector thomas words preceded thomas compared lexicon dylan similar lexical vector cos responding random similarity 
vector thomas words succeeded thomas compared lexicon lexical vectors respond jefferson edison particular pattern coded representations dylan pattern thomas coded representation 
illustrates lexical resonance asymmetric nature retrieval 
shows cosine lexical vector response probe vector abraham words preceded abraham 
lincoln rises lexicon resonance probe vector resonate 
lexical representations remain names chorus instances resonance popular metaphor describing memory retrieval brooks hintzman mewhort johns ratcliff 
randomly excited probe vector 
contrast shows state lexicon response probe vector lincoln words succeeded lincoln 
lincoln responded predominant trace follow abraham see abraham trace precedes lincoln president abe excited pattern lincoln 
furthermore president responds preceding number words kennedy carter 
asymmetric behavior illustrated figures 
new pattern preceding york 
contrast shows state lexicon response pattern succeeding new 
new word preceded york words respond want succeed new york 
table demonstrates asymmetric nature resonance lexicon nearest neighbors cosines sample target words 
resonance may probe lexicon longer general order patterns 
example phrase encoding vector compared lexical vectors cosine computed 
words frequently experienced blank position learning order context surrounding words higher relative cosine order vector 
highly activated words probe refused went came said likes wanted 
table shows highly activated lexical representations various phrases 
column shows words highly excited 
interesting note verbs highly activated bigram information quite different constrained contains bigram constitutes 
verbs fit bigram high frequency bigram information simple verbs successors extremely variable contexts match 
simple verbs fit bigram information fit trigram rarely 
activation rank ordering went increases holographic lexicon table top lexical representations binding preceding succeeding target word king president war sea king king president president war war sea sea luther midas vice ii kennedy civil veterans gull burger henry reagan material nixon revolutionary ended mediterranean otter minos incumbent carter korean baltic johnson trojan hawks tut democratic lincoln world broke arthur lyndon vietnam lamprey rex roosevelt declare aims note 
numbers lexical representations vector cosines 
represents constant placeholder vector encoding 
went progressively fits gram convolutions added 
generally grams probe fit word history order information constrained model response words appear different contexts learning possible retrieve exact word learned particular probe 
table shows exact phrases learned tasa corpus 
italicized word brainstem removed probe position predicted lexicon order information top words activated various probes displayed 
example brainstem larger complex spinal cord exact sentence learned tasa 
brainstem obviously response follow bigram association considered possible response follow 
surrounding words added blank position candidate words higher relative activations constrained 
entire sentence brainstem best fit grams probe considerable difference highly activated word brainstem words 
additional grams add signal appropriately fitting words noise inappropriate ones 
important note retrieval demonstration considers order information advantage context information meaning sentences independent order constrain responses 
frequent words contexts exact word retrieved exact sentence learned 
highly activated words plausible 
full phrases table candidate words target plausibly fit probe correct target best fit 
table shows exact phrases tasa correct target note activation word necessarily increase linearly grams may fit went 
word activation may decrease additional appropriate grams activation rank order may increase 
additional grams may add noise add noise appropriately fitting words noise words fit positional information 
table highly activated words various blank positions said went came world asked worry problem said sun went listened earth kept belonged truth note 
numbers activated words vector cosines 
represents constant placeholder vector encoding 
retrieved highly activated words lexicon best attempt fit position exact instance memory lost averaging 
word experienced small number times possible retrieve exact order instances learned 
word experienced frequently different contexts ability retrieve exact instances diminishes abstracted representation generalizable averaging 
lexical representations undergo abstraction transformation memory experience 
increase reliance lower order grams frequent words 
example final probes table demonstrate beagle error sizing lower order chunks order retrieval gram models 
responses associate interact ball sentence driven lower order grams information object inanimate noun 
augmenting order retrieval position independent semantic information help ameliorate bias 
retrieving decoding resonance 
order probe information unknown positions may estimated surrounding context known words learned experience word transitions 
unknown information may decoded learned history words probe 
simultaneously transitions probe may compared entire lexicon determine words table highly activated words position context neighboring words expanded phrase activations brainstem best brainstem brainstem larger complex spinal cord world best brainstem world arguably profit single double moderate thorough prison fly skills consecutive keen electric eel shocks motors current charges sparks generators electric eel current shock generator electric eel produce discharge volts charges eel eel current pcp emperor penguins yuan penguins scientists emperor penguins come breeding grounds jones mewhort biologists researchers penguins researchers note 
italicized word correct response chunk 
numbers parentheses activated words vector cosines 
table predicting words contexts order context learning resonance 
depending range transitions word learning decoding resonance may return salient signal 
retrieval processes complement frequently agree candidate words may disagree 
example consider probe thomas 
decoding correlation retrieves vector similar cos mildly similar cos randomly similar environment vectors 
transition thomas jefferson packed frequently difficult retrieve 
thomas preceded words pattern stored vectors lexicon 
pattern compared lexicon lexical vectors resonate highest ones jefferson hickey edison wolfe hobbes 
simply averaging cosine decoded vector environmental vector cosine encoded environmental pattern lexical vector averaging decoding resonance provides measure word activation order probe 
vector decoded vector representing probe string 
averaging cosines decoded vector environmental vectors probe vector lexical vectors produces measure word activation ai order probe ai cos ei cos mi resonance decoding agree word word activation emphasized positively negatively retrieval sources compete 
consider probe 
bigram thomas jefferson coded high frequency resonate highly context 
narrow range transition information coded representation decoding retrieves vector highly similar cos randomly similar environmental vectors 
may uniquely decoded 
holographic lexicon phrase activations penguin fly run go get eat move run ought want intend wish hate allowed interfere cope comply grapple ball associate interact interfere kim sat bed looking sad vain unison disgust note 
italicized word correct response chunk 
numbers parentheses activated words vector cosines 
resonance reliably resemble vectors chance decoding summing sources produces response thomas predominantly decoding 
contrast decoding thomas produces jefferson resonance retrieves traces summing sources results list candidates terms activation jefferson edison 
sources needed retrieve order information particularly variable contexts predicting parts speech names 
complementing order retrieval context information 
context information may disambiguate transition information probe shared meaning probe lexical vectors 
example consider probes highway audience 
assume sequence learned stable trigram equally activating went drove spoke full sequences novel audience highway introduce noise order retrieval 
including context information probe emphasizes went drove highway spoke audience shared meaning 
highway interferes spoke audience interferes went drove 
context information emphasizes semantic relationships irrespective matching order instances 
example thomas words highly activated order information jefferson strongest 
predominance bigram thomas jefferson may overridden additional context 
table presents activation associated legal completions thomas activations change sentence context modified 
jefferson word follow thomas general edison word context thomas 
establishing model performance beagle learning context order information produces representation improves limitations semantic space models 
furthermore beagle representations gradually acquired statistical sampling words experi alternatively context information calculated learned memory representations 
ence 
addition great deal word order information retrieved representations 
common storage mechanism may prove unnecessary build transition rules higher order models language comprehension beagle holographic representations input 
proceed important demonstrate context information learned beagle allows model perform similarly model lsa established tasks 
furthermore important determine addition order information representation interferes model ability context information 
common performance evaluator semantic models synonym section toefl landauer dumais turney 
item consists target word alternative words task select alternative similar meaning target 
accuracy set retired items originally landauer dumais criterion dimensionality selection lsa toefl benchmark measuring performance semantic models 
note toefl synonym test models semantics need necessarily perform human levels mechanisms identify synonymy 
comparing performance target alternative cosines models start establish correspondence 
toefl items originally landauer dumais 
lsa beagle trained tasa corpus 
comparisons lsa term space dimensions dimensionality roughly optimal performance toefl items 
beagle standard dimensional vectors 
item alternative highest cosine target selected response 
target correct alternative unknown score assigned item essentially guess 
lsa correctly answered items 
trained context information beagle close correspondence correctly answering items 
trained context order information beagle slightly better correctly answering items 
computed pearson correlation coefficients target alternative cosines lsa context beagle composite beagle 
considering differences learning mechanisms context paragraph lsa sentence beagle lsa context beagle quite similar jones mewhort table example changing word activation context order information probe jefferson edison thomas thomas wrote declaration independence thomas thomas taught civil authority comes god thomas author common sense treaty drawn american thomas thomas wrote human population increased faster food supply note 
numbers activation values target word blank position probe sequence 
boldface indicates correct target fit probe sequence 
pairwise word similarities 
pairwise word similarities lsa composite beagle positively correlated note df pairwise missing items 
tested difference correlations williams ratio correlations see steiger 
correlation lsa context beagle significantly larger correlation lsa composite beagle 
context information beagle reasonably resembles information lsa vectors terms toefl cosines 
addition order information beagle representation similar lsa representation context information 
composite representation performed slightly better toefl similarity lsa decreased relative context representation 
addition providing model sequential dependency adding order information beagle may improve fidelity semantic representation 
limited number comparisons toefl test conducted larger comparison models database constructed mckinley thompson 
constructed database nearly pairs words taken nelson schreiber norms 
word pair computed semantic distance wordnet fellbaum miller compared classic studies human similarity judgments 
demonstrated wordnet semantic distance better predictor human judgments semantic similarity independent semantic measures 
semantic distance wordnet standard compare models 

database contains distance measures wordnet lsa cosines dimensions word pairs 
word pairs computed cosines lsa dimensions beagle context information order information context order information 
table presents correlations mea tom landauer toefl items 
dimensional ratings included 
database independently computed jose quesada university colorado boulder 
sures 
wordnet values distance measures similarity semantic space models negatively correlated wordnet distance 
dimensional lsa solution highly correlated wordnet dimensional solution reported 
factors dimensionality may differed know specifics dimensional version 
beagle context information highly correlated wordnet lsa dimensions beagle order information williams ratio 
composite beagle representation highly correlated wordnet beagle order beagle 
composite representation provides better correspondence semantic distance measures lsa context order representations 
learning context order information composite representation produce data loss due compression 
computed parameter regression model predicting wordnet distance context order cosines model cosines composite representation 
multiple correlation parameter regression accounted half percent variance wordnet semantic distances regression 
suggests data loss due composite storage context order information lexicon minimal 
summary adapted associative memory theory learn semantic information order context dimensional optimization result semantic space representation incorporates order information function model sequential dependency 
addition order information context representation weaken model semantic representation enriches representation wordnet semantic distance measures see jones kintsch mewhort 
section compare correspondence structure learned beagle representations data humans variety semantic tasks 
linking representation human data holographic representations learned beagle contain word history contextual order information 
holographic lexicon table correlations semantic space models semantic distance wordnet variable 
wordnet 
lsa dimensions 
lsa dimensions 
context 
order 
composite note 
context beagle trained context information 
order beagle trained order information 
composite beagle trained context order information 
values 
lsa latent semantic analysis beagle bound encoding aggregate language environment 
section outlines broad range semantic order phenomena accounted directly structure learned representations 
subsection describes similarity structure clusters words formed lexicon focuses semantic categorization typicality acquisition time course clusters 
second subsection focuses predicting human response latency various semantic priming tasks third subsection applies model semantic constraint stem completions 
tasks process model needed produce response latencies errors 
development appropriate process model certainly trivial task purpose section demonstrate beagle vectors contain similarity structure needed process model 
structure semantic categories accumulation common random vectors experience representations words common natural categories develop similar patterns elements 
lexical representations come cluster manner similar organization semantic categories human memory 
variety semantic category effects acquisition trends accounted directly structure learned representations 
typicality 
typical members semantic category processed efficiently atypical ones collins quillian rosch smith 
montague collected subjective responses create empirical category clustering norms asking subjects generate exemplars certain category labels 
example robin frequently produced exemplar bird category produced subjects chicken frequently produced produced subjects 
presumably frequency exemplar production reveals semantic structure category birds 
words listed exemplars category verified faster members category sentence verification tasks rips shoben smith 
example robin bird verified ms faster chicken bird typicality effects easy explain feature list models difficult accommodate semantic network 
featural representation frequent category members fewer overlapping features superordinate list frequent members 
explain typicality semantic network representation pathways frequent members longer frequent members 
case typicality manually built structure semantic representation beagle representation learned automatically experience text 
rosch suggested seeing new bird may classify comparing prototypical bird robin 
typical members category similar cluster densely closer center category 
hierarchical semantic structure natural characteristic vector representations learned beagle 
animals cluster distinctly places vehicles 
animals fish birds snakes tend cluster distinctly 
category labels tend proximal exemplars categories represent 
furthermore names features category wings beak tend proximal exemplars categories features 
figures show dimensional scaling plots sports vehicles respectively 
smaller gray plots show expansions dense regions center category 
figures show typical exemplars positioned nearer algebraic center category gray regions 
rosch subjective rating norms football ranked number subjects example sport basketball ranked number 
hand ranked example sport 
labels category sports vehicle closer typical exemplars 
contrast typical exemplars category sparsely distributed typical exemplars category label 
difficult see representation naturally produce typicality effect typical members closer category center closer category label typical ones 
concrete example typicality consider classic experiment rosch experiment 
rosch primed subjects category label simultaneously exemplars response simply different category judgment 
typicality exemplars defined subjective ratings varied 
basic findings responses faster primed category name different responses faster typical category members atypical ones robin sparrow faster chicken peacock robin car faster chicken tractor 
research cross validated similarity neural states implied rosch typicality response time rt data 
illustrates rosch rt data categories responses plotted beagle predictions euclidean distance category prototype category label stimulus pairs 
distance prototype simulation open circles center category computed mean example vectors category mean distance exemplars prototype typicality bin computed 
category label simulations open triangles mean distance examples category label computed 
distance prototype category label decreased linear function typicality distance prototype distance label 
jones mewhort shows representations learned beagle emulate basic rt findings rosch reasons 
rosch argued semantic categories represented algebraic prototypes bird standard exemplar robin typicality need built artificially model representation processing mechanisms 
alternatively semantic categories defined labels representations learned beagle produce typicality effect 
structure particularly interesting beagle benefit perceptual knowledge birds fruits human subjects presumably 
simple learning routines applied large scale redundancies language sufficient produce semantic typicality effects 
labeling categories exemplars 
typicality data imply exemplars natural category may classified simply basis proximity appropriate label beagle space 
example color high cosine blue red yellow 
test notion exemplar words selected natural categories exemplars appendix 
exemplars classified assigning closest category labels beagle space label vector smallest euclidean distance exemplar vector 
results table 
chance dictate exemplars category correctly labeled 
table shows simple label proximity algorithm classified exemplars better chance predict 
vegetables category classified significantly better chance 
tomato omitted list fruits incorrectly labeled vegetable contextual experience common human mistake tomato technically fruit 
furthermore vegetables produce flowers similarity flower label 
textual experience flowers fruits vegetables quite similarity 
particular interest pattern error responses exemplar misclassified 
final column table shows incorrectly labeled exemplars labels order closer correct label exemplar 
error responses second closest label correct 
error responses certainly random orange fruit misclassified color example incorrect dog exemplars labeled bird animal dog second choice 
difficult define exactly discriminate label word exemplar feature word lexicon 
model know vectors exemplars labels structure learned representations sufficient classify exemplars categories nearest category label 
behavior features beagle space 
feature list representations initially appealing typical exemplars shared semantic features category prototype atypical ones certain common features words replaced equal typicality exemplars subjective norms lexical entries beagle replaced replaced 
addition avoid polysemy problems saw tool replaced screwdriver simulations 
tic category membership wings beak relative unique features 
furthermore words greater number descriptive features tend faster lexical decision naming times lupker 
holographic lexicon 
examples typicality proximity learned representations composite beagle space 
structure sports 
structure vehicles 
beagle bound encoding aggregate language environment 
beagle typical exemplars category typical statistical distributions similar resulting representation puts closer category center category label 
behavior exemplar words feature words language naturally produces representation typical 
rosch experiment typicality data predictions euclidean distance beagle space 
dark squares represent rosch response time rt data responses correspond scale left 
open circles represent distance prototype beagle representations correspond outside right scale 
crossed triangles represent distance label beagle representations correspond inside right scale 
beagle bound encoding aggregate language environment 
med medium 
exemplars similar category features typical ones 
example near neighbors vehicle words representing features descriptive category wheels driver brakes tires seat engine 
features proximal typical vehicle exemplar car atypical exemplar sled product statistical distribution words language 
illustrate similarity semantic features categories descriptive feature words selected categories category labeling simulation words listed table 
category feature words com jones mewhort pared label exemplars category compared category labels exemplars 
category label comparison euclidean distance computed feature correct label wings beak fly feathers bird compared euclidean distance features category labels wings beak fly feathers dog fish sport vehicle disease 
semantic features significantly closer category label labels categories 
table accuracy labeling exemplars proximal label word categories exemplars category category accuracy errors colors purple flower green flower bird fish sports football bird fish birds dogs beagle bulldog bird cities berlin country countries flowers daisy daffodil tulip bird fruits orange color vegetable flower pear bird vegetables fruit flower peas onions fruit flower diseases note 
values sample tests chance df 
errors column lists exemplars incorrectly labeled labels order closer exemplar correct label exemplars lowercase labels uppercase 

table semantic features category distance prototype distance label simulations category features bird wings beak fly feathers dog bark fetch pet fur fish swim scales fins sport players crowd score skill vehicle wheel driver motor brakes disease symptoms treatment pain sickness exemplar comparison euclidean distance computed feature exemplars category listed appendix wings beak fly feathers robin sparrow compared euclidean distance features exemplars categories 
semantic feature words significantly closer exemplars descriptive exemplars categories 
semantic feature words may descriptive categories similar members category contextual experience 
need inherent part semantic representation 
beagle demonstrates distributed representation learned simple mechanisms applied statistical redundancies language represent exemplars labels features representation 
acquisition semantic lexical categories 
previous section learned lexical representations displayed cohesive structure exemplars semantic category 
structure learned sentences corpus 
lexical representations random vectors gradually form structure statistical sampling words corpus development semantic structure lexicon time studied 
example shows dimensional scaling plots exemplars countries fish colors 
shows similarity representations sentences learned structure representations early learning close random 
contrast shows similarity representations entire corpus learned clearly cohesion semantic groups increased noise averaged categories cohesive 
dimensionality beagle fixed pattern elements changes learning development category cohesion studied continuously function experience time course movement 
shows mean cosine learning exemplars colors numbers fish countries listed appendix avoid potential artifact due sentence document order tasa corpus sentences sampled randomly replacement 
displays group cohesion function progressive sampling 
general semantic category cohesion tends develop exponential function learning categories gain holographic lexicon cohesion faster ultimately greater extent 
rate category cohesion development depends frequency contextual variability exemplar words 
illustrates words articles dedicated countries words defined simply usage corpus numbers colors form cohesive categories 
examining continuous learning particularly difficult classic types knowledge representation semantic networks feature lists contemporary semantic space models 
beagle dimensionality fixed learning pattern vector elements changes learning continuous development natural characteristic random accumulation model 
semantic networks examined growth function learning steyvers tenenbaum comparison modern distributed localist models semantic development scope article 
development structure lexicon limited semantic categories 
example studies examined differences concrete words various tasks see review 
generally concrete words dog easier process recall words justice 
addition concrete words normally earlier age acquisition words stahl 
dual coding theory proposes words representing concrete objects encoded verbal visual codes word dog image dog words visual code see pylyshyn criticisms theory 
beagle concrete words similar members class indirect information 
shows time course group cohesion development lexicon concrete words words group sentences sampled random order 
concrete words simulation taken yuille madigan balanced frequency length age acquisition concrete words higher mean rating concreteness rating words see appendix table 
beagle concrete words higher mean interword cosine words 
importantly shows acquisition trends groups different groups progressively cohesive concrete words steeper slope acquisition words 
possible differences concrete words natural statistical variability language 
group concrete words similar denser semantic neighborhoods words 
furthermore cohesion benefit concrete words learned faster words characteristic statistical sampling 
acquisition order information development lexical categories examined beagle lexicon 
general example consider broad finding nouns learned faster verbs fleischman roy gentner gleitman 
month old child jones mewhort 
example semantic category development exemplars countries colors fish 
structure exemplars sentences sampled 
structure exemplars entire text corpus learned 

development semantic category cohesion lexicon function progressive sentence sampling 
lexicon composed predominantly concrete nouns verbs tend stabilize child environment usually split evenly nouns verbs nelson 
equal exposure nouns verbs nouns acquired 
presently dominant hypothesis gleitman gleitman lederer gleitman children concrete nouns represent concrete objects may physically manipulated child environment 
recall versus concrete simulation beagle predicts concreteness structure language environment 
beagle learning 
development cohesion concrete words yuille madigan function progressive sentence sampling 
holographic lexicon benefit nouns verbs reflects structure learned statistical distribution words language 
shows cohesion nouns verbs equated frequency tasa function sentences sampled random order 
words appendix consistent empirical trend nouns cohesive group verbs structure learned faster nouns verbs 
obviously structure sentences tasa different structure child language environment 
structure representations learned beagle consistent nouns group learned faster verbs 
intuitively sense major factor influencing benefit learning nouns faster verbs 
possible finding due part statistical distribution lexical classes language acquisition trends may partly explained progressive sampling statistically redundant language environment 
research needed tease apart effects large scale statistical structure 
addition sources information completely separable influences statistical usage words language 
priming structure semantic representation studied great length similarity priming techniques reviews see mcnamara neely 
common finding priming stimulus processed efficiently preceded related stimulus 
assumption stimulus prime facilitates processing second stimulus target shared information respective mental codes rosch 
semantic priming magnitude facilitation depends semantic similarity prime target 
example nurse processed efficiently preceded doctor preceded bread meyer 

development lexical class cohesion lexicon function progressive sentence sampling 
table priming results 
predictions lexical similarity beagle representations prime target similarity localist models account semantic priming construct spreading activation anderson bower collins loftus 
nodes network activated activation spreads associated pathways related nodes 
spread activation associated nodes partially activated related concept processed 
spreading activation crucial explanatory component semantic networks lorch 
feature list representations contrast semantic priming accounted overlapping features prime target 
robin shares features chair shared features bat sparrow 
greatly debated semantic priming due strength association semantic overlap prime target lucas williams 
aggregate results studies see imply priming exists pairs semantic associated relationship 
addition exists associative boost prime target pairs types relationships 
argued semantic associative distinction false dichotomy mc steyvers purely associated purely semantically related words 
beagle facilitation directly predictable structural similarity lexical representations 
representations contain semantic associative information overlap type cause facilitation prime target pairs share types information yield associative boost 
binary feature representation beagle distributed representations allow shared information types words exemplars category labels features robin bird wings respectively 
furthermore indirectly learned contextual information affords similarity relations mediated priming 
semantic associative priming 
example simulating priming representations learned beagle consider study burgess richards 
prime target pairs relationship semantic deer associative bee honey semantic associative doctor nurse 
measured performance presentations separately visual field simplicity collapse visual fields 
table presents latency data 
experiment naming responses corresponding cosines words beagle 
note high cosine indicates greater similarity turn predicts faster rts condition relative condition lower cosines 
negative priming difference beagle predicts positive jones mewhort 
beagle unrelated related priming unrelated related priming semantic associated note 
beagle bound encoding aggregate language environment 
human data unrelated prime target pairs similar related pairs 
significant facilitation semantic semantic plus associated prime target pairs 
beagle related prime target pairs significantly higher cosines unrelated pairs semantic condition associated condition semantic plus associated condition 
magnitude facilitation predicted beagle differed priming conditions 
student newman post hoc tests revealed differences conditions significant groups homogeneous subsets 
beagle predicts findings facilitation semantic semantic plus associated conditions 
furthermore facilitation predicted greater semantic plus associated condition semantic condition human data 
beagle predict small reliable facilitation effect associated condition finding mirrored data 
stimuli facilitation associated condition experiment lexical decision reliably demonstrated studies see 
extend demonstration facilitation associated prime target pairs predictions generated beagle associated stimulus set shelton martin experiments 
shelton martin robust facilitation priming stimuli faster responses target preceded associated prime coffee cup ms preceded unrelated prime mouse cup ms 
prime target pairs beagle cosines target words associated primes significantly higher targets unrelated primes list 
beagle generally predicts weaker facilitation prime target pairs share associative relationship doctor associative information learned shared context model 
semantic relationships lawyer doctor tend stronger purely associative relationships model pairs relationship semantic steyvers noted called semantic words experiment fact associates nelson 
norms 
associative nurse doctor predicted associative boost similarity result multiple learning sources local global context order information sources simply summed learning 
note certain associative relationships powerful purely semantic ones 
mediated priming 
mediated priming task lorch relationship prime target lion stripes mediated concept tiger 
mediated priming produces subtle effect rt 
observed rt data mediated priming prominently observed measures brain function associated component evoked brain potential recordings 
mckoon ratcliff suggested mediated priming may mediated may reflect weak associative relationship directly prime target 
mediated priming accounted naturally semantic networks spreading activation difficult explain feature list account 
network representation activation node spreads activation connected nodes 
lion node activated activation spreads tiger closely connected lion stripes closely connected tiger 
feature list representation lion stripes overlap fact stripes feature slot distinct representation 
mediated priming naturally occurs beagle representations need process spreading activation 
beagle features elements values representing distribution samples experience 
feature meaning isolation word represented complete pattern elements weights connectionist network 
lion stripes may directly occur text representations shared random vectors contexts 
lion similar shared context tiger stripes 
lion tiger similar stripes similar directly occur 
sense beagle supports mckoon ratcliff notion mediated priming due direct similarity prime target 
mckoon ratcliff account words need simultaneously short term memory encoding 
information shared lion stripes learned statistical behavior language mediated respective relationships tiger 
learned representations similarity lion stripes direct exists lexical entry tiger 
example lorch tested prime target pairs direct relationship tiger stripes relationship mediated concept related lion stripes 
targets conditions varied primes 
table shows lorch rt results corresponding predictions beagle word pairs unrelated primes random re pairings word related list lorch see appendix 
beagle predicts priming facilitation human data 
related prime target pairs significantly holographic lexicon table priming results lorch predictions lexical similarity beagle representations higher cosines unrelated pairs mediated prime target pairs 
furthermore predicted facilitation related condition unrelated related significantly greater predicted facilitation mediated condition unrelated mediated 
beagle representations naturally predict mediated priming effects observed human data relying process construct spreading activation 
beagle similarity prime target pairs direct need mediated representation 
furthermore model predicts greater facilitation prime target pairs direct semantic relationship mediated pairs characteristic human data 
semantic constraint stem completions stem completion stimuli commonly study semantic constraint syntactic ambiguity resolution 
general finding final word sentence identified judged quickly consistent meaning preceding sentence string fischler bloom 
specific meaning stem constrained list candidate completion words 
precise mechanism mechanisms effect subject debate fischler bloom beagle directly predictable fit context order information stem lexicon 
lexical vectors fit meaning order sentence appropriately highly activated presentation stem 
compute context information stem sum learned lexical vectors word stem 
compute order information final position replace position phi vector convolve environmental vectors words position 
example consider boat fits completion word stems 
saved money bought 

sea tossed 
related mediated unrelated lorch beagle note 
beagle bound encoding aggregate language environment 
beagle context information stem activates lexical vectors vaguely related number things meaning stem vague top neighbors verbs notion mediation confusing beagle mediated relationships due directly shared statistical information 
paid purchased earned sold spent received 
target word boat st neighbor context vector sentence 
adding order information computing summed convolutions final position generally constrains words fit grammaticality final position inhibits paid purchased candidates 
addition order information increases boat activation verbs moves rank order 
vagueness stem words highly constrained context order information 
terminal word just boat car house number concrete nouns 
context vector stem hand constrained similar concepts wind water waves shore 
boat th neighbor context vector stem 
adding order information terminal position constrains activations somewhat moving boat th neighbor 
lexical vector boat cosine context order vector stem cosine stem 
ship fits better boat stem worse stem 
study examined effect processing fluency word naming task stem completion stimuli similar example 
specifically high low expectation stems see appendix different terminal words 
subject task read stem press button reveal completion word reading aloud quickly possible 
pronunciation latencies faster completion words preceded high constraint stem ms stem low constraint stem ms stem 
beagle context order information stems computed terminal position types information summed composite probe vector 
cosine computed probe vector lexical vector terminal word stimulus 
model predicts finding greater congruency stem completion context semantically predictive 
specifically high expectation stem completions significantly higher cosines low expectation stem completions 
related study larger stimulus set see appendix varied stem constraint completion words 
example high constraint stem accident covered 
example low constraint stem corner table bit 
glass blood legal completion words stem 
normed stimuli having subjects rate completion predictable stem data interest 
subjects rated completion words predictable high constraint stems low constraint stems 
beagle context order information computed final position stems experiment cosine calculated stem vectors respective completion words 
high constraint stem completions significantly similar low constraint stem completions 
similar study taraban mcclelland experiment stem completion stimuli tease apart effects phrase attachment expectation stem completions 
jones mewhort table mean expectation ratings reading times ms taraban mcclelland experiment stem completion cosines beagle representations attachment type taraban mcclelland beagle expectation reading time cosine npa expected unexpected note 
beagle bound encoding aggregate language environment npa noun phrase attachment verb phrase attachment 
designed sentences terminal word noun phrase attachment npa john ordered pizza verb phrase attachment john ordered pizza enthusiasm 
principle minimal attachment frazier rayner npa sentences take longer read sentences constituent branches 
taraban mcclelland selected npa words expected stem words expectation minimal attachment 
taraban mcclelland normed stimuli having subjects rate expected completion word stem point scale 
expectation subsequently predict reading times experiment 
table presents taraban mcclelland data beagle predictions stimuli 
beagle npa completion words similar stems completions 
key difference stimuli taraban mcclelland stimuli stem completion varied 
completion words conditions varied stems manipulate expectation 
contrast taraban mcclelland stems conditions varied completion words manipulate expectation 
case stems completions coded single vectors beagle manipulations expectation directly predictable learned lexical representations 
experiment taraban mcclelland examined types noun fillers expectation stems 
example stem cleaned storage area 
fillers may fully consistent broom expected filler solvent expected role manager expected attachment odor 
stimuli normed subjects point scale terms expectation completion stem plausibility sentences subjective ratings predicted reading times subsequent experiment 
data table beagle cosine predictions stimulus set 
pattern cosines beagle consistent data taraban mcclelland experiment words experiment lexical entry beagle substituted synonyms exhibited meat shelter 
table mean expectation plausibility ratings reading times ms types completion attachments taraban mcclelland taraban mcclelland experiment stem completion cosines beagle representations attachment type omnibus analysis variance conditions marginally significant 
taraban mcclelland significant difference expectation plausibility ratings expected filler expected role expected attachment conditions see table fully consistent condition rated significantly higher expected filler expected role expected attachment conditions measures 
beagle orthogonal contrast comparing fully consistent condition mean expected filler expected role expected attachment conditions mirrored taraban mcclelland findings 
representations learned beagle provide higher fidelity representation word meaning incorporates context order information 
resulting representations shown possess necessary structure account human data variety semantic word prediction tasks 
general discussion purpose article demonstrate holographic lexicon representing word meaning word order learned simple summation association mechanism applied large scale statistical redundancies text 
furthermore broad range psycholinguistic phenomena accounted directly structure lexical representations learned way 
model intended model language memory model provides account representations higher order models language comprehension kintsch 
notion account behavior possible structure knowledge representation building rules complexity model 
estes noted clear priori know performance occurs situation describe terms rules 
hardwired rules complex mechanisms resort modeling simple explanations exhausted 
gibson warned perceptual theorists postulate computational mechanisms perceptual system necessary information perception obtained environment sensory system 
similarly simon reminded cognitive theorists build complexity model algorithms behavior understood terms simple organism responding redundancies complex envi holographic lexicon taraban mcclelland beagle plausibility reading time cosine fully consistent expected filler expected role expected attachment note 
beagle bound encoding aggregate language environment 
ronment 
discussing path taken ant beach simon noted ant path irregular complex hard describe 
complexity really complexity surface beach complexity ant simon 
reiterates simon advice domain knowledge representation reminding theorists build unnecessary complexity representation processing mechanisms easily explained structure learned simple mechanism applied statistical redundancies complex language environment 
level comprehension complex rules may recruited account behavior possible rules learned environment 
built rules account language comprehension default modeling approach examined behavior complex account structure simple representation 
beagle requires minimum innate control structure learning illustrates wide range data may explained directly statistical abstraction language environment 
beagle assumes lexical representation word pattern elements arbitrary number dimensions 
true value element word representation estimated statistical sampling 
exists distribution values particular element successive observations word language noise averaged true value emerges central limits theorem 
furthermore knowledge representation model simple possible contain unnecessary complexity 
beagle demonstrates distributed representations word meaning usage automatically learned simple learning mechanisms having complexity built artificially knowledge representation case classic feature list semantic network representations 
comparing beagle semantic space models beagle extends existing semantic space models introducing new way implement core principle inducing word meaning usage text 
beagle improves criticisms existing semantic space models incorporating word order exploiting incremental learning algorithm 
furthermore adapts learning representation principles associative memory theory murdock model uses mechanisms learn word meaning may form memory representations effective accounting judgments frequency recency murdock smith recognition serial recall mewhort murdock free recall franklin mewhort 
mechanisms allow model retrieve word transition information stored lexicon 
major difference representations learned beagle learned lsa beagle includes consideration order information addition context information 
hyperspace analogue language hal burgess lund lund burgess measure distance word steps moving window learns explicitly encode order information mechanism retrieve sequential dependencies 
priming data article example lsa hal predicts pattern priming human data hal underestimates magnitude associative priming lsa overestimates associative priming underestimates semantic priming 
addition hal unable predict mediated similarity burgess 
variety priming data see jones 
priming example behavior depends semantic associative information representation considers types information correctly simulate aggregate human data 
beagle represents context order information composite representation giving higher fidelity representation word meaning 
product convolution order encoding mechanism model invert routine retrieve word transitions representations 
model word meaning sequential dependency information requiring additional storage assumptions 
note convolution correlation primitive model sequential dependency virtue demonstrates learned positional information may retrieved semantic memory reason beagle takes small step empiricist theory language 
beagle currently replace generative models language suggests reduced role rule processing 
occurrence models bayesian methods consider words appear contexts apart griffiths steyvers griffiths steyvers tenenbaum hoffman smith humphreys steyvers griffiths press 
specifically topic models griffiths steyvers tenenbaum probabilistic methods infer generative topics documents created 
idea document mixture generative topics topic probability distribution words 
griffiths steyvers blei tenenbaum integrated generative processes probabilistic topic model hidden markov model fuse model meaning sequential dependencies 
beagle model considers syntactic semantic information processes text 
model tested automated lexical class tagging document classification fitting human data comparison models point data common tasks 
difference beagle semantic space models concerns nature word context 
lsa context jones mewhort specifically document topic models griffiths steyvers tenenbaum 
hal concept context moves word window continuously text corpus bridging sentences 
beagle word context specifically sentence order information computed sentence correctly consider syntactic information 
beagle trained context information similarity structure representations learned similar learned lsa trained text corpus 
changing beagle compute context information paragraph document sentence produces representations similarity subtly lsa 
appear size context particularly important computation context information beagle 
possible beagle particularly sensitive initial conditions 
models hal lsa topics initial data constant runs term document matrix case lsa 
models produce representation multiple training runs input corpus 
contrast beagle begins random noise gradually accumulates structure statistical sampling 
environmental representations word created random different training runs corpus seed stored 
follows lexical representations word runs randomly similar 
semantic similarity structural similarities words lexicon lexicons runs different pattern interword cosines remarkably similar 
representation dog different training runs similarity dog cat highly congruent runs 
compared similarity structure multiple lexicons trained corpus interword similarity structures lexicons quite consistent different random initial conditions 
provided initial conditions beagle close random exact initial conditions unimportant 
criticism common semantic space models srns role supervision learning word meanings criticism valid beagle 
children obviously benefit feedback supervision learning word meanings mcclelland noted unsupervised models analogous learning language listening radio cited elman 
semantic space models learn word meanings need feedback supervision certainly important factor human word learning semantic models need account 
elman suggested feedback derived stimulus environment srn predicts word sequence sentences paragraphs related sentences paragraphs tom landauer personal communication suggested additional random vector representing paragraph context added calculation sentence context word encountered new paragraph 
produce greater similarity words appeared paragraphs 
similar usage random vectors represent context see dennis humphreys 
topics model stochastic gibbs sampling 
looks ahead see correct word feedback adjust connection weights 
plate shown srn learn feedback error correction elman notion supervision srn may unnecessary 
modularity composite systems traditionally widespread assumption knowledge word meaning knowledge lexical class knowledge grammatical usage separate types information involving different forms representation stored separately 
information word meaning stored lexicon dictionary type definition knowledge grammatical usage represented form rules production systems 
beagle means model syntax model memory produce limited syntactic behavior simply inverting learning routines structure memory representation 
directly obvious point structure memory longer simulate transition behavior rules relied novel strings long range dependencies 
addition representing types information vector patterns beagle represents vector pattern 
composite representation containing superposition context order information functions tasks reported concatenated system context order information represented distinct vectors 
keeping occam razor appear need separate representations single representation accounts data equally 
meaning order information may represented form representation murdock demonstrated memory items associations 
context information order information represented vector multiple meanings lexically ambiguous words stored need multiple representations 
multiple meanings ambiguous word simultaneously activated word processed foss tanenhaus seidenberg meaning clearly dominant burgess seidenberg tanenhaus 
results taken evidence multiple meanings polysemous word distinct lexical representations 
beagle polysemous word bank lexical representation 
pattern elements representing bank meaning dominated frequent financial institution sense information river shore sense stored pattern elements may disambiguated word context 
sentence context sufficiently biased meaning ambiguous word enhance activation congruent meaning 
effect observed response latency measures van 
frequency meaning prior context affect activation different word meanings donnell rayner 
different contexts bank fishing river bank different meanings bank emerge beagle lexicon lexical representation bank responds contexts 
holographic lexicon multiple meanings polysemous words stored holographic representation different meanings emerge single representation depending context order sentence 
language comprehension extremely complicated behaviors 
claim full theory behaviors 
claim knowledge underpinning behaviors learned acquired simple mechanisms operating large scale 
lsa shown word contextual history learned shown order history learned furthermore types information stored composite holographic representation 
short pushed lsa view language new level bringing syntactic issues range empiricist program 
claim beagle captures complexity syntax natural language fair claim beagle pushes empiricist program closer goal 
models higher order comprehension simplified adopt beagle order information 
order information may allow models limit application rule mechanisms 
learning representation model limited text may applied statistical redundancies environments 
article trained beagle text 
text impoverished version full input available human system 
modern models consider grounding meaning perception action see glenberg de vega graesser press progress 
text convenient set data train beagle 
model learn regularities visual auditory input equally appropriate front representation 
integrating multisensory data statistical models needed role supervision environmental feedback statistical learning 
anderson bower 

human associative memory 
washington dc winston 
lorch 

depth automatic spreading activation mediated priming effects pronunciation lexical decision 
journal experimental psychology learning memory cognition 
montague 

category norms verbal items categories replication extension connecticut category norms 
journal experimental psychology monographs pt 

booth 

probabilistic representation formal languages 
th annual ieee symposium switching automata theory pp 

new york institute electrical electronics engineers 
brooks 

concept formation memory instances 
rosch lloyd eds cognition categorization pp 

hillsdale nj erlbaum 
burgess lund 

dynamics meaning memory 
dietrich markman eds cognitive dynamics conceptual representational change humans machines pp 

mahwah nj erlbaum 
burgess seidenberg tanenhaus 

context lexical access implications nonword interference lexical ity resolution 
journal experimental psychology learning memory cognition 
burgess richards 

semantic associative priming cerebral hemispheres words words don 
places 
brain language 
chomsky 

aspects theory syntax 
cambridge ma mit press 
chomsky 

rules representations 
new york columbia university press 
christiansen chater 

connectionist model recursion human linguistic performance 
cognitive science 


mediated priming lexical decision task evidence event related potentials reaction time 
journal memory language 
collins loftus 

spreading activation theory semantic processing 
psychological review 
collins quillian 

retrieval time semantic memory 
journal verbal learning verbal behavior 
collins quillian 

language user 
tulving donaldson eds organization memory pp 

new york ny academic press 
deerwester dumais furnas landauer harshman 

indexing latent semantic analysis 
journal american society information science 
dennis humphreys 

context noise model episodic word recognition 
psychological review 
elman 

finding structure time 
cognitive science 
elman 

distributed representations simple recurrent networks grammatical structure 
machine learning 
elman 

learning development neural networks importance starting small 
cognition 
elman 

language dynamical system 
port van gelder eds mind motion explorations dynamics cognition pp 

cambridge ma mit press 
elman 

alternate view mental lexicon 
trends cognitive science 
estes 

classification cognition 
oxford england oxford university press 
fellbaum 

wordnet electronic lexical database 
cambridge ma mit press 
firth 

synopsis linguistic theory 
firth ed studies linguistic analysis pp 

oxford england blackwell 
fischler bloom 

automatic attentional processes effects sentence contexts word recognition 
journal verbal learning verbal behavior 
fischler bloom 

effects constraint validity sentence contexts lexical decisions 
memory cognition 
fleischman roy 

verbs harder learn nouns initial insights computational model intention recognition situated word meaning 
bara barsalou bucciarelli eds proceedings th annual conference cognitive science society pp 

mahwah nj erlbaum 
foss 

effects ambiguity sentence comprehension 
journal verbal learning verbal behavior 
franklin mewhort 

analysis immediate memory free recall task 
li eds high performance computing systems applications pp 

new york kluwer academic 
frazier rayner 

making correcting errors jones mewhort sentence comprehension eye movements analysis structurally ambiguous sentences 
cognitive psychology 
roberts 

signals linear systems 
new york wiley 
gentner 

nouns learned verbs linguistic relativity versus natural partitioning 
ed language development vol 

language cognition culture pp 

hillsdale nj erlbaum 
gibson 

senses considered perceptual systems 
boston houghton mifflin 
gleitman gleitman lederer 

human simulation vocabulary learning 
cognition 
glenberg de vega graesser 
eds 
press 
symbols embodiment meaning workshop debate 
griffiths steyvers 

probabilistic approach semantic representation 
gray eds proceedings fourth annual conference cognitive science society pp 

mahwah nj erlbaum 
griffiths steyvers blei tenenbaum 

integrating topics syntax 
saul weiss bottou eds advances neural information processing systems vol 
pp 

san mateo ca morgan kaufmann publishers 
griffiths steyvers tenenbaum 

topics semantic representation 
manuscript submitted publication 
heiser 

selecting stimulus set prescribed structure empirical confusion frequencies 
british journal mathematical statistical psychology 
hintzman 

minerva simulation model human memory 
behavior research methods instruments computers 
hoffman 

unsupervised learning probabilistic latent semantic analysis 
machine learning journal 
howard kahana 

distributed representation temporal context 
journal mathematical psychology 


semantic priming due association strength feature overlap 
review 
psychonomic bulletin review 
jelinek lafferty 

computation probability initial substring generation stochastic context free grammars 
computational linguistics 
jones kintsch mewhort 

high dimensional semantic space accounts priming 
journal memory language 
kintsch 

role knowledge discourse comprehension construction integration model 
psychological review 
kintsch 

comprehension paradigm cognition 
new york cambridge university press 
kintsch 

predication 
cognitive science 


structure process semantic memory evidence event related potentials reaction time 
journal experimental psychology general 


context build semantics 
psychonomic bulletin review 
laham 

automated content assessment text latent semantic analysis simulate human cognition 
unpublished doctoral dissertation university colorado boulder 
landauer dumais 

latent semantic analysis measurement knowledge 
kaplan burstein eds educational testing service conference natural language processing techniques technology assessment education 
princeton nj educational testing service 
landauer dumais 

come know 
practical problem theory 
hermann johnson eds basic applied memory research vol 

theory context pp 

mahwah nj erlbaum 
landauer dumais 

solution plato problem latent semantic analysis theory acquisition induction representation knowledge 
psychological review 
landauer foltz laham 

latent semantic analysis 
discourse processes 
landauer laham schreiner 

passage meaning derived word order 
comparison latent semantic analysis humans 
langley eds proceedings nineteenth annual conference cognitive science society pp 

mahwah nj erlbaum 
burgess 

mediated priming high dimensional semantic space effect direct semantic relationships cooccurrence 
brain cognition 
lucas 

semantic priming association meta analytic review 
psychonomic bulletin review 
lund burgess 

producing high dimensional semantic spaces lexical occurrence 
behavior research methods instruments computers 
mckinley thompson 

semantic distance norms computed electronic dictionary wordnet 
behavior research methods instruments computers 
marcus santorini marcinkiewicz 

building large annotated corpus english penn treebank 
computational linguistics 
masson 

distributed memory model semantic priming 
journal experimental psychology learning memory cognition 
stahl 

influence word meaning acquisition reading vocabulary children 
reading writing 
mckoon ratcliff 

spreading activation versus compound cue accounts priming mediated priming revisited 
journal experimental psychology learning memory cognition 
mcnamara 

semantic priming perspectives memory word recognition 
new york psychology press 
de sa seidenberg 

nature scope featural representations word meaning 
journal experimental psychology general 
metcalfe 

composite holographic associative recall model 
psychological review 
mewhort johns 

sharpening echo model short term recognition memory 
memory 
mewhort 

serial recall letter strings 
eds relating theory data essays honor bennet murdock pp 

hillsdale nj erlbaum 
meyer 

facilitation recognizing pairs words evidence dependence retrieval operations 
journal experimental psychology 
miller 

language communication 
new york mcgraw hill 
miller 
ed 

wordnet line lexical database special issue 
international journal lexicography 
miller 

nouns wordnet 
fellbaum ed wordnet electronic lexical database pp 

cambridge ma mit press 
miller selfridge 

verbal context recall meaningful material 
american journal psychology 
murdock 

theory storage retrieval item associative information 
psychological review 
murdock 

item associative information distributed memory model 
journal mathematical psychology 
murdock 

derivations chunking model 
journal mathematical psychology 
murdock smith 

judgments frequency holographic lexicon recency distributed memory model 
journal mathematical psychology 


modality concreteness set size effects free reconstruction order task 
memory cognition 


human memory research data theory 
pacific grove ca brooks cole 
neely 

semantic priming effects visual word recognition selective review current findings theories 
besner humphreys eds basic processes reading visual word recognition pp 

hillsdale nj erlbaum 
nelson schreiber 

university south florida word association rhyme word fragment norms 
retrieved november edu nelson 

concept word sentence interrelations acquisition development 
psychological review 
nelson 

individual differences language development implications development language 
developmental psychology 


accessing lexical ambiguities sentence comprehension effects frequency meaning contextual bias 
memory cognition 
osgood 

ease individual judgment processes relation polarization attitudes culture 
journal social psychology 
osgood 

nature measurement meaning 
psychological bulletin 
osgood 

exploration semantic space personal diary 
journal social issues 


imagery mental processes 
new york holt rinehart winston 


mental representations dual coding approach 
oxford england oxford university press 
yuille madigan 

concreteness imagery meaningfulness values nouns 
journal experimental psychology pt 

palmer gildea kingsbury 

proposition bank annotated corpus semantic roles 
computational linguistics 


limits occurrence tools theories language research 
discourse processes 


number features effects semantic processing 
memory cognition 
lupker 

impact feedback semantics visual word recognition number features effects lexical decision naming tasks 
psychonomic bulletin review 
plate 

holographic reduced representations 
ieee transactions neural networks 
plate 

holographic reduced representations csli lecture notes 
stanford ca csli publications 
plaut 

semantic associative priming distributed attractor network 
cognitive science society ed proceedings seventeenth annual conference cognitive science society pp 

hillsdale nj erlbaum 
pustejovsky 

generative lexicon 
cambridge ma mit press 
pylyshyn 

mind eye tells mind brain critique mental imagery 
psychological bulletin 
pylyshyn 

mental imagery debate analog media versus tacit knowledge 
psychological review 
ratcliff 

theory memory retrieval 
psychological review 
rips shoben smith 

semantic distance verification semantic relations 
journal verbal learning verbal behavior 
rosch 

internal structure perceptual semantic categories 
moore ed cognitive development acquisition language pp 

new york academic press 
rosch 

cognitive representations semantic categories 
journal experimental psychology general 
salton 

studies automatic text analysis document retrieval 
journal association computing machinery 
salton wong yang 

vector space model automatic indexing 
communications association computing machinery 


effects context classification words nonwords 
journal experimental psychology human perception performance 


psychology duffy trans 
london allen unwin 
original published di eugenio 

extending latent semantic analysis features dialogue act classification 
association computational linguistics ed proceedings nd annual meeting association computational linguistics pp 

east pa association computational linguistics 
donnell rayner 

eye movements lexical ambiguity resolution investigating subordinate bias effect 
journal experimental psychology human perception performance 
servan schreiber cleeremans mcclelland 

graded state machines representation temporal contingencies simple recurrent networks 
machine learning 
shelton martin 

semantic automatic semantic priming 
journal experimental psychology learning memory cognition 
simon 

sciences artificial rd ed 
cambridge ma mit press 
smith humphreys 

evaluation unsupervised semantic mapping natural language concept mapping 
behavior research methods 
smith shoben rips 

structure process semantic memory featural model semantic decisions 
psychological review 
smolensky 

tensor product variable binding representation symbolic structures connectionist systems 
artificial intelligence 
gleitman 

hard label concepts 
hall waxman eds weaving lexicon pp 

cambridge ma mit press 
horn ruppin edelman 

unsupervised learning natural languages 
proceedings national academy sciences usa 
jones mewhort steiger 

tests comparing elements correlation matrix 
psychological bulletin 
steyvers 

modeling semantic orthographic similarity effects memory individual words 
unpublished doctoral dissertation indiana university bloomington 
steyvers griffiths 
press 
probabilistic topic models 
landauer mcnamara dennis kintsch eds latent semantic analysis road meaning 
mahwah nj erlbaum 
steyvers tenenbaum 

large scale structure semantic networks statistical analyses model semantic growth 
cognitive science 


accessing lexical ambiguity different types sentential contexts 
journal memory language 
tanenhaus seidenberg 

combinatory lexical information language comprehension 
altmann ed cognitive models speech processing pp 

cambridge ma mit press 
taraban mcclelland 

constituent attachment thematic role assignment sentence processing influences contentbased expectations 
journal memory language 
turney 

mining web synonyms pmi ir versus lsa toefl 
de raedt flach eds proceedings th european conference machine learning pp 

freiburg germany european conference machine learning 
van der van den 

empirical confusion matrix continuous line capitals 
perception psychophysics 
van 

ambiguous words context event related potential analysis time course meaning activation 
journal memory language 


illusions familiarity 
journal experimental psychology learning memory cognition 


routes remembering remembering 
journal experimental psychology general 
wiemer hastings 

adding syntactic information lsa 
gleitman joshi eds proceedings second annual conference cognitive science society pp 

mahwah nj erlbaum 
wiemer hastings 

rules syntax vectors semantics 
moore stenning eds proceedings third annual conference cognitive science society pp 

mahwah nj erlbaum 
williams 

comparison regression variables 
journal royal statistical society series 
williams 

automatic priming semantic 
european journal cognitive psychology 
learning context information bound encoding aggregate language environment beagle model straightforward described detail model description section main text article word context history sum environmental vectors word tokens occurred sentences 
learning retrieval order information necessarily simplified model description section algorithms encoding decoding order information described greater detail 
encoding order information learning order information beagle produces directional associations permuting argument vectors positions differently prior pairwise convolution adapted plate 
program initializes encoding remapping functions created positions respectively 
function creates scrambling elements vector passed scrambling order determined randomly initialization constant learning 
vector order elements scrambled differently 
contains elements different order expected cosine zero 
encoding dog bite bite dog produce unique associations order arguments differ convolution commutative dog bite bite dog 
example encoding position sequence abcd operations computed yield order vector sentence code bind bind bind bind cd bind cd ob decoding circular correlation learned items may decoded order information holographic vector inverse circular correlation decode environmental vectors memory properly encoding mapping functions undone appropriate order 
decoding mappings inverse respectively 
dog encoded dog bite feed dog bite feed dog summed representation bite may decoded approximately equal dog 
lexical vector function dog decodes facsimile feed 
example decoding right left word luther demonstrated main text article specifically holographic lexicon appendix learning retrieving order information lexicon continue luther martin luther king 
grams included probe value decoded response constrained 
grams larger bigram decoding computed slightly different way left right blank position 
example assume lexical vector chunk abc coded 
information determine vector fits ab information traces coded needed 
contains information vector succeeds may decoded trace contains information vector succeeds conditional preceding decode may built correlated specifically succeeds bigram sequence ab precedes symbol vector retrieved probed similar vector retrieved probed ab conditional preceding similar coding done left right convolution decoding backwards number steps slightly complicated 
assume 
information determine vector fits bc iteratively unpacked 
contains information vector preceded may decoded contains information vector preceded conditional succeeding trace packed vectors time left right specifically iteratively unpacked right left predict position 
approximately facsimile entire operation decode symbol precedes memory may written furthermore contains dependency appeared position seen position ab information disambiguate blank position 
chunks retrieve order information decoding complicated demonstrate 
concrete example consider decoding blank position triplet martin luther 
king luther luther luther martin luther martin luther luther luther martin 
martin luther martin adding decoded vectors produces vector similar pattern elements king 
decoding position luther king martin luther luther luther king luther luther king luther luther king 
luther king king decoding middle position martin king come sources jones mewhort luther martin martin martin king king king king martin martin king martin martin king 
martin king king sentences collocation phrases may progressively unpacked holographic vectors 
chunk example lexical vector martin decoded right luther retrieved 
martin decoded left word reliably retrieved 
martin luther decoded right king retrieved sequence words king decoded right jr retrieved producing learned collocate martin luther king jr jr decoded right consistent word retrieved 
descriptive source code test data demonstrations beagle may online www indiana edu beagle phrase doctor martin luther king jr tasa 
appendix list exemplars category colors sports fruit countries red football apple canada blue baseball orange mexico green basketball banana australia yellow hockey grape france purple germany white plum japan black pear china gray soccer lemon brazil orange lime spain cyan cherry india magenta boxing netherlands golf cuba pink poland brown coconut russia diseases vegetable flowers dogs alzheimer squash cancer rose beagle peas daisy bulldog lotus diabetes leukemia hepatitis onions tulip poodle peppers meningitis beans sunflower influenza malaria daffodil tuberculosis birds fish cities vehicles robin trout boston car sparrow cod ottawa truck hawk shark london bicycle chicago automobile pigeon munich bus denver van duck salmon berlin taxi tokyo train swan herring houston motorcycle eagle canberra plane crow dublin boat bass oakland trolley penguin flounder detroit ship goose pittsburgh bike holographic lexicon appendix list nouns verbs lexical class simulation nouns verbs car go sun drink ball eat earth read tree move girl walk house get road bus play door die rabbit hide box speak bed kick dog think table ride toy fly received october revision received july accepted july 
