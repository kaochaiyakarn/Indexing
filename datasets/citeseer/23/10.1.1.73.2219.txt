learning bounds generalized family bayesian posterior distributions tong zhang ibm watson research center yorktown heights ny watson ibm com obtain convergence bounds concentration bayesian posterior distributions true distribution novel method simplifies enhances previous results 
analysis introduce generalized family bayesian posteriors show convergence behavior generalized posteriors completely determined local prior structure true distribution 
important surprising robustness property hold standard bayesian posterior may concentrate exist bad prior structures places far away true distribution 
consider sample space measure respect field 
statistical inference nature picks probability measure unknown 
assume density respect 
bayesian paradigm statistician considers set probability densities respect indexed assumption true density represented randomly picked prior distribution 
quantities appearing derivations assumed measurable 
set samples 
xn xi independently drawn unknown distribution optimal bayesian method derived optimal inference respect posterior distribution 
bayesian procedure optimal nature picks prior statistician known procedures desirable properties frequentist point view admissibility bayesian 
theoretical point view necessary understand behavior bayesian methods assumption nature picks prior statistician 
respect fundamental issue bayesian analysis bayesian inference posterior distribution converge corresponding inference true view bayesian paradigm method generate statistical inferencing procedures don assume bayesian prior assumption true 
particular assume 
unknown distribution number observations approach infinity 
general question bayesian posterior distribution concentrated true underlying distribution sample size large 
referred consistency bayesian posterior distribution certainly fundamental issue understanding behavior bayesian methods 
problem drawn considerable attention statistics 
classical results include average consistency results consistency theorem asymptotic convergence results bernstein von mises theorem parametric problems 
infinite dimensional problems choose prior carefully bayesian posterior may concentrate true underlying distribution leads inconsistency 
authors gave conditions guarantee consistency bayesian posterior distributions convergence rates obtained 
convergence rates studied works heavy empirical process theory 
purpose develop finite sample convergence bounds bayesian posterior distributions novel approach simplifies analysis leads tighter bounds 
heart approach new posterior averaging bounds related pac bayes analysis appeared machine learning works 
new bounds independent interests fully explore consequences obtain correct convergence rates statistical estimation problems squares regression 
motivated learning bounds introduce generalized family bayesian methods show convergence behavior relies prior mass small neighborhood true distribution 
surprising consider example shows standard bayesian method puts positive prior mass true distribution may get inconsistent posterior exist undesirable prior structures far away true distribution 
regularization formulation bayesian posterior measure assume observe samples 
xn independently drawn true underlying distribution shall call probability density wx respect depends observation measurable posterior distribution 
define generalized bayesian posterior respect xi xi 
call bayesian posterior 
standard bayesian posterior denoted 
probability density respect define kl divergence kl wd kl wd ln 
consider real valued function denote expectation respect 
similarly consider real valued function denote eq expectation respect true underlying distribution ex denote expectation respect observation key starting point analysis simple observation relates bayesian posterior solution entropy regularized density respect estimation 
formulation techniques analyzing regularized risk minimization problems investigated author applied obtain sample complexity bound bayesian posterior distributions 
proof regularization formulation straight forward shall skip due space limitation 
proposition density respect ln xi xi kl wd 

proposition indicates generalized bayesian posterior minimizes regularized empirical risk possible densities respect prior 
need study behavior regularized empirical risk minimization problem 
may define true risk replacing empirical expectation expectation respect true underlying distribution kl kl wd kl eq ln kl divergence non negative number 
quantity widely measure closeness distributions clearly bayesian posterior approximate solution empirical expectation 
term measures average kl divergence density 
term second term non negative know immediately distribution concentrated empirical process techniques typically expect bound term 
unfortunately case kl defined implies long non zero concentration density kl 
may non zero probability sample size approaches infinity 
remedy consider distance function defined 
statistics considers divergence defined eq 
divergence defined kl lim 
statistical literature convergence results specified hellinger distance 
mention learning bound derived trivial 
consistent discussion corresponding may converge 
additional assumptions boundedness kl exists bounded divergence 
posterior averaging bounds entropy regularization inequality follows directly known convex duality 
example see explanation :10.1.1.10.5041
proposition assume measurable real valued function density respect kl wd ln exp 
main technical result forms basis lemma assume wx posterior density respect depends measurable 
lemma consider posterior wx 
inequality holds measurable real valued functions lx ex exp wx lx ln exe lx kl ex expectation respect observation proof 
proposition obtain wx lx ln exe lx kl ln exp lx ln exe lx 
applying theorem interchange order integration exe exe lx ln ex exp lx exe lx ln ex exp lx 
corollary straight forward consequence lemma 
note bayesian method loss form 
theorem posterior averaging bounds notation lemma 

xn samples independently drawn consider measurable function real number event holds probability exp wx xi kl expected risk bound wx ln eq exp exe wx ln eq exp ex wx xi kl proof sketch 
bound direct consequence markov inequality 
second bound obtained fact ex exp exp ex follows jensen inequality 
bounds immediately applicable bayesian posterior distribution 
leads exponential tail inequality second leads expected risk bound 
analyzing bayesian methods detail section shall briefly compare results called pac bayes bounds obtained estimating left hand side hoeffding inequality appropriately chosen 
shall estimate left hand side bernstein style bound useful general statistical estimation problems corollary notation theorem assume sup 
probability exp wx eq wx wx xi kl exp eq eq proof sketch 
follow standard derivations bernstein inequality outlined known non decreasing turn implies ln eq exp eq eq eq applying bound left hand side theorem finish proof 
may simple bound obtain xi wx eq wx kl 
inequality holds data independent choice 
may easily turn bound allows depend data known techniques see example 
optimize resulting bound similar pac bayes bound 
typically optimal order kl rate convergence right hand side better 
interesting case exists constant eq eq beq 
condition appears theoretical analysis statistical estimation problems squares regression loss function non negative classification 
appears analysis maximum likelihood estimation log loss shall see log loss directly handled framework theorem 
modified version condition occurs analysis classification problems problem separable 
shall assume holds 
follows corollary wx eq wx xi kl 
inequality holds data independent easily turn bound allows depend standard techniques 
shall list final result purpose 
parameter optimized hard check resulting bound significantly better wx xi 
self bounding condition holds theoretical analysis statistical estimation problems 
obtain correct convergence behavior cases including bayesian method interested inequality inadequate essential bernstein type bound 
useful point analyze problems needs appropriately chosen data independent lead correct minimax rate convergence 
note choose constant possible achieve bound converges fast 
shall point kl divergence version pac bayes bound developed loss related techniques lead rate fast ln near zero errors 
bernstein style bound generally applicable necessary complicated statistical estimation problems squares regression 
convergence bounds bayesian posterior distributions shall analyze finite sample convergence behavior bayesian posterior distributions theorem 
exponential tail inequality provides detailed information discussion expected risk bound simplicity 
case slightly tighter results obtained applying hoeffding exponential inequality directly left hand side theorem method corollary 
analyze bayesian method ln theorem 
consider 
wx bayesian posterior parameter defined 
consider arbitrary data independent density respect obtain theorem chain equations exe ln exe ln eq exp ln ex xi ln xi kl ex ln xi kl wd xi ex sup ln xi xi ex sup ln xi xi defined 
note inequality follows theorem second inequality follows proposition 
empirical process bound second term improved precise bounding method shall skip due lack space 
difficult see see proposition proposition skip derivation due space limitation inf ln exp 
fact ln simplify left hand side obtain exe ln ex sup ln xi xi 
shall compare analysis previous results 
consistent concept previous studies shall consider quantity set indicator function 
intuitively probability mass bayesian posterior region distance away divergence 
markov inequality immediately obtain bound ln ex sup ln xi xi 
estimate right hand side 
due limitation space shall consider simple truncation estimation leads correct convergence rate non parametric problems yields unnecessary ln factor parametric problems correctly handled precise estimation 
introduce notation essentially prior measure radius kl ball kl kl kl 
definition kl addition shall define upper bracketing introduced denoted minimum number non negative functions fi respect eq fi fi 
ex sup ln xi xi ex ln ln fj xi xi ln exe ln fj xi ln xi ln 
obtain kl ln ln bound immediately implies consistency convergence rate theorem bayesian posterior distribution theorem consider sequence bayesian prior distributions parameter space may different different sample sizes 
consider sequence positive numbers sup ln kl sn sn sn probability 
ln sup sn sn sn probability 
claim implies bayesian posterior concentrated ball divergence rate convergence op 
note determined local property true distribution immediately implies long kl bayesian method consistent 
second claim applies standard bayesian method 
consistency requires additional assumption depends global properties prior may somewhat surprising condition necessary 
fact counterexample shows standard bayesian method inconsistent condition kl 
standard bayesian procedure ill behaved put sufficient amount prior true distribution 
consistency theorem relies upper entropy number 
convergence rates established 
obtained rate convergence result standard bayesian method covering definitions 
definitions covering hellinger covering works obtain rate convergence non parametric bayesian methods 
possible derive bounds different covering definitions analysis shall details 
shall point works assumptions completely necessary 
example definition kl requires additional assumptions eq ln stronger condition needed analysis 
shall mention bound form theorem known produce optimal convergence rates non parametric problems see examples 
formulated extended family bayesian algorithms empirical minimization entropy regularization 
derived general posterior averaging bounds entropy regularization suitable analyzing bayesian methods 
new bounds independent interests lead bernstein style exponential inequalities crucial obtaining correct convergence behavior statistical estimation problems squares regression 
posterior averaging bounds obtain new convergence results generalized family bayesian posterior distributions 
results imply bayesian method robust standard bayesian method convergence behavior completely determined local prior density true distribution 
standard bayesian method optimal certain averaging sense behavior heavily dependent regularity prior distribution globally 
happens standard bayesian method put emphasis difficult part prior distribution degrades estimation quality easier parts interested 
able guess true distribution putting large prior mass neighborhood bayesian method accidentally bad choices 
difficult design bayesian priors 
new theoretical insights obtained imply completely understands impact prior safer bayesian method 
acknowledgments author andrew barron ron meir matthias seeger helpful discussions comments 
andrew barron mark schervish larry wasserman 
consistency posterior distributions nonparametric problems 
ann 
statist 
diaconis david freedman 
consistency bayes estimates 
ann 
statist 
discussion authors 
ghosh aad van der 
convergence rates posterior distributions 
ann 
statist 
mcallester 
pac bayesian stochastic model selection 
machine learning 
ron meir tong zhang 
generalization error bounds bayesian mixture algorithms 
journal machine learning research 
robert 
bayesian choice decision theoretic motivation 
springer verlag new york 
seeger 
pac bayesian generalization error bounds gaussian process classification 
jmlr 
shen larry wasserman 
rates convergence posterior distributions 
ann 
statist 
