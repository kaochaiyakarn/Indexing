fundamentals analyzing mining data streams graham cormode labs research park avenue florham park nj usa 
scenarios network analysis utility monitoring financial applications generate massive streams data 
streams consist millions billions simple updates hour processed extract information described tiny pieces 
survey provides problems data stream monitoring techniques developed years help mine data avoiding massive flows information 
particular tutorial introduces fundamental techniques create compact summaries data streams sampling sketching synopsis techniques 
describes extract features clusters association rules 
lastly see methods detect process generating stream evolving indicating important change occurred 
keywords data streams sampling sketches association rules clustering change detection 
years growing interest study analysis data streams flows data large usually impractical store completely data 
analyzed produced high quality results guaranteed matter outcomes observed stream progresses 
tutorial surveys key ideas techniques developed analyze mine massive data streams 
see longer survey algorithmic perspective 
motivation studying data streams comes variety areas scientific data generation satellite observation experiments particles generate terabytes data short amounts time sensor networks may hundreds thousands nodes readings high rate communications networks generate huge quantities meta data traffic passing 
cases information processed analyzed variety reasons monitor system analyze experiment ensure service running correctly 
massive size input typically feasible store convenient access 
operate resources smaller size input sublinear guarantee quality answer particular computations data 
disparate settings general framework study streaming model 
fact variations model depending form input may take algorithm respond 
models arrivals arrivals departures 
basic model data streams arrivals 
stream consists quantity tuples items describe input 
typically tuple simple small object indicate example identity particular object interest weight value associated arrival 
network observation packet interpreted tuple indicating intended destination packet size packet payload bytes 
application packet interpreted tuple identity concatenation source destination packet weight indicating single packet 
typically interpret streams defining massive implicit vectors indexed item names entries usually sum associated counts interpretations may possible 
richer model allows departures additional positive updates entries implicit vector may negative 
captures general situations earlier updates revoked observations negative values feasible 
case assumption tuple input stream processed seen revisited stored explicitly stream algorithm limited internal memory 
randomization approximation 
models natural fundamental questions shown require space linear input answer exactly 
example test separate workshop data stream analysis san italy march streams encode number occurrences item requires store space linear number distinct items immense 
able progress typically allow approximation returning answer correct small fraction error randomization allowing algorithms random choices fail small probability 
algorithms randomization approximation refer approximations 
update time query time space usage 
evaluate algorithms operate streams typically look behavior respect additional features update time time process stream update 
query time time information stored answer question interest 
space usage amount memory algorithm keep information 
typically measured terms parameters stream number tuples number different items parameters 
effective streaming algorithm measures particularly space sublinear ideally poly logarithmic log log constant streaming sketches summaries section outline fundamental approaches coping streaming data drawing representative sample creating compact sketch stream 
random sampling reservoir mining algorithms applied draw representative sample data stream 
question ensure sample drawn uniformly stream continuously growing 
example want draw sample items stream length want sample roughly items 
items arrive ensure probability item sampled 
retain items skewed prefix stream representative 
solutions possible ensure continuously maintain uniform sample stream 
idea reservoir sampling dates back eighties 
easiest describe wish draw sample size 
initialize sample item stream 
replace current sample ith item stream seen throwing random bits simulate coin probability landing heads keeping ith item sampled item observe heads 
simple exercise prove seeing items probability retained sample precisely generalize technique draw sample items replacement performing independent repetitions algorithm replacement picking items replacing ith probability 
drawback approach easily generalize distributed streams wish sample uniformly union 
example consider trying sample stream formed network traffic crossing atlantic pacific oceans 
feasible operate jointly streams 
alternative sampling algorithm refer min wise sampling analogy alternate technique known min wise hashing 
item stream pick random label real number range 
retain item smallest random label seen far 
straightforward observe item equal chance getting smallest tag due symmetry procedure picks uniformly stream 
run algorithm distributed streams merge results get item picked uniformly disjoint union streams picking retained item smallest label 
application estimating entropy 
empirical entropy sequence characters computed finding number occurrences fi character computing fi log fi entropy network monitoring applications detect anomalies 
number possible workshop data stream analysis san italy march items large need different approach approximate entropy 
build estimator entropy follows sampling position stream min wise sampling counting number subsequent occurrences stream character position build unbiased estimate log log estimate reliable improved average repetitions different random samples 
shown give estimator median log repetitions form estimator 
works large small results reliable estimator 
modifications technique generate estimator see details 
sketches estimation data stream problems solved just sample 
data structures effect include contribution entire input just items picked sample 
example consider trying count number distinct objects stream 
easy see items included sample tell distinct 
streaming algorithm gets see item turn better shall see 
refer sketch compact data structure summarizes stream certain types query 
typically linear transformation stream imagine stream defining vector algorithm computes product matrix vector effective matrix small representation defined implicitly hash functions 
highlight popular sketch algorithms count min sketch 
count min sketch array counters size log log hash functions 
update mapped log counters row incremented reflect update 
data structure estimate frequency fi item error probability space log 
flajolet martin sketch 
flajolet martin sketch bitmap length approximately log item mapped hash function entry bitmap probability maps entry entry entry 
item stream map bit hash function set bit 
position significant bitmap indicates logarithm number distinct items seen repetitions randomly chosen hash functions improves accuracy 
space log sufficient approximation ams sketches 
alon matias szegedy sketch described terms count min sketch 
go update counter multiply value update hash function item updated half items mapped hash function half 
sum squares counters row gives high quality estimate sum squares frequency counts 
computation variations thereof heart data stream analyses 
approximation formed space log 
common feature sketch algorithms rely hash functions item identifiers relatively easy implement fast compute 
practical streaming data management systems implement sketches sprint system gigascope operate network data streams gigabit speeds 
implementations sketches web including www cs rutgers edu code index html 
stream data mining algorithms building ideas sampling sketching design algorithms specific analysis data mining tasks 
discuss popular problems association rule mining change detection clustering 
association rule mining classic problem data mining association rule mining 
large collection transactions ti subset possible items example sets items bought supermarket workshop data stream analysis san italy march goal find rules form support rule fraction input contains members rule ti ti 
confidence rule number input transactions contains members rule divided number containing conditions left side ti ti ti 
general seeks find rules support confidence exceeding specific thresholds 
exponentially possible rules careful strategies designed search efficiently 
typically problem reduced finding frequent itemsets subsets items high support threshold 
itemsets association rules determined 
clearly problem especially challenging input transactions observed streaming fashion limited resources available process 
question finding frequent itemsets sets size necessary precursor solving general problem challenge set possible items large attracted significant interest 
sketching techniques outlined applied describe deterministic non randomized approaches 
algorithm shows problem approximated space 
tracks set items associated counts 
item associated counter counter incremented item replaces item smallest count count incremented 
shown simple algorithm gives desired accuracy implemented efficiently 
ways find frequent items extended frequent itemsets 
method outlined manku motwani attempts available space fully possible 
new transaction generates subsets stores compact trie structure 
space full uses pruning algorithm frequent items algorithms delete frequent itemsets track error estimated counts item 
gives efficient somewhat scalable solution general convenient non trivial worst case bound space required accuracy 
variations problem studied finding itemsets correspond ordered subsequences sequential patterns substrings input transactions 
change detection monitoring stream values fundamental question distribution values changed 
want know things changed detect anomalies deviation expected trigger alert 
tell problem data feed caused distribution shift 
built data mining algorithm particular model change indicated model may longer valid need rebuild 
change 
change behaviour frequency subset items change patterns 
take definition underlying distribution frequencies changes 
aim non parametrically explicitly fixing model expect data fit 
propose technique statistical bootstrapping identify change occurred 
consider case input consists series points high dimensional space value categorical 
expect see exact points times space partitioning algorithm window define regions compute relative frequencies region set empirical probabilities window sliding window 
applied fixed window sliding window size points 
test change compute kullback leibler distance kl log 
order test distance significance bootstrapping idea compute distances randomly assigning points windows sets computing distance 
high quantile th percentile distances boundary measured kl distance exceeds steps declare change occurred 
procedure implemented efficiently streaming fashion keeping appropriate data structures observing sliding window advances recompute kl distance scratch compute incrementally previous value operations 
technique turns quite efficient practice requiring tens microseconds update 
extensions variations possible variant formulations change tests kernel methods workshop data stream analysis san italy march clustering notion cluster familiar talk cancer clusters crime clusters indicating high local density events 
formally set items clustering places items similar clusters ensures items different clusters different 
natural try extend clustering stream mean stream large store point cluster allocated 
typically seek number clusters smaller number points clustered 
seeing stream output just clusters mapping points clusters implicit point mapped closest cluster 
give simple example clustering stream optimizing center objective attempting minimize diameter maximum distance points cluster 
algorithm arises guessing diameter clustering value point allocated cluster 
subsequent point stream far existing cluster new cluster containing new point created allocated existing cluster 
guess clusters created 
reasonably close true diameter diameter stream clustering factor best possible cluster radius 
trying different guesses parallel discarding generate clusters build deterministic clustering algorithm 
stream clustering algorithms get complex 
notion core sets small subset input solving problem subset gives approximation solution full input 
hierarchical approach solving problem exactly small subset data fits memory merging solutions get approximate solution full problem 
different techniques needed guarantee results clustering objective functions median means 

agrawal imielinski swami 
mining association rules sets items large databases 
sigmod 

alon matias szegedy 
space complexity approximating frequency moments 
stoc pages 

broder charikar frieze mitzenmacher 
min wise independent permutations 
stoc 

chakrabarti cormode mcgregor 
near optimal algorithm computing entropy stream 
soda 

charikar chekuri feder motwani 
incremental clustering dynamic information retrieval 
stoc 

cormode muthukrishnan 
improved data stream summary count min sketch applications 
journal algorithms 

cormode muthukrishnan zhuang 
divide continuous clustering distributed data streams 
icde 

cranor johnson spatscheck shkapenyuk 
gigascope stream database network applications 
sigmod 

krishnan venkatasubramanian yi 
information theoretic approach detecting changes multi dimensional data streams 
interface 

flajolet martin 
probabilistic counting algorithms database applications 
journal computer system sciences 

manku motwani 
approximate frequency counts data streams 
vldb 

agrawal el abbadi 
efficient computation frequent top elements data streams 
icdt 

muthukrishnan 
data streams algorithms applications 
publishers 

ye bhattacharyya 
general purpose continuous ip backbone traffic analysis platform 
research report rr atl sprint atl 

vitter 
random sampling reservoir 
acm trans 
mathematical software march 
electronic journal symbolic data analysis workshop data stream analysis san italy march 
