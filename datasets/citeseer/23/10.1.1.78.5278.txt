analysis greedy active learning strategy sanjoy dasgupta university california san diego dasgupta cs ucsd edu core search problem active learning schemes better understand extent adaptive labeling improve sample complexity 
give various upper lower bounds number labels need queried prove popular greedy active learning rule approximately strategy minimizing number labels 
increasingly common phenomenon classification tasks unlabeled data abundant labels considerably harder come 
genome sequencing projects instance producing vast numbers peptide sequences reliably labeling structural information requires time close attention 
distinction labeled unlabeled data captured standard models pac framework motivated field active learning learner able ask labels specific points charged label 
query points typically chosen unlabeled data set practice called pool learning :10.1.1.13.8629
creating query points synthetically including rich body theoretical results approach suffers problems practical viewpoint queries produced quite unnatural bewildering human classify second queries picked underlying data distribution limited value terms generalization 
focus pool learning 
interested active learning generalization guarantees 
suppose hypothesis class vc dimension want classifier error rate distribution joint input label space 
theory tells supervised setting need labeled points drawn fixed level confidence henceforth ignore 
get away substantially fewer labels unlabeled points able adaptively choose points label 
fewer querying strategies follow 
toy example illustrating potential active learning 
suppose data lie real line classifiers simple thresholding functions hw hw 
vc theory tells underlying distribution classified perfectly hypothesis called realizable case draw random labeled examples return classifier consistent 
suppose draw unlabeled samples lay points line hidden labels sequence followed sequence goal discover point transition occurs 
accomplished simple binary search asks just log labels 
active learning gives exponential improvement number labels needed adaptively querying log labels automatically infer rest 
generalized binary search 
far looked extremely simple learning problem 
complicated hypothesis classes sort generalized binary search possible 
search space look 
supervised learning realizable case usual bounds specify sample complexity roughly labeled points target error rate 
pick unlabeled points try find hypothesis consistent hidden labels adaptively querying just 
know sauer lemma classify points considered jointly different ways effect size reduced 
finite set effective hypothesis class 
example size corresponding intervals points xi split real line 
possibly learn target hypothesis labels revealed narrow regions 
possible pick possibilities labels 
binary search possible just log labels needed 
unfortunately hope generic positive result kind 
toy example linear separator 
show situation different pick collection unlabeled points unit sphere assume hidden labels correspond perfectly linear separator 
target hypotheses identified querying labels 
active learner required identify exactly right hypothesis close 
little variations don help 
benign situations expect target hypothesis identifiable labels 
put differently worst case target hypotheses active learning gives improvement sample complexity 
hopefully average respect distribution target hypotheses number labels needed small 
instance bad case target hypothesis chosen uniformly random identified querying just log labels expectation 
motivates main model 
average case model count expected number labels queried target hypothesis chosen distribution interpreted bayesian setting accurate think merely device averaging query counts bearing final generalization bound 
natural choice uniform existing active learning schemes reflects underlying combinatorial structure problem hurt deal directly 
chosen mask structure instance set linear separators set convex regions proportional volume region 
problem continuous combinatorial 
expected number labels needed identify target hypothesis chosen 
average case setting possible get away labels sample complexity supervised learning problem defined 
show answer sadly 
benefit active learning really function specific hypothesis class particular pool unlabeled data 
depending expected number labels needed lies range constants ideal case log perfect binary search worst case labels randomly chosen queries notice exponential gap top bottom range 
simple querying strategy achieves close minimum expected number labels minimum number 
main result property holds variant popular greedy scheme ask label evenly divides current effective version space weighted 
doesn necessarily minimize number queries just greedy decision tree algorithm need produce trees minimum size 
uniform expected number labels needed greedy strategy ln times strategy 
give bound arbitrary show corresponding lower bounds uniform non uniform cases 
variants greedy scheme underlie active learning heuristics described optimal literature 
rigorous validation scheme general setting 
performance guarantee significant recall log log minimum number queries possible 
preliminaries input space space labels unknown underlying distribution want select hypothesis function class vc dimension accurately predict labels points assume problem realizable hypothesis gives correct prediction point 
suppose points 
xm ym drawn randomly standard bounds give function want hypothesis error modulo fixed confidence level need pick hypothesis consistent labeled points 
suppose just pool unlabeled data 
xm available 
possible labelings points form subset effective hypothesis class 
xm 
sauer lemma tells 
want pick unique consistent hidden labels querying just 
deterministic search strategy represented binary tree internal nodes queries xi label leaves elements accommodate randomization instance allow random choice query point letting internal nodes tree random coin flips 
main result theorem unaffected generalization 
xm identify target hypotheses need see labels 
bad news claim hypothesis class linear separators set distinct data points perimeter unit circle target hypotheses identified querying labels 
proof 
see consider realizable labelings labeling points negative 
labeling li points negative xi 
impossible distinguish cases seeing labels 

rephrase example terms learning linear separator error suppose input distribution density perimeter unit circle 
matter density target hypotheses force ask labels improvement sample complexity supervised learning 
example bad target hypotheses large imbalance probability mass positive negative regions 
adding extra dimension extra point exactly example modified bad hypotheses balanced 
return original case 
hypotheses lie depth query tree rest 
suppose convenience 
xm clockwise order unit circle 
hij hij labels xi xj positive wraps remaining points negative negative positive 
possible construct query tree hij lies depth log 
target hypothesis chosen uniformly expected number labels queried log place hopes average case analysis 
log 
final hypothesis considered point doesn exactly right hamming distance correct 
similar example forces queries 
main result distribution analyze search strategies number labels require averaged target hypotheses drawn 
terms query trees average depth leaf chosen 
specifically tree leaves include support 
quality tree labels needed leaf depth 
tree average depth 
answer sadly 
claim pick 
input space size hypothesis class vc dimension defined domain property chosen uniform query tree 
proof 
consist points 
xm consist hypotheses positive exactly inputs 
order identify particular element querying method discover exactly points xi nonzero 
construction order queries asked irrelevant 
rest simple probability calculation 
average case model seen example intelligent querying results exponential improvement number labels required help 
generic scheme comes close minimizing number queries minimum number 
natural candidate greedy strategy 
current version space 
unlabeled xi hypotheses label xi positive ones label negative 
pick xi sets largest 
nearly equal mass min show minimizing queries strategy 
theorem distribution suppose optimal query tree requires labels expectation target hypotheses chosen 
expected number labels needed greedy strategy ln minh 
case uniform approximation ratio ln 
show matching lower bounds uniform non uniform cases 
analysis greedy active learner lower bounds greedy scheme greedy approach optimal doesn take account way query search space specifically effect query quality queries 
instance consist dense clusters permits rapid binary search 
version space subregions process ultimately optimal initially slower shrinking hypothesis space alternatives 
concrete example type gives rise lower bound 
claim power concept class hn size uniform optimal tree average height qn log greedy active learning strategy produces tree average height qn log log log 
non uniform greedy scheme deviate substantially optimality 
claim hypothesis class elements distribution ranges value optimal tree average depth greedy tree average depth 
proofs lower bounds appear full available author website 
upper bound overview 
lower bounds quality greedy learner things get worse 
basic argument uniform show optimal tree requires queries expectation query expectation cut chunk mass 
root query greedy tree tg cf 
johnson set cover analysis 
things get trickier try show rest tg uses just queries average may need queries certain hypotheses 
subtrees tg correspond version spaces queries needed roots subtrees cut version space 
worst case model proof approximate optimality known related context saw claim model trivial situation 
average case model especially arbitrary weights require care 
details 
want space discuss issues arise proving main theorem leave actual proof full 
key concept define quality query turns need monotonically decreasing go active learning proceeds version space shrinks 
rules natural entropy notions 
suppose version space possible query xj 
subset labels xj positive ones label negative average probability mass measured eliminated xj say xj shrinks understanding expectation 
shrinkage easily seen monotonicity property need 
lemma xj shrinks shrinks expect optimal tree short query shrinks considerably 
concretely definition shrinkage suggest queries provide shrinkage current version space mass queries needed 
isn entirely true second effect need just query regardless 
roughly speaking lots hypotheses significant mass left effect dominates second takes 
smoothly incorporate effects notion collision probability 
distribution support cp chance random draws identical 
lemma suppose query shrinks 
pick query tree leaves include restriction cp 
corollary pick tree leaves include exist query shrinks cp 
current version space small collision probability query split sizeable chunk form basis proof induction 
cp large say greater 
case mass particular hypothesis exceeds combined shrink just insignificant amount subsequent greedy query iterations greedy queries 
turns roughly number iterations optimal tree needs target greedy procedure reject identify target 
rejected time shrunk considerably 
combining cases cp get lemma proved full yields main theorem immediate consequence 
lemma denote particular query tree query tree 
corresponds subtree ts ts ln minh related promising directions attempting summarize wide range proposed active learning methods instance discuss basic techniques rely :10.1.1.20.8521:10.1.1.16.4036:10.1.1.5.5447
greedy search 
technique abstracted rigorously validated 
foundation schemes cited 
algorithmically main problem query selection rule immediately tractable approximations necessary 
linear separators consists convex sets chosen proportional volume query selection involves estimating volumes convex regions tractable techniques inconvenient 
tong koller investigate margin approximations efficiently computable svm technology 
opportunistic priors 
trick learner takes look unlabeled data places bets hypotheses 
uniform bet leads standard generalization bounds 
algorithm places weight certain hypotheses instance large margin final error bound excellent guessed right worse usual guessed wrong 
technique specific active learning analyzed 

interesting line investigates flexible family priors specified pairwise similarities data points 

bayesian assumptions 
analysis seen sort prior belief assumption nature shares belief particular generalization bound depend 
bayesian assumption immediate benefit active learning stage remaining version space weighted prior largely agreement unlabeled data legitimate output remaining hypotheses 
non bayesian setting legitimate 
hypothesis class consists probabilistic classifiers bayesian assumption way approximate greedy selection rule map estimate expensive summation posterior 

terms theoretical results considers tradeoff labels generalization error greedy scheme realized sampling analyzed bayesian setting 
authors show possible achieve exponential improvement number labels needed learn linear separators data target hypothesis chosen uniformly unit sphere 
intriguing question holds general data distributions 
directions 
looked case acceptable error rate fixed goal minimize number queries 
fixing number queries asking best average error rate possible 
words query tree fixed depth leaf annotated remaining version space treating element point predictions pool data error leaf depends hamming diameter querying strategy producing low diameter leaves 
widely classifiers linear separators 
existing active learning schemes ignore rich algebraic structure arrangement hyperplanes 

am grateful anonymous nips reviewers careful detailed feedback 
angluin 
queries concept learning 
machine learning 
angluin 
queries revisited 
proceedings twelfth international conference algorithmic learning theory pages 
baum lang 
query learning poorly human oracle 
international joint conference neural networks 
bjorner las sturmfels white ziegler 
oriented matroids 
cambridge university press 
cohn ghahramani jordan 
active learning statistical models 
journal artificial intelligence research 
dasgupta long lee 
theoretical analysis query selection collaborative filtering 
machine learning 
freund seung shamir tishby 
selective sampling query committee algorithm 
machine learning 
johnson 
approximation algorithms combinatorial problems 
journal computer system sciences 
kearns vazirani 
computational learning theory 
mit press 
mccallum nigam :10.1.1.13.8629
employing em pool active learning text classification 
fifteenth international conference machine learning 
roy mccallum 
optimal active learning sampling error reduction 
twentieth international conference machine learning 
shawe taylor bartlett williamson anthony 
structural risk minimization data dependent hierarchies 
ieee transactions information theory 
tong koller 
support vector machine active learning applications text classification 
journal machine learning research 
zhu lafferty ghahramani 
combining active learning semi supervised learning gaussian fields harmonic functions 
icml workshop 
