umass tdt margaret connell ao feng raghavan shah james allan center intelligent information retrieval department computer science university massachusetts amherst ma connell allan cs umass edu 
center intelligent information retrieval umass amherst submitted runs tasks hierarchical topic detection topic tracking new event detection link detection 
describe models experiments training results tasks 

hierarchical topic detection task replaces topic detection previous tdt evaluations 
year htd existing results compare 
vector space model baseline bounded clustering reduce time complexity simple parameter tuning 
model description topic detection classifies stories different topics htd requires 
entities story topic 
views topic structure inter related events gives hint new task 
experiments show time locality useful attribute event organization help solve complexity problem tdt 
tdt collection contains stories different languages running time traditional clustering algorithms take acceptable huge collection 
know stories event tend close time need compare story local stories collection 
algorithm steps bounded nn event formation bounded agglomerative clustering building hierarchy 
step stories original language source taken time ordered 
stories processed incoming story compared certain number stories 
number approximately number stories token file value baseline run 
similarity cosine similarity tf idf term vectors current story similar previous story larger threshold baseline run current story assigned event similar previous submission tdt workshop story belongs 
new event created 
step list events source language class 
events class sorted time order time stamp story 
bounded agglomerative clustering events works 
take certain number events number called default value sorted events list 
iteration find closest event pair combine event earlier 
process continues branch branch iterations number clusters left branch 
take half get new events cluster branch clusters left 
process continues events clustered 
number clusters left branch th events number 
shows get optimal branching factor 
evaluation cost parameters year optimal value take branch baseline run 
list clusters clustered bounded agglomerative clustering algorithm shown 
process continues number clusters smaller value proportional square root number stories class 
clusters language difference sources combined sorted time order single round clustering performed 
clusters languages mixed clustered cluster left root node hierarchy multi lingual experiment machine translation arabic mandarin stories simplify similarity calculation 
clusters containing story 
stories non leaf nodes allowed evaluation cluster replaced corresponding story beneficial travel cost 
training tdt collection closest tdt time training corpus 
tdt contains newswire broadcast stories tdt newswire 
order comparable take newswire stories tdt corpus includes nyt ann afp xin 
lot parameters tuned training set 
branch average branching factor bounded agglomerative clustering algorithm 
determines height hierarchy 
threshold threshold event formation decide new event created 
agglomerative clustering algorithm source stops number clusters smaller controls mix clusters different sources 
maximal window size agglomerative clustering 
reduce time complexity agglomerative clustering take forever 
story compared stories nn event clustering 
idea comes time locality event threading 
sim similarity function comparing term vectors 
preliminary experiments show cluster sizes vary lot algorithm 
clusters close root node contains thousands stories lot clusters singletons 
nn agglomerative clustering algorithms favor large clusters 
hierarchy balanced modified similarity calculation give smaller clusters chances 
similarity cluster centroids number stories cluster events time ordered story constant control favor smaller clusters get 
parameters tuned training set 
results year submitted runs condition 
baseline run parameters run shown 
knn bound number previous stories compared incoming story knn algorithm 
default value 
sim similarity function 
cosine similarity cluster centroid simply story vector story comparison 
weigh vector weighing scheme 
idf number stories collection df number stories term appears 
thresh threshold knn 
default 
maximal number clusters agglomerative clustering 
baseline run 
branch average branching factor 
take optimal value evaluation parameters 
constant decides clusters different sources mixed 

similar similarity function normalized cluster size see equation 
difference increases umass 
table shows performance runs tdt table contains results official tdt submission 
normalized travel cost different versions normalization schemes 
normalization factor old scheme expected travel cost reach leaf node minimal spanning tertiary tree 
new scheme assumes level dag 
system tdt mul eng output table umass htd trained tdt system tdt eng nat tdt mul eng output table umass htd tdt old evaluation plan strict scheme 
system pay attention branching factor penalized lot 
chuk ict get normalized travel cost respectively mul eng run old normalization scheme 
tno job 
normalization cost old normalization 
new easier achieve 
easy normal hierarchy beat normalized travel cost sense 
large travel cost normalization 
travel cost smaller detection cost encourage everybody focus detection cost 
overlapping clusters allowed probably lead highly overlapping clusters 
get power set coming extreme case 
think result want see starting htd 
opinion prefer old normalization scheme 
changes needed detection cost travel cost comparable 
normalized travel cost old scheme larger detection cost set smaller weight smaller 
second binary tree wins year maximal height real systems deep hierarchy 
suggest increase decrease push optimal branching factor 
umass year evaluation mainly high detection cost 
factors may useful improve performance 
small branching factor reduce detection cost travel cost 
smaller branching factor get clusters different granularities chance match topic 
optimal branching factor shows advantage small branching factor reducing travel cost 
experiments limited branching factor proved assumption 
assigning story multiple clusters decrease rate dramatically 
tno observed experiments 
assumption temporal locality useful event threading topics 
experiments submission show larger window size improve performance 
source language clustering hierarchy assumes stories source topic result worse 
removing restriction shows improvement submitted runs 
year submission tno better participants detection cost travel cost 
participant allows story assigned clusters may useful reduce detection cost 
binary tree may reason small detection cost 

topic tracking creating training newswire corpus created subset tdt corpus emulates characteristics tdt corpus close possible 
known participants priori topics tdt english stories test set rest topics multi lingual stories 
emulate statistics sorted tdt topics number test stories english sources 
collected top topics highest number english test stories english topics 
removed non english stories topics 
remaining topics tdt sampled topics follows 
topic estimated standard deviation number stories language chose top ones standard deviation minimum number stories language 
topics expected contain nearly equal proportion test stories languages expected imitate multilingual topics test corpus 
idea information language specific distribution stories multi lingual topics tdt corpus safest assume uniform distribution 
formed training topics english topics rest multi lingual 
unsupervised tracking topic tracking task small number training stories particular topic 
topic represented centroid average vector representatives training stories 
incoming stories compared centroid topic 
cosine similarity story centroid exceeds threshold story considered topic topic 
linking task different thresholds chosen condition 
determined experiments run newswire subsample tdt corpus explained previous section tdt training data 
tdt tracking task submitted runs required conditions 
tracking runs deferral 
tracking system parameters model translation condition number terms vector threshold value adaptation carried 
story link detection experimented models traditional vector space model relevance models 
adaptation allows addition incoming story topic representation recomputes topic centroid 
adaptation uses additional threshold determines new story similar centroid added topic 
new centroid computed story vectors time new story added topic 
adaptation number terms added model updated variable parameter 
stories exist english arabic mandarin english translations english mandarin compare chinese stories arabic stories means compare native language trans lated form 
translation condition indicates comparisons english native language 
submitted runs 
tf idf umass method uses vector space model 
cross language similarities calculated provided english translations incrementally updated corpus statistics 
vector length constrained terms 
centroids adapted 
threshold 
threshold trained specially sampled portion tdt corpus explained section 
tfidf adaptation umass method similar baseline case described included adaptation 
seen experiments tdt adaptation degrade performance vector terms 
happens adaptation may allow inclusion new stories stray topic include confusing terms 
limited number terms centroid top highest weighted terms terms 
maximum stories added centroid vector 
required conditions manual transcription adapting threshold threshold 
numbers obtained training training data obtained explained section 
tf idf language specific comparisons adaptation umass run chose comparisons stories native language possible 
training stories english start stories arrived compared english training stories 
chinese story arabic story highly similar centroid vector english arrives seed chinese arabic centroid vector explained 
training story system created centroid vectors centroid collection called global centroid native english stories 
incoming story native english system calculated similarity english centroid system calculated similarity global centroid 
initially centroid native arabic mandarin stories 
determine start new arabic mandarin centroid topic global threshold topic 
threshold average similarity pairs stories global centroid 
global centroid story threshold set experiments tdt data 
similarity incoming story global centroid greater threshold incoming story new native centroid 
native topic centroids established adaptation threshold computed average similarity pairs stories native centroid 
threshold changes story added centroid 
incoming native english stories compared english statistics incrementally updated stories taken native english sources 
case story considered update incoming story 
top weighted terms vector considered maximum stories added centroid vector 
required conditions threshold adaptation threshold initialized 
thresholds trained specially sampled portion tdt corpus refer section 
submitted run threshold system year 

relevance models adaptation umass application relevance models topic tracking somewhat different 
wanted take advantage streaming nature tracking task re cast relevance models terms unsupervised topic adaptation 
denote topic tracked subsequent story stream 
similarity topic measured asymmetric clarity adjusted divergence language models background probability language model story refers relevance model topic computational constraints chose estimate relevance model smoothed maximum likelihood ml estimates 
hand constructed incrementally fashion 
start setting tion zero 
initialize equa training documents start tracking similarity story exceeds pre defined threshold update topic model 
update involves setting original training stories stories scored adaptation threshold keep zero documents recom pute allow equation adaptations 
systems enhancements years tdt tracking research results graph shows det curves umass umass umass training corpus consists newswire stories sampled section 
find little improvement due adaptation tf idf model 
det curve relevance model general better reflected min cost 
supervised tracking supervised tracking assumption stories training stories test stories arrive stream 
test story system output confidence score decision just unsupervised tracking 
story delivered assumption user reads immediately provides relevance judgment immediately 
evaluation addition evaluated min cost evaluated su metric trec filtering runs 
supervised tracking runs lot approaches unsupervised tracking certain modifications fact relevance judgments obtained document delivered 
probability false alarms probability random performance unsup db adapt topic weighted curve nt unsup db adapt tw min det norm cost unsup db adapt topic weighted curve nt unsup db adapt tw min det norm cost unsup db rm topic weighted curve nt unsup db rm tw min det norm cost det curve unsupervised tracking results adaptation unsupervised tracking supervised tracking danger adapting documents false alarms 
supervised tracking adapt threshold document delivered document relevant topic 
adapt top terms 
evidence advantages supervision probability 
impact supervision min cost experiment constructed supervised version umass tf idf adaptation called umass compared best performing unsupervised system 
get det curves corresponding clearly demonstrates impact supervision 
false alarms probability random performance sup db tf idf adapt topic weighted curve nt sup db tf idf adapt tw min det norm cost unsup db rm topic weighted curve nt unsup db rm tw min det norm cost det curve supervised unsupervised 
impact language specific comparisons supervised learning study effect native language hypothesis creating supervised version umass tf idf native models adaptation called umass 
det curve comparing umass umass shown 
negative feedback supervised tracking obtain judgments document delivered 
get judgments hits false alarms 
adapt negative feedback scheme rocchio 
probability false alarms probability random performance sup db adapt topic weighted curve nt sup db adapt tw min det norm cost sup db topic weighted curve nt sup db tw min det norm cost det curve native language comparisons centroid vector document delivered 
document topic document topic 
negative feedback useful improving su metric 
incrementing threshold runs played idea incrementing threshold system doing badly 
basically system delivering large number non relevant documents user incremented threshold 
specifically ratio relevant non relevant documents delivered fell incremented threshold 
incrementing threshold helped improve su cost 
systems submitted submitted runs official evaluation 
cases training done specially sampled newswire portion tdt corpus described 

relevance models umass identical system umass unsupervised tracking adaptation basis relevant documents 

relevance models increase thresholds umass identical system described 
addition threshold incremented described 
tf idf adaptation negative feedback increase threshold umass vector space system database umass unsupervised tracking 
addition negative feedback increase thresholds described implemented 
identical umass described negative feedback increase thresholds implemented 
training set umass umass performed reasonably metrics su mincost 
results discussed section 
results discussion table shows mincost su development set test set tdt data 
find systems perform metric perform 
system su mincost su mincost dev dev test test umass umass umass umass umass highest su tdt set obtained umass 
systems obtained reasonably high su utilities systems quite low min costs 
umass best system metrics done su utility mincost 
baseline runs represent story vector term space coordi nate represents weight particular term story 
weight calculated usual product defined number times term occurs story row term frequency document length average length documents 
formula 
number stories collection contain occurrences terms total number stories collection 
parameters calculated incrementally deferral limit computed total number documents seen far time deferral period 
runs deferral 
start tracking task creating cluster training stories particular topic 
runs considering story training 
centroid cluster vector representation training story 
case baseline adaptation cluster change 
incoming stories compared centroid training cluster measuring similarity 
cosine similarity function simply measures inner product 
tf idf adaptation language specific comparisons umass vectors 
vector normalized unit length iden identical umass unsupervised tracking tical number terms 
terms ranked vector space model databases ex weights 
number unique terms exceeds vector cept adaptation done relevant stories 
length system select top number terms centroid incoming story vectors 

tf idf adaptation language specific comparisons baseline run single database sto negative feedback increase threshold umass ries english 
original arabic mandarin stories translated provided machine translators 
stories reduced root form krovetz stemmer dictionary stemmer 
words removed standard inquery words list 

new event detection basic model cosine similarity equation metric determine similarity story story arrived earlier 
belief novelty story taken similarity closest neighbor past 
preprocessing version open source lemur system tokenize data remove words stem create document vectors 
words included list inquery stem stemming algorithm implementation provided part lemur 
named entities extracted bbn identifinder 
collection wide statistics incremental start empty collection update statistics stories arrive 
maximum decision deferral period source files threshold decisions set 
model run tdt collection trained english newswire stories previous tdt collections 
example english newswire stories tdt tdt tdt training model runs tdt 
systems fielded umass model story overlap named entities non named entities closest matching stories table measured 
scores evidence original confidence score assigned story modified 
final confidence score story calculated formula 
www cs cmu edu lemur stories cosine similarity computed similar terms named terms excluding story document entities named entities table features determine final confidence score story stories ordered terms decreasing cosine similarity probability umass system run tdt false alarms probability random performance baseline topic weighted curve tw min det norm cost topic weighted curve system umass tw min det norm cost det curves comparing performance baseline cosine system umass 
formula designed boost confidence scores old stories leave new story confidence scores untouched 
large number old stories low confidence scores due reasons documents multiple topics topic diffuse 
step 
checks original confidence score low warrant investigation 
possible stories topic linked named entities non named entities step 
designed detect groups terms form link stories 
done original confidence score doubled 
steps 



check see stories share high degree similarity due named entities entities 
case confidence scores slowly reduced back original value 
numerical values formula obtained training data 
shows performance umass tdt 
table summarizes performance systems tdt collection 
table summary performance umass systems tdt collection 
umass umass umass umass topic weighted minimum cost umass story compared stories preceding 
highest cosine similarity returned confidence score story old 
umass story compared maximum preceding stories highest coordination match 
highest cosine similarity returned confidence score story old 
umass umass similar umass considers closest matches uses different formula 
rationale 

link detection tdt link detection cross lingual task existing tdt system see performed new tdt data 
methods story representation similarity measure run 
vector space model cosine similarity second relevance modeling 
models fully documented 
compare stories link detection cosine similarity vector space tdt system previous years 
story represented vector terms term weights number occurrences term story total number documents collection number documents containing term 
collection statistics computed incrementally documents stream deferral period test story arrives 
deferral period link detection 
relevance models statistical technique estimating language models small samples text 
suppose short string text large collection documents language model estimated relevance model mixture language models document collection 
document models weighted posterior probability producing query posterior computed follows effect equation assigns high weights doc uments generated relevance model interpreted massive query expansion technique 
apply relevance modeling story link detection represent story small number terms find relevance model story measure similarity models 
story pruned terms 
terms lowest probability occurring chance picking words randomly collection hypergeometric distribution 
length story number times word number times occurs collection statistics computed incrementally documents stream deferral period test story arrives 
deferral period link detection 
estimated relevance models stories similarity models measured symmetrized clarity adjusted divergence 
occurs size collection total refers relevance model estimated story background general english probability computed entire collection deferral period 
roi post processing experiments tried omitted 
important issue multi language link detection best compare stories 
comparisons stories english provided machine translated versions mandarin arabic stories 
alternative compare stories original language possible 
choice belief similarity measure accurate possible errors translation avoided 
stories pair originally english similarity comparison done english 
comparison done chinese chinese comparison done arabic stories arabic 
cross language comparisons done english 
corpus statistics incrementally updated separately language 
english stories lower cased stemmed kstem stemmer 
words removed 
native arabic stories converted unicode utf windows cp encoding normalized stemmed light stemmer 
words removed 
native mandarin stories converted unicode utf gb 
stopped list chinese words set stopping rules 
bigrams character pairs 
submissions 
baseline umass run vector space model stories machine translated english 
vector constrained highest terms 
story vectors weighted tf idf weighting scheme 
threshold determined training newswire portion tdt corpus set 
deferral period 

relevance model english umass rm run language model approach stories machine translated english 
number words story pruned hypergeometric distribution 
threshold determined training newswire portion tdt corpus set deferral period 

native language comparisons umass run vector space model stories compared native language possible 
threshold determined training newswire portion tdt corpus set 
deferral period 

native language comparisons relevance model umass drm run language model approach stories compared native language possible 
number words story pruned hypergeometric distribution collection appropriate language 
threshold determined training newswire portion tdt corpus set 
deferral period 
shows comparative performance submitted conditions tdt newswire data 
relevance modeling improves performance vector space model comparisons native language stories possible improves performance comparisons stories english 
clear picture relevance modeling native language comparisons improves performance reinforcing findings tdt data 
minimum cost cost detection algorithm tdt tdt tdt cosine tfidf relevance model cosine tfidf relevance model table performance different algorithms link detection 
results probability link tdt cos rm cos rm random performance false alarm probability det curve story link detection tdt newswire training data probability tdt link cos rm cos rm random performance false alarm probability det curve story link detection tdt data table show somewhat different pattern tdt tdt 
minimum cost lower tdt 
relevance model native languages outperforms tdt 
clear graph native languages vector space model win suspicious results 
investigation chinese segmenter native mandarin stories prior stopping transforming bigrams native language comparisons improve performance 
clear relevance modeling general win 
looks low thresholds 
choices tdt thresholds tdt training runs low 
table shows minimum cost corresponding threshold runs revised data 
graph submitted conditions native mandarin stories revised seen 
see native language comparisons give better results comparisons vector space model relevance model case 
tdt relevance model outperform vector space model high recall probability low 
probability link tdt revised cos rm cos fix rm fix random performance false alarm probability det curve revised story link detection tdt data minimum cost threshold detection algorithm tdt tdt cosine tfidf relevance model cosine tfidf relevance model table revised performance different algorithms link detection 
results acknowledgments supported part center intelligent information retrieval part sd number 
opinions findings recommendations expressed material author necessarily reflect sponsor 

allan feng 
flexible intrinsic evaluation hierarchical clustering tdt 
proc 
acm twelfth international conference information knowledge management pages nov 
james allan alvaro margaret connell steve townsend ao feng feng larkey victor lavrenko raghavan 
umass tdt research summary 
proceedings tdt evaluation unpublished 
james allan jamie callan feng malin 
inquery trec 
proceedings trec 
james allan jin rajman wayne gildea lavrenko 
topic novelty detection 
summer workshop 
daniel bikel richard schwartz ralph weischedel 
algorithm learns name 
machine learning 
thorsten brants chen 
system new event detection 
proceedings th annual international acm sigir conference research development retrieval pages 
acm press 
james callan bruce croft stephen harding 
inquery retrieval system 
proceedings dexa rd international conference database expert systems applications pages 
robert krovetz 
viewing morphology inference process 
proceedings th annual international acm sigir conference research development information retrieval pages 
acm press 
larkey feng margaret connell victor lavrenko 
language specific models multilingual topic tracking 
proceedings sigir pp 
sheffield england 
ramesh ao feng peng james allan 
event threading news topics 
acm thirteenth international conference information knowledge management 
