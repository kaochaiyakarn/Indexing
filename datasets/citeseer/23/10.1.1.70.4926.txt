gaussian process latent variable models visualisation high dimensional data neil lawrence department computer science university sheffield regent court portobello street sheffield dp neil dcs shef ac uk introduce new underlying probabilistic model principal component analysis pca 
formulation interprets pca particular gaussian process prior mapping latent space observed data space 
show prior covariance function constrains mappings linear model equivalent pca extend model considering restrictive covariance functions allow non linear mappings 
general gaussian process latent variable model evaluated approach visualisation high dimensional data different data sets 
additionally non linear algorithm leading twin kernel pca mapping feature spaces occurs 
visualisation high dimensional data achieved projecting data set lower dimensional manifold 
linear projections traditionally preferred due ease computed 
approach visualising data set dimensions project data principal components 
forced choose priori components project sensibly choose associated largest eigenvalues 
probabilistic reformulation principal component analysis pca informs choosing components choice maximises likelihood data 
integrating latent variables optimising parameters probabilistic pca ppca formulated latent variable model set centred dimensional data denoting latent variable associated data point may write likelihood individual data point ppca model gaussian distributed unit covariance maximising likelihood data set solution design matrix 
probabilistic principal component analysis latent variable models factor analysis fa independent component analysis ica require marginalisation latent variables optimisation parameters 
consider dual approach optimising probabilistic model turns equivalent pca 
integrating parameters optimising latent variables specifying prior distribution row matrix integrating obtain marginalised likelihood th corresponding log likelihood tr tr parameters marginalised may focus optimisation likelihood respect gradients respect may implies solution algebraic manipulation formula leads matrix dimension latent space columns eigenvectors diagonal matrix th element th eigenvalue arbitrary orthogonal matrix note eigenvalue problem developed easily shown equivalent solved pca see formulation pca manner key step development kernel pca replaced kernel 
probabilistic pca model shares underlying structure differs optimise marginalise marginalise optimise 
marginalised likelihood optimising recognised product independent gaussian processes linear covariance function natural extension non linearisation mapping latent space data space non linear covariance function 
solution solution dependent disregard 
independent component analysis correct rotation matrix placed constraints orientation axes matrix recovered 
gaussian process latent variable models saw previous section pca interpreted gaussian process mapping latent space data space locale points latent space determined maximising gaussian process likelihood respect refer models class gaussian process latent variable models 
principal component analysis process prior inner prod uct matrix section develop alternative considering prior allows non linear processes specifically focus popular rbf kernel takes form element th row th column scale parameter denotes kronecker delta 
gradients respect latent points combining chain rule 
gradients may combination non linear optimiser scaled conjugate gradients scg obtain latent variable representation data 
furthermore gradients respect parameters kernel matrix may computed jointly optimise solution naturally unique linear case described solution subject arbitrary rotation may expect multiple local minima 
illustration scg illustrate simple gaussian process latent variable model turn multi phase oil flow data 
twelve dimensional data set containing data known classes corresponding phase flow oil pipeline stratified annular homogeneous 
illustration computational reasons data sub sampled data points 
shows visualisations data pca algorithm required iterations scg 
positions model initialised pca see www dcs shef ac uk neil matlab code 
gradient optimisation rbf latent space shows results clearly superior terms greater separation different flow domains achieved linear pca model 
additionally gaussian process perform mapping means uncertainty positions points data space 
formulation level uncertainty shared dimensions may visualised latent space 
subsequently done varying intensity background pixels 
unfortunately quick analysis complexity algorithm shows gradient step requires inverse kernel matrix operation rendering algorithm impractical data sets interest 
strictly speaking model represent mapping gaussian process maps distribution data space point 
apparent weakness model may easily rectified allow different levels uncertainty output dimension constrained model allows visualise uncertainty latent space preferred 
visualisation oil data pca linear uses rbf kernel 
crosses circles plus signs represent stratified annular homogeneous flows respectively 
plot indicate precision manifold expressed data space latent point 
optimised parameters kernel practical algorithm main components revised computationally efficient optimisation process sparsification 
kernel methods may sped sparsification representing data set subset points known active set 
remainder inactive set denoted informative vector machine selects points sequentially reduction posterior process entropy induce 
latent variable optimisation 
point inactive set shown project data space gaussian distribution mean denotes kernel matrix developed active set column vector consisting elements th column correspond active set 
variance note appear inverse gradients respect depend data independently optimise likelihood respect full set optimised pass data 
kernel optimisation 
likelihood active set optimised respect gradient evaluations costing algorithm summarises order implemented steps 
note whilst optimise points active set repeatedly active set practice looked map solutions optimisations specifying unit covariance gaussian prior matrix respectively 
algorithm algorithm modelling 
require size active set number iterations initialise pca 
iterations select new active set ivm algorithm 
optimise respect parameters scaled conjugate gradients 
select new active set 
point active set optimise respect scaled conjugate gradients 
points remain original location 
experiments follow iterations active set size experiments run shot basis statements effects significant modification parameters 
results data sets oil flow data previous section available points include comparison generative topographic mapping gtm 
full oil flow data set visualised gtm latent points laid grid rbf nodes rbf 
parameters latent variable model notice gtm artificially latent space locations latent points 
follow visualisation sub set digits digit greyscale version usps digit data set 
modelled face data set consisting images video sequence digitised images originally video sequence expect underlying dimensionality data images produced smooth way time thought piece string embedded high dimensional pixel space 
ordered results visualisation code performing experiments available fromhttp www dcs 
shot mean algorithm experiment run setting random seed values 
producing visualisation dataset leave open criticism shot result lucky 
data sets follows shot approach problems multiple local minima removes temptation preferentially selecting results 
digit images visualised latent space 
followed plotting images random order plotting image overlap existing image 
digits plotted 
note little space taken ones thin line running visualisation may contrasted visualisation similar data set 
suggest ones easier model require large region latent space 
shef ac uk neil avi video files visualisation results experiments data model digits model faces 
discussion empirically rbf model gives useful visualisations range datasets 
strengths method include ability optimise kernel parameters generate fantasy data point latent space 
probabilistic process obtain error bars position manifolds visualised imposing greyscale image latent space 
kernels collide twin kernel pca eigenvalue problem provides maxima respect linear kernel exploited kernel pca 
consider twin kernel pca replaced kernel functions 
twin kernel pca longer undertaken eigenvalue decomposition algorithm suitable mechanism determine values parameters kernel 
top fantasy faces model face data 
faces created uniformly spaced ordered points latent space visualising mean distribution data space 
plots show sequence unfolding starting top left moving right 
ideally transition images smooth 
bottom examples data set closest corresponding fantasy images latent space 
full sequences entire dataset available web avi files 
stochastic neighbor embedding 
consider written introduced vector length constant redefined entropy may add obtain kl recognised kullback leibler kl divergence distributions 
stochastic neighbor embedding sne minimises kl divergence visualise data 
sne vector discrete 
generative topographic mapping 
generative topographic mapping radial basis function network perform mapping latent space observed space 
marginalisation latent space achieved expectation maximisation computing entropy requires full rank true general forced adding jitter 
em algorithm 
radial basis function network special case generalised linear model interpreted gaussian process 
interpretation gtm particular covariance function 
special feature gtm manner latent space represented set uniformly spaced delta functions 
view having delta function associated data point positions delta functions optimised gtm data point associated different fixed delta functions 
new class models probabilistic modelling visualisation high dimensional data 
provided strong theoretical grounding approach proving principal component analysis special case 
real world data sets showed visualisations provided model cluster data reasonable way 
model advantage various spectral clustering algorithms years common gtm truly generative underlying probabilistic interpretation 
suffer artificial suffered gtm 
theoretical analysis suggested novel non linearisation pca involving kernel functions 
aaron hertzmann comments manuscript 
becker thrun obermayer editors 
advances neural information processing systems volume cambridge ma 
mit press 
bishop james 
analysis multiphase flows dual energy gamma neural networks 
nuclear instruments methods physics research 
bishop svens williams 
gtm principled alternative self organizing map 
advances neural information processing systems volume pages 
mit press 
bishop svens williams 
gtm generative topographic mapping 
neural computation 
hinton roweis 
stochastic neighbor embedding 
becker pages 
lawrence seeger herbrich 
fast sparse gaussian process methods informative vector machine 
becker pages 

netlab algorithms pattern recognition 
advances pattern recognition 
springer berlin 
code available www ncrg aston ac uk netlab 
roweis saul hinton 
global coordination local linear models 
dietterich becker ghahramani editors advances neural information processing systems volume pages cambridge ma 
mit press 
sch lkopf smola 
ller 
kernel principal component analysis 
proceedings international conference artificial neural networks icann page lausanne switzerland 
tipping 
sparse kernel principal component analysis 
leen dietterich tresp editors advances neural information processing systems volume pages cambridge ma 
mit press 
tipping bishop 
probabilistic principal component analysis 
journal royal statistical society 
