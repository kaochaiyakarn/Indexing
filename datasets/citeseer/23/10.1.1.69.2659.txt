scidac iop publishing journal physics conference series doi understanding failures computers schroeder garth gibson computer science department carnegie mellon university forbes ave pittsburgh pa usa mail cs cmu edu garth cs cmu edu 
computers year away pressing need anticipate compensate probable increase failure application interruption rates 
researchers designers integrators available far little detailed information failures interruptions smaller computers experience 
information available suggests application interruptions far common coming decade largest applications may large fractions computer resources checkpoints restarting checkpoint interruption 
reviews sources failure information compute clusters storage systems projects failure rates corresponding decrease application effectiveness discusses coping strategies application level checkpoint compression system level process pairs fault tolerance supercomputing 
need public repository detailed failure interruption records particularly concerning projections architectural family machines widely disputed 
introduces computer failure data repository issues call failure history data publish 

hardest problems high performance computing hpc installations avoiding coping recovering failures 
coming petaflops clusters require simultaneous control hundreds thousands millions processing storage networking elements 
large number elements involved element failure frequent making increasingly difficult applications forward progress 
success computing depend ability provide reliability availability scale 
researchers practitioners spent decades investigating approaches avoiding coping recovering failures progress area hindered lack publicly available failure data real large scale systems 
collected analyzed number large data sets failures high performance computing hpc systems 
data sets cover node outages hpc clusters failures storage systems 
data sets large scale trends assumptions commonly applied computing systems design project potential machines decade expectations failure rates mean time application interruption consequential application utilization full machine checkpoint restart fault tolerance balanced system design method matching storage bandwidth memory size aggregate computing power 
surprisingly growth aggregate computing power continues growth computing power computer resources may spent conventional fault recovery methods 
envision applications denied half system resources years example 
discuss alternative actions may compensate unacceptable iop publishing percentage scidac iop publishing journal physics conference series doi systems hardware software network environment human unknown percentage systems hardware software network environment human unknown 
breakdown failures root cause 
bar shows breakdown systems particular hardware platform right bar shows aggregate statistics lanl systems 
trend 
briefly discuss efforts publicly available data sets computer failure data repository hosted usenix 
increasing importance frequent failures application execution assert computer application designers need ready access raw data describing computer failures occurred existing machines 

data sources 
node outages hpc clusters primary data set obtained collected los alamos national laboratory lanl covers high performance computing systems including total machines processors 
systems smp clusters processors node comprising total nodes processors 
remaining clusters consist numa boxes processors adding total nodes processors 
data contain entry failure occurred year time period resulted application interruption node outage 
data covers aspects system failures software failures hardware failures failures due operator error network failures failures due environmental problems power outages 
failure data includes start time time system node affected categorized root cause information 
best knowledge largest failure data set studied literature date terms time period spans number systems processors covers publicly available researchers see raw data 
section provide results analyzing data 

storage failures interest large scale cluster failure originates key role high bandwidth storage checkpoint restart strategies application fault tolerance 
part larger effort doe scidac ii data storage institute anticipate explore challenges storage systems computing 
storage failures masked interrupting applications raid technology reconstructing failed disk impact storage performance noticeably failures occur storage system recovery tools take days bring large filesystem back online user precious data 
disks traditionally viewed reliable hardware component due mechanical aspects disk 
able obtain data sets referred hpc hpc describing disk drive failures occurring hpc sites data sets referred com com collected large internet service provider 
data sets vary duration month years cover total hard drives different vendors include scsi fibre channel disk drives 
provide results analyzing data section 
detailed results see 
scidac iop publishing journal physics conference series doi number failures year year system introduced number failures year processor year system introduced 
average number failures lanl system year 
average number failures system year normalized number processors system 
systems hardware type color 

cluster node outages 
common root cause node outages 
question ask causes node outage 
lanl data provides root cause breakdown failures human environment network software hardware unknown 
left shows relative frequency high level root cause categories 
hardware single largest component failures assigned category 
software second largest contributor failures sites attributed software 
trends similar look right fraction total repair time attributed different root cause categories 
important note number failures undetermined root cause significant 
fraction hardware failures larger fraction undetermined failures fraction software failures close undetermined failures conclude hardware software largest contributors failures 
conclude failure sources human environment network insignificant 

nodes fail 
question frequently node outages occur words long expect application run interrupted node failure 
left shows average number node failures observed year lanl systems 
systems hardware technology chip model shaded color 
graph indicates failure rate varies widely systems failures year system failures year 
note failure rate failures year means application running nodes system interrupted forced recovery times day 
applications running systems require large number nodes weeks computation complete failure recovery frequent events application execution 
wonder causes large differences failure rates different systems 
turns main reason differences systems vary widely size 
shows system average number failures year normalized number processors sockets system 
normalized failure rates show significantly variability systems leads interesting propositions 
failure rate system grows proportional number processor chips system 
second little indication systems hardware get reliable time technology changes 
shows systems sorted time went production normalized failure rates exhibit increasing decreasing trend moving oldest system system 
scidac iop publishing journal physics conference series doi annual replacement rate 
arr arr arr hpc hpc hpc hpc com com com 
comparison datasheet solid dashed line graph observed field 
bar graph corresponds drive population 
dotted line represents weighted average data sets 

storage failures 
drives need replaced 
common way estimate rate drive replacement mean time failure mttf inverse annual failure rate afr specified drive datasheet provided vendor 
modern drives datasheet typically range hours suggesting annual failure replacement rate arr 
data field experience disk replacements differs datasheet specifications disk reliability 
shows annual failure rate suggested horizontal solid dashed line observed data sets weighted average arr disks years old dotted line 
hpc hpc hpc com cover different types disks graph contains bars type disk 
shows significant discrepancy observed arr datasheet afr data sets 
datasheet high 
observed data set type factor higher datasheet 
average arr data sets weighted number drives data set 
removing com data exhibits highest average arr times higher 
natural question arises observed disk replacement rates higher field datasheet mttf suggest drives years operation 
discussions vendors customers identified number possible reasons 
customers vendors agree definition drive faulty 
fact disk replaced implies failed possibly customer specific health test 
health test conservative lead replacing drive vendor tests find healthy 
second datasheet typically determined accelerated stress tests certain assumptions operating conditions disks temperature stay threshold workloads duty cycles powered hours patterns certain data center handling procedures followed 
practice operating conditions ideal assumed tests determine datasheet 

drive age affect replacement rates 
disk replacement rates commonly assumed vary system life cycle drives age 
year drive operation commonly characterized early failures infant mortality 
years failure rates expected approximately steady state years wear starts kick result failure rates increase 
data sets drive populations drives compute nodes filesystem nodes scidac iop publishing journal physics conference series doi hpc type hpc drives study failure rate pattern function age 
interesting observations 
replacement rates rising significantly years early years lifecycle 
replacement rates hpc nearly double year year 
observation suggests wear may start earlier expected leading steadily increasing replacement rates system useful life steady failure rates drive lifetime 
second observe hardly infant mortality 
hpc file system nodes hpc drives arr drives higher months year months year 
case hpc compute nodes infant mortality limited month operation steady state estimate datasheet mttf 

projections computers section looks technology trends combines results data analysis provide projections reliability availability hpc systems 

projected failure rates model essential prediction number processor chips growing time increasing failure rates fault tolerance overheads 
argue follows 
expect integrators deliver computers long standing trends shown top org aggregate compute performance doubles year 
second expect processor chip designers provide little increase clock speed increase number processor cores processor chip commonly referred socket new core processor era fast rate estimated doubling years 
models follow bracket prediction rates growth cores doubling months 
hpc programmers manage increase thread parallelism impressive rate achieving top org trends require processor chip sockets system year growth rate factor year respectively 
proposed data section predicts failure rates grow proportional number sockets system indication failure rate socket decrease time technology changes 
number sockets systems increases achieve top org performance trends expect system wide failure rate increase 
projections attempt quantify increase failure rate expect see systems 
lanl data show optimistic estimate failure rate year socket 
data predict failure rates change increasing numbers cores processor chip core reasonable predict failure mechanisms operate chip level possibly highly optimistic assumption failure rates increase number chip sockets number cores chip 
baseline projections model jaguar system oak ridge national laboratory ornl expanded system 
jaguar processor sockets dual core tb main memory storage bandwidth gb 
architecture machines lanl roadrunner differs jaguar hybrid nodes employing vector graphics processors predictions failure rates little different jaguar include separate lines graphs plots expected increase failure rate corresponding decrease mean time interrupt assumptions 
graphs shows zero increase failure rate cores socket stretch failure rates peak machines top curves expected grow dramatically 
suggestion blue gene machines experience lower failure rates data argument dramatically different projection blue gene computers scidac iop publishing journal physics conference series doi number failures year months months months year mean time interrupt min months months months year 
expected growth failure rate left decrease right assuming number cores socket grows factor months respectively number sockets increases stay top org 

modeling checkpoint overhead observing dramatic increase failure rates brings question increase failure rates affect utility systems 
fault tolerance hpc systems typically implemented checkpoint restart programming 
checkpoint restart programs applications periodically stops useful write checkpoint disk 
case node failure application restarted checkpoint recomputes lost results 
effective application utilization system amount time doing useful executing user code writing checkpoints redoing lost node failure 
utilization depend time takes write checkpoint mean time interrupt mt time checkpoints computed mt time write checkpoint depends total amount memory system fraction memory application needs checkpoint able recover bandwidth specifically conservative assume demanding applications may utilize checkpoint entire memory 
system jaguar tb memory gb storage bandwidth means system wide checkpoint take order minutes 
projections assume balanced system model bandwidth memory grow proportion compute power time write checkpoint stay constant time 
remaining variable equation checkpoint interval balance trade amount lost minimized small versus total time system spends writing checkpoints minimized large checkpoint interval chosen mt 
shows projection effective application utilization time assumptions 
observe utilization drastically decreasing time 
example case number cores chip doubles months utilization drops zero meaning system spend time writing checkpoints recovering lost situation clearly unacceptable 
remainder section consider different strategies predicted drop utilization 

grow number sockets system data suggests failure rate grows proportional number sockets keeping number sockets constant increase failure rate 
means failing achieve top org aggregate performance trends outcome hpc community appears determined avoid increasing performance processor chip faster currently percentage memory checkpoint scidac iop publishing journal physics conference series doi utilization months months months year 
effective application utilization time 
projected 
chip designers consider see return era rapid decreases processor cycle time power consumption implications 
remaining alternative increasing number cores chip faster probably effective possible memory bandwidth chip keep 
think number processor chip sockets increase continually 

individual sockets reliable increase failure rates prevented individual processor chip sockets reliable year socket increase proportionally number sockets system time 
chip designers offer expectation happening favorite choice system integrators new request proposed machines hold system constant 
unfortunately data recall indicates hardware reliable time suggesting long number sockets rising system wide drop 

partition systems number interrupts application sees depends number processor chips parallel 
way increase seen application run sub partition machine nodes machine 
revisiting equations section reveals approach effective cutting number nodes application runs half cut amount lost due interrupts due writing checkpoints half 
way view approach smaller machines big machine 
solution works small applications appealing demanding hero applications largest new computers justified 
think approach extensively solve problem demanding applications 

take checkpoints faster basic premise checkpoint restart approach fault tolerance decreasing storage bandwidth increases proportion total performance called balanced system design 
achieving balance meaning doubling storage bandwidth year difficult challenge storage systems way cope increasing failure rates increase storage bandwidth 
example assuming number sockets failure rate grows year effective application utilization stay checkpoints taken time year 
shows increase storage bandwidth necessary keep failure rates assuming number cores doubles months respectively 
note required increase bandwidth orders magnitude higher expected increase bandwidth disk drive commonly assumed year shown bottom line scidac iop publishing journal physics conference series doi bandwidth increase months months months expected drive year 
growth bandwidth necessary checkpoints cheap compensate increased failure rate due growth number sockets assuming number cores socket grows factor months respectively 
drives sockets months months months year 
ratio number disks number sockets growing disk bandwidth compensate increased failure rate due growth number sockets 

means increase bandwidth come growth total number drives 
growth significantly higher expected growth number sockets 
shows ratio number drives number chip sockets provide necessary bandwidth 
treating price chip price drive fixed reasonable business goals implies fraction cost going storage total system cost increase significantly 
possible desirable 
second option decrease amount memory checkpointed growing total memory fast better compression application data leading fraction memory written checkpoint 
growing total memory slower balanced rate help reduce total system cost independently may acceptable demanding applications 
compression appealing approach 
achieving higher checkpoint speedups purely compression require significantly better compression ratios year 
right shows right axis maximum fraction balance sized memory application write back checkpoint order provide necessary speedup compensate increasing failure rates 
note early year application checkpoint total memory 
mark crossed options diskless checkpointing checkpoint written volatile memory node disk hybrid approaches viable 
recommend application capable compressing checkpoint size pursue path considering increasing number cycles go checkpointing compute time needed compression may time spent 

process pairs fault tolerance checkpoint restart consider alternative techniques currently standard checkpoint restart methods fault tolerance 
traditional method fault tolerance hasn applied hpc systems process pairs checking computations 
process pairs eliminate cost associated writing back checkpoints lost case failure 
process pairs expensive requires giving hardware introduces overheads keep processors checking 
method works keep utilization sacrifice appropriate 
scidac iop publishing journal physics conference series doi number drives year 
number drives systems 
number concurrent reconstructions year 
number concurrent reconstructions system 

recommendations demanding applications applications justify largest computers see increasing failure rates trends top org continue 
standard checkpoint restart fault tolerance strategy effective utilization machines running demanding applications fall 
relying machine manufacturing counter trend recommended historical data relying storage bandwidth counter expensive best 
recommend applications consider spending increasing number cycles compressing checkpoints 
recommend experimentation process pairs fault tolerance supercomputing 

storage projections storage bandwidth computers required maintain balanced system design difficult scaling issues storage system designers coming 
individual disk bandwidth grows rate year significantly slower year growth rate top org predicts 
keep number disk drives system increase impressive rate 
projects number drives system necessary just maintain balance 
shows current technology trends continue hpc system top top org chart need disk drives 
managing number independent disk drives delivering bandwidth application extremely challenging storage system designers 
second disk drive capacity keep growing year continuously increasing amount needed reconstruct failed drive time needed complete reconstruction 
trends decrease physical size diameter drives help limit increase reconstruction time single step decreases limited poorer cost effectiveness smaller disks 
think realistic expect increase reconstruction time year 
assuming today reconstruction times hours drives system fail year average recall section project number concurrent reconstructions going hpc systems shown 
indicates year average nearly concurrent reconstructions progress time 
analysis consider current trend expensive error correcting codes disk arrays order tolerate second failures long reconstructions 
quite clear storage systems designers spending large fraction efforts fault tolerance inside storage systems application fault tolerance depends 
minimum reconstruction time shorter order hours reconstruction usually performed maximum speed order interfere foreground workload 
example common disk array hpc systems takes days reconstruction 
scidac iop publishing journal physics conference series doi 
computer failure data repository described part broader research agenda goal analyzing making publicly available failure data large variety real production systems 
built public computer failure data repository hosted usenix association 
goal repository accelerate research system reliability filling nearly empty collection public data detailed failure data variety large production systems 
point organizations contributed data described section agreed data publicly available part repository 
los alamos national laboratory national energy research scientific computing center nersc data available public web sites 
encourage computing organizations collect publish failure data systems repository 

related large scale studies failures real production systems scarce probably result reluctance owners systems release failure data 
existing studies limited months data covering typically failures 
commonly cited studies failure analysis stem late early computer systems significantly different today 
studies published storage failures reporting arr rates similar range observe data 

acknowledgments gary high performance computing division los alamos national lab ray scott robin pittsburgh supercomputing center collecting providing data helping interpret data 
national energy research scientific computing center nersc making failure data collected systems publicly available 
people organizations provided data remain unnamed 
discussions relating high systems mark dave fox lawrence livermore national lab 
members companies pdl consortium including apc cisco emc hewlett packard hitachi ibm intel network appliance oracle seagate symantec interest support 
material supported department energy award number de fc er research sponsored part army research office agreement number daad 

bibliography computer failure data repository 
usenix org 
lanl raw data information available 
www lanl gov projects data 
nersc raw data information related failures available url 
nersc gov 
scientific discovery advanced computing scidac data storage institute 
www scidac org 
top supercomputing sites 
www top org 
bodik husbands patterson williams yelick 
landscape parallel computing research view berkeley 
technical report ucb eecs eecs department university california berkeley dec 
bressoud schneider 
hypervisor fault tolerance 
acm trans 
comput 
syst 
chapin rosenblum devine lahiri gupta 
hive fault containment shared memory multiprocessors 
sosp proceedings fifteenth acm symposium operating systems principles 

afr problems definition calculation measurement commercial environment 
proc 
annual reliability maintainability symposium 

specifying reliability disk drive industry 
proc 
annual reliability maintainability symposium 
scidac iop publishing journal physics conference series doi elnozahy alvisi 
wang johnson 
survey rollback recovery protocols messagepassing systems 
acm comput 
surv 
gray 
computers done 
proc 
th symposium reliability distributed software database systems 
gray 
census tandem system availability 
ieee transactions reliability 

hpc file system issues perspectives 
presentation la ur slides available www dtc umn edu disc presentations pdf 
iyer 
measurement modeling computer reliability affected system activity 
acm trans 
comput 
syst 
iyer 
failure data analysis lan windows nt computers 
proc 
th ieee symposium reliable distributed systems 
koch 
new roadrunner supercomputer 
presentation sc 

lin siewiorek 
error log analysis statistical modeling heuristic trend analysis 
ieee transactions reliability 

architecture tandem nonstop system 
acm proceedings acm conference page new york ny usa 
acm press 
meyer wei 
analysis workload influence dependability 
proc 
international symposium fault tolerant computing 
murphy gent 
measuring system software reliability automated data collection process 
quality reliability engineering international 
wolski 
modeling machine availability enterprise wide area distributed computing environments 
euro par 
oppenheimer ganapathi patterson 
internet services fail done 
usenix symposium internet technologies systems 
patterson gibson katz 
case redundant arrays inexpensive disks raid 
proc 
acm sigmod international conference management data 
pinheiro weber 
failure trends large disk drive population 
proc 
fast conference file storage technologies 
plank li 
faster checkpointing parity 
proc 
th international symposium fault tolerant computing 
plank li 
diskless checkpointing 
ieee trans 
parallel distrib 
syst 
roth 
path oak ridge national laboratory 
data storage workshop supercomputing 
zhang 
failure data analysis large scale heterogeneous server environment 
proc 
international conference dependable systems networks dsn 
schroeder gibson 
large scale study failures high performance computing systems 
proc 
international conference dependable systems networks dsn 
schroeder gibson 
disk failures real world mttf hours mean 
fast proceedings th conference usenix conference file storage technologies 
schwarz baker van manasse shah 
disk failure investigations internet archive 
session nasa ieee conference mass storage systems technologies msst 
simon 
computing 
slides presentation acts workshop 
acts nersc gov events workshop slides simon pdf june 
tang iyer 
failure analysis modelling vax cluster system 
proc 
international symposium fault tolerant computing 
vaidya 
case level distributed recovery schemes 
proceedings acm sigmetrics conference 
van gray 
empirical measurements disk failure rates error rates 
msr tr 
xu iyer 
networked windows nt system field failure data analysis 
proc 
pacific rim international symposium dependable computing 
yang 
sun 
comprehensive review hard disk drive reliability 
proc 
annual reliability maintainability symposium 
young 
order approximation optimum checkpoint interval 
commun 
acm 

