conditional random sampling sketch sampling technique sparse data kenneth church microsoft research microsoft way redmond wa church microsoft com ping li department statistics stanford university stanford ca stat stanford edu trevor hastie department 
statistics stanford university stanford ca hastie stanford edu develop conditional random sampling crs technique particularly suitable sparse data 
large scale applications data highly sparse 
crs combines sketching sampling converts sketches data conditional random samples online estimation stage sample size determined retrospectively 
focuses approximating pairwise distances comparing crs random projections 
boolean data crs provably better random projections 
show real world data crs outperforms random projections 
technique applied learning data mining information retrieval database query optimizations 
conditional random sampling crs sketch sampling technique effectively exploits data sparsity 
modern applications learning data mining information retrieval datasets large highly sparse 
example term document matrix sparse 
sampling large scale sparse data challenging 
conventional random sampling randomly picking small fraction performs poorly samples zeros 
heavy tailed data estimation errors random sampling large 
alternatives random sampling various sketching algorithms popular random projections min wise sketches 
sketching algorithms designed approximating specific summary statistics 
specific task sketching algorithm outperforms random sampling 
hand random sampling flexible 
example set random samples estimate lp pairwise distances multi way associations 
conditional random sampling crs combines advantages sketching random sampling 
important applications concern pairwise distances distance clustering classification multi dimensional scaling kernels 
large training set web scale computing pairwise distances exactly time consuming infeasible 
data matrix rows columns 
example term document matrix total number word types total number documents 
modern search engines general number data points number features 
computing pairwise associations aa called gram matrix machine learning costs daunting large various sampling methods proposed approximating gram matrix kernels 
example normal random projections approximate aa ar ar entries 
reduces cost min 
full version www stanford edu publications crs tr pdf sampling techniques critical databases information retrieval 
example database query optimizer seeks highly efficient techniques estimate intermediate join sizes order choose optimum execution path multi way joins 
conditional random sampling crs applied estimating pairwise distances norm multi way associations 
crs estimating joint histograms way multi way 
focuses estimating pairwise distances inner products refer readers technical report estimating joint histograms 
early concerned estimating way multi way associations boolean data 
compare crs normal random projections approximating distances inner products cauchy random projections approximating distances 
boolean data crs bears similarity broder sketches important distinctions 
showed boolean data crs improves broder sketches roughly halving estimation variances 
procedures crs conditional random sampling stage procedure 
sketching stage scan data matrix store fraction non zero elements data point sketches estimation stage generate conditional random samples online pairwise way multi way name algorithm conditional random sampling crs 
sampling sketching procedure original permuted postings global view sketching stage 
sketches provides global view sketching stage 
columns sparse data matrix randomly permuted 
non zero entries considered called postings 
sketches simply front postings 
note actual implementation need maintain permutation mapping column ids 
data matrix random samples postings sketches data matrix rows 
column ids random ds columns constitute random sample 
ui denotes ith row 
postings consist tuples id value sketches ki entries postings sorted ascending ids 
example ds min 
excluding obtain samples directly sampled ds columns data matrix 
apparently sketches uniformly random samples may estimation task difficult 
show sketches random samples pairwise group wise 
constructs conventional random samples data matrix show generate retrospectively random samples sketches 
column randomly permuted construct random samples simply ds columns data matrix columns ds real applications 
sparse data store non zero elements form tuples id value structure called postings 
denote postings pi row ui 
shows postings data matrix 
tuples sorted ascending ids 
sketch ki postings pi ki entries smallest ki ids pi shown 
central observation exclude elements sketches ids larger ds min max id max id obtain exactly samples directly sampled ds columns data matrix 
way convert sketches random samples conditioning ds differs pairwise know 
estimation procedure estimation task crs extremely simple 
construct conditional random samples sketches effective sample size ds compute distances inner products samples multiply estimate original space 
ds show improve estimates advantage marginal information 
ds denote conditional random samples size ds obtained crs 
example ds non zero 
denote inner product squared distance distance respectively iu random samples simple linear estimators mf ds ds mf ds ds ds mf ds computational cost 
th sketching stage requires generating random permutation mapping length linear scan non zeros 
generating sketches rn costs fi fi number non zeros ith row fi pi 
estimation stage need linear scan sketches 
conditional sample size ds large cost estimating distance pair data points ds 
theoretical variance analysis crs give theoretical analysis variances crs 
simplicity ignore finite population correction factor ds due sample replacement consider mf ds ds assuming sample replacement samples ds conditional ds 
var mf ds ds dx var mf ds ds iu dx unconditional variance simply iu ds ds xd var mf var mf ds ds iu dx dx iu iu 
var var ds var ds var ds conditionally unbiased 
closed form expression known know max similar ds ds jensen inequality 
asymptotically increase inequality equality max ds max numbers non zeros respectively 
see proof 
extensive simulations verify errors usually 
similarly derive variances mf mf summary obtain xd var mf ds var mf ds var mf ds iu denote dx max iu max dd max dd 
sparsity term max reduces variances significantly 
max variances reduced factor compared conventional random coordinate sampling 
brief random projections give brief random projections compare crs 
normal random projections widely learning data mining 
random projections multiply data matrix random matrix generate compact representation ar estimating distances typically consists entries call normal random projections 
consists cauchy 
impossibility result ruled estimators metrics dimension reduction 
denote rows corresponding original data points introduce notation marginal norms normal random projections case consists 
easy show linear estimators inner product squared distance unbiased variances nrp mf vt var nrp mf var nrp mf nrp mf 
assuming margins known provides maximum likelihood estimator denoted nrp mle asymptotic variance var nrp mle 
cauchy random projections dimension reduction case consisting entries cauchy 
proposed estimator absolute sample median 
proposed variety nonlinear estimators including sample median estimator bias corrected geometric mean estimator bias corrected maximum likelihood estimator 
analog johnson lindenstrauss jl lemma dimension reduction proved bias corrected geometric mean estimator 
list maximum likelihood estimator derived accurate 
crp mle crp mle crp mle solves nonlinear mle equation kx shows crp mle var crp mle crp mle 
crp mle 
general stable random projections dimension reduction lp generalized bias corrected geometric mean estimator general stable random projections dimension reduction lp provided theoretical variances exponential tail bounds 
course crs applied approximating lp distances 
improving crs marginal information reasonable assume know marginal information marginal norms numbers non zeros marginal histograms 
leads sharper estimates maximizing likelihood marginal constraints 
boolean data case express mle solution explicitly derive closed form asymptotic variance 
general real valued data joint likelihood available propose approximate mle solution 
boolean data data estimating inner product estimating way contingency table cells 
margin constraints degree freedom 
hard show mle solution denoted mle cubic equation ds 
asymptotic variance mle proved var mle real valued data ds 
practical solution assume parametric form bivariate data distribution prior knowledge solve mle considering various constraints 
suppose samples bivariate normal moments determined population moments ds ds ds population means 
ds ds ds suppose known mle ut denoted mle mle ds similar lemma solution cubic equation 
mle fairly robust observe biases quite noticeable 
general bias variance trade especially large 
intuitively reason crude assumption bivariate normality works fixed margins removed large extent non normal component data 
theoretical comparisons crs random projections reflected variances general data types crs better random projections depends competing factors data sparsity data heavy 
important scenarios crs outperforms random projections 
boolean data case marginal norms numbers non zeros mi ui fi 
plots ratio var mf var nrp mf verifying crs considerably accurate var mf var nrp mf max max 
plots var mle var nrp mle possible range data ratio 
close random projections appear accurate 
occur absolute variances small zero ratio matter 
variance ratio variance ratio variance ratio variance ratio var mf variance ratios var nrp mf show crs smaller variances random projections marginal information 

plot spaced 
variance ratio variance ratio variance ratio variance ratio ratios var mle var nrp mle show crs usually smaller variances random projections nearly independent data suppose data points independent strictly uncorrelated second order easy show variance crs smaller var mf max var nrp mf ignore data sparsity 
crs better estimating inner products nearly independent data 
obtained inner products infer distances easily margins easy obtain exactly 
high dimensions case data points weakly correlated 
comparing computational efficiency previously mentioned cost constructing sketches rn nd precisely fi 
cost normal random projections reduced sparse random projections 
possible crs considerably efficient random projections sampling stage 
estimation stage crs costs compute sample distance pair 
cost random projections 
small difference concern 
empirical evaluations compare crs random projections rp real data including randomly sampled documents nsf data sparsity documents news group data sparsity class corel image data sparsity 
estimate pairwise inner products distances crs rp 
pair obtain runs average absolute errors 
compare median errors percentage crs better random projections 
results figures 
panel dashed curve indicates sample data point equal sample size 
crs adjust sample size sparsity reflected solid curves 
adjust sample sizes roughly 
data points divided groups sparsity 
data different groups assigned different sample sizes crs 
random projections average sample size 
nsf newsgroup data crs overwhelmingly outperforms rp estimating inner products distances marginal information 
crs outperforms rp approximating distances margins 
corel data crs outperforms rp approximating inner products distances margins 
rp considerably outperforms crs approximating distances distances margins 
note corel image data sparse considerably heavy tailed nsf newsgroup data 
ratio median errors percentage inner product sample size sample size distance distance sample size sample size distance margins inner product distance distance margins distance sample size sample size sample size sample size nsf data 
upper panels ratios crs rp random projections median absolute errors values indicate crs better 
bottom panels percentage pairs crs smaller errors rp values indicate crs better 
dashed curves correspond fixed sample sizes solid curves indicate crudely adjust sketch sizes crs data sparsity 
case crs overwhelmingly better rp approximating inner products distances margins 
applications distances large sparse datasets 
propose new sketch method conditional random sampling crs provably better random projections important special cases boolean data nearly independent data 
general non boolean data crs compares favorably theoretically empirically especially take advantage margins easier compute distances 
proposed sparse random projections reduce cost dk 
ratio median errors percentage inner product distance sample size sample size sample size distance margins sample size inner product sample size distance sample size distance distance sample size distance margins sample size newsgroup data 
results quite similar nsf data 
case obvious adjusting sketch sizes helps crs 
ratio median errors percentage inner product distance distance distance margins sample size sample size sample size sample size inner product sample size acknowledgment distance sample size distance corel image data 
sample size sample size distance margins chris burges david heckerman chris meek andrew ng art owen robert tibshirani various helpful conversations comments discussions 
bingham dhillon matthias hein datasets 
achlioptas 
database friendly random projections johnson lindenstrauss binary coins 
journal computer system sciences 
achlioptas mcsherry sch lkopf 
sampling techniques kernel methods 
nips pages 
vempala 
algorithmic theory learning robust concepts random projection 
machine learning 
bingham mannila 
random projection dimensionality reduction applications image text data 
kdd pages 
charikar 
impossibility dimension reduction 
journal acm 
broder 
resemblance containment documents 
compression complexity sequences pages 
dhillon modha 
concept decompositions large sparse text data clustering 
machine learning 
drineas mahoney 
nystrom method approximating gram matrix improved kernel learning 
journal machine learning research dec 
indyk 
stable distributions pseudorandom generators embeddings data stream computation 
focs pages 
li 
sparse stable random projections estimators tail bounds stable random projections 
technical report arxiv org ps cache cs pdf pdf 
li church 
sketches estimate associations 
hlt emnlp pages 
li church 
sketch algorithm estimating way multi way associations 
computational linguistics appear 
li church hastie 
conditional random sampling sketched sampling technique sparse data 
technical report department statistics stanford university 
li church hastie 
nonlinear estimators tail bounds dimensional reduction cauchy random projections 
arxiv org ps cache cs pdf pdf 
li hastie church 
improving random projections marginal information 
colt pages 
li hastie church 
sparse random projections 
kdd pages 
vempala 
random projection method 
american mathematical society providence ri 
