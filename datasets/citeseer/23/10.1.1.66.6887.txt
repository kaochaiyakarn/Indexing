agenda research multi agent learning yoav shoham rob powers computer science department stanford university stanford ca shoham powers cs stanford edu survey ai multi agent reinforcement learning learning stochastic games 
tracing representative sample literature argue exciting suffers fundamental lack clarity problem problems addressed 
propose defined problems multi agent reinforcement learning single view suited ai adequately addressed 
conclude remarks believe progress problem 
reinforcement learning rl active research area ai years 
growing interest extending rl multi agent domain 
technical point view taken community realm markov decision problems mdps realm game theory particular stochastic markov games sgs 
body ai multi agent rl small couple dozen papers topic time writing 
contrasts literature learning ai literature learning game theory cases finds hundreds thousands articles books 
despite small number discuss papers 
trace representative historical path literature 
concentrate called bellman heritage multi agent rl learning watkins dayan bellman equations bellman 
specifically discuss littman claus boutilier hu wellman bowling veloso littman greenwald hall serrano course analyzing papers mention 
section trace bellman heritage summarize results obtained 
results unproblematic cases zero sum sgs aka team pure coordination sgs attempt extend general sum sgs problematic 
copyright american association artificial intelligence www aaai org 
rights reserved 
section trace back technical awkwardness results view misguided focus nash equilibrium ingredient learning algorithm evaluation criterion 
believe problem runs deeper basic lack clarity exact problem addressed 
section argue distinct defined problems addressed attempt map existing categories 
identify feel interesting ai note barely addressed line research 
section comments think go tackling 
bellman heritage multi agent rl section review representative sample literature 
start algorithms summarize results reported 
terminology notation 
agent stochastic game sg tuple 
set agents indexed 
set agent stage games usually thought games normal form see samet exception 

ai set actions pure strategies agent note assume agent strategy space games notational convenience substantive restriction 

rn ri immediate reward function agent stochastic transition function specifying probability game played game just played actions taken 
markov decision problem mdp agent sg mdp simpler structure 
minimax nash start single agent learning algorithm watkins dayan computing optimal policy mdp unknown reward transition functions max known certain assumptions way actions selected state time learning converges optimal value function simplest way extend multi agent sg setting just add subscript formulation learning agent pretend environment passive qi ai qi ai ri vi vi max qi ai ai ai authors tested variations algorithm sen hale 
approach ignores multi agent nature setting entirely 
updated regard actions selected agents 
justified opponents choices actions stationary fails opponent may adapt choice actions past history game 
cure problem define values function agents actions qi qi ri vi left question update complex nature values 
definition player zero sum sgs littman suggests minimax learning algorithm updated minimax values littman max min 
extended general sum sgs minimax longer motivated settings 
alternative try explicitly maintain belief regarding likelihood agents policies update induced expectation values vi max ai pi qi ai 
approach spirit belief procedures game theory fictitious play brown rational learning kalai lehrer pursued claus boutilier claus boutilier 
joint action learners specifically adopt procedures fictitious play probability action stage game assumed past empirical frequency 
procedure defined general sum game claus boutilier procedure directly bellman equations bellman dynamic programming procedures mdps known reward transition functions 
consider context common payoff team games 
stage game common payoff outcome agents receive payoff 
payoff general different different outcomes agents problem coordination called games pure coordination 
zero sum common payoff sgs special properties discuss section relatively straightforward understand problem learning 
situation different general sum games picture pretty 
early contribution nash learning hu wellman generalization learning general sum games 
nash updates values nash equilibrium game defined values vi 
qn 
abuse notation expression represents game qi denotes payoff matrix player denotes nash payoff player 
course general nash equilibria nash payoff may unique 
nash taken apply general sum sgs interpreted nondeterministic procedure 
focus hu wellman special class sgs 
littman articulated explicitly reinterpreting nash friend foe algorithm littman 
informative view algorithms applying different special class sgs 
friend class consists sgs execution algorithm values players define game globally optimal action profile meaning payoff agent joint action payoff joint action 
foe class execution algorithm values define game saddle point 
defined number players simplicity show updated player game friend maxa foe maxp mina friend updates similarly regular learning foe updates minimax 
greenwald ce learning similar nash uses value correlated equilibrium update greenwald hall serrano vi cei 
qn 
nash requires agents select unique equilibrium payoff issue authors address explicitly suggesting possible selection mechanisms 
convergence results referenced main criteria measure performance algorithms ability converge equilibrium self play 
littman minimax learning proven converge limit correct values game guaranteeing convergence nash equilibrium self play 
results standard assumptions infinite exploration conditions learning rates proofs convergence single agent learning 
claus boutilier claus boutilier conjecture independent learners belief joint action learners mentioned converge equilibrium common payoff games conditions decreasing exploration offer formal proof 
nash learning shown converge correct values classes games defined earlier friend games foe games 
ce learning shown converge nash equilibria subset set correlated equilibria number empirical experiments formal results 
focus equilibria 
previous section summarized developments multi agent rl editorial comments 
discuss critically 
results concerning convergence nash quite awkward 
nash attempted treat general sum sgs convergence results constrained cases bear strong similarity known cases zero sum games common payoff games 
analysis interesting generalizes conditions existence saddle point guaranteed limited zero sum games existence globally optimal nash equilibrium payoff guaranteed limited games 
conditions payoffs quite restrictive hold games defined intermediate values execution protocol 
hard find natural classes games satisfy properties special cases difficult verify outset game satisfies properties 
note original single agent qlearning nash concentrates learning correct case nash equilibrium game 
obvious turn procedure guiding play zero sum games 
multiple optimal equilibria exist players need oracle coordinate choices order play converge nash equilibrium begs question learning coordination 
view unsatisfying aspects bellman heritage nash onwards weak awkward convergence assurances limited applicability assumption oracle manifest deeper set issues 
summarized question justifies focus nash equilibrium 
certain local debate regarding initial formulation results resolved papers bowling bowling littman littman hu wellman journal version article hu wellman 
nash appeals nash equilibrium ways 
uses execution algorithm 
second uses convergence yardstick evaluating algorithm 
troubling ways 
max min strategy employed minimax nash equilibrium strategy prescriptive force 
best equilibrium identifies conditions learning purport say prior 

manifestation lack prescriptive force existence multiple equilibria thorny problem game theory limiting focus games uniquely identified equilibrium assuming oracle merely sweeps problem rug 

argument playing equilibrium strategy games dependent circular assumption opponents seek equilibrium strategy 
able justify assumption players unbounded computational ability calculating nash equilibrium large game prove intractable 
concerned specific details nash descendants concerned convergence nash equilibrium evaluation criterion 
bowling veloso articulate yardstick clearly bowling veloso 
put forward criteria learning algorithm multi agent setting learning converge stationary policy terminate best response play agent property called hannan consistency game theory hannan 
particular conditions require self play learning terminate stationary nash equilibrium 
useful criterion weak ignores fact playing extended sg 
confront centrality nash equilibrium game theory question play central role ai 
return section briefly view answer 
defined agendas multi agent learning view root difficulties field lacked clearly defined problem statement 
section identify think coherent research agenda multi agent rl 
fact generously offer agendas 
identify view appropriate ai game theoretic point view 
agenda descriptive asks humans learn context learners see erev roth ho chong 
name game said literature learning game theory repeated games special case sgs revolves entirely question learning procedure leads nash equilibrium 
opinion game theory unclear motivation doing 
comment section focus article 
show experimentally certain formal model learning agrees people behavior typically laboratory experiments 
typically undertaken psychologists experimental game theorists experimentally inclined social scientists 
second agenda computational nature 
views learning algorithms iterative way compute solution concepts 
fictitious play originally proposed way computing sample nash equilibrium adaptive procedures proposed computing solution concepts example computing equilibria local effect games leyton brown tennenholtz 
tend efficient computation methods constitute quick dirty methods easily understood implemented 
agendas intertwined learning game theory researchers propose various dynamics perceived plausible sense explaining human behavior proceed investigate converge equilibria 
key concern game theory successful theory support notion nash kinds equilibrium plays central role non cooperative game theory 
main limitation line research agreed objective criterion judge reasonableness dynamics 
agendas prescriptive 
ask agents people programs learn 
involves distributed control dynamic systems 
need desire decentralize control system operating dynamic environment case local controllers adapt choices 
direction naturally modeled repeated stochastic common payoff team game attracted attention ai years 
proposed approaches evaluated value achieved joint policy resources required terms computation communication time required learn policy 
case role equilibrium analysis agents freedom deviate prescribed algorithm 
researchers interested agenda access large body existing ai fields control theory distributed processing computation 
remaining prescriptive agendas assume learning takes place self interested agents 
understand relationship agendas worthwhile explicitly note obvious fact reinforcement learning single multi agent setting specific form acting actions conditioned runtime observations world 
question best learn specialized version general question best act 
remaining prescriptive agendas diverge interpret best 
call equilibrium noted game theory somewhat unusual unique having notion equilibrium associated dynamics give rise equilibrium 
agenda 
expected game theory adopt perspective differs commonly studied game theory fact explicitly rejected place fudenberg kreps 
seen pursued outside game theory tennenholtz 
agenda described follows 
traditional view non cooperative game theory notion optimal strategy meaningless replaced notions best response predominantly nash equilibrium learning strategy just strategy extended game ask vector learning strategies agent forms equilibrium 
course meaningful precise game played including payoff function information structure 
particular context sgs specify aggregate payoff agent limit average sum discounted rewards 
focus agenda naturally focus identifying classes learning strategies form equilibria different classes stochastic games 
final prescriptive agenda shall call ai agenda pending descriptive title 
name viewed bit ironic part approach taken ai believe sense field 
agenda somewhat asks best learning strategy agent fixed class agents game 
retains design stance ai asking design optimal effective agent environment 
just happens environment characterized types agents inhabiting 
raise question parameterize space environments return section 
objective agenda identify effective strategies environments interest 
effective strategy achieves better payoff environment selected class opponents 
class opponents motivated reasonable containing problems interest 
convergence equilibrium valuable serves goal maximizing payoff need careful discussing payoff stochastic game specify aggregate payoffs individual matrix games 
say ai agenda fact alien past multi agent rl ai discussion implies 
cited earlier concentrates comparing convergence rates algorithms self play see preliminary analysis comparing performance algorithms environments consisting learning agents 
hu wellman stone littman experimental strands tied formal research agenda particular convergence analyses 
striking exception chang kaelbling chang kaelbling return section 
ai agenda quite prevailing spirit game theory 
precisely adopts optimal agent design perspective consider equilibrium concept central necessarily relevant 
essential divergence approaches lies attitude bounded rationality 
traditional game theory assumed away outset positing perfect reasoning infinite mutual modeling agents 
struggling ways gracefully back assumptions appropriate 
fair say despite notable exceptions cf rubinstein bounded rationality largely unsolved problem game theory 
contrast ai approach embraces bounded rationality starting point adds elements mutual modeling appropriate 
result fewer elegant theorems general greater degree applicability certain cases 
applies general situations complex strategy spaces particular multi agent learning settings 
said equilibrium agenda ai agenda quite different areas overlap looks closely 
discuss section order parameterize space environments start grapple traditional game theoretic notions type spaces 
furthermore imagines learning algorithms evolve time imagine algorithms evolve equilibrium validating game theory agenda 
may advise thoughts long term outcome evolution stills provides guidance behave short term prior convergence 
case trading agent competition tac serves illustrate point 
tac wellman wurman series competitions computerized agents trade non trivial set interacting markets 
think tac setting allow application game theoretic ideas 
fact teams certainly gave thought teams behave class opponents programs engaged computation nash equilibria modeling beliefs agents part sophisticated attempts send specific signals agents 
situation sufficiently complex programs concentrated simpler tasks predicting prices different markets treating external events opposed influenced program 
reasonably argue competition team continue improve tac agent eventually agents settle equilibrium learning strategies 
believe true principle argument compelling game fairly simple played long time horizon 
tac strategy space rich convergence happen lifetime 
case provides guidance win competition 
say words ai agenda reconsider bellman heritage discussed earlier fit categorization 
minimax fit ai agenda highly specialized case zero sum games objective minimizing worst case payoff set possible opponents 
common payoff sgs superficially reminiscent ai agenda probably fits better dai agenda payoff function interpreted payoff agents designer 
general evaluating performance self play separate argument required reasonable class opponents expect 
nash descendants feel somewhat followers equilibrium agenda restricted set equilibria stochastic games 
fail resonate ai agenda unclear class environments achieve payoff 
pursuing ai agenda ai agenda calls categorizing strategic environments populations agent types agent designed interact 
agent types may come distribution case hope design agent maximal expected payoff distribution case different objective called example agent maximal minimum payoff 
case need way speak agent types 
question best represent meaningful classes agents representation calculate best response 
won say best response calculation note computationally hard problem 
example known general best response player sg non computable 
touch question parameterize space agents challenge 
objective propose specific taxonomy agent types provide guidance construction useful taxonomies different settings 
agents categorized strategy space 
space strategies complex categorization trivial 
coarse way limiting strategy space simply restrict family 
example assume agent belongs class joint action learners sense claus boutilier 
principle orthogonal way restricting strategy space place computational limitations agents 
example constrain finite automata bounded number states 
kinds limitations left large space reason disciplined approaches winnowing space 
particular strategies opponent function beliefs restricting assumptions beliefs 
approach taken chang kaelbling chang kaelbling extent stone littman look limited set possible strategies beliefs 
general example assume opponent rational learner sense kalai lehrer place restrictions prior model commonly pursued bounded rationality neyman papadimitriou yannakakis rubinstein 
concerned equilibrium analysis impacted limitations clear technical results obtained directly contribute ai agenda agents strategies 
note slippery slope asks second agent computational limitations strategy space recursively beliefs agent computational powers strategy space beliefs 
brings realm type spaces zamir interaction type spaces bounded rationality territory see gmytrasiewicz durfee 
research done weaving different considerations coherent comprehensive agent taxonomy 
settle open problem focus briefly question best evaluate competing methods environment defined 
powers shoham attempted define set criteria guide effort designing learning algorithms stochastic games 
criteria set requirements minimal payoff achieved classes opponents 
particular require algorithm define target set required achieve optimal payoff simultaneously guaranteeing achieves payoff security level strategy minus opponent 
demonstrate algorithm provably meets criteria target set class opponents actions independent game history 
continuing develop algorithms perform optimally versus intriguing sets opponents ultimately environment interest set existing multi agent learning algorithms 
set diverse easily fit specific formally defined class opponents strive approximate sampling 
implemented wide variety algorithms including described game theory literature 
empirical environment able compare performance algorithms display highly competitive performance new algorithm method empirical testing line ai agenda see 
interesting observations came tournament setting complicated algorithms aren necessarily effective evaluated environment adaptive agents 
multi agent setting learning teaching inseparable 
choice agent informed agent past behavior impacts agent behavior 
reason neutral term multi agent adaptation apt 
doesn quite ring multi agent learning wage linguistic battle useful keep symmetric view mind thinking pursue ai agenda 
particular helps explain greater sophistication asset 
example consider infinitely repeated game chicken yield dare yield dare presence opponent attempts learn agent strategy play best response exam ple fictitious play system claus boutilier best strategy agent play stationary policy agent soon learn yield 
watch crazy policy stone littman bully strategy stone littman oscar wilde tyranny weak 
notice success simple strategy function environment competes agents strategy tends fare emphasizing importance specifying class opponents wishes perform 
concluding remarks reviewed previous multi agent rl argued believe clear fruitful research agenda ai multi agent learning 
critical remarks previous give impression don appreciate researchers 
truth 
best friends colleagues belong group greatly educated inspired ideas 
look forward new innovative results sure see field hope comments may contribute healthy debate goal 
supported part darpa benchmark stanford graduate fellowship 
want members multi agent research group stanford university helpful advice feedback project 
bellman 
dynamic programming 
princeton university press 
bowling veloso 
rational convergent learning stochastic games 
proceedings seventeenth international joint conference artificial intelligence 
bowling 
convergence problems general sum multiagent reinforcement learning 
proceedings seventeenth international conference machine learning 
brown 
iterative solution games fictitious play 
activity analysis production allocation 
new york john wiley sons 
ho chong 
sophisticated ewa learning strategic teaching repeated games 
journal economic theory 
chang kaelbling 
playing believing role beliefs multi agent learning 
proceedings nips 
claus boutilier 
dynamics reinforcement learning cooperative multiagent systems 
proceedings fifteenth national conference artificial intelligence 
erev roth 
predicting people play games reinforcement leaning experimental games unique mixed strategy equilibria 
american economic review 
fudenberg kreps 
learning mixed equilibria 
games economic behavior 
gmytrasiewicz durfee 
decision theoretic approach coordinating multiagent interactions 
proceedings twelfth international joint conference artificial intelligence 
greenwald hall serrano 
correlated learning 
nips workshop multiagent learning 
hannan 
approximation bayes risk repeated plays 
contributions theory games 
hu wellman 
multiagent reinforcement learning theoretical framework algorithm 
proceedings fifteenth international conference machine learning 
hu wellman 
learning agents dynamic multiagent system 
journal cognitive systems research 
hu wellman 
multiagent learning 
journal machine learning 
samet 
learning play games extensive form valuation 
economics 
kalai lehrer 
rational learning leads nash equilibrium 
econometrica 
leyton brown tennenholtz 
games 
proceedings eighteenth international joint conference artificial intelligence 
littman 
generalized reinforcement learning model convergence applications 
proceedings th international conference machine learning 
littman 
markov games framework multi agent reinforcement learning 
proceedings th international conference machine learning 
littman 
friend foe learning games 
proceedings eighteenth international conference machine learning 
zamir 
formulation bayesian analysis games incomplete information 
international journal game theory 

non computable strategies discounted repeated games 
economic theory 
neyman 
bounded complexity justifies cooperation finitely repeated prisoner dilemma 
economic letters 
papadimitriou yannakakis 
complexity bounded rationality 
stoc 
powers shoham 
new criteria new algorithm learning multi agent systems 
advances neural information processing systems 
forthcoming 
rubinstein 
modeling bounded rationality 
mit press 
sen hale 
learning coordinate sharing information 
proceedings twelfth national conference artificial intelligence 
stone littman 
implicit negotiation repeated games 
meyer tambe eds eighth international workshop agent theories architectures languages atal 
tennenholtz 
efficient learning equilibrium 
advances neural information processing systems volume 
cambridge mass mit press 
watkins dayan 
technical note learning 
machine learning 
wellman wurman 
trading agent competition research community 
ijcai workshop agent mediated electronic trading 
