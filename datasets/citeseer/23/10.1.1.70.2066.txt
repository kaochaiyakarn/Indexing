learning selectively conditioned forest structures applications dbns classification brian machine learning department carnegie mellon university pittsburgh pa cs cmu edu dealing uncertainty bayesian network structures maximum posteriori map estimation bayesian model averaging bma intractable due number possible directed acyclic graphs 
prior decomposable classes graphs efficient learning take place fixed orderings limited degree 
show map estimates bma selectively conditioned forests scf combination classes computed efficiently ordered sets variables 
apply scfs temporal data learn dynamic bayesian networks having intra timestep forest inter timestep limited degree structure improving model accuracy dbns combination structures 
apply scfs bayes net classification learn selective na bayes classifiers 
argue built feature selection selective augmented bayes classifiers preferable similar non selective classifiers empirical evidence 
bayesian networks proven useful modeling relationships variables 
optimally efficiently learning data relationships important building accurate models structure learning difficult due number possible graphs requirement acyclicity 
common approach structure learning find maximum posteriori map estimate graph structure bayesian network 
second approach probabilistically aver anind dey human computer interaction institute carnegie mellon university pittsburgh pa anind cs cmu edu andrew robotics institute carnegie mellon university pittsburgh pa ri cmu edu forest structure selectively conditioned structure combination selectively conditioned forest structure 
age possible structures bayesian model averaging bma 
provide novel methods obtaining map structure estimate bma class structures ordered sets fully observed discretized variables set variables connected tree graph variable limited number parents previous set ordering 
call class structures selectively conditioned forests scfs 
provide algorithm capable finding map scf structure time polynomial number variables set amount training data exponential limit number parents node 
runtime algorithm 
additionally provide method bma scf structures run time 
influenced investigate selectively conditioned forests temporal data modeling problems 
gathering temporal data difficult observe necessary variables high temporal granularity obtain true causal model underlying physical phenomena 
possible different observation features result high order model 
pairs variables appear instantaneous correlation timestep 
scf dynamic bayesian network dbn contains sets inter timestep intra timestep dependencies 
increase time complexity finding intra timestep dependencies addition dependencies linear number variables learning map structure 
show real datasets adding intra timestep structure improves model accuracy additional parents intra timestep variables unobserved inference 
second application method generalize friedman 
tree augmented na bayes classifier employing map scf learn selective forest augmented na bayes increase asymptotic time complexity 
generalizations tan reduce model parameterization improving classification accuracy removing unnecessary dependencies noisy features 
selective na bayes classifiers interest langley sage prior structures obtained approximately heuristics exact map estimation 
remainder organized follows 
section review structure learning bayesian model averaging relate similar approaches dealing structure uncertainty dbns augmented na bayes classifiers 
provide algorithm efficiently learning map estimate bayesian model averaging scf class structures sections 
sections apply map scf temporal data classification problems learn scf dbns selective forest augmented na bayes classifiers evaluate resulting models real datasets illustrate benefits intra temporal dependencies temporal modeling selectivity classification 
section provide extensions 
background related bayesian networks allow compact representation probability distributions sets variables 
joint probability dataset xn factors directed acyclic graph follows xn xi parents xi denoted determined values xi values xi 
conditional probability xi parents determined set parameters 
machine learning settings unknown learned data 
common approach structure learning employ prior graph structures parameters limits complexity maximum posteriori map estimate graph structure 
map argmax argmax argmax xi argmax ls xi log argmax ls xi log ls xi log xi shown equation structure parameters priors usually chosen entire function decomposes sum local score function ls evaluations equation local priors parameters structure 
common priors bayesian networks distributed variables bayesian dirichlet bd bayesian dirichlet equivalent bde heckerman uniform bayesian dirichlet equivalent bdeu buntine 
general class directed acyclic graphs finding map estimate np hard chickering decomposable map functions 
restricted classes structures map estimate polynomial time tree structures reduction maximum spanning tree solution edges weighted mutual information chow liu fixed orderings limited degree combinatorial search buntine 
class structures variable restricted parents occurring previously provided ordering variables 
restrictive classes graphs heuristic edge operations heckerman employed search structures polynomial time 
guarantees searches find map estimate 
contrast learn exact map estimate combination tree structures fixed orderings limited degree polynomial time 
bayesian model averaging bma inquiries probabilistically averaged possible graph structures parameters 
example probability new test data averaged possible models weighted training data xi ls log general bayesian networks number possible structures consider bayesian model averaging intractable 
efficient bma employed structures limited degree friedman koller tree structures meila jaakkola de sufficient statistics efficiently computed decomposable conjugate prior 
bayesian model averaging tree structures efficient computation relies combinatoric result graph theory matrix tree theorem west 
provide method perform bayesian model averaging class scfs generalization theorem 
temporal datasets represent important domain structure learning 
dynamic bayesian networks dbns extension bayesian networks temporal data arbitrary length 
dbn consists set variables repeated consecutive timesteps 
edges dbns generally restricted directed backwards time 
graph structure slice dbns consists structure timestep structure repeated consecutive timestep 
slice dbns structure learning applied learn initial timestep structure repeated structure friedman 
conditional chow liu algorithm obtains map estimate class structures variable parent previous timestep addition fixed set condition variables 
algorithm cooper herskovits employed learn structure dbns fixed number parents previous timestep 
conditional chow liu tree structures limited inter timestep parents subclasses scfs allow combination inter timestep intra timestep dependencies learned dynamic bayesian networks 
show combination dependencies produces accurate models structures limited exclusively inter timestep intra timestep dependencies 
structure learning employed successfully augment bayes net classifiers ways relax independence assumptions bayesian model averaging employed deal structure uncer tainty small datasets 
friedman 
augment na bayes classifier tree structure connecting feature variables addition edges class feature 
call new classifier tree augmented na bayes tan 
find relaxation feature conditional independence assumption helps improve classification accuracy numerous datasets 
selective na bayes langley sage relaxes direct dependency requirement feature class variable allowing built feature selection 
bayesian model averaging employed tan de selective na bayes dash cooper resulting increased accuracy small datasets large amounts structure uncertainty 
selective forest augmented na bayes classifier provides relaxation tan constraints 
allows features interconnected forest structure optionally dependent class variable 
class structures simple case selectively conditioned forests 
heuristic approaches employed create structure pruning edges tan classifier approach find optimal map estimate employ bayesian model averaging 
learning map scf structures focus situation set variables divided sets st st values st st variables st conditioned variables st 
graph structure similarly split edges parents variables st st denote subgraphs gt gt 
restriction variables st parents st choice parents variables st st independent cycles formed spanning sets 
map argmax log st gt st log gt log st gt log gt argmax gt xi st argmax ls xi log cmap map ls xi log map structure map obtained combining map structure gt conditional map structure cmap gt st shown equation 
cmap differs map estimate variables cmap parents disjoint condition set st addition parents set st 
selectively conditioned forests scfs impose additional constraints cmap graph structure variable xt st parents st denote variable xt parent st forest interconnect variables st 
sets variables algorithm 
intra set parents create directed cycles 
simple observation allows efficient optimization 
intra set parent inter set parents independently maximized combinatorially evaluating limited subset parent variables st equation 
argmax forest argmax forest argmax forest max ls xi log max ls xi log ls xi log argmax ls xi log dynamic algorithm follows equations 
presupposes possible intra set parent finds best possible set inter set parents equation parent step 
denote combined best parent set xi inter set parent score sets weight edge intra set parent child xi directed graph step 
solution equation maximum directed spanning forest graph time complexity algorithm characterized st st cost performing calculations 
root forest weight contributes total structure score 
reduce additional vertex weight root considered maximization modified version chu liu edmonds algorithm chu liu edmonds 
algorithm cmap algorithm st st 
st st argmax ls xi log st xj max ls xi log st xj st 
create full directed graph st weight ej weight vi 
find weighted vertices graph yielding root vertex set edges ea 
root root ai ai scf bayesian model averaging bayesian model averaging queries probabilistically averaged possible models 
scfs equation rewritten terms intra set parents 
equation follows decomposition employed buntine 
ls xi log ls log inner summation equation averages possible inter set parents variable parent 
possible inter set parents create directed cycles 
employ matrix tree theorem directed graphs efficiently average tree forest structures 
theorem tutte construct matrix aij wk wi matrix created removing row column det tn tk wi tn directed spanning tree rooted efficient computation obtained theorem generalized forests weighted roots augmenting graph new root edges 
equation efficiently calculated structures choice direction single edge arbitrary 
lead ill conditioned matrix 
heuristic edge direction map estimate employed direct edges 
weight settings 
wi ls ls scf dynamic bayesian networks selectively conditioned forest algorithms inspired dynamic bayesian networks 
class graphs repeating slice dbn mixture inter timestep dependencies connecting variables xt variables xt intra timestep dependencies connecting variables xt 
formulation lends naturally scfs xt conditional set xt target set 
scfs allow intra timestep forest structure limited number inter timestep parents 
example class scf dbns shown 
partially unrolled scf dbn forest structure inter timestep parents limited 
dbn variables timestep inter timestep dependencies limited running times map scf bma scf 
resulting structure average structures optimizes xt xt making scfs suited prediction 
experiments compare average withheld predictive performance log xt xt map scf bma scf map estimates subclasses scf different temporal datasets 
office variables discretizations sensor values measuring keyboard mouse usage sound levels motion status office door minute level granularity 
approximately timesteps data 
scf algorithm train data test remaining withheld data 
second dataset bayesian automated taxi bat dbn forbes synthetic dbn modeling vehicle status variables 
generate timesteps training data timesteps testing data 
train bdeu sample size uniform structure prior 
office structure map intra map inter map inter map inter map scf map scf bma scf bma scf true model bat bat average withheld log probability timestep data current timestep data 
results shown 
structure model uses marginal probabilities variable 
intra model learns forest structure connect variables timestep 
inter model allows variable parents previous timestep 
structure map estimates map scf performs best classes expect learning superclass models structures 
significance datasets map scf model outperforms inter models suggesting importance intra temporal dependencies dbns 
bma scf equivalent map scf office dataset 
smaller bat experiments structure uncertainty high bma scf outperforms map scf significantly 
selective forest augmented na bayes show scfs applied bayesian network classification create generalization tan classifier performs similarly clean data relevant features offering built feature selection improved performance data random irrelevant features 
resulting classifiers selective tree augmented na bayes stan selective forest augmented na bayes new provide method bma method evaluation exact map estimated stan 
illustrates characteristic differences na bayes augmented na bayes models 
selectivity integrates feature selection structure learning augmented na bayes classi na bayes classifiers augmentation 
na bayes tree augmented nb tan nb fan selective tree augmented nb stan selective forest augmented nb 

map learned map scf algorithm class condition set st features target set st map stan forest augmented na bayes fan learned simple modifications map scf algorithm impose different structure restrictions 
generalization tan classifier increased generality augmented bayes classifiers lead decreased classification accuracy friedman 
generative models maximize joint likelihood bayes optimal classifier maximizes conditional likelihood class variable feature values 
map selective augmented bayes classifiers risk yielding small markov class variable maximizing joint likelihood primarily term 
words features explain away features needing class variable 
friedman 
require class variable parent feature deal problem degrade classification accuracy presence irrelevant features 
experiments doesn fan propose addressing issue adjusting graph structure prior prefer including class variable parent requiring 
modify objective function penalize excluding class variable 
argmax log argmax log argmax st ls xi penalty indicator function statement true zero parameterize penalty 
penalty st setting yields fan classifier setting yields classifier 
sparse datasets features independent class variable features useful discriminatively weakly expect value extremes produce better results fan classifier 
experiments evaluate classifiers uci datasets newman 
remove examples missing features 
perform fold cross validation entropy discretization continuous variables fayyad irani fold 
bdeu prior map scf algorithm prior weight uniform structure prior 
breast cancer crx wine car zoo cmc ecoli average nb tan fan stan classification accuracy rates na bayes augmented na bayes classifiers uci datasets results experiment show classification accuracy rates various na bayes classifiers different uci datasets average classification accuracy 
find significant difference average classification accuracy na bayes classifier augmented classifiers insignificant differences augmented classifiers 
results second set experiments show robustness differ breast cancer crx wine car zoo cmc ecoli average nb tan fan stan classification accuracy rates uci datasets irrelevant features added 
breast cancer crx wine car zoo cmc ecoli average nb tan fan stan classification accuracy rates uci datasets irrelevant features added 
ent classifiers irrelevant features 
settings experiment add uniform random binary noise features 
nb tan fan significant accuracy degradations amount irrelevant features increases stan models effectively ignore irrelevant features 
methods exist performing feature selection wrapper kohavi john increase time complexity algorithm heuristics map scfs incorporate part structure learning process structure augmented classifiers 
addition selectivity increase run time complexity versus tan feature selection employs statistically justified bayesian scoring metrics 
final experiment demonstrates situation selective classifier non selective counterpart fan employing exclusion penalty equation better classifica average classification accuracy stan tan fan naive bayes number noisy features added average classification accuracy rates uci datasets versus number irrelevant features added 
tion accuracy original fan classifier achieved 
construct synthetic dataset binary features weakly dependent class variable fi random binary features independent class variable 
generate examples train classifier varying exclusion penalty constants test set separately generated examples 
average experiments 
prediction accuracy exclusion penalty effect penalization factor fan exclusion penalty versus classification accuracy learning features weakly correlated class additional irrelevant features results shown 
extremes graph classifier fan classifier 
fan classifier exclusion penalty 
believe real datasets similar happy discriminative benefit including potentially relevant features balanced detrimental effect including irrelevant features class variable markov blanket shown built feature selection provides equivalent classification accuracy fan relatively clean data far greater robustness noisy irrelevant features 
additionally described graph prior modified better balance benefits large markov blanket class variable versus including irrelevant features 
believe evidence similar run time implementation complexities suggest preferred commonly employed tan classifier bayes net classification 
shown existing classes structures allow efficient structure learning tree structures limited fixed orderings combined allowing efficient structure learning 
call combined class structures selectively conditioned forests 
algorithms efficiently learning map estimates scfs bayesian model averaging class scfs 
prior scfs learned approximately 
demonstrated usefulness class structures domains 
applied dynamic bayesian networks scfs learn structures inter timestep dependencies 
showed empirically combination dependencies improves predictive model accuracy 
additionally showed bayesian model averaging helps improve predictive model accuracy small training datasets 
applied generative classification models scfs yield selective forest augmented na bayes classifiers 
showed empirically classifiers perform tan noise free data better tan noisy data 
believe noise robustness comparable run time costs implementation complexity preferable classifier tan 
plans continuations twofold 
apply scfs datasets missing values learning algorithms extended hidden markov models instance 
second empirical analysis classifier warranted 
specifically empirically measure benefits exclusion penalty prior bayesian model averaging important problem classification 
buntine 
theory refinement bayesian networks 
proc 
uai pp 

de 
tractable bayesian learning tree augmented naive bayes models 
icml pp 

chickering 
learning bayesian networks npcomplete 
proc 
ai statistics pp 

chow liu 
approximating discrete probability distributions dependence trees 
ieee transactions information theory 
chu liu 
shortest arborescence directed graph 
science sinica 
cooper herskovits 
bayesian method induction probabilistic networks data 
machine learning 
dash cooper 
model averaging prediction discrete bayesian networks 
mach 
learn 
res 
edmonds 
optimum branchings 
journal research national bureau standards 
fayyad irani 
multi interval discretization continuous valued attributes classification learning 
proc 
ijcai pp 

forbes huang kanazawa russell 
bayesian automated taxi 
ijcai pp 

friedman koller 
bayesian bayesian network structure bayesian approach structure discovery bayesian networks 
proc 
uai pp 

friedman geiger goldszmidt 
bayesian network classifiers 
machine learning 
friedman murphy russell 
learning structure dynamic probabilistic networks 
proc 
uai pp 

heckerman geiger chickering 
learning bayesian networks combination knowledge statistical data 
machine learning 
smyth robertson 
conditional chow liu tree structures modeling discrete valued vector time series 
proc 
uai pp 

kohavi john 
wrappers feature subset selection 
artificial intelligence 
langley sage 
induction selective bayesian classifiers 
proc 
uai pp 

meila jaakkola 
tractable bayesian learning tree belief networks 
proc 
uai pp 

newman blake merz 
uci repository machine learning databases 
url www ics uci edu mlearn mlrepository html 

new synthesis bayesian network classifiers cardiac spec image interpretation 
phd thesis university toledo 
tutte 
dissection equilateral triangles equilateral triangles 
proc 
cambridge phil 
soc 
west 
graph theory nd edition 
prentice hall 
