statistical dependency analysis support vector machines yamada graduate school information science japan advanced institute science technology ishikawa japan yamada ac jp yuji matsumoto graduate school information science nara institute science technology nara japan aist nara ac jp propose method analyzing word word dependencies deterministic bottom manner support vector machines 
experimented dependency trees converted penn treebank data achieved accuracy word word dependency 
result little worse date phrase structure parsers looks satisfactorily accurate considering parser uses information phrase structures 
number statistical parsers proposed attained performance :10.1.1.14.298
known uses learning penn treebank syntactic anno text training data comparable performance hardly obtainable applied texts quite different expert domains medical legal documents 
build statistical parsers high accuracy various domains prepare parsed annotated training data target domains 
annotate penn treebank style phrase struc tures sentences annotators need acquainted deep linguistic theories phrase structure rules 
unrealistic impossible train experts specialized scientific domain penn style phrase structure annotation texts expertise 
word word dependency structure far easier understand amenable annotators knowledge target domain lack deep linguistic knowledge 
annotating simpler structure useful reaching consensus annotators expected construction training data noise free 
focus dependency structure analysis hoping possibility preparing sufficient training data noise 
statistical parsers phrase structure trees intensively studied mentioned little paid attention statistical dependency analysis english sentences 
eisner probabilistic models propose methods dependency parsing accuracies satisfactorily high 
propose new method statistical dependency parsing english rolls royce motor cars said nnp np np vp sbar expects sales remain steady cars nnp nnp vbd prp vbz prp nnp nns vp np vb vp adjp jj vp pp qp np cd nns rolls royce nnp nnp motor nnp said vbd cars prp prp phrase structure tree dependency tree phrase structure tree sentence rolls royce motor cars said expects sales remain steady cars dependency tree corresponding 
sentences deterministic bottom manner 
learning algorithm apply support vector machines svms relying ability cope large scale feature spaces 
rest organized follows 
section describe overview svms advantage dependency analysis 
section shows deterministic parsing algorithm parsing actions 
section describe method learn dependency structures svms 
section reports experiments penn treebank section discusses related 
final section gives 
support vector machines support vector machines svms binary classifiers maximum margin strategy introduced vapnik 
suppose training examples xi yi xi feature vector dimensional feature space yi class label positive negative xi 
svms find hyperplane correctly separates training examples maximum margin distance hyperplanes 
optimal hyperplane maximum margin obtained solving quadratic programming 
min yi xi expects vbz sales nnp nns remain vb steady constant experiment fixed called slack variable jj cars nns cd margin positive example negative example positive support vector negative support vector overview support vector machine non separable case see details 
optimal hyperplane written follows 
sign xi lagrange multiplier corresponding constraint called kernel function calculates similarity arguments svms estimate label unknown example sign positive 
advantages svms statistical dependency analysis high gen performance high dimensional feature spaces 
svms optimize pa rameter separate hyperplane maximum margin strategy 
strategy guarantees theoretically low generalization error unknown example high dimen sional feature space 
ii learning combination multiple features possible virtue polynomial kernel functions 
svms deal non linear classification kernel functions 
especially polynomial function kernel optimal hyperplane effect account combination features causing large amount computational cost 
owing advantages train rules dependency structure analysis features including part speech tags word combination 
svms discriminative classifiers generative probabilistic models naive bayes classifiers maximum entropy models 
absolute value eq represents distance example optimal hyperplane appropriate score cost dynamic programing search best answer widely probabilistic models 
employ deterministic parsing algorithm dependency structure svms 
deterministic dependency analysis parsing actions parser constructs dependency trees left right word order input sentences parsing actions shift right left 
actions applicable neighboring words referred target nodes 
shift means construction dependencies target nodes point focus simply moves right 
shows example shift action 
result shift shows target nodes move right saw girl 
prp saw vbd dt girl nn prp saw vbd dt girl nn example action shift shows states action 
result action right action constructs dependency relation neighboring words left node target nodes child right 
example action right 
applying action child girl modifies girl 
point target nodes saw girl executing action current algorithm keeping right frontier focus point unchanged 
prp saw vbd dt girl nn prp saw vbd girl nn dt example action right shows states action 
result action left action constructs dependency relation neighboring words right node target nodes child left opposite action right 
shows example left action 
note left right action applicable dependent child child complete subtree dependent children exist 
parser guarantee parser able see surrounding context target nodes 
seen section 
parsing algorithm parsing algorithm consists procedure estimation appropriate parsing actions contextual information surrounding target nodes ii parser constructs prp saw vbd girl nn dt prp saw vbd girl nn dt example action left shows states action 
result action dependency tree executing estimated actions 
shows pseudo code parsing algorithm 
input sentence wn pn initialize wn pn construction true start construction true break construction true get contextual features estimate action model construction left right construction false parsing algorithm execution denote indexes left right target nodes sequence nodes consisting elements tm element represents root node sub tree constructed parsing process initially ti pairs word pos tag wi pi 
estimation appropriate parsing action functions get contextual features estimate action 
function get contextual features extracts contextual features surrounding detail features described section 
function estimate action estimates appropriate parsing action shift right left model 
note training model annotated dependency trees training sentences model answers correct actions 
analysis test sentences model learned svms 
floor nn left context target nodes right context sellers nns dt node features context resort nn jj wp vbd criticized vbn example contextual information rb role variable construction check actions construct dependencies sentence 
value true means model estimated shift actions target nodes parsing process parser stops analysis outputs subtrees action possible 
construction equal false target nodes moved sentence parsing process repeated size equal 
learning dependency structure svms training dependency structure training sentence parsed algorithm 
estimation parsing action context corresponds example svms 
multi class classification problem construct binary classifiers corresponding action left vs right left vs shift right vs shift pairwise method 
decision pairwise method action maximum number votes svms 
features learning indexes left right target nodes left context defined nodes left side target nodes tl right context defined right tr 
context lengths represents numbers nodes left right contexts 
shows example context lengths 
case left context sellers right context 
pos tags words candidate features nodes left right context 
feature represented triplet position target nodes denotes feature type value feature 
represents node left context denotes left right node target nodes 
denotes right context 
feature type value summarized table 
table ch pos ch lex ch pos ch lex information child nodes context called child features 
analysis prepositions auxiliary verbs relative pronouns information children preference dependencies constructed 
child features predicates modifies plural noun sellers modifies singular noun resort 
child features dynamically determined step analysis 
table summary feature types values 
type value pos part speech pos tag string lex word string ch pos pos tag string child node modifying parent node left side ch lex word string child node node modifying parent node left side ch pos pos tag string child node modifying parent node right side ch lex word string child node modifying parent node right side example features follows pos lex pos nns lex sellers ch pos dt ch lex pos lex pos nn lex resort ch pos jj ch lex pos wp lex ch pos vbd ch lex pos lex 
grouping training example reducing learning cost computational cost training svms roughly proportional number training examples 
large amount training sentences penn treebank standard set section number training examples 
difficult learn training examples 
cope problem divided training examples groups 
current implementation divide provisionally training examples pos tag left target node 
svms os set pairwise svms learning training examples including feature pos pos 
analysis test sentences estimation parsing action performed svms corresponding pos tag left target node 
example pos tag left target node nn analysis test sentences parser estimates actions svms nn experiments experiments standard data set penn treebank section training data section test data 
convert phrase structure trees penn treebank dependency structure trees head rules similar collins ones 
part speech tagging training data annotated tags intact 
test sentences pos tagged nakagawa tagger revision learning svms tagging accuracy test data experiments 
sentences section test set include words number words punctuation marks 
performance parser evaluated types measure 
dependency accuracy root accuracy complete rate applied words excluding punctuation marks 
dependency accuracy dep 
acc 
root accuracy root acc 
complete rate comp 
rate results number correct parents total number parents number correct root nodes total number sentences number complete parsed sentences total number sentences firstly investigate performance degrees polynomial kernel functions 
table illustrates dependency accuracy different number degrees polynomial kernel functions 
best result obtained 
results superior 
results suggest learning dependency structure requires take account combination multiple features solely single features 
accuracy lower 
may overfitting known curse dimensionality number dimension feature spaces account combination features larger 
table dependency accuracies degrees polynomial kernel functions context length 
features described table 
dep 
acc 
root acc 
comp 
rate investigate performance different length context 
table illustrates results length context 
best dependency accuracy root accuracy model best result complete rate 
demonstrates depen dency accuracy depends context length longer right context contributes performance 
result worse 
reason lies features effective parsing included context 
table dependency accuracies length context kernel function features described table 
context length dep 
acc 
root acc 
comp 
rate investigate features contribute dependency accuracy especially focusing child features 
examine sets features child features pos lex table lexical features child nodes pos lex ch lex ch lex table part speech pos tag features child node pos lex ch pos ch pos table pos tag parent node md wp wp pos tag child nn nns nnp vb vbd vbg vbn vbp vbz md pos tag lexical feature child nodes pos tag features child 
pos tag lexical features child nodes features table table dependency accuracies sets features context length 
kernel function 
dep 
acc 
root acc 
comp 
rate num 
features table shows results feature set 
accuracy lower 
result shows dynamic dependencies estimated svms effective parsing process deterministic manner 
best result comparable result 
accuracy superior feature set include lexical information appeared training examples higher dimensional feature space feature set 
results suggest lexical information effective features dependency analysis 
comparison related compare parser statistical parsers maximum entropy inspired parser proposed charniak probabilistic generative model proposed collins 
parsers produce phrase structure sentences output phrase structure section converted dependency trees head rules parser input sentences collins parser assigned part speech nakagawa tagger comparison 
didn nakagawa tagger input sentences assigned part speech word input sentences 
parse trained best setting kernel function context length feature set table 
table shows result comparison 
best result collins models comparable 
result parser lower parsers natural information phrase structure solely rely word level dependency relation 
analysis near leaf dependency trees features parser similar phrase structure parsers disadvantage parser 
example analysis dependencies tested part speech tags aux auxiliary verb categories added head rules 
head rule verb phrase modified 
tags included tag set penn treebank occurred outputs 
table comparison related 
charniak collins parser model model model dep 
acc 
root acc 
comp 
rate leaf acc 
corresponding base noun phrase dependency tree able predicate subtree part noun phrase parent nodes part speech tag 
fact leaf acc 
parser denotes dependency accuracy leaf nodes comparable collins table 
analysis nearby root dependency trees phrase structure parsers richer information parser 
parser estimates dependency relation top nodes subtrees corresponds clause sentence verb phrase including head sentence 
structure subtrees similar parent node verb children consist nouns prepositions adverbs 
difficult estimate parent node correctly 
phrase structure parsers head information phrase label phrase vp sbar 
labels phrases predictors estimating dependencies relation 
root accuracy collins parser table higher parser 
parser accuracy looks result estimating dependency relations information label phrases 
related link grammar analyzes word word dependencies call links relation called connector represented subject sentence object verb 
parser analyze word word dependencies possible incorporate connector parser adding label connector actions left right right subject left object 
eisner propose methods dependency parsing probabilistic model reported experiments penn treebank 
best result experiments achieved dependency accuracy 
impossible compare results results directly 
size training test data different experiments 
didn evaluate sentences contained conjunction 
compared parser collins parser older version experiments 
result collins parser dependency accuracy data set experiments 
parser tested various sentences contained conjunction achieved accuracy 
performance parser better eisner 
denominator leaf acc 
number leaf nodes annotated human numerator number leaves estimated parents correctly parser 
focus dependency structure analysis hoping possibility preparing sufficient training data noise various domains proposed method determin istic dependency analysis support vector machines 
experimented dependency trees converted penn treebank data achieved accuracy 
result little worse date phrase structure parsers looks satisfactorily accurate considering parser uses information phrase structures 
includes improvement accuracy 
current implementation parser discriminate kinds structure dependency tree dependencies relation target nodes ii dependency relation target nodes node child complete subtree 
parser executes action shift kinds structure 
order cope problem need divide action shift kinds actions shift wait corresponding ii respectively 
new actions behavior parsing process target nodes move right learned different class svms 
application various domains 
order evaluate effectiveness dependency structure necessary apply parser quite different domains 
apply parser biological medical abstracts medline 
internet grateful med development team national medline med line 
daniel sleator davy temperley 
parsing english link grammar 
technical report technical report cmu cs carnegie mellon university october 
eugene charniak 
maximum entropy inspired parser 
proceedings sec ond meeting north american chapter association computational linguistics naacl pages 
jason eisner 
empirical comparison probability models dependency grammar 
technical report ircs ircs univ pennsylvania 
jason eisner 
new probabilistic models dependency parsing exploration 
proceedings th international conference computational linguistics coling pages 
michael collins 
new statistical parser bigram lexical dependencies 
proceedings th annual meeting association computational linguistics pages 
michael collins 
generative lexicalised models statistical parsing 
proceed ings th annual meeting association computational linguistics jointly th conference eacl pages 
mitchell marcus beatrice santorini mary ann marcinkiewicz 
building large annotated corpus english penn treebank 
computational linguistics 
nakagawa taku yuji matsumoto 
revision learning application part speech tagging 
proceedings association computational linguistics pages 
adwait ratnaparkhi 
learning parse natural language maximum entropy models 
machine learning 
vladimir vapnik 
nature statistical learning theory 
new york 
vladimir vapnik 
statistical learning theory 
wiley interscience publication 
