labeled rtdp improving convergence real time dynamic programming bonet computer science department university california los angeles los angeles ca usa bonet cs ucla edu rtdp heuristic search dp algorithm solving non deterministic planning problems full observability 
relation dynamic programming methods rtdp bene ts rst evaluate entire state space order deliver optimal policy second deliver policies pretty fast 
hand rtdp nal convergence slow 
introduce labeling scheme rtdp speeds convergence retaining anytime behavior 
idea label state solved heuristic values greedy policy de ned converged states reached greedy policy 
due presence cycles labels computed recursive bottom fashion general show computed quite fast overhead compensated recomputations avoided 
addition labeling procedure label state solved improves heuristic value relevant state 
results number labeled rtdp trials needed convergence number rtdp trials bounded 
practical point view labeled rtdp lrtdp converges orders magnitude faster rtdp faster heuristic search dp algorithm lao 
lrtdp converges faster value iteration heuristic suggesting lrtdp quite general scope 
rtdp algorithm solving non deterministic planning problems full observability understood heuristic search dynamic programming dp procedure barto bradtke singh 
rtdp involves simulated greedy searches heuristic values states visited updated dp fashion making consistent heuristic values possible successor states 
provided heuristic lower bound admissible goal reachable state positive probability updates yield key properties rst rtdp trials runs trapped loops eventually reach goal second repeated trials yield optimal heuristic values optimal policy closed set states copyright american association arti cial intelligence www aaai org 
rights reserved 
ctor geffner departament de tecnologia universitat pompeu fabra barcelona espa hector geffner upf es includes initial state barto bradtke singh bertsekas 
rtdp corresponds generalization korf lrta non deterministic settings algorithm underlies gpt planner bonet geffner 
relation dp algorithms normally non deterministic settings value policy iteration bertsekas puterman rtdp bene ts rst focused evaluate entire state space order deliver optimal policy second anytime behavior produces policies fast policies improve smoothly time 
features follow simulated greedy exploration state space paths resulting greedy policy considered paths considered 
undesirable effect strategy paths tend ignored rtdp convergence slow 
introduce labeling scheme rtdp speeds convergence rtdp quite dramatically retaining focus anytime behavior 
idea label state solved heuristic values greedy policy de ned converged states reached greedy policy 
due presence cycles labels computed recursive bottom fashion general show computed quite fast overhead compensated recomputations avoided 
addition labeling procedure fails label state solved improves heuristic value relevant state 
result number labeled rtdp trials needed convergence number rtdp trials bounded 
practical point view show empirically problems labeled rtdp lrtdp converges orders magnitude faster rtdp faster heuristic search dp algorithm lao hansen zilberstein 
experiments lrtdp converges faster value iteration heuristic function suggesting lrtdp may quite general scope 
organized follows 
cover rst basic model non deterministic problems full feedback value iteration rtdp section 
assess anytime convergence behavior methods section introduce labeling procedure rtdp discuss theoretical properties section 
nally run empirical evaluation resulting algorithm section brief discussion section 
preliminaries provide brief review models needed characterizing non deterministic planning tasks full observability algorithms proposed solving 
follow presentation bonet geffner see bertsekas excellent exposition eld 
deterministic models feedback deterministic state models newell simon nilsson basic action models ai consist nite set states nite set actions state transition function describes actions map state 
classical planning problems understood terms deterministic state models comprising 
discrete nite state space 
initial state 
non empty set goal states 
actions applicable state 
deterministic state transition function 
positive action costs 
solution model type sequence actions 
generates state trajectory 
sn si ai action ai applicable si sn goal state ai si sn solution optimal total cost si ai minimal 
non deterministic models feedback non deterministic planning problems full feedback modeled replacing deterministic transition function transition probabilities pa resulting model 
discrete nite state space 
initial state 
non empty set goal states 
actions applicable state 
transition probabilities pa reaching state action 
positive action costs 
numbers pa real non negative sum states 
ideas apply slight modi cation non deterministic planning transitions modeled means functions mapping non empty set states bonet geffner 
states si result action ai predictable assumed fully observable providing feedback selection action ai 
states fully observable 
models known markov decision processes mdps speci cally stochastic shortest path problems bertsekas 
due presence feedback solution mdp action sequence function mapping states actions 
function called policy 
policy assigns probability state trajectory 
starting state product transition probabilities pai si si ai si 
assume actions goal states costs produce changes pa expected cost associated policy starting state weighted average probability trajectories multiplied cost si si 
optimal solution control policy minimum expected cost possible initial states 
optimal solution models form guaranteed exist goal reachable state positive probability bertsekas 
optimal solution necessarily unique 
interested computing optimal near optimal policies 
assume initial state system known suf cient compute partial policy closed relative 
partial policy prescribes action taken subset states 
refer states reachable probability including partial policy closed relative 
refer states reachable optimal policy relevant states 
traditional dynamic programming methods value policy iteration compute complete policies heuristic search dp methods rtdp lao focused aim compute closed partial optimal policies smaller set states includes relevant states 
traditional heuristic search algorithms achieve means suitable heuristic functions provide admissible estimates expected cost reach goal state dynamic programming heuristic function de nes greedy policy argmin pa expected cost resulting states assumed 
call greedy action heuristic value function denote optimal expected cost state goal discounted mdp formulations see puterman bertsekas 
uses mdps ai see example sutton barto boutilier dean hanks barto bradtke singh russell norvig 
known greedy policy optimal optimal cost function due possible presence ties greedy policy unique assume ties broken systematically static ordering actions 
result value function de nes unique greedy policy optimal cost func tion de nes unique optimal policy 
de ne relevant states states reachable optimal policy constitute minimal set states optimal value function needs de ned 
value iteration standard dynamic programming method solving non deterministic states models computing optimal cost function plugging greedy policy 
optimal cost function unique solution xed point equation bellman min pa called bellman equation 
stochastic shortest path problems border condition needed goal states value iteration solves plugging initial guess right hand obtaining new guess left hand side 
form value iteration known asynchronous value iteration bertsekas operation expressed min pa vector size initialized arbitrarily initialized replaced equality assignment 
expression updating state value called state update simply update 
standard synchronous value iteration states updated parallel asynchronous value iteration selected subset states selected update 
cases known states updated nitely value function converges eventually optimal value function 
case concerns model assumption needed convergence bertsekas goal reachable positive probability state 
practical point view value iteration stopped bellman error residual de ned difference left right sides def min pa states suf ciently small 
discounted mdp formulation bound policy loss difference expected cost policy expected cost optimal policy obtained simple expression discount factor maximum de nition relevant states restricted barto bradtke singh includes states reachable optimal policy 
rtdp state repeat state goal pick best action update hash update stochastically simulate state algorithm rtdp algorithm termination conditions see text residual 
stochastic shortest path models similar closed form bound bound computed expense bertsekas hansen zilberstein 
execute value iteration maximum residual smaller bound bound policy loss higher desired process repeated smaller 
reasons take basic task computation value function residuals smaller equal parameter states 
assume initial state known check residuals states reached greedy policy de nition known results proceed 
say cost vector monotonic iff min pa known results barto bradtke singh bertsekas theorem assumption optimal values model non negative nite admissibility preserved updates 
real time dynamic programming rtdp simple dp algorithm involves sequence trials runs starting initial state goal state algorithm common routines de ned alg 
rtdp trial result simulating greedy policy updating values states visited 
rtdp asynchronous value iteration algorithm single state selected update iteration 
state corresponds state visited simulation successor states selected corresponding probability 
combination powerful rtdp different state return argmin qvalue state qvalue action return state update value qvalue pa value state action pick probability pa return state residual return value qvalue algorithm state routines 
pure greedy search general asynchronous value iteration 
proofs properties rtdp barto bradtke singh bertsekas bertsekas tsitsiklis model assumed 
theorem bertsekas tsitsiklis assumption holds rtdp greedy policy trapped loops forever eventually reach goal trial 
rtdp trial terminates nite number steps 
theorem barto 
assumption holds initial value function admissible lower bound repeated rtdp trials eventually yield optimal values relevant states 
note asynchronous value iteration rtdp requirement states updated nitely 
states may updated 
hand proof optimality lies fact relevant states updated 
results important 
classical heuristic search algorithms ida solve optimally problems states 
korf taylor lower bounds admissible heuristic functions allows bypass states space 
rtdp rst dp method offers potential general non deterministic setting 
course cases quality heuristic functions crucial performance classical heuristic search aimed discovery new effective heuristic functions see korf 
lao hansen zilberstein heuristic search dp algorithm shares feature 
ef cient implementation rtdp hash table needed storing updated values value function initially values function heuristic function table empty 
time value state table updated entry table created 
states table states table 
convergence rtdp asymptotic number trials convergence bounded low probability transitions taken eventually positive probability taken arbitrarily large number steps 
practical reasons value iteration de ne termination rtdp terms bound residuals termination criterion rtdp barto bradtke singh 
precisely terminate rtdp value function converges initial state relative positive parameter de nition value function converges state relative parameter residuals states reachable greedy policy smaller equal 
provided termination condition shown approaches rtdp delivers optimal cost function optimal policy relevant states 
convergence anytime behavior practical point view rtdp positive negative features 
positive feature rtdp anytime behavior quickly produce policy smoothly improve time 
negative feature convergence termination slow 
illustrate features comparing anytime convergence behavior rtdp relation value iteration vi couple experiments 
display anytime behavior methods plotting average cost goal greedy policy function time 
cost changes time result updates value function 
averages taken running simulations trials rtdp simulations update vi 
curves fig 
smoothed moving average points rtdp points vi 
domain experiments racetrack barto bradtke singh 
problem domain characterized racetrack divided cells see fig 

task nd control driving car set initial states set goal states minimizing large ring large square elapsed time rtdp vi elapsed time quality pro les average cost goal vs time value iteration rtdp 
number time steps 
states tuples dx dy represent position speed car dimensions 
actions pairs ax ay instantaneous accelerations ax ay 
uncertainty domain comes assuming road slippery result car may fail accelerate 
precisely action ax ay intended effect time time action effects correspond action 
car hits wall velocity set zero position left intact different barto bradtke singh car moved start position 
experiments consider shapes ring states large ring full square states large square 
cases task drive car extreme racetrack opposite extreme 
optimal policies problems turn expected costs number steps respectively percentages relevant states reachable optimal policy respectively 
shows quality policies function time value iteration rtdp 
cases initial value function informative heuristic rtdp 
quality pro le rtdp better produces policy practically produced eventually vi half time 
time rtdp yields policy average cost close optimal vi yields policy average cost times higher 
problem rtdp hand convergence 
examples rtdp produces optimal policies times seconds respectively converges sense de ned minutes 
value iteration hand converges far considered single initial state simple modify algorithm extend theoretical results case multiple initial states 
rtdp vi racetrack ring 
initial goal positions marked left right seconds respectively 
labeled rtdp fast improvement phase rtdp slow convergence phase consequences exploration strategy greedy simulation privileges states occur paths resulting greedy policy 
states relevant current value function updating produces large impact 
states paths needed convergence states appear simulations 
suggests potential way improving convergence rtdp adding noise simulation 
take different approach interesting consequences practical theoretical point view 
different adding noise action selection rule done reinforcement learning algorithms sutton barto reinforcement learning noise needed action selection rule guarantee optimality thing needed rtdp 
full dp updates rtdp partial dp updates td learning preserve admissibility 
roughly forcing rtdp greedy states keep track states value function converged avoid visiting states 
accomplished means labeling procedure call 
refer graph states reachable state greedy policy current value function greedy graph refer set states graph greedy envelope 
invoked state procedure searches greedy graph rooted state residual greater 
state state labeled solved returns true 
returns true labels solved current value function converged case labels solved states greedy envelope de nition converged 
hand state greedy envelope residual greater possibly states updated returns false 
updates crucial accounting theoretical practical differences rtdp labeled version rtdp call labeled rtdp lrtdp 
note due presence cycles greedy graph possible general type recursive bottom labeling procedures common ao implementations nilsson state labeled solved successors states solved 
labeling procedure sound incomplete presence cycles may fail label solved states value function converged 
code shown alg 

label state procedure represented bit hash table denoted solved 
state labeled solved iff solved true 
note depth search stores states visited closed list avoid loops duplicate 
time space complexity worst case tighter bound sv sv refers greedy envelope value function distinction relevant greedy envelope may smaller complete state space certain cases depending domain quality initial value heuristic function 
additional detail procedure shown alg 
performance reasons search greedy graph stopped state residual greater states collected update closed list 
states act fringe states search proceed beneath children greedy graph added labeling procedures solving graphs cyclic solutions hinted hansen zilberstein authors mention reasons preclude backward induction procedures computing state values graphs cyclic solutions preclude bottom recursive labeling procedures standard ao implementations 
state float rv true open closed solved open push open open pop closed push check residual residual rv false continue expand state foreach pa solved open closed open push rv true label relevant states foreach closed solved true update states residuals ancestors closed closed pop update return rv algorithm open list 
presence monotonic value function dp updates non decreasing effect result establish key property theorem assume admissible monotonic value function 
call labels state solved increases value state decreasing value 
consequence solve models form 
say model solved value function converges initial state simple prove theorem provided initial value function admissible monotonic goal reachable state procedure solved solves model number iterations greater iteration complexity 
lrtdp state float solved state float visited solved insert visited visited push check termination goal states goal break pick best action update hash update stochastically simulate state try labeling visited states reverse order visited visited pop break algorithm lrtdp 
solving procedure theorem novel interesting suf ciently practical 
problem attempt label states solved labeling 
greedy envelope strongly connected graph state converge rst 
particular states close goal normally converge faster states farther 
motivation alternative procedure labeled rtdp algorithm 
lrtdp trials rtdp trials trials terminate solved stated reached initially goal states solved invoke procedure reverse order unsolved state visited trial back procedure returns false state 
labeled rtdp terminates initial state labeled solved 
code lrtdp shown alg 

rst property lrtdp inherited rtdp theorem provided goal reachable state initial value function admissible lrtdp trials trapped loops terminate nite number steps 
novelty optimality property lrtdp bound total number trials required convergence theorem provided goal reachable state initial value function admissible mono tonic lrtdp solves model number trials bounded 
empirical evaluation section evaluate lrtdp empirically relation rtdp dp algorithms value iteration standard dynamic programming algorithm lao extension ao algorithm introduced hansen zilberstein dealing graphs cyclic solutions 
value iteration baseline performance practical algorithm computes complete policies needs memory states state space 
lao hand heuristic search algorithm rtdp lrtdp computes partial optimal policies need consider entire space 
interested comparing algorithms dimensions anytime convergence behavior 
evaluate anytime behavior plotting average cost policies different methods function time described section convergence anytime behavior 
likewise evaluate convergence displaying time memory number states required 
perform analysis domain independent heuristic function initializing value function heuristic function 
obtained solving simple relaxation bellman equation min pa expected value taken possible successor states replaced min value min min pa clearly optimal value function relaxation lower bound compute need running labeled variant lrta algorithm korf deterministic variant rtdp convergence state heuristic value needed 
standard dp methods lrta require evaluate entire state space order compute values separate hash table containing values produced calls kept memory values reused calls doing single source shortest path problems need opposed single potentially costly sources shortest path problem set states large 
total time taken computing heuristic values heuristic search procedures lrtdp lao quality estimates shown separately 
brie see time signi cant significant time taken lrtdp 
see time remains competitive respect value iteration lrtdp algorithm heuristic 
call heuristic de ned relaxation hmin 
easy show heuristic admissible monotonic 
large ring large square elapsed time rtdp vi lao lrtdp elapsed time quality pro les average cost goal vs time rtdp vi lrtdp heuristic problem rel 
hmin hmin small large track small large small large small large table information different instances size expected cost optimal policy percentage relevant states heuristic time spent seconds computing hmin 
problems problems consider instances racetrack domain barto bradtke singh discussed earlier 
consider instances small barto bradtke singh track hansen zilberstein tracks ring square small large corresponding shapes 
table contains relevant information instances size expected cost optimal policy percentage relevant states 
columns show information heuristic hmin value hmin total time involved computation heuristic values lrtdp lao algorithms different instances 
interestingly times roughly equal algorithms different problem instances 
note heuristic provides pretty tight lower bound somewhat expensive 
run experiments heuristic hmin heuristic 
taken source code lao 
problems multiple initial states value hmin table average intial states 
algorithms rtdp vi lao lrtdp consider algorithms comparison vi rtdp lrtdp variant lao called improved lao hansen zilberstein 
due cycles standard backward induction step backing values ao replaced lao full dp step 
ao lao gradually grows explicit graph envelope originally including state iteration computes best policy graph 
lao stops graph closed respect policy 
practical point view lao slow improved lao variation lao hansen zilberstein gives properties lao runs faster 
algorithms implemented 
results obtained sun fire gb ram clock speed mhz 
results curves fig 
display evolution average cost goal function time different algorithms averages computed explained problems large ring large square 
curves correspond 
rtdp shows best pro le quickly producing policies average costs near optimal lrtdp close vi farther 
table shows times needed convergence seconds results values similar 
times rtdp reported exceed cutoff time convergence minutes instances 
note heuristic lrtdp converges faster vi problems problem track lrtdp running hansen zilberstein implementation lao different algorithm different properties 
particular lao maintain optimal policy explicit graph iterations 
algorithm small large track small large small large small large vi lrtdp table convergence time seconds different algorithms initial value function times rtdp shown exceed cutoff time convergence minutes 
faster times shown bold font 
algorithm small large track small large small large small large vi hmin hmin lrtdp hmin table convergence time seconds different algorithms initial value function hmin times rtdp shown exceed cutoff time convergence minutes 
faster times shown bold font 
time taken vi 
takes longer solves problems reasonable time 
reason lrtdp rtdp behave absence informative heuristic function focused search updates quickly boost values appear relevant soon provide heuristic function heuristic function learned sense korf 
difference approaches lrtdp hand classical dp methods vi prominent informative heuristic hmin seed value function 
table shows corresponding results rst rows show convergence times vi lrtdp initial values obtained heuristic hmin time spent computing heuristic values excluded 
average times displayed column table times roughly equal different methods standard deviation case average time 
clearly lrtdp heuristic information effectively vi computation heuristic values expensive lrtdp hmin account time compute heuristic values remains competitive vi lrtdp 
large square time times 
similar occurs small small ring 
course results improved speeding computation heuristic hmin currently working 
summary introduced labeling scheme rtdp speeds convergence retaining anytime behavior 
due presence cycles labels computed recursive bottom fashion standard ao implementations computed quite fast means search procedure label state solved improves value states nite amount 
labeling procedure interesting theoretical practical properties 
theoretical side number labeled rtdp trials number rtdp trials bounded practical side labeled rtdp converges faster rtdp appears converge faster value iteration lao exhibiting better anytime pro le 
labeled rtdp converges faster vi heuristic suggesting proposed algorithm may quite general scope 
currently working alternative methods computing approximating heuristic hmin proposed variations labeling procedure obtaining practical methods solving larger problems 
acknowledgments eric hansen shlomo zilberstein making code lao available 
bonet supported nsf onr afosr dod muri program usb fellowship 
barto bradtke singh 
learning act real time dynamic programming 
arti cial intelligence 
bellman 
dynamic programming 
princeton university press 
bertsekas tsitsiklis 
neuro dynamic programming 
athena scienti bertsekas 
dynamic programming optimal control vols 
athena scienti bonet geffner 
planning incomplete information heuristic search belief space 
chien kambhampati knoblock eds proc 
th international conf 
arti cial intelligence planning scheduling 
breckenridge aaai press 
bonet geffner 
gpt tool planning uncertainty partial information 
cimatti geffner giunchiglia rintanen eds proc 
ijcai workshop planning uncertainty partial information 
bonet geffner 
planning control arti cial intelligence unifying perspective 
applied intelligence 
boutilier dean hanks 
planning un der uncertainty structural assumptions computational leverage 
proceedings 
hansen zilberstein 
lao heuristic search algorithm nds solutions loops 
arti cial intelligence 
korf taylor 
finding optimal solutions puzzle 
clancey weld eds proc 
th national conf 
arti cial intelligence 
portland aaai press mit press 
korf 
real time heuristic search 
arti cial intelligence 
korf 
finding optimal solutions rubik cube patterns databases 
kuipers webber eds proc 
th national conf 
arti cial intelligence 
providence ri aaai press mit press 
newell simon 
human problem solving 
englewood cliffs nj prentice hall 
nilsson 
principles arti cial intelligence 
tioga 
puterman 
markov decision processes discrete stochastic dynamic programming 
john wiley sons russell norvig 
arti cial intelligence modern approach 
prentice hall 
sutton barto 
reinforcement learning 
mit press 
