doi cache blocking sparse matrix vector multiply works rajesh richard james demmel katherine yelick received november revised september published online march springer verlag new performance models compact data structures cache blocking applied sparse matrix vector multiply spm 
extend prior models relaxing assumption vectors fit cache find new models accurate predict optimum block sizes 
addition determine criteria predict cache blocking improves performance 
conclude architectural suggestions memory systems execute spm faster 
keywords performance optimization sparse matrix multiplication memory hierarchies performance modeling overview consider problem building high performance implementations sparse matrix vector multiply spm call source vector destination vector 
making spm fast complicated modern hardware architectures overhead manipulating sparse demmel yelick computer science division university california berkeley soda hall berkeley ca usa mail cs berkeley edu mail cs berkeley edu demmel mail demmel cs berkeley edu yelick mail yelick cs berkeley edu data structures 
unusual see spm run peak floating point performance single processor fig 

contrast optimizing dense matrix kernels dense blas performance depends nonzero structure matrix may known run time 
prior sparsity system version im developed algorithm generator search strategy spm quite effective practice 
sparsity generators employed variety performance optimization techniques including register blocking cache blocking multiplication multiple vectors 
cache blocking differs register blocking cache blocking reorders memory accesses increase temporal locality register blocking compresses data structure reduce memory traffic 
focuses performance models cache blocking sect 
asks fundamental questions limits exist performance tuning extending prior models accounting tlb translation look aside buffer buffer virtual physical address translations enabling accurate selection optimal cache block sizes 
cache blocking increases complexity data structures represent matrix adding extra set row pointers block 
trade needs benefit added temporal locality outweighs added costs accessing extra data structures facilitate increase temporal locality 
explore subset matrices sparse register blocking benefit 
matrices cache blocking significant impact performance 
simplify analysis consider especially sparse matrices cache blocking noticeable advantages disadvantages 
assume matrices stored row oriented format 
matrices stored column oriented data structure roles reversed 
classify set matrices see benefits cache blocking concluding cache blocking effective simultaneously fit cache fits cache nonzeros distributed matrix opposed band matrix non zero density sufficiently high high register blocking appreciable difference 
matrix exhibit properties cache blocking significant impact performance apply cache blocking matrix 
properties exist choice block size important blocking greatly increase high greatly decrease low performance compared unblocked versions 
aims construct models allow predict best block size static matrix machine properties 
traditional static models cache behavior select tile sizes dense kernels applied sparse kernels due presence indirect irregular memory accesses known run time 
number notable attempts model performance 
jalby heras fraguela developed sophisticated cache blocking sparse matrix vector multiply works probabilistic cache models assume uniform distribution non zero entries :10.1.1.18.8802
models differ ability account cross interference misses 
model sect 
differs prior consider multi level memory hierarchies including tlb explicitly model execution time addition cache misses 
gropp bounds similar ones develop analyze tune computational fluid dynamics code detailed performance study fracture mechanics code itanium 
considers tuning matrices come variety domains explores performance modeling cache block size selection 
due space limitations high level intuition summary data 
refer reader full report details 
software algorithms described available optimized sparse kernel interface 
collection low level primitives provide automatically tuned computational kernels sparse matrices solver libraries applications 
summary cache blocking optimization matrix data structures original matrix represented compressed sparse row format csr 
csr nonzero elements row column indices stored sequentially memory arrays called value col idx fig 
indexing array called block ptr identify new rows start 
essence cache blocking breaks csr matrix multiple smaller csr matrices stores sequentially memory 
addition new array called row start created points start row cache blocks 
diagram shown fig 

main goals choose size blocks get reuse caching source destination vectors keeping cost associated extra overhead low 
henceforth refer assume register blocking 
discuss compress size block row start rse optimization exploit fact cache block just sparse matrix 
technique allows easy recursion multiple levels cache blocking 
row start matrices especially band matrices blocked possible cache block non zeros exist rows 
cache block example nonzero elements tenth rows rest cache block empty 
basic cache blocked data structure loop zero rows doing useful 
row start block ptr col idx val fig 
cache blocking data structures 
order avoid unnecessary accesses new vector contains row start rs row re information cache block created point nonzero rows cache block 
new indexing information performance sensitive size cache block 
performance results shown optimization improve performance 
matrix suite pointers nonzero row sufficient nonzero rows clustered list nonzero rows added unnecessary overhead 
exploiting cache block structure described cache blocked matrix thought smaller sparse matrices stored sequentially memory 
exploit fact calling prior sparse matrix vector multiplication routines smaller matrix passing appropriate part source destination vectors arguments 
advantage handling multiplication fashion inner loops generated independently code cache blocking code previously written non cache blocked implementations reused 
optimization allows easy recursion multiple levels cache blocking 
tests indicate function call overhead negligible number cache blocks matrix usually small compared total memory operations 
analytic models section create analytic upper lower bounds performance modeling various levels memory hierarchy 
describe performance model model different parts memory system 
cache blocking sparse matrix vector multiply works contribute model 
create load model discuss analytic upper lower bounds number cache misses level 
examine upper lower bounds tlb misses develop complex relation tlb upper lower bounds yield accurate estimate actual number tlb misses 
techniques extended construct accurate cache models practice initial estimate tlb misses sufficiently accurate predict block sizes 
performance model performance model similar added latency term account tlb misses :10.1.1.18.8802
model execution time follows 
want upper bound performance lower bound time assume overlap latencies computation memory accesses 
hi number hits cache level mi number misses 
execution time hi mem tlb access time cycles seconds cache level lowest level cache mem memory access time 
hits loads loads number loads cache block size see sect 

assuming perfect nesting caches level access level hi mi mi 
tlb nested account assuming tlb misses overlapped misses levels serviced cache misses nonzero matrix entries leads floating point multiply floating point add 
get estimate upper bound performance lower serviced 
performance expressed mflop eq 
lower lower bound misses ith cache level discussed convert mflop similarly get lower bound performance letting mi upper upper upper bound misses ith cache level discussed 
order take tlb effects account estimate number cycles needed process tlb order eq 
match measured performance 
incorporate upper bound model setting equal tlb model described sect 

previously estimated values latencies 
load model assume cache block data structure described sect 

count number loads required spm follows 
matrix non zeros 
henceforth assume register blocking done optimal sparse test matrices 
define new variable equal number cache blocks cache block size produced 
case nonzeros uniformly randomly distributed matrix 
true depends nonzero structure matrix 
loads counted manner loads src vector matrix dest vector eq 
matrix terms counted follows 
count loads element matrix column index actual value 
cache block load corresponding rsj rej 
cache block load elements block ptr array twice index bound row starting bound row 
load element row start twice pointer row cache blocks start row cache blocks 
cache block calculate result corresponding destination vector updates corresponding destination vector 
assume matrix multiplication done compressed sparse row total number reads source vector fewest number loads occur matrix cache blocked case equals equals 
cache blocking decrease number loads 
cache blocks greatly increase overhead 
cache model develop upper lower bounds number cache misses lead lower upper bounds performance mflops respectively 
start cache 
cache line size integers 
assume double precision number represented twice number bytes integer 
order estimate minimum number cache misses occur take total amount data access divide line size 
give total number lines matrix source destination vectors take assuming data perfectly aligned 
cache blocking sparse matrix vector multiply works lower bound lower misses lower src vector matrix dest vector find lower bounds level cache simply replace appropriate line size 
find upper bound assume entry matrix loaded lower bound assume access source access destination vectors conflict capacity misses 
upper bound upper misses upper src vector matrix dest vector indicates access source vector 
second term number times access destination vector 
stream matrix entries access element number misses depend conflict capacity 
notice load model sect 
cache model section predict advantages cache blocking show increase data structure overhead 
tlb model simple load cache models cache blocking benefit 
turns main benefit cache blocking increased temporal locality source vector seen number tlb misses model 
experimental data sect 
fact show improvements cache misses captured model 
model turn adequate predicting cache block sizes 
estimate lower bounds tlb misses simply take total size data access divide page size 
quantity minimum number pages data resides minimum number compulsory misses tlb 
estimate upper bound assume load matrix page 
assume take tlb access source vector destination vector 
equations identical cache models eq 
eq 
replace line size page size 
test matrices incurred tlb misses itanium platform hardware tlb counters 
modeling performance merely lower upper bound models take increased locality cache blocking account lower bound cache misses calculate upper bound performance counts compulsory misses 
blocking adds overhead data storage amount overhead occurs blocking 
factor need accurate model 
previously observed matrices noticeable increase number tlb misses source vector occupies large fraction tlb 
number tlb misses orders magnitude higher incorrect block size chosen chose try accurately estimate number tlb misses combination lower upper bound models 
fig 
see distinct categories block sizes yielded performance matrix suite table section itanium 
category matrices matrices showed best performance column block size equaled th tlb 
second category matrices matrices added overhead blocking hurt performance performance best column block size log row dimension tlb size histogram peak performance itanium linux log column dimension tlb size fig 
histogram block sizes itanium 
row column block size shown shade cell represents number matrices performance peak block size chosen 
graphical way representing optimum cache block sizes variety matrices impact performance 
define tlb size number entries tlb multiplied page size 
itanium mb doubles probably due early eviction source vector lru page replacement policies 
cache blocking sparse matrix vector multiply works exceeded number columns matrix blocking column direction 
notice performance depend heavily row block size large conclude blocking done row dimension 
capture behavior performance model modified version tlb model combines upper bound lower bound create reasonable estimate number misses 
main aims expose penalty temporal locality accessing source vector 
account penalty tlb model switches upper bound model estimate number misses column block size large 
optimal block size percentage tlb size changes machine machine optimal crossover point switching lower upper bound models vary platform platform 
tlb counter data available itanium model platform 
models platforms similar 
tlb model tlb upper min tlb lower equation shows model calculate approximate number tlb misses itanium 
variables follows page size integers number tlb entries tlb number non zero columns 
variable define fraction tlb filled source vector 
empirical data itanium matrices optimal column block size th tlb column block size tlb switch upper bound model 
upper bound scaled fraction source vector overflows tlb 
switch performed matrix dense average number nonzeros nonzero column greater ensuring blocking provides reuse 
conditions fail lower bound model tlb misses 
itanium empirically determined values 
data shows model applied itanium closely predicts noticeable jump number tlb misses matrices matrices matrices cache blocking significant benefits 
predict cache block sizes 
refine model verify platforms 
peaks upper bound performance model correlate better peaks actual performance matrices 
actual definition large varies different platforms itanium set tlb 
model peaks upper bound model show blocking idea case 
evaluate models sect 

verification analytic model evaluate spm set matrices principle large sparse cache blocking significant effect 
properties matrices chosen referenced table 
evaluate performance model true hardware counters papi predict performance henceforth called papi model compare model estimates lower upper bound cache tlb misses henceforth termed analytic lower upper bound models 
cache memory latencies derived published processor manuals curve fitting experimental saavedra barrera memory system microbenchmark maps benchmarks 
due space limitations summary full data 
shows evaluation models sect 
performance line performance cache blocking best performance shows performance optimum cache block size exhaustive search power cache block sizes 
best rc analytic model line shows performance cache block size chosen analytic model 
best cache block size running analytical model various power cache block sizes 
analytic upper lower bounds show performance predicted models 
papi model line performance actual cache values hardware counters table matrix benchmark suite application area dimension nonzeros density dense matrix statistical experimental design linear programming lp lp latent semantic indexing column wise expansion lsi row wise expansion lsi row wise stamping lsi queuing model mutual exclusion lp problem italian scheduling italian scheduling small lp web connectivity graph wg wg mmd reordering wg rcm reordering note matrices just modified versions matrix cache blocking sparse matrix vector multiply works mflop performance model verification itanium linux base performance best performance best rc analytic model analytic upper bound model analytic lower bound model papi model matrix 
fig 
performance model intel itanium plugged execution time model 
shown fig 
analytic model sect 
performance factor itanium implying time unaccounted 
relative performance function block size predicted meaning model heuristic choosing block size 
performance optimal block sizes chosen maximize performance papi model best itanium implying model heuristic models accurate 
furthermore case matrix analytic model similarly predictions itanium yielding best performance 
notice base performance matrix itanium lower predicted lower bound performance 
examination actual cache misses matrix shows actual number misses closer modeled upper bound matrices indicating access source destination causing cache misses 
indicate memory latencies calculate lower bound performance optimistic 
validate hypothesis 
shows heuristic pentium power compared itanium tlb model itanium describe performance pentium power 
motivates different set tlb model parameters terms eq 
platforms described sect 

availability hardware tlb counters help model validation platforms 
shows upper bound performance performance power matrix caused prefetch engine correctly predicting access stream reducing number compulsory misses 
mflop mflop performance model verification power aix matrix base performance best performance best rc analytic model analytic upper bound model analytic lower bound model performance model verification pentium linux base performance best performance best rc analytic model analytic upper bound model analytic lower bound model papi model matrix 
fig 
performance model ibm power intel pentium 
note matrix run platforms due memory limitations evaluation matrices platforms matrix structure speedups matrix varied machines best speedups table observed matrices 
best speedups occurred matrices 
matrices matrices small row dimensions large column dimensions nonzeros scattered matrix 
furthermore largest increases cache cache blocking sparse matrix vector multiply works table speedups matrices platforms matrix 
platform itanium pentium power matrix platform itanium pentium power table shows performance optimum cache block divided performance non blocked implementation platform matrix 
highlighted values top speedups platform fraction misses fraction misses remaining blocking itanium linux best data cache best tlb misses matrix 
fig 
effect cache blocking data cache tlb itanium 
plot shows effect cache blocking cache misses tlb misses measured papi 
best data cache line number cache misses optimum block size divided number cache misses occur unblocked implementation 
best tlb misses line analogous line tlb 
matrices showed largest performance gains matrices see table showed greatest drop cache misses implying cache blocking having desired effect misses increased occurred matrices largest speedups implying cache blocking increased locality 
shows effect cache blocking 
plots show matrices largest speedups largest drop number cache misses tlb misses 
addition matrices show little change optimum cache misses implying cache blocking little effect matrices 
matrices sparse effectively reuse accessing source vector blocking help source vector large 
matrices densities higher matrices matrix matrices helped cache blocking provided column block size large greater elements matrix matrices matrices 
reuse blocking pay 
find general matrices row dimension column dimension benefit cache blocking 
smaller row dimension implies overhead added cache blocking small number rows limited 
larger column dimension implies unblocked implementations may lack locality 
matrix large column dimension blocking yield performance improvement 
performed additional experiments random banded matrices confirming theoretical jalby :10.1.1.18.8802
expected cache blocking help band relatively narrow natural access pattern optimal pays band grows 
case rse optimization smooths differences performance block sizes 
platform evaluation certain matrices matrix experienced significant performance gains cache blocking itanium power speedup drastic pentium 
expect average number cycles access memory grows cache blocking provide improvement performance cache blocking allows reduce expensive accesses main memory 
behavior cache blocked spm number implications architecture systems 
tlb misses reduced cache blocking avoided selecting large page sizes 
second hardware support cacheable non cacheable accesses memory useful access helped caches accesses matrix 
separate paths prevent cache conflicts matrix data source vector data 
contrast increased associativity partially addresses issue allows premature eviction old source vector elements matrix elements 
verify impact separate memory paths hybrid scalar vector architecture cray 
cache blocking significantly reduces cache misses spm particularly large small distribution nonzeros nearly random nonzero density sufficiently high 
conditions appear matrix find tlb misses important factor execution time 
new performance bounds models incorporate effect tlb implicitly cache blocking sparse matrix vector multiply works modeling capacity conflict misses ignored prior models :10.1.1.18.8802
new models predict optimal near optimal cache block size leading speedups 
includes improving accuracy models levels memory hierarchy obtain accurate memory latencies 
accurate models lead accurate heuristics decide cache block sparse matrix platform matrix structure 
analyze problem novel architectures 

bilmes demmel lam chin portable highperformance ansi coding methodology application matrix multiply university tennessee lapack working note 
browne dongarra garner london scalable cross platform infrastructure application performance tuning hardware counters 
proceedings supercomputing november 
fraguela zapata memory hierarchy performance prediction sparse blocked algorithms 
parallel proc lett 
gropp keyes smith realistic bounds implicit cfd codes 
proceedings parallel computational fluid dynamics pp 

alt mazurkiewicz fracture mechanics intel itanium architecture case study 
workshop epic architectures compiler technology acm micro austin tx 
heras perez rivera modeling improving locality irregular problems sparse matrix vector product cache memories case study 
europe pp 

im optimizing performance sparse matrix vector multiplication 
phd thesis university california berkeley may 
demmel yelick performance modeling analysis cache blocking sparse matrix vector multiply 
technical report ucb csd university california berkeley eecs dept 

saad basic toolkit sparse matrix computations www cs umn 
edu research arpa html 
saavedra barrera cpu performance evaluation execution time prediction narrow spectrum benchmarking 
phd thesis university california berkeley february 
carrington wolter modeling application performance convolving machine signatures application profiles 
jalby characterizing behavior sparse algorithms caches 
proceedings supercomputing 
optimized sparse kernel interface bebop cs berkeley 
edu 
demmel yelick lee performance optimizations bounds sparse matrix vector multiply 
proceedings supercomputing baltimore md usa november 
automatic performance tuning sparse matrix kernels 
phd thesis university california berkeley 
whaley dongarra automatically tuned linear algebra software 
proceedings supercomputer 
