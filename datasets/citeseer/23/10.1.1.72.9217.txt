journal artificial intelligence research submitted published perseus randomized point value iteration pomdps spaan science uva nl nikos vlassis vlassis science uva nl informatics institute university amsterdam kruislaan sj amsterdam netherlands partially observable markov decision processes pomdps form attractive principled framework agent planning uncertainty 
point approximate techniques pomdps compute policy finite set points collected advance agent belief space 
randomized point value iteration algorithm called perseus 
algorithm performs approximate value backup stages ensuring backup stage value point belief set improved key observation single backup may improve value belief points 
contrary point methods perseus backs randomly selected subset points belief set sufficient improving value belief point set 
show idea extended dealing continuous action spaces 
experimental results show potential perseus large scale pomdp problems 

major goal artificial intelligence build intelligent agents russell norvig 
intelligent agent physical simulated able autonomously perform task characterized sense think act loop uses sensors observe environment considers information decide executes chosen action 
agent influences environment acting detect effect actions sensing environment closes loop 
interested computing plan maps sensory input optimal action execute task 
consider types domains agent uncertain exact consequence actions 
furthermore determine full certainty state environment single sensor reading environment partially observable agent 
planning kinds uncertainty challenging problem requires reasoning possible futures possible histories 
partially observable markov decision processes pomdps provide rich mathematical framework acting optimally partially observable stochastic environments astr aoki sondik lovejoy kaelbling littman cassandra :10.1.1.107.9127
pomdp defines sensor model specifying probability observing particular sensor reading specific state stochastic transition model captures uncertain outcome executing action 
agent task defined reward receives time step goal maximize discounted cumulative reward 
assuming discrete models pomdp framework allows capturing uncertainty introduced transition observation model defining operating belief state agent 
belief ai access foundation 
rights reserved 
spaan vlassis state probability distribution states summarizes information regarding past 
belief states allows transform original discrete state pomdp continuous state markov decision process mdp turn solved corresponding mdp techniques bertsekas tsitsiklis 
optimal value function pomdp exhibits particular structure piecewise linear convex exploit order facilitate solving 
value iteration instance method solving pomdps builds sequence value function estimates converge optimal value function current task sondik 
value function parameterized finite number hyperplanes vectors belief space partition belief space finite amount regions 
vector maximizes value function certain region action associated optimal action take beliefs region 
computing value function estimate looking step deeper requires account possible actions agent take subsequent observations may receive 
unfortunately leads exponential growth vectors planning horizon 
computed vectors useless sense maximizing region empty identifying subsequently pruning expensive operation 
exact value iteration algorithms sondik cheng kaelbling search value iteration step complete belief simplex minimal set belief points generate necessary set vectors horizon value function 
typically requires linear programming costly high dimensions 
zhang zhang argued value iteration converges optimal value function exact value iteration steps interleaved approximate value iteration steps new value function upper bound previously computed value function 
results speedup total algorithm linear programming needed order ensure new value function upper bound previous complete belief simplex 
general computing exact solutions pomdps intractable problem papadimitriou tsitsiklis madani hanks condon calling approximate solution techniques lovejoy hauskrecht 
practical tasks compute solutions parts belief simplex reachable encountered interacting environment 
motivated approximate solution techniques focus sampled set belief points planning performed hauskrecht poon roy gordon pineau gordon thrun spaan vlassis possibility mentioned lovejoy :10.1.1.12.7112
idea planning complete belief space agent intractable large state spaces planning carried limited set prototype beliefs sampled letting agent interact randomly environment 
pbvi pineau instance builds successive estimates value function updating value gradient points dynamically growing belief set 
describe perseus randomized point value iteration algorithm pomdps vlassis spaan spaan vlassis 
perseus operates large set beliefs gathered simulating random interactions agent pomdp environment 
belief set number value backup stages performed 
perseus randomized point value iteration pomdps algorithm ensures backup stage value point belief set improved decrease 
contrary point methods perseus backs random subset belief points key observation single backup may improve value points set 
allows compute value functions consist small number vectors relative belief set size leading significant speedups 
evaluate performance perseus benchmark problems literature show competitive methods terms solution quality computation time 
furthermore extend perseus compute plans agents continuous large discrete set actions disposal spaan vlassis 
examples include navigating arbitrary location rotating pan tilt camera desired angle 
pomdp solution techniques targets discrete action spaces exceptions include application particle filter continuous state action space thrun certain policy search methods ng jordan baxter bartlett 
report experiments domain agent equipped proximity sensors move continuous heading distance experimental results navigation task involving mobile robot omnidirectional vision perceptually aliased office environment 
remainder structured follows section review pomdp framework ai perspective discuss exact methods solving pomdps tractability problems 
outline class approximate value iteration algorithms called point techniques 
section describe discuss perseus algorithm extension continuous action spaces 
related approximate techniques pomdp planning discussed section 
experimental results problem domains section 
wrap section 
partially observable markov decision processes partially observable markov decision process pomdp models repeated interaction agent stochastic environment parts hidden agent view 
agent goal perform task choosing actions fulfill task best 
stated agent compute plan optimizes performance measure 
assume time discretized time steps equal length start step agent execute action 
time step agent receives scalar reward environment performance measure directs agent maximize cumulative reward gather 
reward signal allows define task agent give agent large positive reward accomplishes certain goal small negative reward action leading 
way agent steered finding plan accomplish goal fast possible 
pomdp framework models stochastic environments agent uncertain exact effect executing certain action 
uncertainty captured probabilistic transition model case fully observable markov decision process mdp sutton barto bertsekas tsitsiklis 
mdp defines transition model specifies probabilistic effect action changes state 
extending spaan vlassis mdp setting pomdp deals uncertainty resulting agent imperfect sensors 
allows planning environments partially observable agent environments agent determine full certainty true state environment 
general partial observability stems sources multiple states give sensor reading case agent sense limited part environment sensor readings noisy observing state result different sensor readings 
partial observability lead perceptual aliasing different parts environment appear similar agent sensor system require different actions 
pomdp captures partial observability probabilistic observation model relates possible observations states 
formally pomdp assumes time step environment state agent takes action receives reward environment result action environment switches new state known stochastic transition model 
markov property entails depends previous state action agent perceives observation may conditional action provides information state known stochastic observation model 
sets assumed discrete finite generalize continuous section 
order agent choose actions successfully partially observable environments form memory needed observations agent receives provide unique identification transition observation model pomdp transformed belief state mdp agent summarizes information past belief vector 
belief probability distribution forms markovian signal planning task 
beliefs contained dimensional simplex means represent belief numbers 
pomdp problem assumes initial belief instance set uniform distribution states representing complete ignorance regarding initial state environment 
time agent takes action observes belief updated bayes rule normalizing constant 
discussed goal agent choose actions fulfill task possible compute optimal plan 
plan called policy maps beliefs actions 
note contrary mdps policy function continuous set probability distributions policy characterized value function defined expected discounted reward agent gather starting belief bt bt bt bt bt bt discount rate 
discount rate ensures finite sum usually chosen close 
policy maximizes called optimal policy specifies optimal action execute perseus randomized point value iteration pomdps current step assuming agent act optimally time steps 
value optimal policy defined optimal value function satisfies bellman optimality equation hv max bo bellman backup operator bellman 
holds ensured solution optimal 
approximated iterating number stages see section stage considering step 
problems finite planning horizon piecewise linear convex smallwood sondik infinite horizon tasks approximated arbitrary value function 
parameterize value function vn stage finite set vectors hyperplanes vn 
additionally vector action associated optimal take current step 
vector defines region belief space vector maximizing element vn 
regions form partition belief space induced piecewise linearity value function 
examples value function state pomdp shown fig 

set vectors vn stage value belief vn max denotes inner product 
gradient value function vector arg max policy 
exact value iteration computing optimal plan agent means solving pomdp classical method value iteration puterman 
pomdp framework value iteration involves approximating applying exact dynamic programming operator approximate operator initially piecewise linear convex value function 
commonly produced intermediate estimates 
piecewise linear convex 
main idea value iteration algorithms pomdps value function vn particular belief point easily compute vector arg max unknown set vectors 
denote operation backup 
computes optimal vector belief back projecting vectors current horizon value function step returning vector maximizes value particular defining ra max spaan vlassis vn max ra vn ra max max ra ra max max max gi 
applying identity maxj arg maxj twice compute vector backup follows backup arg max gb ra arg max gi 
computing vector backup straightforward locating minimal set points required compute vectors costly 
region belief space maximal family algorithms tries identify regions sondik cheng kaelbling 
corresponding region called witness point existence region 
set exact pomdp value iteration algorithms focus searching belief space consider enumerating possible vectors pruning useless vectors monahan cassandra littman zhang 
example exact value iteration consider straightforward way computing due monahan 
involves calculating possible ways constructed exploiting known structure value function 
operate independent particular longer applied 
include ways selecting ga ga ra denotes cross sum operator 
unfortunately stage number vectors exponential generated vn regions generated vectors empty vectors useless identifying subsequently pruning requires linear programming introduces considerable additional cost state space large 

cross sum sets defined rk 
rk 
perseus randomized point value iteration pomdps zhang zhang proposed alternative approach exact value iteration designed speed exact value iteration step 
turns value iteration converges optimal value function exact value update steps interleaved approximate update steps new value function vn computed vn vn vn 
additionally requires value function appropriately initialized choosing single vector components equal mins 
vector represents minimum cumulative discounted reward obtainable pomdp guaranteed zhang zhang compute vn backing witness points vn number steps 
saw backing set belief points relatively cheap operation 
vn number vectors created applying backup witness points vn set linear programs solved ensure vn vn 
repeated number steps exact value update step takes place 
authors demonstrate experimentally combination approximate exact backup steps speed exact value iteration 
general computing optimal planning solutions pomdps intractable problem reasonably sized task papadimitriou tsitsiklis madani 
calls approximate solution techniques 
describe line research approximate pomdp algorithms focus planning fixed set belief points 
approximate value iteration major cause intractability exact pomdp solution methods aim computing optimal action possible belief point 
instance series value functions size grows exponentially planning horizon 
natural way sidestep intractability settle computing approximate solution considering finite set belief points 
backup stage reduces applying fixed number times resulting small number vectors bounded size belief set 
motivation approximate methods ability compute successful policies larger problems compensates loss optimality 
approximate pomdp value iteration methods operating fixed set points explored lovejoy subsequent works hauskrecht poon pineau spaan vlassis 
pineau 
instance approximate backup operator computes value backup stage set backup fixed set belief points general assumption underlying called point methods updating value gradient vector resulting policy generalize effective beliefs outside set assumption realistic depends pomdp structure contents intuition problems set spaan vlassis reachable beliefs reachable arbitrary policy starting forms low dimensional manifold belief simplex covered densely relatively small number belief points 
crucial control quality computed approximate solution makeup number schemes build proposed 
instance regular grid belief simplex computed triangulation lovejoy 
options include extreme points belief simplex random grid hauskrecht poon 
alternative scheme include belief points encountered simulating pomdp generate trajectories belief space sampling random actions observations time step lovejoy hauskrecht poon pineau spaan vlassis 
sampling scheme focuses contents beliefs encountered experiencing pomdp model 
pbvi algorithm pineau instance point pomdp algorithm 
pbvi starts selecting small set beliefs performs number backup stages expands sampling beliefs performs series backups repeats process satisfactory solution allowed computation time expires 
set bt grows simulating actions bt maintaining new belief points furthest away points bt 
scheme heuristic bt cover wide area belief space comes cost requires computing distances bt 
backing bt pbvi algorithm generates stage approximately bt vectors lead slow performance domains requiring large bt 
section point pomdp value iteration method require backing compute backups subset seeing computed solution effective complete set result limit growth number vectors successive value function estimates leading significant speedups 

randomized point backup stages introduced pomdp framework models agents inhabiting stochastic environments partially observable discussed exact approximate methods computing successful plans agents 
describe perseus approximate solution method capable computing competitive solutions large pomdp domains 
perseus perseus approximate point value iteration algorithm pomdps vlassis spaan spaan vlassis 
value update scheme perseus implements randomized approximate backup operator increases decrease value belief points operator efficiently implemented pomdps shape value function 
key idea value backup stage improve value points belief set updating value gradient randomly selected subset points 
perseus randomized point value iteration pomdps backup stage value function vn compute value function vn improves value build value function vn upper bounds vn necessarily require linear programming vn vn 
agent randomly explore environment collect set reachable belief points remains fixed complete algorithm 
initialize value function single vector components equal mins zhang zhang 
starting perseus performs number backup stages convergence criterion met 
backup stage defined follows auxiliary set containing non improved points perseus backup stage vn 
set vn 
initialize 
sample belief point uniformly random compute backup 

vn add vn add arg max vn 
compute vn vn 

go 
small number vectors sufficient improve vn especially steps value iteration 
idea compute vectors randomized greedy manner sampling increasingly smaller subset keep track set non improved points consisting new value vn lower vn 
start backup stage vn set means initialized indicating need improved backup stage 
long empty sample point compute backup 
improves value vn step add vn update vn computing inner product new 
hope improves value points points removed long empty sample belief points add vectors 
ensure termination backup stage enforce shrinks adding vectors improves value generated 
vn step ignore insert copy maximizing vector vn vn 
point considered improved removed step belief points vector maximizing vn 
procedure ensures shrinks backup stage terminate 
pictorial example backup stage fig 

perseus performs backup stages convergence criterion met 
pointbased methods convergence criteria considered instance bound difference successive value function estimates vn vn 
option track number policy changes number different optimal action vn compared vn lovejoy 
vn vn spaan vlassis vn vn example perseus backup stage state pomdp 
belief space discussion depicted axis axis represents 
solid lines vectors current stage dashed lines vectors previous stage 
operate beliefs indicated tick marks 
backup stage computing vn vn proceeds follows value function stage start computing vn sampling add backup vn improves value sample add backup vn improves value improved backup stage finished 
key observation underlying perseus algorithm belief backed resulting vector improves value belief points results value functions relatively small number vectors compared poon pineau 
experiments show number vectors grows modestly number backup stages vn 
practice means afford larger point methods positive effect approximation accuracy dictated bounds pineau 

furthermore compared methods build set various heuristics pineau smith simmons build cheap requires sampling random trajectories starting 
duplicate entries affect probability particular sampled value update stages size vn 
perseus randomized point value iteration pomdps alternative single fixed set collected fixed policy algorithm resample new bt th backup stage fixed intervals policy 
approach justified fact agent executing optimal policy probably visit small subset beliefs tested scheme affect solution quality perseus trade offs achieve additional computational cost sampling multiple sets note similar policy learning fixed set sampled states adopted algorithms lagoudakis parr ng schneider 
backups perseus fixed set viewed particular instance asynchronous dynamic programming bertsekas tsitsiklis 
asynchronous dynamic programming algorithms full sweeps state space order states backed arbitrary 
allows algorithm focus backups may high potential impact instance prioritized sweeping algorithm solving fully observable mdps moore atkeson andre friedman parr 
drawback notion exact planning horizon somewhat lost general performing backup stages computed plan considering steps 
backing non improved belief points asynchronously perseus focuses interesting regions reachable belief space sampling random ensures eventually taken account 
ensure value particular belief point decreases guaranteed perseus converge proof requires observing added vector poon vlassis spaan 
explained perseus handle large belief sets obviating dynamic belief point selection strategies proposed hauskrecht poon pineau 

note parameter set user size complexity resulting policy mildly dependent size interesting issue new vectors generated backup stage perseus may affect speed convergence algorithm 
general smaller size vn value function faster backups operator linear dependence vn 
hand consecutive value functions may differ arbitrarily size observed cases new value function fewer vectors old value function hard derive bounds speed convergence perseus complicates analysis involved trade offs 
mainly identified cases small number new vectors added value function 
case initial backup stages initialized low large large negative immediate reward 
case single vector may improve points number backup stages value function reached sufficient level 
second case near convergence value function converged certain regions belief space 
sampling belief point region result near copy old vector 
case provides evidence value function initialized low adding single vector efficient way correct case may viewed providing evidence convergence perseus 
spaan vlassis extension large continuous action spaces attractive feature perseus naturally extended large continuous action spaces due improve principle backup stage 
note backup operator involves maximization actions action space finite small cache advance transition observation reward models achieve optimized implementation backup operator 
large continuous action spaces full maximization actions clearly infeasible resort sampling techniques 
idea replace full maximization actions sampled max operator performs maximization random subset ri littman 
means compute models fly sampled action requires algorithm parameterized model family generate needed models action input 
generated models cached case action considered iterations see experimental section called old actions 
sampled max operator suited backup scheme perseus require values belief points decrease consecutive backup stages 
particular replace backup operator new backup operator backup defined follows spaan vlassis backup arg max gb random set actions drawn gb defined 
sampled action generate pomdp models fly mentioned models compute required vectors gb backup backup operator simply replace backup operator step section 
full maximization case need check step vectors generated actions improves value particular belief point 
keep old vector associated action selected previous backup stage 
concerning sample complexity backup operator derive simple bounds involve number actions drawn probability find action terms value improvement 
easily show probability best action actions selected uniformly random best fraction actions log log 
practice various sampling schemes possible vary way constructed 
identified number proposal distributions sample actions uniform gaussian distribution centered best known action particular dirac distribution 
take account policy computed far focusing current action associated input belief recorded vn sampling uniformly random uses knowledge 
actions sampled uniformly random viewed exploring actions distributions exploiting current knowledge 
select makeup choose combination distributions mentioned allowing explore exploit time 
experiments see section implement backup operator number different combinations analyze effects 
perseus randomized point value iteration pomdps 
related section reported class approximate solution techniques pomdps focus computing value function approximation fixed set prototype belief points 
broaden picture approximate pomdp solution methods 
related overview provided hauskrecht 
heuristic control strategies proposed rely solution underlying mdp 
popular technique mdp littman cassandra kaelbling simple approximation technique treats pomdp fully observable solves mdp value iteration 
resulting values define control policy arg maxa 
qmdp effective domains policies computes take informative actions mdp solution assumes uncertainty regarding state disappear action 
mdp policies fail domains repeated information gathering necessary 
way sidestep intractability exact pomdp value iteration grid belief simplex fixed grid lovejoy bonet variable grid brafman zhou hansen 
value backups performed grid point value grid point preserved gradient ignored 
value non grid points defined interpolation rule 
grid methods differ mainly grid points selected shape interpolation function takes 
general regular grids scale problems high dimensionality non regular grids suffer expensive interpolation routines 
alternative computing approximate value function policy search methods search policy restricted class controllers 
instance policy iteration hansen bounded policy iteration bpi poupart boutilier search space bounded size stochastic finite state controllers performing policy iteration steps 
options searching policy space include gradient ascent meuleau kim kaelbling cassandra kearns mansour ng ng jordan baxter bartlett aberdeen baxter heuristic methods stochastic local search boutilier 
particular pegasus method ng jordan estimates value policy simulating bounded number trajectories pomdp fixed random seed takes steps policy space order maximize value 
policy search methods demonstrated success cases searching policy space difficult prone local optima 
approach solving pomdps heuristic search lave hansen smith simmons 
defining initial belief root node methods build tree branches pairs recursively induces new belief node 
methods bear similarity perseus focus reachable beliefs 
differ way belief points selected back methods branch bound techniques maintain upper lower bounds expected return fringe nodes search tree 
hansen proposes policy iteration method represents policy finite state controller uses belief tree focus search areas belief space controller spaan vlassis name tiger grid hallway hallway tag continuous navigation table characteristics problem domains 
improved 
applicability large problems limited full dynamic programming updates 
hsvi smith simmons approximate value iteration technique performs heuristic search belief space beliefs update bounds similar lave 
alternative approach maintaining uncertainty estimates approximate value function gaussian processes tuttle ghahramani 
compression techniques applied large pomdps reduce dimensionality belief space facilitating computation approximate solution 
roy gordon thrun apply exponential family pca sample set beliefs find lowdimensional representation approximate solution sought 
non linear compression effective requires learning reward transition model reduced space 
model learned compute approximate solution original pomdp mdp value iteration 
alternatively linear compression techniques preserve shape value function poupart boutilier 
property desirable allows exploit existing pomdp machinery 
instance linear compression applied preprocessing step bpi poupart boutilier perseus poupart 
literature pomdps continuous actions relatively sparse thrun ng jordan baxter bartlett 
thrun applies real time dynamic programming pomdp continuous state action space 
beliefs represented sets samples drawn state space values approximated nearest neighbor interpolation growing set prototype values updated line exploration sampling bellman backups 
pegasus handle continuous action spaces cost sample complexity polynomial size state space theorem ng jordan 

experiments show experimental results applying perseus benchmark problems pomdp literature pomdp domains testing perseus problems continuous action spaces 
table summarizes domains terms size belief set gathered simulating trajectories interactions agent pomdp environment starting random state sampled perseus randomized point value iteration pomdps time step agent picked action uniformly random 
domains discount factor set 
discrete action spaces hallway hallway tiger grid problems introduced littman maze domains commonly test scalable pomdp solution techniques littman brafman zhou hansen pineau smith simmons spaan vlassis poupart 
tag domain pineau order magnitude larger problems benchmark problem pineau smith simmons boutilier poupart boutilier spaan vlassis poupart 
benchmark mazes littman 
introduced benchmark maze domains tiger grid hallway hallway 
navigation tasks objective agent reach designated goal state quickly possible 
agent observes possible combination presence wall directions plus unique observation indicating goal state hallway problem landmarks available 
step agent take actions stay place move forward turn right turn left turn 
transition observation model noisy 
table compares performance perseus algorithms 
problem sampled set beliefs executed perseus times problem different random seeds 
average expected discounted reward computed trajectories starting random states drawn perseus runs computed policy 
reported reward average trajectories 
perseus reaches competitive control quality small number vectors resulting considerable speedup 
tag goal tag domain described pineau 
robot search moving opponent robot tag 
chasing robot observe opponent occupy position time execute tag action order win game receive reward 
opponent location reward robot penalized reward motion action takes 
opponent tries escape tagged moving away chasing robot probability remaining location 
chasing opponent robot start random location 
chasing robot perfect information regarding position movement actions north east south west deterministic 
state space represented cross product states robots 
robots located positions depicted fig 
opponent tagged state resulting total states 
tag 
perseus qmdp results section computed matlab intel pentium iv ghz results obtained different platforms time comparisons rough 
state space 
vectors time nr 
vectors 
spaan vlassis time value 
reward time time policy changes 
reward 
tag state space chasing opponent robot performance perseus 
large benchmark problem compared pomdp problems studied literature exhibits sparse structure 
applied perseus belief set points 
fig 
show performance perseus averaged runs error bars indicate standard deviation runs 
evaluate computed policies tested trajectories steps times starting positions sampled starting belief 
fig 
displays value estimated expected discounted reward averaged trajectories number vectors value function estimate number policy changes number different optimal action vn compared vn 
regarded measure convergence point solution methods lovejoy 
see experiments perseus reaches solutions virtually equal quality size 
table compares performance perseus state art methods 
results show tag problem perseus displays better control quality method computes solution order magnitude faster methods 
specifically solution computed beliefs consists vectors pbvi maintains vector indicates randomized backup stage perseus justified takes advantage large size value function grows moderately planning horizon leading significant speedups 
interesting compare variations bpi bias poupart poupart boutilier 
bias focuses perseus randomized point value iteration pomdps tiger grid hsvi perseus pbvi bpi grid mdp results tiger grid 
hallway perseus hsvi pbvi bpi mdp results hallway 
hallway pbvi hsvi perseus bpi mdp results hallway 
tag perseus hsvi bpi bbsls bpi pbvi mdp results tag 
table experimental comparisons perseus algorithms 
perseus results averaged runs 
table lists method average expected discounted reward size solution value function controller size time seconds compute solution 
sources pbvi pineau bpi bias poupart boutilier bpi bias poupart hsvi smith simmons grid brafman poon bbsls boutilier approximate read 
reachable belief space incorporating initial belief dramatically increases performance solution size computation time reach control quality perseus 
continuous action spaces applied perseus domains continuous action spaces agent equipped proximity sensors moving continuous heading distance navigation task involving mobile robot omnidirectional vision perceptually aliased office environment 
continuous navigation state space 
spaan vlassis example image 
environment 
continuous action space domains points indicate states depicts goal state 
environment continuous navigation problem black square represents agent beams indicate range proximity sensors 
problem panoramic image corresponding prototype feature vector ok induced ok 
darker dot higher probability 
continuous navigation tested approach navigation task simulated environment agent move continuous heading distance 
continuous navigation environment represents hallway highly perceptually aliased see fig 

agent inhabiting hallway equipped proximity sensors observing compass direction 
assume proximity sensor detect wall range resulting total number possible sensor readings 
agent sensor system noisy probability correct wall configuration observed observations returned equal probability 
task reach goal location located open area walls near agent detect 
agent initialized random state environment learn movement actions take order reach goal fast possible 
perseus assumes finite discrete state space set possible locations agent need discretize space performed simple means clustering random subset possible locations resulting grid locations depicted fig 

agent actions defined parameters heading agent turns distance intends move direction 
executing action transports gaussian distribution centered expected resulting position defined current position translated meter direction 
standard deviation gaussian transition model di means agent wants travel uncertainty regarding resulting perseus randomized point value iteration pomdps position 
distance parameter limited interval heading ranges 
movement penalized reward step reward obtainable goal location 
test feasibility perseus continuous action spaces compute successful policies sampling actions random experimented number different sampling schemes backup operator 
scheme defined makeup au composed samples distributions au uniformly random gaussian distribution centered best known action far standard deviation dirac distribution best known action 
describe number samples distribution au 
tested schemes sampling single action uniformly random gaussian distribution adding schemes resulting sampling actions uniform gaussian distributions including old action 
scheme explores option sampling action particular distribution tested 
option try best known old action particular relatively cheap cache transition observation reward model time chosen previous backup stage 
problem set belief points 
evaluate control quality computed value functions collected rewards sampling trajectories random starting locations particular time intervals policy computed far 
trajectory stopped maximum steps agent reached goal collected reward properly discounted 
results averaged runs perseus different random seed computed matlab intel xeon ghz 
fig 
shows results sampling schemes mentioned 
top row displays control quality indicated average discounted reward 
fig 
see just sampling single action uniformly random gives performance extending include best known action improves control quality 
gaussian sampling schemes learn slower take small steps action space 
additional disadvantage gaussian sampling need user specify standard deviation 
fig 
depicts control quality schemes sample distributions different values shows tested variations reach similar control quality trying actions particular slow learning 
looking size value function fig 
see resulting value function smaller scheme tested 
appears experiment sampling actions increases chance finding high quality action generalizes fewer vectors eventually needed reach control quality higher computational cost backup stage 
note tested schemes number vectors value function remains orders magnitude lower size belief points confirming efficient behavior perseus randomized backup scheme 
obtain insight effect sampling different distributions computed relative frequency occurrence maximizing action au executing backup check vector computed returned reward vectors origin best action time reward 
time number vectors 
time spaan vlassis uniform uniform old gauss gauss old uniform uniform old gauss gauss old improved uniform improved old improved origin maximizing action 
reward vectors origin best action uniform gauss old uniform gauss old uniform gauss old time reward 
time number vectors 
uniform gauss old uniform gauss old uniform gauss old time improved uniform improved gauss improved old improved origin maximizing action 
perseus results continuous navigation problem averaged runs 
left column shows performance au right column displays 
top row figures display average discounted reward obtained vs computation time figures middle row show size value function bottom row details origin maximizing vector see main text 
reward perseus randomized point value iteration pomdps time reward 
discrete discrete discrete uniform old vectors discrete discrete discrete uniform old time number vectors 
origin best action time improved uniform improved old improved origin best action 
performance perseus domain averaged runs 
action improves record action originated backup stage normalize counts respect total number backups backup stage including improve 
resulting frequencies plotted bottom row fig 
sampling schemes sampling uniform old fig 
sampling action distributions fig 

see time relative frequency best known action grows improved old number instances sampled actions improves drops zero improved 
frequencies actions sampled uniform gaussian distribution improved uniform resp 
improved gauss resulting best action improving drop 
observations confirm intuition sampling actions random perseus effectively explore action space advantageous algorithm time progresses algorithm able exploit actions turn useful 
arbitrary heading navigation evaluate perseus continuous actions realistic problem compare discretized action spaces include domain 
problem adapted spaan vlassis mobile robot omnidirectional vision navigate highly perceptually aliased office environment see fig 

mem robot database contains set approximately panoramic images collected manually driving robot meters office environment 
robot decide move arbitrary direction actions parameterized heading ranging 
applied technique continuous navigation domain grid state space states fig 
assume gaussian error resulting position 
observation model compressed images pca applied means clustering create dimensional prototype feature vectors 
fig 
shows inverse observation model observation fig 
displays image database closest particular prototype obser 
memorable database provided tsukuba research center japan real world computing project 
spaan vlassis vation 
task reach certain goal state reward obtained action yields reward 
belief set contained belief points 
compared continuous action extension perseus discretized versions problem applied regular perseus fixed discrete action set headings equal separation offset random angle prevent bias 
fig 
displays results perseus au schemes turned give similar results discrete action spaces 
fig 
shows sampling continuous results control quality discrete version needs time reach backup requires generate transition observation reward models fly 
discrete cases benefit optimized implementation cache transition observation reward models continuous action scheme needs computation time match performance outperform 
employing continuous scheme perseus exploits ability move arbitrary heading find better policy discrete cases 
see providing robot fine grained action space leads better control quality problem discretization headings appears fine grained control performance 
fig 
plots number vectors value function scheme see reaching control quality continuous discrete version need similar amount vectors 
fig 
shows relative frequency occurrence maximizing action old detailed section 
fig 
see time best known action exploited frequency instances sampled action improves value diminished near zero 

partially observable markov decision process pomdp framework provides attractive principled model sequential decision making uncertainty 
models interaction agent stochastic environment inhabits 
pomdp assumes agent imperfect information parts environment hidden agent sensors 
goal compute plan allows agent act optimally uncertainty sensory input uncertain effect executing action 
unfortunately expressiveness pomdps counterbalanced intractability computing exact solutions calls efficient approximate solution techniques 
considered line research approximate point pomdp algorithms plan sampled set belief points 
perseus randomized point value iteration algorithm planning pomdps 
perseus operates large belief set sampled simulating random trajectories belief space 
approximate value iteration performed belief set applying number backup stages ensuring backup stage value point belief set improved key observation single backup may improve value belief points 
contrary point methods perseus backs randomly selected subset points belief set sufficient improving value belief point set 
experiments confirm allows compute value functions consist small number vectors relative belief set size leading significant speedups 
performed experiments benchmark problems perseus randomized point value iteration pomdps literature perseus turns competitive methods terms solution quality computation time 
extended perseus compute plans agents large continuous set actions disposal sampling actions action space 
demonstrated viability perseus pomdp problems continuous action spaces continuous navigation task robotic problem involving mobile robot omnidirectional vision 
analyzed number different action sampling schemes compared discretized action spaces 
perseus extended deal structured state spaces poupart poupart hoey boutilier continuous observation spaces hoey poupart continuous state spaces porta spaan vlassis 
explore alternative compact representations guestrin koller parr theocharous murphy kaelbling applying perseus cooperative multiagent domains extending approaches emery montemerlo gordon schneider thrun becker zilberstein lesser goldman chaib draa 
acknowledgments bruno scherrer geoff gordon pascal poupart anonymous reviewers comments 
research supported progress embedded systems research program dutch organization scientific research nwo dutch ministry economic affairs technology foundation stw project aes 
aberdeen baxter 

scaling internal state policy gradient methods pomdps 
international conference machine learning sydney australia 
andre friedman parr 

generalized prioritized sweeping 
advances neural information processing systems 
mit press 
aoki 

optimal control partially observable markovian systems 
journal franklin institute 
astr 

optimal control markov processes incomplete state information 
journal mathematical analysis applications 
ng schneider 

policy search dynamic programming 
advances neural information processing systems 
mit press 
baxter bartlett 

infinite horizon policy gradient estimation 
journal artificial intelligence research 
becker zilberstein lesser goldman 

solving transition independent decentralized markov decision processes 
journal artificial intelligence research 
bellman 

dynamic programming 
princeton university press 
bertsekas tsitsiklis 

parallel distributed computation numerical methods 
prentice hall 
spaan vlassis bertsekas tsitsiklis 

neuro dynamic programming 
athena scientific belmont ma 
poupart hoey boutilier 

decisiontheoretic approach task assistance persons dementia 
proc 
int 
joint conf 
artificial intelligence 
bonet 

epsilon optimal grid algorithm partially observable markov decision processes 
international conference machine learning pp 
sydney australia 
morgan kaufmann 
brafman 

heuristic variable grid solution method pomdps 
proc 
national conference artificial intelligence 
boutilier 

stochastic local search pomdp controllers 
proc 
national conference artificial intelligence san jose ca 
cassandra littman zhang 

incremental pruning simple fast exact method partially observable markov decision processes 
proc 
uncertainty artificial intelligence providence rhode island 
cheng 

algorithms partially observable markov decision processes 
ph thesis university british columbia 


controlled random sequences 
theory probability applications 
emery montemerlo gordon schneider thrun 

approximate solutions partially observable stochastic games common payoffs 
proc 
int 
joint conference autonomous agents multi agent systems 
guestrin koller parr 

solving factored pomdps linear value functions 
ijcai workshop planning uncertainty incomplete information 
hansen 

finite memory control partially observable systems 
ph thesis university massachusetts amherst 
hansen 

solving pomdps searching policy space 
proc 
uncertainty artificial intelligence pp 

hauskrecht 

value function approximations partially observable markov decision processes 
journal artificial intelligence research 
hoey poupart 

solving pomdps continuous large discrete observation spaces 
proc 
int 
joint conf 
artificial intelligence 
kaelbling littman cassandra 

planning acting partially observable stochastic domains 
artificial intelligence 
kearns mansour ng 

approximate planning large pomdps reusable trajectories 
advances neural information processing systems 
mit press 
lagoudakis parr 

squares policy iteration 
journal machine learning research 
perseus randomized point value iteration pomdps littman cassandra kaelbling 

learning policies partially observable environments scaling 
international conference machine learning san francisco ca 
lovejoy 

computationally feasible bounds partially observed markov decision processes 
operations research 
madani hanks condon 

undecidability probabilistic planning infinite horizon partially observable markov decision problems 
proc 
national conference artificial intelligence orlando florida 
meuleau kim kaelbling cassandra 

solving pomdps searching space finite policies 
proc 
uncertainty artificial intelligence 
monahan 

survey partially observable markov decision processes theory models algorithms 
management science 
moore atkeson 

prioritized sweeping reinforcement learning data time 
machine learning 
ng jordan 

pegasus policy search method large mdps pomdps 
proc 
uncertainty artificial intelligence 
papadimitriou tsitsiklis 

complexity markov decision processes 
mathematics operations research 
chaib draa 

online pomdp algorithm complex multiagent environments 
proc 
int 
joint conference autonomous agents multi agent systems 
pineau gordon thrun 

point value iteration anytime algorithm pomdps 
proc 
int 
joint conf 
artificial intelligence acapulco mexico 
poon 

fast heuristic algorithm decision theoretic planning 
master thesis hong kong university science technology 
porta spaan vlassis 

robot planning partially observable continuous domains 
robotics science systems mit cambridge ma 
poupart boutilier 

value directed compression pomdps 
advances neural information processing systems 
mit press 
poupart boutilier 

bounded finite state controllers 
advances neural information processing systems 
mit press 
poupart 

exploiting structure efficiently solve large scale partially observable markov decision processes 
ph thesis university toronto 
poupart boutilier 

approximate scalable algorithm large scale pomdps 
advances neural information processing systems 
mit press 
puterman 

markov decision processes discrete stochastic dynamic programming 
john wiley sons new york ny 
spaan vlassis roy gordon 

exponential family pca belief compression pomdps 
advances neural information processing systems 
mit press 
roy gordon thrun 

finding approximate pomdp solutions belief compression 
journal artificial intelligence research 
russell norvig 

artificial intelligence modern approach nd edition 
prentice hall 
lave 

markovian decision processes probabilistic observation states 
management science 
smallwood sondik 

optimal control partially observable markov decision processes finite horizon 
operations research 
smith simmons 

heuristic search value iteration pomdps 
proc 
uncertainty artificial intelligence 
sondik 

optimal control partially observable markov decision processes 
ph thesis stanford university 
spaan vlassis 

point pomdp algorithm robot planning 
proceedings ieee international conference robotics automation pp 
new orleans louisiana 
spaan vlassis 

planning continuous actions partially observable environments 
proceedings ieee international conference robotics automation pp 
barcelona spain 
sutton barto 

reinforcement learning 
mit press 
ri littman 

generalized markov decision processes reinforcement learning algorithms 
tech 
rep cs brown university department computer science 
theocharous murphy kaelbling 

representing hierarchical pomdps dbns multi scale robot localization 
proceedings ieee international conference robotics automation 
thrun 

monte carlo pomdps 
advances neural information processing systems 
mit press 
tuttle ghahramani 

propagating uncertainty pomdp value iteration gaussian processes 
tech 
rep gatsby computational neuroscience unit 
vlassis spaan 

fast point algorithm pomdps 
proceedings annual machine learning conference belgium netherlands pp 
brussels belgium 
nips workshop planning real world whistler canada dec 
zhang zhang 

speeding convergence value iteration partially observable markov decision processes 
journal artificial intelligence research 
zhou hansen 

improved grid approximation algorithm pomdps 
proc 
int 
joint conf 
artificial intelligence seattle wa 

