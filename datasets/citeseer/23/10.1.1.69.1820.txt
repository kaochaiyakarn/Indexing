modeling human motion binary latent variables graham taylor geoffrey hinton sam roweis dept computer science university toronto toronto canada hinton roweis cs toronto edu propose non linear generative model human motion data uses undirected model binary latent variables real valued visible variables represent joint angles 
latent visible variables time step receive directed connections visible variables time steps 
architecture line inference efficient allows simple approximate learning procedure 
training model finds single set parameters simultaneously capture different kinds motion 
demonstrate power approach synthesizing various motion sequences performing line filling data lost motion capture 
website www cs toronto edu publications nips advances motion capture technology fueled interest analysis synthesis complex human motion animation tracking 
models physics masses springs produced impressive results sophisticated energy learning methods estimate physical parameters motion capture data 
want generate realistic human motion need model complexities real dynamics difficult analytically learning essential 
simplest way generate new motion sequences data concatenate parts training sequences 
method transform motion training data new sequences learning adjusting style characteristics :10.1.1.124.515
focus model driven analysis synthesis avoid complexities involved imposing physics constraints relying pure learning approach knowledge model comes data 
data modern motion capture systems high dimensional contains complex non linear relationships components observation vector usually represent joint angles respect skeletal structure 
hidden markov models model data efficiently rely single discrete state multinomial represent history time series 
model bits information past history require hidden states 
avoid exponential explosion need model distributed componential hidden state representational capacity linear number components 
linear dynamical systems satisfy requirement model complex non linear dynamics created non linear properties muscles contact forces foot ground myriad factors 
energy model vectors real values general distributed binary representations hidden state directed models time series inference difficult 
restricted boltzmann machine rbm model probability distribution observation vector time frame posterior latent variables factorizes completely making inference easy 
typically binary logistic units visible data hidden variables application data comprised joint angles continuous 
modified rbm visible units linear realvalued variables gaussian noise 
graphical model layer visible units layer hidden units undirected connections layers connections layer 
setting hidden units distribution visible unit defined parabolic log likelihood function extreme values improbable log vi ci vi const standard deviation gaussian noise visible unit 
practice rescale data zero mean unit variance 
fixing learning expect model predict data higher precision 
main advantage undirected energy model directed belief net inference easy hidden units conditionally independent states visible units observed 
conditional distributions assuming hj bj vi ci logistic function gaussian bj ci biases hidden unit visible unit respectively wij symmetric weight 
maximum likelihood learning slow rbm learning works approximately follow gradient function called contrastive divergence 
learning rule wij data recon expectation hidden unit activations respect data distribution second expectation respect distribution reconstructed data 
reconstructions generated starting markov chain data distribution updating hidden units parallel sampling eq 
updating visible units parallel sampling eq 

expectations states hidden units conditional states visible units vice versa 
learning rule hidden biases just simplified version eq 
bj hj data hj recon 
conditional rbm model rbm described models static frames data incorporate temporal information 
model temporal dependencies treating visible variables previous time slice additional fixed inputs 
fortunately complicate inference 
add types directed connections autoregressive connections past configurations time steps visible units current visible configuration connections past current hidden configuration 
addition directed connections turns rbm conditional rbm 
experiments chosen 
tunable parameters need types directed connections 
simplify discussion assume refer order model 
setting parameters gradient quadratic log likelihood overwhelm gradient due weighted input binary hidden units provided value vi visible unit far bias ci 
trained model probabilities feature conditional data visible units 
shown hidden unit model sequence contains order walking sitting standing times walking running 
rows represent features columns represent sequential frames 
inference difficult standard rbm 
data time hidden units time conditionally independent 
contrastive divergence training 
change update visible hidden units implement directed connections treating data previous time steps dynamically changing bias 
contrastive divergence learning rule hidden biases eq 
equivalent learning rule temporal connections determine dynamically changing hidden unit biases ij hj data recon 
ij log linear parameter weight connecting visible unit time hidden unit similarly learning rule autoregressive connections determine dynamically changing visible unit biases 
ki recon ki weight visible unit time visible unit autoregressive weights model short term temporal structure leaving hidden units model longer term higher level structure 
training states hidden units determined input receive observed data input receive previous time slices 
learning rule remains standard rbm different effect states hidden units influenced previous visible units 
attempt model frames sequence 
hidden layer visible layer architecture model experiments previous time steps learning model motion need proceed sequentially training data sequences 
updates conditional past time steps entire sequence 
long isolate chunks frames size depending order directed connections mixed formed mini batches 
speed learning assemble chunks frames balanced mini batches size 
randomly assign chunks different mini batches chunks mini batch uncorrelated possible 
save computer memory time frames replicated mini batches simply indexing simulate chunking frames 
approximations training procedure relies approximations chosen experience training similar networks 
training replaced vi eq 
eq 
expected value expected value vi computing probability activation hidden units 
compute step reconstructions data stochastically chosen binary values hidden units 
prevents hidden activities transmitting unbounded amount information data reconstruction 
updating directed visible hidden connections eq 
symmetric undirected connections eq 
stochastically chosen binary values hidden units term data replaced hj expected value second term reconstruction 
took approach reconstruction data depends binary choices selecting hidden state 
infer hiddens reconstructed data probabilities highly correlated binary hidden states inferred data 
hand reconstruction binary choice hiddens reconstruction doesn correlate terms point including extra noise 
lastly note fine tuning procedure making crude approximation addition contrastive divergence 
inference step conditional past visible states approximate ignores smoothing 
directed connections exact inference model include forward backward pass sequence currently perform forward pass 
avoided backward pass missing values create problems undirected models hard perform learning efficiently full posterior 
compared hmm lack smoothing loss offset exponential gain representational power 
data gathering preprocessing data cmu graphics lab motion capture database see acknowledgments 
processed data consists joint angles derived cmu mit markers plus root near base back orientation displacement 
datasets original data captured hz downsampled hz 
joint angle dimensions original cmu data constant values eliminated 
remaining joint angles degrees freedom 
joint angles root orientation converted euler angles exponential map parameterization 
done avoid gimbal lock discontinuities 
mit data expressed exponential map form need converted 
treated root specially encodes transformation respect fixed global coordinate system 
order respect physics wanted final representation invariant ground plane translation rotation gravitational vertical 
represented ground plane translation incremental forwards vector incremental sideways vector relative direction person currently facing represented height non incrementally distance ground plane 
represented orientation gravitational vertical incremental change represented rotational degrees freedom absolute pitch roll relative direction person currently facing 
final dimensionality data vectors cmu data mit data 
note eliminated exponential map dimensions constant zero corresponding joints single degree freedom 
mentioned sec 
component data normalized zero mean unit variance 
advantage model fact data need heavily preprocessed dimensionality reduced 
brand hertzmann apply pca reduce noise dimensionality 
autoregressive connections model thought doing kind whitening data 
manually segment data cycles sample regular time intervals quaternion spherical interpolation 
dimensionality reduction problematic wider range motions modeled 
experiments training model updates described demonstrate ways learned structure human motion 
direct demonstration exploits fact probability density model sequences model generate de novo number synthetic motion sequences 
video files sequences available website mentioned motions hand motion editing software 
note keep reservoir training data sequences generation need weights model valid frames initialization 
causal generation learned model done line smoothing just learning procedure 
visible units time steps determine effective biases visible hidden units current time step 
keep previous visible states fixed perform alternating gibbs sampling obtain joint sample conditional rbm 
picks new hidden visible states compatible visible history 
generation requires initialization time steps visible units implicitly determine mode motion synthetic sequence start 
randomly drawn consecutive frames training data initial configuration 
generation walking running sequences single model demonstration train single model data containing walking running motions learned model generate types motion depending initialized 
trained sequences walking sequences jogging subject cmu database 
downsampling hz training data consisted frames 
training model generate walking top running bottom motion see videos website 
skeleton frames apart 
shows walking sequence running sequence generated model alternating gibbs sampling probability hidden units conditional current previous visible vectors 
training data contain transitions walking running vice versa model continue generate walking running motions depending initialized 
learning transitions various styles second demonstration show model capable learning homogeneous motion styles transitions training data contains hidden unit trained passes training data third order model directed connections 
weight updates mini batch size 
order sequences randomly permuted walking running sequences distributed training data 
examples transitions 
trained sequences mit database containing long examples running jogging transitions styles 
downsampling hz provided frames 
training done model trained identical hidden unit model trained top model see sec 

resulting level model generate data 
video available website demonstrates model ability stochastically transition various motion styles single generated sequence 
introducing transitions noise third demonstration show transitions motion styles generated transitions absent data 
model data described sec 
learned separate sequences walking running 
generate sampling procedure time stochastically choose hidden states current previous visible vectors add small amount gaussian noise hidden state biases 
encourages model explore hidden state space deviating far current motion 
applying noisy sampling approach see generated motion occasionally transitions learned styles 
transitions appear natural see video website 
filling missing data due nature motion capture process adversely affected lighting environmental effects noise recording motion capture data contains missing unusable data 
markers may disappear dropout long periods time due sensor failure occlusion 
majority motion editing software packages contain interpolation methods fill missing data leaves data smooth 
methods rely starting points missing data marker goes missing sequence na interpolation 
methods past data single missing marker fill marker missing values joint angles highly correlated substantial information placement marker gained 
trained model ability easily fill missing data regardless occur sequence 
due approximate inference method rely backward pass sequence ability fill missing data line 
filling missing data model similar generation 
simply clamp known data visible units initialize missing data reasonable example value previous frame alternate stochastically updating hidden visible units known visible states held fixed 
demonstrate filling trained model exactly described sec 
walking running sequence left training data test data 
walking running test sequences erased different sets joint angles starting halfway test sequence 
sets joints left leg entire upper body 
seen video files website quality filled data excellent hardly distinguishable original ground truth test sequence 
demonstrates model ability predict angles rotation left hip 
walking sequence length frames compared model performance nearest neighbor interpolation simple method frame values known dimensions compared example training set find closest match measured euclidean distance normalized angle space 
unknown dimensions filled matched example 
reconstruction model stochastic repeated experiment times report mean 
missing leg mean squared reconstruction error joint model measured normalized joint angle space summed frames interest 
nearest neighbor interpolation error greater 
missing upper body mean squared reconstruction error joint model 
nearest neighbor interpolation error greater 
normalized joint angle frame normalized joint angle frame model successfully fills missing data previous values joint angles temporal connections current angles joints rbm connections 
shown angles rotation left hip joint plot third similar 
original data shown solid line model prediction shown dashed line results nearest neighbor interpolation shown dotted line see video website 
higher level models trained model add layers deep belief network 
previous layer kept sequence hidden state vectors driven data treated new kind fully observed data 
level architecture alter number units trained exact way 
upper levels network model higher order structure 
greedy procedure justified variational bound 
twolevel model shown 
consider special cases higher level model 
keep visible layer th order directed connections standard ar model gaussian noise 
take hidden layer model delete level autoregressive connections sets visible hidden directed connections simplified model trained stages learning static iid model pairs triples time frames inferred hidden states train fully observed sigmoid belief net captures temporal structure hidden states 
discussion higherlevel models introduced generative model human motion idea local constraints global dynamics learned efficiently conditional restricted boltzmann machine 
trained models able efficiently capture complex non linearities data sophisticated pre processing dimensionality reduction 
model designed human motion mind lend high dimensional time series 
relatively low dimensional unstructured data example model single isolated joint single layer model expected difficulty cyclic time series contain subsequences locally similar occur different phases cycle 
possible preserve global phase information higher order model higher dimensional data full body motion capture unnecessary configuration joint angles angular velocities phase ambiguity 
single layer version model performs better higher dimensional data 
models hidden layers able implicitly model longer term temporal information mitigate effect 
demonstrated model effectively learn different styles motion transitions styles 
differentiates approach pca approaches accurately model cyclic motion additionally build separate models type motion 
ability model transition smoothly dependent having sufficient examples transitions training data 
plan train larger datasets encompassing transitions various styles motion 
augment data static skeletal identity parameters essence mapping person unique identity set features able generative model different people generalize individual characteristics type motion 
model limited single source data 
hope integrate low level vision data captured time motion learn correlations vision stream joint angles 
acknowledgments data set project obtained cs cmu edu 
database created funding nsf eia 
second data set project obtained people csail mit edu sig stf 
matlab playback motion generation videos neil lawrence motion capture toolbox www dcs shef ac uk neil 
lecun bottou bengio haffner gradient learning applied document recognition proceedings ieee vol 
pp 
november 
liu hertzmann popovic learning physics motion style nonlinear inverse optimization acm trans 
graph vol 
pp 

forsyth brien motion synthesis annotations proc 
siggraph 
brand hertzmann style machines proc 
siggraph pp 

li wang 
shum motion texture level statistical model character motion synthesis proc 
siggraph pp 

boulic thalmann fua style motion synthesis computer graphics forum vol 
pp 

welling rosen zvi hinton exponential family application information retrieval proc 
nips 
freund haussler unsupervised learning distributions binary vectors layer networks proc 
nips 
hinton training products experts minimizing contrastive divergence neural comput vol 
pp 
aug 
hinton learning multilevel distributed representations high dimensional sequences tech 
rep tr university toronto 
teh hinton rate coded restricted boltzmann machines face recognition proc 
nips 
hsu pulli popovi style translation human motion acm trans 
graph vol 
pp 

practical parameterization rotations exponential map graph 
tools vol 
pp 

hinton osindero 
teh fast learning algorithm deep belief nets neural comp vol 
pp 

