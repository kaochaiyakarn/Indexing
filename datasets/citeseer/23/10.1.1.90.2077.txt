ucfs novel user space high performance custom file system web proxy servers jun wang rui min zhu yiming hu department electrical computer engineering computer science university cincinnati cincinnati oh mail rmin uc edu corresponding author yiming hu phone fax web proxy caching servers play key role today web infrastructure 
previous studies shown disk major performance bottlenecks proxy servers 
conventional file systems proxy server workloads high overheads 
presents novel user space custom file system called ucfs drastically improve performance proxy servers 
ucfs user level software component proxy server manages data raw disk disk partition 
entire system runs user space easy inexpensive implement 
portability maintainability 
ucfs uses efficient memory meta data tables eliminate overhead meta data searches updates 
includes novel file system called cluster structured file system cfs 
similar log structured file systems lfs cfs uses large disk transfers significantly improve disk write performance 
cfs markedly improve file read operations generate garbage 
comprehensive simulation experiments representative real world traces show ucfs significantly improve proxy server performance 
example ucfs achieves times better performance state art squid server running unix fast file system ffs times better squid asynchronous ffs times better improved squidml 
index terms file systems 
web proxy servers 
disk os 
performance improvement 
portion reported workshop performance architecture web servers paws 
world wide web large distributed information system providing access shared data objects 
web proxy caching recognized effective scheme alleviate service bottleneck reduce network traffic minimize user retrieval latency 
different kinds proxy caches cooperative hierarchical web caching architectures large independent root caching servers institution leaf level caching servers widely 
various studies shown disk biggest performance hurdles large web proxy servers 
web cache hit rate grows logarithmic fashion amount traffic size client population 
modern proxy servers significant document rate typically 
means incoming requests generate disk operation create new web documents disk 
proxy servers low document hit rate ram cache 
large ram say mb hits ram 
hits go disk files disk hits resulting high demand disk os 
hits modified hits ims hits reply code occur client requests file modification date certain time 
proxy servers invalidate stale files maintaining consistency generating frequent updates disk 
current proxy servers designed run top general purpose file system unix file system ufs ffs 
systems performance advantages generalpurpose computing environments highly specialized workload proxy servers result poor performance discussed 
need better solutions cope increasing demand proxy server performance 
state art proxy servers squid squidml squid public web proxy cache software widely caches research real world environments 
squid uses file system host operating system unix file system ufs handle cached web documents disks 
improved squid scheme memory mapped files called squidml reduce disk os 
problems current approaches proxy servers squid squidml proxy servers regular file systems unix fast file system ffs mmap function manage cached web documents disks 
approaches major drawbacks 
studies shown web documents web proxy traffic kb 
known ffs conventional file systems efficiently handle small files leading poor performance :10.1.1.11.5338:10.1.1.14.8906:10.1.1.26.8749:10.1.1.117.5365
example rosenblum ousterhout pointed file systems utilize disk bandwidth 
mainly disk bandwidth wasted disk seeking rotational latencies small disk requests dominated systems 

file systems ufs uses synchronous writes file meta data updates order ensure file system consistency furthermore systems periodically flush dirty data buffer linux file systems option asynchronous updates 
cache disk improve data reliability 
policies cause considerable performance degradation 
critical general purpose systems explain earlier proxy caches operations unnecessary data reliability major concern 
data lost fetched original web server 
research file systems soft updates ffs file systems developed ameliorate problem limitations discussed section 

ffs file read operation may suffer high penalty especially case directory lookup cache name cache file read may generate disk reads level directory inodes read file inode write file inode update file access time reads file data 
proxy servers may consist millions files name cache hit rate limited experiments 
name cache compete meta data data caches limited memory size 
large files may penalty accessing indirect blocks 

squidml uses files manage cached files disk 
mmap reduces overheads associated file system performance improvement limited 
discussed cached files normally small fit ram stored disk 
small expensive requests frequently needed order swap files 

problem squid squid hashes urls universal resource locators level directory entries 
urls pointed web site different directories destroying host locality stream 
suggested virtual memory files smaller page size virtual memory preserving host locality reduce disk os 
method limited benefits virtual memory optimized handle proxy traffic 
machines bit process address spaces normally gb available users 
method support sized web documents complex multi processing architecture 
possible solutions order solve mentioned problems possible solutions 
drawbacks 
log structured file system lfs significantly improve performance small writes :10.1.1.117.5365
major problems solution 
lfs currently supported major os 
second lfs may suffer serious performance degradation garbage collection overhead workload high 
seltzer pointed cleaner overhead reduces lfs performance disk full 
lfs performance degraded quickly disk near full 
lfs improve read performance important proxy servers 
similar lfs disk caching disk dcd uses large logs improve small write performance 
dcd works device device driver level better portability 
dcd suffers overhead 
operation background process transfers data cache disk normal data disk 
improve read performance 
soft updates improve file system performance eliminating overheads associated synchronous meta data updates 
major improvement general propose file systems soft updates currently supported operating systems free bsd 
importantly fundamental problems proxy servers solve 
dominated small disk writes deriving creating new small files 
regular file data meta data fit ram accessed disk 
soft updates solve problems 
soft updates speed reads 
file systems metadata logging similar problems 
ffs asynchronous mode ffs async asynchronous updating metadata 
improves performance cost sacrificing consistency 
ffs async solve problem small disk writes improve disk read performance 
may develop new file system kernel optimized proxy servers replace ffs 
approach costly difficult implement maintain new file system kernel 
poses huge portability problem file system designs closely tied kernel structures vary different operating systems different versions os 
solution notice database systems face similar problem 
file systems satisfy performance demand high performance database systems database systems choose manage data file meta data raw disks 
order achieve significantly better performance propose proxy server manage data meta data pass file system 
meta data include proxy servers file systems 
dedicated raw disk raw disk partition design data meta data managing algorithms optimized proxy server applications 
algorithms run user level part regular proxy server process 
result easy implement maintain portability 
need change kernel 
rest describe design web proxy server file system ucfs user level custom file system proxy caching network applications 
ucfs user level software component proxy server server applications manage files 
uses efficient memory meta data structures eliminate disk os related meta data searches updates 
furthermore employs novel file system called cluster structured file systems cfs takes full advantage locality web accesses 
cfs write performance similar lfs large disk transfers 
lfs cfs significantly improves read performance 
importantly cfs generate garbage cleaning overhead 
organization remainder organized follows 
section describe design operation ucfs 
section describes experimental methodology 
section presents simulation results analyses 
section discusses related 
section summarizes new strategy section discusses 
design ucfs data structure definitions order describe design clearly define terminologies ucfs 

ram buffer cache user level ram buffer cache ucfs different buffer cache regular file system 

disk level cache represents disks proxy servers store cached files 

url request request received proxy server web clients web servers protocols 

url file file identified host path information url request 

file meta data consists file attributes including file name size access time 
cached file url web document cached unique file proxy server 
file ram buffer cache disk level cache 
consists file meta data file data 
system architecture system consists major components discussed detail rest section 

memory file lookup table 
table contains location information memory disk cached files 
track files proxy cache 

cluster structured file system cfs 
key design 
cfs works similar lfs writes tries group small writes fix sized large write buffer write disk single large disk write order achieve high performance fully utilizing disk bandwidth 
major differences cfs lfs 
cfs clusters basic units reads 
disk reads large disk requests 
strategy cfs reads efficient small reads lfs ffs 
importantly cfs generate garbage disk garbage major problem lfs 

disk cluster table 
table consists status disk clusters 

ram buffer cache 
cache holds hot files accessed soon 

locality grouping algorithm 
algorithm identifies access locality 
files accessed web site grouped cluster read written disk 
rationale files accessed files cluster accessed soon 
proxy systems cache files larger kb 
proxy logs analyzed large files take files accessed 
servers cache dynamic documents 
subsections describe system component details 
memory file lookup table url request comes proxy server needs quickly decide url file cached ram disk file located 
design goal disk needed process 
result file meta data files proxy cache kept tables ram 
large proxy server cache millions files tables carefully designed sure sizes reasonable 
main challenge store url strings cached files 
maintained hash table 
cached file system entry contains url string file location absence information url string url request represent unique identity url file 
server receives url request compute hash value url find corresponding entry table compare incoming url request url string hash table entry decide hit solution may result hash table requires huge ram space url strings long hundreds bytes 
real world proxy traces average url length varies bytes bytes 
solution md solution store md checksums urls hash table saving actual url strings 
shown entry meta data hash table consists bytes plus bit 
md checksum url string calculated unique key 
md checksum occupies bytes hash table entry 
note actual url string file saved file data disk 
addition entry uses bytes store disk cluster index find appropriate cluster discussed shortly disk contains file 
special cluster index shows file disk memory 
additional mem bit identify file copy ram 
implies file memory copy 
note possible file copies ram disk 
performance optimization explained section 
bytes save modified time url file value reply modified header 
information proxy server determine cached file stale valid 
squid implements similar hash table memory data structure save file meta data cached files 
consists bytes small pointer architectures intel sparc mips 
addition byte cache key md checksum associated 
map cache directory location fields 
unfortunately policy converts disk path name 
access file file system needs perform expensive name translation order get disk addresses 
md checksum bytes ab cdef ab ab cdef ab cdef ed fb performance space requirements cluster index mem bit memory file lookup table memory file lookup table file meta data searches updates take place ram 
file metadata updates applied memory discussed data reliability major concern proxy server 
disk directory searches inode accesses needed 
significantly reduces overhead file writes reads 
new hash function hash lookup table 
comparisons usually take instructions 
computing hash value takes instructions byte key 
case length md checksum 
memory file lookup table take large memory space 
currently allow cache entries lookup hash table 
total size table mb 
small portion total ram proxy server mb ram 
price ram drops rapidly systems ram 
design compact compared systems 
example similar sized squid hash table consumes times ram 
lookup table full means files proxy cache proxy server delete files proxy cache replacement algorithm see section details 
files disk level cache deleted corresponding entries memory file lookup table reclaimed 
cluster structured file system cfs key entire design ucfs 
cfs divides disk space large fix sized clusters basic read write units 
cluster size kb kb 
simulation find kb cluster size best performance 
note cfs maintain directory structures 
objects disk files 
directory information taken care memory file lookup table 
disk layout shows disk layout cfs 
disk cluster may contain multiple small files common case studies shown currently web documents kb 
hand large file span multiple clusters 
file meta data consists fields stored file data 
url string cached file obtained request 

file size 

modified time file 

number 
records number accesses file cached proxy 

cluster array 
file larger cluster span multiple clusters 
array records cluster numbers remaining clusters file file size larger cluster 
array entries 
cluster size kb maximum file size allowed kb kb 
system cache files larger kb 
array contents ignored file smaller cluster 
addition normal data clusters disk subsystem pre allocates super block disk save important information cluster size pointers disk copies file lookup table disk cluster table 
writes superblock cluster cluster cluster cluster file meta data data url size timestamp cluster array file data meta meta data data data large file data 
metadata data file file file file disk layout table ucfs writes cfs simple 
ram buffer full proxy server evict inactive files memory put write buffer ram 
size buffer multiple cluster size 
write buffer fills entire buffer written disk clusters large cluster sized disk writes 
case lfs large writes cfs efficient performance seek rotational overheads relatively small compared data transfer time 
reads reads cfs different file systems lfs ffs 
clusters basic unit cfs reading file entire cluster contains file read single large disk memory 
important reasons choosing design reading entire cluster contains files single file fact doing prefetching extra overhead system groups files cluster locality section file cluster accessed files cluster accessed soon 
doing prefetching cluster read need disk access 
note servers system throughput number processed requests second system response time important 
cfs prefetching improve disk response time disk throughput saves small reads loading files single performance improved prefetched files hit ram evicted 
regular file system ffs prefetching improve response time 
negative impact disk throughput increased number disk requests 
example ffs prefetches small files needs extra os 
files accessed requests wasted hurt performance 
second eliminate file meta data overhead store file data atomic storage unit disk 
file meta data read memory example disk simulated average takes ms read kb file block 
hand takes ms read kb cluster 
disk seek rotational latency independent request sizes dominate disk access time 
time file data read ram buffer cache 
accesses updates invalidations file meta data done memory 
ffs file meta data separated file data located different disk locations multiple operations associated file meta data read write files 
cluster reads allow cfs completely eliminate problem garbage major problem lfs 
garbage generated individual files disk cluster segment lfs invalidated 
cfs cluster read ram entire disk cluster marked blank files ram 
proxy server invalidate file disk stale file cluster containing file read memory file meta data file checked anyway 
files ram disk anymore invalidation files happen ram disk 
result garbage generated 
improved read write policy description prefetching read little bit simplistic 
actual design immediately invalidate disk cluster hit file read ram 
files cluster ram moved back disk write files back disk files modified read 
read cluster memory just mark cluster ram disk 
operation done disk cluster table 
time set mem field memory lookup table file cluster 
shows files cluster valid copy memory 
notice cluster index field memory lookup table records previous cluster number file located disk 
think files cluster virtual cluster ram buffer cache 
files belonging cluster may may stay ram re grouped invalidated updated 
file cluster evicted back disk cfs check files cluster cold buffer intact prefetching 
cfs simply marks cluster disk deletes files ram buffer cache 
file cluster mem field memory lookup table set back show file valid copy disk 
unchanged cluster index field records correct cluster index file 
additional write back disk needed 
files cluster invalidated updated disk cluster invalidated immediately marking disk cluster table ram 
cluster index memory file lookup table cleared 
shows prefetched files valid copy memory 
improved scheme saves disk write back operation keeping copies disk memory file 
implement additional tags controls disk cluster table memory file lookup table 
quantifying overhead large prefetching reads proxy server finds requested file disk read cluster containing file ram buffer cache 
pointed large read fact doing prefetching minimal extra cost 
example cluster kb files kb time read files single large read ms close reading small file ms 
words system prefetches additional files ram small overhead ms hidden cost 
additional kb files prefetched replace kb files memory generate additional large write write replaced files back disk 
prefetched files accessed extra large write overhead replacements necessary 
files prefetched hit overhead unavoidable 
evaluate worth perform large reads cfs overhead calculate total cost large prefetching read 
compare cost ffs operations needed complete job 
cost cfs smaller ffs worthwhile implement policy 
typical cfs cluster size kb contains small files average file size kb 
cfs worst case latency reading disk hit file calculated factor right side equation overhead large read 
second factor right side overhead writing replaced files back disk 
result represents uncommon worst case overhead cfs 
prefetched files replacing files necessary write back process overhead 
discussed section cfs write operation avoided files cluster modified 
files prefetched hit file accessed clients files memory 
files defined prefetch hit files 
calculate prefetch hit rate add prefetch tag file meta data file indicate prefetched file 
memory variable count total hits prefetched files ram 
number include repeated hits file 
calculate prefetch hit rate experiments hit rate varied 
mean standard deviation 
numbers discussed details section 
average prefetch hit rate files prefetched large read average small files prefetched accessed clients 
calculate total latency ffs complete amount reading small files hit file prefetched files 
squid ffs discussed small file read disk may result additional file meta data operations reads level directory inodes read file inode write file inode update file access time 
ffs implements directory lookup cache capture repeated accesses inodes hit rate limited busy proxy server may accommodate millions files 
experiments rate varied 
assume rate 
penalty ffs file read total latency ffs read small files computed follows disk model reading writing kb kb data take ms ms respectively 
calculate saving factor disk latency typical large prefetch read result indicates large reads cfs performance 
please note result conservative worst case overhead cfs 
situation prefetch hit rate speedup large prefetch read small ffs read calculated situation cfs slightly better read performance ffs 
large files file larger size cluster span clusters 
clusters need contiguous disk 
cluster array file meta data located cluster large file indicate cluster numbers remaining clusters 
contents array ignored file smaller cluster 
large files common proxy server workloads 
traces files larger kb smaller kb files larger kb non 
ucfs performance reading writing large files clusters 
system uses large disk os files 
minimal disk request size cluster size request size larger clusters stored disk contiguously 
ffs traditional file systems hand may small expensive os handle files disk fragmented 
data structures cfs cfs uses memory disk cluster table manage clusters system 
cluster system corresponding entry includes fields access time cfs access update counter 
total number cluster 
valid status 
flag indicates cluster empty valid disk valid disk memory 
memory file lookup table track files maximum data capacity cfs reach gb average file size kb 
clusters disk level cache 
assuming entry disk cluster table occupies bytes total memory requirement disk cluster table mb 
timer documents reported files accessed life time proxy server worth caching 
previous researches fourth total requests timers approximately files referenced timers 
problem immediately know cached document timer 
wait specific time period say days 
find cached file accessed period assume timer file 
delete free memory disk spaces 
replacement policy hours files cluster accessed cluster invalidated 
simulation clusters consist timer files 
system deletes old clusters system disk full lfu aging algorithm 
ucfs replaces deletes files disk clusters meta data entries memory file lookup table updated emptied 
cfs maintains variable memory records current disk head position disk block address called cdp 
cfs write clusters disk checks disk cluster table see cdp neighbored space free clusters accommodate incoming write requests ram buffer cache 
strategy allows issuing large disk write consists contiguous multiple clusters 
maximizes disk bandwidth saves disk seeks rotational latencies 
ram buffer cache design implementation hot buffer head file file 
input pool requests 
coming url file medium buffer cold buffer file file 
file file file 
file file 
swap pages disk 
output pool 
swap pages disk grouped page host locality ram buffer cache ucfs architecture ram buffer cache shown 
objects ram buffer cache files 
real clusters ram buffer cache 
described section system maintains virtual cluster concept save write back operations disk 
ram buffer cache holds active files memory 
addition ram buffer cache full works locality grouping algorithm group cached files different clusters host temporal locality writing disk 
files memory stored buffer lru list mru file top 
note cache files larger kb proxy server 
number chosen result squid cache age reported implement hierarchical ram buffer cache logical buffers capacity thresholds hot buffer medium buffer cold buffer take total ram buffer cache capacity respectively 
buffer uses head pointers pointers maintain logical division doubly linked lru list 
memory variables control capacity logical buffers 
logical buffers follows 
hot buffer accessed files put hot buffer 
hot buffer full migrate lru file hot buffer medium buffer 

medium buffer cfs reads cluster disk prefetches small files cluster memory 
cluster prefetched files loaded 
prefetched files receive hits move hot buffer 
proxy server sees cache create new file fetch content file web server 
new file placed medium buffer 
put newly created files prefetched files medium hot buffer files potential timers want pollute hot buffer 
medium buffer reaches limit capacity migrate mru file hot buffer replace lru file cold buffer 

cold buffer files cold buffer accessed evicted memory 
buffer full ucfs groups files locality large clusters cold buffer 
ucfs swaps full clusters memory output pool 
cfs fetches ready clusters pool writes disk large disk writes 
implementing level ram buffer cache successfully prevent timer file polluting hot buffer keep real hot files ram buffer cache 
locality grouping algorithms key assumptions design cfs files residing cluster locality 
file read files cluster read soon 
assumption allows adopt cluster sized large read policy allows prefetching eliminates garbage 
locality order validate assumption analyzed log traces proxy servers 
investigate user access pattern web session 
user may visit different web sites time typically retrieve related files web site 
html file embedded files image files audio video files accessed 
html file contains hyper text links html files accessed user 
kind locality try capture grouping 
experiments validate observation 
chose html file accessed user collected files accessed user short period minutes cluster 
selected user visited web site 
html file accessed new user access nearly half files cluster 
previous studies shown similar results explored web proxy accesses locality web client session :10.1.1.31.6715:10.1.1.12.2253
level url resolution grouping algorithm developed simple efficient algorithm called level url resolution grouping algorithm investigation 
details algorithm 
algorithm algorithm decide evicted file allocate write buffer procedure create file pointer fp move fp lru file lru list current file half capacity cold buffer extension lru file html move fp file lru list endwhile current file html html file half capacity cold buffer move fp lru file lru list allocate size int clusters write buffer fill file write buffer record file evicted file release file lru list 
allocate write buffer algorithm start group system chose file swapped 
file evicted allocate write buffer grouping 
size write buffer multiple clusters 
process described 
system decides file allocates write buffer choose candidate files locality associated evicted file scope half cold buffer put write buffer 
process described 
write buffer full algorithm selects files access frequency evicted file small files algorithm algorithm group files locality write buffer procedure grouping input parameter evicted file output parameter write buffer create file pointer fp move fp lru file lru list iteration fill files locality write buffer space full current file half capacity cold buffer file web site host path write buffer large save file fill file write buffer release file lru list move fp file lru list endif iteration fill small files write buffer reduce internal fragmentation free space write buffer larger bytes find files access times evicted file small files bytes size fill write buffer half capacity cold buffer fit policy grouping 
locality grouping algorithm kb free space write buffer bytes 
filling cluster algorithm reduces internal fragmentation cluster 
selecting file evict algorithm gives preference html files 
html file related files accessed including embedded images files pointed embedded urls html file 
ucfs system operations previous section describes design ucfs component 
section show components 
request processing proxy server receives url request client server passes url ucfs 
ucfs calculates md checksum searches memory file lookup table 
possible outcomes 
cache file cached server 
case proxy fetch file original server put medium buffer ram 

memory hit 
file cached ram 
proxy server decides file valid 
file transferred client directly 

memory stale hit 
ucfs finds file cached ram 
proxy server decides file valid anymore 
proxy server replaces file ram latest version fetched original web server 

disk hit 
ucfs finds file cached disk 
proxy server decides file valid 
ucfs finds cluster containing file loads entire cluster medium buffer 
cluster disk invalidated 
server sends file client 
cluster may contain small files small files prefetched ram 

disk stale hit 
ucfs finds file cached disk 
proxy server decides file valid anymore 
ucfs cfs fetches cluster contains file disk ram invalidates disk cluster 
server replaces file ram new version fetched web server 
design generate garbage invalidation happens ram 
files cluster prefetched ram 
files accessed soon 

ims hit web proxy servers part ims requests 
requests typically verify validate expiration status files web clients proxy server web servers 
may affect networking performance generate overheads 
ucfs processes checking stale status files 
valid hits stale hits proxy server cache hit memory disk valid hit stale hit 
valid hit means cached file valid copy original web object 
stale hit means cached file dated longer valid 
proxy server re fetch file web server 
url document system modified time stored memory lookup table time stamp associated file data 
proxy values expire time server reply headers communicate web server decide file valid stale 
policy similar algorithm adopted squid 
power cycles re boot pointed proxy servers data reliability major concern 
cached files server lost crash retrieved original web servers 
major performance degradation file fetched subsequence accesses file satisfied proxy cache 
fact real production proxy server crashes rare event 
squid run unix systems proven highly reliable 
power failure problems easily solved inexpensive ups power supply 
ucfs deal regular shutdown re boot process 
server needs shut maintenance ups detects power failure ucfs shuts saves memory file lookup table disk cluster table files memory reserved superblock space disk 
proxy server restarts ucfs simply reads important data structures files superblock space order written disk 
cached files ready need fetched servers 
experimental methodology trace driven simulation evaluate performance design compared baseline systems squid ffs squid ffs async squidml ffs 
squid ffs squid running top regular unix ffs file system 
squid popular state art web proxy server writing 
ffs popular unix file systems 
squid ffs async squid running top unix ffs mounted asynchronous mode 
asynchronous ffs uses asynchronous writes perform disk updates 
avoids overhead synchronous updates traditional ffs better performance 
file system consistency may sacrificed event system crash 
squidml ffs squidml running ffs 
squidml better performance squid uses mmap virtual memory operations reduce disk os small files 
baseline systems represent state art proxy server technology widely 
ucfs simulator developed ucfs system simulator top widely comprehensive disk simulator 
update disk models slightly order simulate state art disk drives 
simulated gb disk drive 
chose quantum atlas gb drive basic model largest model currently provided triple sectors track expand capacity 
megabit second mb sec internal transfer rate 
order accurate comparison assume system mb ram quite standard memory size prevailing proxy servers 
trace logs collected real world large proxy servers exactly mb ram configuration 
see section details 
total mb ram mb cache file data 
remaining mb ram reserved hash table os overhead 
little bit conservative reality hash table ucfs takes mb leaving memory caching file data 
cfs cluster size kb stated 
chose size results show best performance evident 
proxy server simulator processes comprehensive list url requests including tcp hit tcp mem hit tcp tcp refresh hit tcp refresh requests described details 
simulators baseline systems squid implementations available possible measure performance traces 
result built simulators traces disk models ucfs compare performance 
squid ffs squid ffs async assume squid runs unix ffs file system 
assume system mb memory ucfs 
reported squid uses nearly ram store meta data hashing table urls assume mb ram squid cache data remaining ram meta data os overhead 
squid uses ffs manage files 
translates url unique numeric file name hashing level file path saves disk 
unix ffs simulator ported bsd ffs design top simulator 
ffs simulator provides detailed simulation guarantee valid simulation fair comparison 
simulates ffs cylinder groups cylinder group inode regions free blocks management ffs fragments management kb disk block sizes kb fragment sizes directory look cache improve meta data look 
hit rate observed simulation 
implementing ffs async ignore sync fsync tags show synchronous writes buffer cache 
writes asynchronous ffs async mode 
squidml ffs new way reduce disk compared squid 
idea circumvent file system abstractions store cached files large memory mapped file 
disk space memory mapped file allocated accesses file entirely managed virtual memory system 
specifically url requests smaller kb mmap process requests 
documents larger kb handled modified squid cache architecture 
stores cached objects server directory assuming cache linked objects tend access directory 
segment sizes bytes bytes retrieve objects memory mapped file 
frequency cyclic replacement algorithm 
trace driven simulation study idea 
developed squidml simulator realize scheme 
virtual memory simulator ffs simulator built top simulator 
files larger sizes handled ffs simulator smaller kb processed virtual memory mmap file 
simulation assume total system memory mb squid ffs ucfs 
ffs buffer caches mapping share half memory mb 
remaining ram reserved memory hash table meta data squid os overheads 
workload models chose large suites real world web proxy traces evaluate system 
large web proxy server traces set traces nlanr caches widely network research fields 
nlanr traces consist traces nlanr caches 
pa sj self service independent proxy caches 
web proxy servers organized hierarchy web proxy server architecture 
communicate sibling servers web documents 
traces may show locality independent web proxy servers 
caches receive requests day serving gb data client caches world 
obtained traces nlanr 
configurations caches summarized table 
traces consist logs week period 
unable retrieve complete traces remaining caches period unstable ftp server 
name city ram cache mb size gb bo boulder pa palo alto pb pittsburgh sj san jose uc urbana champaign table configurations nlanr caches name total url total data document hit name requests traffic gb rate bo pa pb sj uc table characteristics nlanr traces preprocessing traces nlanr access log traces consist comprehensive list information cached proxy servers 
speed simulation simulated requests replies specified 
filtered cgi dynamic document requests unrelated tcp requests tcp denied tcp client refresh tcp kept tcp hit tcp mem hit tcp tcp refresh hit tcp refresh tcp ref fail hit tcp ims hit tcp ims requests 
table shows statistic summary post processed nlanr web proxy server traces 
order measure system performance stable state avoid transient effect requests warm system started measuring performance point 
simulation results performance analysis section compare performance ucfs squid ffs squid ffs async squidml ffs 
disk performance total disk latency compares total disk latency week period 
shows speedups ucfs baseline systems calculated clear ucfs reduces total disk latency dramatically 
ucfs achieves significant improvement squid ffs squid ffs async squidml 
ucfs improves performance proxy server times squid ffs times squid ffs async times squidml ffs 
total disk latency hours squid ffs squid ffs async squidml ffs ucfs week disk latency report bo pa pb sj uc performance speedup times performance speedup total disk latency squidml ffs squid ffs squid ffs async bo pa pb sj uc total disk latency ucfs squid ffs squid ffs async squidml ffs average disk response time compares average disk response time ucfs baseline systems 
disk response time represents total time takes swap file disk level cache url operation 
reading file disk response time equals long proxy server wait get file file system 
writing file represents long takes file system return write 
shows speedups ucfs baseline systems calculated clearly see ucfs achieves dramatic performance improvements squid ffs squid ffs async squidml ffs 
squid ffs url operation spends ms disk squidml ffs needs ms disk serve url operation 
squid ffs async avoids expensive synchronous writes faster systems average disk response times ms ucfs hand needs ms disk io handle url operation 
improvement upto times baseline systems 
disk response time url request ms disk response time squid ffs squid ffs async squidml ffs ucfs bo pa pb sj uc performance speedup times performance speedup disk response time squidml ffs squid ffs squid ffs async bo pa pb sj uc average disk response time ucfs squid ffs squid ffs async squidml ffs understand system behaviors draw histogram disk response time ms range 
shows histogram disk response time uc trace 
results traces similar shown 
ucfs shows url requests hit ram buffer took ms finish 
ucfs utilizes memory efficiently 
uses large writes quickly write small files disk buffer space available writes 
large reads effectively prefetch files ram greatly improving read cache hit rate 
url requests response times higher ms mainly large disk reads writes 
shows squid ffs operations took ms finish implying lower ram buffer cache hit rate 
url requests finished ms times needed access files disk 
url requests took ms complete 
believe high costs results high metadata overheads addition normal disk access latencies including directory lookups case name cache misses creating new files new objects expensive synchronous updates file meta data 
shows results squid ffs async 
squid ffs async eliminates expensive synchronous writes small files 
result url operations finished ms requests spent ms additional expensive file meta data reads name cache misses 
requests spent ms writing reading file disk level cache 
squidml ffs requests spent ms disk time 
uses mmap functions avoid meta data operations squidml ffs shows better performance shifting group slow requests squid ffs higher ms left ms squidml ffs uses small expensive writes reads page data memory 
operations spent ms ms time disk response 
exclusive percent exclusive percent ucfs proxy server disk response time ms squid ffs async disk response time ms exclusive percent exclusive percent squid ffs disk response time ms squidml ffs disk response time ms histograms disk response time ucfs squid ffs squid ffs async squidml ffs uc workload disk reads writes gain insights ucfs achieve performance categorized disk accesses reads writes studied performance separately 
compares total read latency total write latency systems 
shows ucfs speedup baseline systems 
squid ffs worst read write performance systems 
squid url request cache generate file write proxy server retrieves document original server writes disk 
disk hit result file read disk 
squid ffs writes web document disk generates small disk os 
writes level directory inodes write file inode writes web object 
squid ffs reads web document disk generates multiple small disk os reads level directory inodes read file inode write file inode update file access time reads file data 
ffs name cache eliminate reads inodes overhead excessively high 
name cache hit rate limited busy proxy server may deal millions files 
name cache compete meta data data caches limited memory 
squidml uses files manage cached data smaller kb 
reduces eliminate meta data update overheads associated file open close operations 
small portion cached file data kept ram moment system frequently page data 
operation results inefficient small disk accesses 
squidml uses ffs handle files larger kb account proxy traffic 
result squidml ffs better read performance better write performance squid ffs 
ffs async eliminates expensive synchronous metadata updates significantly improve write performance squid ffs times 
ffs async uses small expensive disk requests read write data disk disk bandwidth utilization severely limited 
squid ffs async directly improve read performance indirectly improve read response time slightly reduction disk bandwidth competition 
ucfs hand demonstrates drastically better performance baseline systems reads writes 
example indicates ucfs shows times better write performance squid ffs times better write performance squid ffs async times better read performance squidml ffs 
explained ucfs achieves impressive result writes eliminating metadata overheads importantly amortizing small disk writes overhead single large write 
example average file size kb cluster size kb cluster contains files 
file write result disk write 
ucfs significantly improves read latencies large reads efficient prefetching 
ucfs non garbage cfs performance degradation problem 
document hit rates calculated total document hit rates including hits ram disks systems 
shown table ucfs achieves comparable document hit rate systems 
important ucfs achieves file system performance hurting web proxy server document hit rate 
reason ucfs baseline systems slightly different document hit rates different replacement policies include buffer management policies timer document management algorithms 
read latency squid ffs squid ffs async squidml ffs ucfs total disk read latency bo pa pb sj uc write latency squid ffs squid ffs async squidml ffs ucfs total disk write latency bo pa pb sj uc total disk read write latency ucfs squid ffs squid ffs async squidml ffs speedup times total read latency speedup ucfs squid ffs read speedup squid ffs async read speedup squidml ffs read speedup bo pa pb sj uc memory hit rate prefetch hit rate read write speedup speedup times total write latency speedup ucfs squid ffs write speedup squid ffs async write speedup squidml ffs write speedup bo pa pb sj uc shown ucfs improves performance proxy servers areas efficient memory data structure eliminate meta data os cfs design allows grouping small writes efficient large writes cfs design reads files large clusters effectively prefetching related files ram little extra overhead cfs design avoids garbage problem altogether 
key design decision cfs large clusters basic read unit 
design allows prefetching files completely eliminating garbage 
avoiding garbage important advantage benefit large clusters largely determined memory hit rate prefetched files memory hit accesses near performance gain prefetching limited 
prefetches may pollute cache 
prefetch hit rate lead higher memory hit rate 
table compares memory hit rates ucfs squid ffs squid ffs async squidml 
ram size set mb 
memory hit rates represent percentages name squid ffs ffs async squidml ucfs bo pa pb sj uc average table document hit rate nlanr caches hit documents ram memory cache see ucfs higher memory hit rates baseline systems 
new scheme achieves better memory hit rates squid ffs squidml ffs respectively 
table shows prefetch hit rate defined section ucfs 
improved memory hit rate due successful prefetching indicating locality grouping algorithm works 
name squid ffs ffs async squidml ucfs bo pa pb sj uc average table memory hit rates different architectures trace name bo pa pb sj uc average ucfs prefetch hit rate table prefetch hit rates different architectures performance impact cfs cluster sizes results cfs cluster size kb 
varied cluster size kb kb order find optimal cluster size 
ram size fixed mb 
shows workloads kb best performance 
larger clusters difficult hold similar locality files cluster leading wasted prefetching traffic 
smaller clusters utilize full disk bandwidth disk seeks rotational latencies overheads increase 
performance impact ram sizes studied performance systems different ram sizes 
data space limitation observation summarized follows 
ram memory caches ram buffer caches ucfs buffer cache baseline systems 
disk latency ms cluster size impact average disk latency url ucfs bo pa pb sj uc average disk latency url request ucfs different cluster size period changing memory sizes limited impacts proxy server performance 
example increasing ram size mb mb results response time improvement squid 
surprise previous studies pointed web cache hit rate grows logarithmic fashion amount traffic size client population 
ucfs performs slightly better memory size increases 
speed ucfs squid ffs increases slightly memory size increases shown 
believe locality grouping algorithm better chances find file candidates buffer size increases 
related speedup times speedup average latency ucfs squid ffs bo pa pb sj uc io response time speedup different memory sizes unix file system papers tried improve unix file system performance 
sprite lfs bsd lfs developed solve small writes problem ffs 
far major commercial operating system supporting 
ganger proposed solution called soft updates enhance performance ffs safe asynchronous write 
ganger kaashoek improved ffs disk bandwidths small files embedded inodes explicit grouping 
progress current general purpose file systems poor performance proxy server workloads 
general issues web proxy servers large web proxy server popular research topic 
number papers show web proxy traces excellent long term locality 
evaluation showed document hit ratios ranged large commercial web proxy servers 
research papers reported similar hit rates web proxy servers 
web prefetching web proxy servers proxy software parse html prefetch documents links embedded images 
kaashoek suggested web prefetching technique fetch lined images places files adjacent html files reads unit effectively prefetching images 
piggyback cache validation krishnamurthy wills proposed piggyback invalidation policies 
piggyback cache validation pcv piggyback server invalidation psi tried reduce cache validation traffic proxy caches servers 
ideas original web server information partition group affiliated related web documents 
proxy caches web servers exchange check validate information group 
documents group validated schemes reduce average networking cost staleness ratio 
ucfs groups affiliated documents apply piggyback invalidation techniques ucfs reduce networking cost maintain strong coherency 
requesting web document web server ucfs piggyback validation requests files cluster 
file systems web proxy servers studies suggested disk traffic significant overhead web proxy servers 
observed disk delays contribute total hit response time 
experiments showed different level proxy caches squid disk response times ranged ms ms result similar values observed squid ffs baseline system 
suggested proxies multiple disks balance load reduce long seeks disk partitions 
proposed methods reduce overhead proxies maintaining directory store web documents host mmap file virtual memory handle small objects reduce meta data operations 
markatos proposed similar method improve file system performance storing objects similar sizes directory called buddy 
developed technique large continuous disk writes called stream reduce disk latency 
calculating average disk latency url operation simulation concluded squid spent ms latency url operation mb buffer cache 
result similar values obtained squid ffs baseline system 
iyengar disk storage allocation algorithms managing persistent storage minimize number disk seeks allocations 
claimed algorithms result considerable performance improvements database file systems web related workloads 
performance improvement large proxy servers limited methods 
tried improve performance top current file system get quite limited effect 
build simple highly efficient custom file system raw disk 
compared methods save meta data overhead maximize utilization disk bandwidth 
user space file system combined disk subsystem web proxy server related load 
specialized file systems commercial specific systems developed web proxy server market 
network appliance provides web proxy cache service web clients support proxy agents 
improved performance building special operating system application specific file system executing closed hardware 
design details available 
multimedia storage servers provide guaranteed quality service reading writing streaming media implementing specialized data placement request scheduling algorithms provide services 
presents novel user level custom file system called ucfs high performance web proxy servers 
system significant advantages traditional approaches 
ucfs manages file data meta data raw disk disk partition 
limitation existing general purpose file system free design efficient algorithms optimized web proxy cache applications 
ucfs runs completely user space easy inexpensive implement maintain 
need modify os kernel 

design excellent portability runs part regular user space program 
ucfs need special support os achieve high performance 

cluster structured file system cfs design algorithms group documents similar locality cluster significantly improve read write performance 

log structured approaches garbage collection cfs cfs generate garbage 

efficient memory file lookup tables completely eliminate overhead caused meta data os 
system avoids overhead periodically flushing dirty data disk 
extensive simulation confirms ucfs drastically improve proxy server performance 
running week long nlanr traces ucfs reduces squid ffs disk os 
achieves times better performance squid ffs times better squid ffs async times better performance squidml ffs terms disk response time 
pointed traces research root level traces 
traces lowerlevel caches institution leaf level proxy caches normally better locality 
believe scheme better performance lower level proxy servers grouping algorithm better 
believe ucfs improve performance network applications web servers file caching servers 
os major bottleneck applications 
example ucfs system exist regular file system accelerate web server 
users maintain web documents reliable familiar regular file system 
web server accesses web page time file copied ucfs system file go ucfs file updated 
doing dramatically improve web server performance operations transparent users 
trying improve locality grouping algorithms order improve prefetch hit rates 
try implement ucfs test performance real world environment 
study issues applying ucfs web related platforms web servers 
acknowledgments supported part national science foundation career award ccr ohio board computer science collaboration 
wang min wu hu boosting performance internet servers user level custom file systems proceedings nd workshop performance architecture web servers paws 
published special vol 
acm sigmetrics performance evaluation reviews boston ma pp 
feb 
almeida cao measuring proxy performance wisconsin proxy benchmark tech 
rep computer science department university wisconsin madison april 
richardson grunwald reducing disk web proxy server caches proceedings usenix annual technical conference usenix berkeley ca pp 
usenix association june 
gribble brewer system design issues internet middleware services deductions large client trace proceedings usenix symposium internet technologies systems usits monterey ca dec 
measured access characteristics world wide web client proxy caches proceedings usenix symposium internet technologies systems usits monterey ca dec 
wessels third cache report oct 
measurement factory performance study squid proxy world wide web journal special edition www characterization performance evaluation 
wessels squid frequently asked questions 
www org doc faq faq html toc 
ousterhout douglis beating bottleneck case log structured file systems tech 
rep computer science division electrical engineering computer sciences university california berkeley oct 
rosenblum ousterhout design implementation log structured file system acm transactions computer systems vol 
pp 
feb 
seltzer bostic mckusick staelin implementation log structured file system unix proceedings winter usenix san diego ca pp 
jan 
hu yang dcd disk caching disk new approach boosting performance proceedings rd international symposium computer architecture isca philadelphia pennsylvania pp 
may 
hagmann reimplementing cedar file system logging group commit proceedings eleventh acm symposium operating systems principles austin tx pp 
nov 
acm operating systems review 
ganger patt metadata update performance file systems usenix symposium operating system design implementation osdi pp 
nov 
ganger mckusick soules patt soft updates solution metadata update problem file systems acm transactions computer systems vol 
pp 
may 
schindler ganger nagle higher disk head utilization extracting free bandwidth busy disk drives proceedings conference operating system design implementation osdi san diego oct 
markatos secondary storage management web proxies proceedings nd usenix symposium internet technologies systems usits berkeley ca pp 
usenix association oct 
seltzer smith balakrishnan chang padmanabhan file system logging versus clustering performance comparison proceedings usenix new orleans la pp 
jan 
hu yang design implementation dcd device driver unix proceedings usenix technical conference monterey california pp 
jan 
unix internals new frontiers 
prentice hall 
rivest md homepage umbc edu cs md md html 
jenkins new hash functions hash table lookup dr dobb journal sept 
williamson eager traffic analysis web proxy caching hierarchy ieee network vol 
pp 
may june 
arlitt williamson trace driven simulation document caching strategies internet web servers simulation journal vol 
pp 
jan 
breslau cao fan phillips shenker web caching zipf distributions evidence implications proceedings infocom conference mar 
barford bestavros bradley crovella changes web client access patterns characteristics caching implications world wide web special issue characterization performance evaluation 
ganger patt system level models evaluate subsystem designs ieee transactions computers vol 
pp 
june 
quantum 
quantum atlas tm kii disk drives 
wessels nlanr cache readme april 
ftp ircache nlanr net traces readme 
fielding gettys mogul frystyk masinter leach berners lee hypertext transfer protocol www cis ohio state edu cgi bin rfc rfc html june 
ganger kaashoek embedded inodes explicit grouping exploiting disk bandwidth small files proceedings usenix technical conference january pp 
jan 
williams abrams abdulla fox removal policies network caches world wide web documents proceedings acm sigcomm conference applications technologies architectures protocols computer communications vol 
acm sigcomm computer communication review new york pp 
acm press aug 
group www collector prefetching proxy server www 
ac jp products html 
kaashoek engler anger wallach server operating systems sigops european workshop sept 
krishnamurthy wills study piggyback cache validation proxy caches world wide web proceedings usenix symposium internet technologies systems berkeley pp 
usenix association dec 
krishnamurthy wills piggyback server invalidation proxy cache coherency proceedings th international www conference brisbane australia apr 
kant mohapatra scalable internet servers issues challenges performance evaluation review 
caching proxies proceedings rics performance madison wi pp 
jun 
file placement web cache server proceedings th acm symposium parallel algorithms architectures 
iyengar jin challenger efficient algorithms persistent storage allocation proceedings th ieee symposium mass storage systems san diego california april 
danzig architecture deployment 
www com technology level html 
high performance web caching white 
martin narayanan rastogi silberschatz fellini multimedia storage server chapter 
kluwer academic publishers 
