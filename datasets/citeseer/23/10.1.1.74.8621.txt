journal instruction level parallelism submitted published stream algorithms architecture volker csail mit edu computer science artificial intelligence laboratory massachusetts institute technology stata center street cambridge ma henry hoffmann hank mit edu lincoln laboratory massachusetts institute technology wood street lexington ma anant agarwal agarwal csail mit edu computer science artificial intelligence laboratory massachusetts institute technology stata center street cambridge ma wire delay power consumption primary obstacles continued scaling microprocessor performance 
fundamentally issues addressed emerging breed tiled microarchitectures including raw trips scale replicate programmable processing elements small amounts memory communicate chip networks characterized extremely low latencies high bandwidth 
goal show tiled microarchitectures permit energy efficient high performance computations algorithms mapped properly 
propose decoupled systolic architecture canonical tiled microarchitecture supports resource requirements decoupled systolic algorithms designed specifically architecture 
develop analytical framework reasoning efficiency performance decoupled systolic algorithms 
particular define stream algorithms class decoupled systolic algorithms achieve computational efficiency asymptotically large numbers processors 
focus attention class regularly structured computations form foundation scientific computing digital signal processing 

approach physical limits signal propagation delays power consumption emerge primary focus engineering trade offs 
tiled microarchitectures cope large signal propagation delays large chip areas short wires exposing scheduling wire resources software 
tiling provides path increasing clock frequency primary performance growth power consumption dominant constraint 
continuation established vlsi design practices produce silicon chips generate heat packaging technology dissipate 
situation energy efficiency primary design aspect microarchitectures 
approach increase energy efficiency boosting computational efficiency 
key lies finding match algorithms microarchitecture 
show achieve maximal computational efficiency regular applications programmable tiled microarchitecture 
energy efficiency direct consequence hardware utilization measured operations time unit 
note applications utilize underlying hardware resources useful computation highest performing energy efficient 
hard ai access foundation morgan kaufmann publishers 
rights reserved 
hoffmann agarwal ware utilization depends statistical distribution executed instruction types instruction schedule course data determine voltages toggle data wires 
previous studies shown risc processors tiled microarchitectures raw energy consumed functional units performing useful 
majority energy dissipated clock network chip memories 
show tiled microarchitectures perform energy efficient computations propose decoupled systolic architecture dsa canonical tiled microarchitecture executing decoupled systolic algorithms 
dsa consists array compute tiles peripheral memory tiles chip memory system 
name suggests decoupled systolic algorithms decouple memory accesses computation execute systolic fashion 
define stream algorithms class decoupled systolic algorithms achieve computational efficiency dsa asymptotically large numbers processors 
comparison existing systolic algorithms efficiency target 
crux stream algorithms reduction power consumption memory modules asymptotically insignificant compared power consumed useful computation 
article contributions 
demonstrate construction exist formulations regular algorithms stream algorithms definition achieve computational efficiency asymptotically large numbers processors dsa 

design methodology guide design stream algorithms defined entity 
resulting rigorous approach improves understanding tiled microarchitectures microarchitectural design point enables systematic restructuring algorithms communicates perspective interaction algorithms architecture 

efficiency analysis allows identify machines decoupled systolic architectures explicitly incorporating area cost tile local memories penalizing unbounded local memories 
particularly important context emerging tiled microarchitectures individual tiles small silicon footprints 

identified dsa canonical tiled microarchitecture resembles systolic machines streaming applications achieves asymptotic energy efficiency important class algorithms 
believe insight transforms known constraint tiled microarchitectures small local memories feature related energy efficiency 

experimental results executing stream algorithms existing tiled microarchitecture emulating decoupled systolic architecture 
results indicate stream algorithms just theoretical interest achieve high performance energy efficiency practical system sizes 
computational efficiency shall denote utilization floating point units study algorithms dominated floating point arithmetic 
enable simple counting arguments algorithmic design efficiency analysis distinguish computation memory accesses account contributors including clock network 
choice fact computation memory accesses order effects determine computational energy efficiency realm algorithmic analysis 
viewing energy efficiency consequence computational efficiency algorithm designer focus goal maximizing 
stream algorithms architecture remainder organized follows 
section decoupled systolic architecture 
section defines notion stream algorithms 
discuss algorithmic design process transforming application stream algorithm sample applications stream algorithms 
matrix multiplication section fundamental stream algorithm workhorse part stream algorithms 
obvious stream algorithms triangular solver section convolution section 
analyze stream algorithm argue achieves optimal computational efficiency asymptotically executed decoupled systolic architecture 
discuss related section summarize results including emulation experiments raw section conclude section 
decoupled systolic architecture decoupled systolic architecture dsa canonical tiled microarchitecture consisting array processing elements replicated silicon area connected chip network 
tiled microarchitectures single chip parallel machines organization primarily determined propagation delay signals wires 
enable high clock frequencies large chip areas tiled microarchitectures short wires span fraction side length chip registers pipeline signal propagation 
short wires turn introduce scheduling problem space time cope propagation signals distances longer reachable single wire 
moving data wires distributing operations processors equally important scheduling goals 
scheduling problem received attention context vlsi design parallel computation parallelizing compiler design instruction level parallelism past 
speed advantage short wires gone unnoticed 
fact systolic arrays proposed kung leiserson late aimed part exploiting speed short wires 
lacking chip area support programmable structures early systolic arrays designed special purpose circuits particular application customized problem size 
systolic systems warp programmable reap benefits systolic arrays regular applications 
believe significant area energy efficiency systolic arrays merit reexamination face architectural similarities tiled microarchitectures 
decoupled systolic architecture serves canonical tiled microarchitecture 
result design analysis various foundational decoupled systolic algorithms 
execute decoupled systolic algorithms efficiently tile needs small bounded amount local memory algorithmic feature exploited dsa 
tiles arranged array set memory tiles periphery illustrated 
array consists computation tiles denoted memory tiles denoted periphery 
letter denote network size array analogous problem size algorithm 
decoupled systolic algorithm solves large problem breaking smaller systolic subproblems storing input data intermediate results peripheral memories 
input 
network size canonical network parameter 
number tiles network size determined network topology 
example linear array size contains computation tiles memory tiles dimensional array contains computation tiles memory tiles 
dsa dimensional square array side length hoffmann agarwal decoupled systolic architecture dsa array computation tiles surrounded memory tiles shown network size data supplied computation tiles memories continuous stream network eliminating load store memory accesses computation tiles 
consequence decoupled systolic algorithms decouple memory accesses computation move memory accesses critical path 
decoupling memory accesses computation permits energy efficient microarchitecture majority power consumed computation memory accesses 
tile architecture decoupled systolic architecture types tiles computation tiles memory tiles 
peripheral memory tiles consist computation processor augmented additional local memory backed large chip memory system 
name suggests primary involves memory accesses including address computations 
assume memory deliver throughput loads store clock cycle 
section focuses key architectural features computation tiles 
computation tile sketched simple general purpose programmable core comprising integer unit floating point unit multiply add module multi ported general purpose register file 
analysis decoupled systolic algorithms revealed registers suffice 
focus attention datapath omits control logic small instruction buffer 
assume single issue order pipelined fpu allows issue multiply add operation clock cycle 
functional units including divider square root unit pipelined critical application performance due frequent 
referring computational efficiency typically mean utilization fpu 
particular computational efficiency refers fpu executing operation clock cycle 
arguably important feature dsa design chip network 
interconnect uses register mapped network allows instructions access network register names 
raw dsa incorporate separate switch proces stream algorithms architecture gpr iu fpu computation tile contains general purpose register file gpr integer unit iu floating point unit fpu multiply add module 
processor connects fifo neighbors 
sor programmed routing 
flexibility provided separate switch processor particular routes bypassing main processor simply needed stream algorithms 
dsa emulated top tiled microarchitecture raw modest loss efficiency discussed section 
illustrated fifo connect synchronize neighboring processors 
implementation stream algorithms demands existence bidirectional connections neighboring tiles 
fifo blocking exposed programmer mapping register names instruction set 
outgoing ports mapped write registers semantics fifo push operation incoming ports readonly registers semantics fifo pop operation 
raw prefer network tightly integrated pipelined functional units 
accordingly bypass wires commonly feed signals back operand registers connect individual pipeline stages outgoing network fifo tight network integration ensures latency single clock cycle communication neighboring computation tiles allows efficient pipelining results operations different pipeline depths processor array 
decoupled systolic architecture uses wide instruction word schedule multiple simultaneous data movements network functional units network register file network 
typical stream instruction consists parts 
operation floating point multiply add compound instruction 
multiplies values arriving network ports adds product value general purpose register simultaneously routes incoming values neighboring tiles specified part instruction 
value arriving port routed outgoing port value arriving port outgoing port instructions dsa block operands available 
small fifo deep deepest functional unit eases problem scheduling instructions substantially 
exists trade instruction width area hoffmann agarwal occupied corresponding wires tile 
analysis stream algorithms assume data movements specified route part instruction 
chip network discussion register mapped network previous section nearly completes description chip network 
fact chip network consists little wires connecting neighboring tiles switch tile programmed connections wires functional units 
wires span distance longer side length tile relatively short fast 
vlsi implementation balances side length tile critical path length tile 
provide brief network organization means programming example 
programmer point view essential features register mapped network enables distribute computation multiple tiles network temporary storage reduce critical path length computation abandoning local memory accesses conventional load store instructions 
simple example illustrating decoupling computation consider computation sum shows version addition conventional load store processor architecture left side space distributed tile version dsa network size right hand side 
memory load store architecture mem decoupled systolic architecture dataflows load store architecture tiles dsa 
conventional load store processor addition requires instructions load operands memory general purpose registers perform addition store result variable symbol indicate denote memory locations version dsa distribute operations tiles computation tile memory tiles 
execute instruction tile network temporary storage mem mem memory tile memory tile computation tile stream algorithms architecture memory tile introduced section names network mapped registers read write access networks depending instruction 
memory tile loads operand local memory writes network register similarly memory tile loads operand straight network register computation tile uses instruction read operands network registers write result back network register memory tile reads sum network register stores local memory 
network register names refer directions code position independent particular tile 
note register mapped network acts distributed register file organized fifo furthermore load store version addition occupies instruction slots tile tile version occupies instruction slot tiles 
critical path length program terms instruction slots times smaller tile version 
consider element wise addition vectors scalar numbers 
case replicate loop array index tiles proper loop unrolling code achieves throughput addition clock cycle 
contrast equivalent loop load store version achieves throughput addition clock cycles 
investing times space improves throughput factor 
computational efficiency versions 
load store version fourth instruction useful addition tile version tile computation tile operates computational efficiency memory tiles implement data movement 
see sections design stream algorithms allows utilize computation tiles dsa computational efficiency 
chip network single chip dsa embedded larger system 
desire access larger chip memory system 
addition sake scalability want interconnect multiple chips assemble larger stream fabrics 
dsa significant portion silicon real estate invested chip network seamless connectivity memory dsa chips essential scalability programmability 
number pins provided chip packaging technology imposes serious limitation number network wires may cross chip boundary 
extend chip network chip boundary need enormous number data pins 
array sides memory tiles networks directions bits 
multiplying factors results hoffmann agarwal minimum number data pins single chip 
concrete word size bits require data pins word size bits require data pins 
number pins technologically infeasible foreseeable area inefficient 
compromising network size match availability pins better alternative invest chip network reduces number wires needed cross chip boundary 
prime candidate network fat tree permits concentration switches number wires connected switch root tree small compared total number wires connected switches leaves 
shows fat tree network arranged forms back plane memory tiles 
reduction wires switches fat tree quite flexible allowing pick number wires cross chip boundary constraints packaging technology 
chip boundary memory tiles connect back plane fat tree network chip world including memory system 
number data pins chip package raises question wires wish single chip dsa fat tree network back plane 
assume fat tree connects chip large pool chip memory banks memory tiles acting stream buffers stream caches 
organization memory hierarchy allow stream data chip speed required ensure computational efficiency 
meet requirement need focus attention bandwidth chip network 
analysis stream algorithms revealed bandwidth needed chip boundary factor smaller bandwidth offered extending chip network 
fact stream algorithms largest bandwidth requirements stream svd example require sets incoming sets outgoing streams sides array 
corresponds utilizing just sides tiles bidirectional network stream algorithms architecture tile leading aggregate memory bandwidth word size bits fat tree network requires data pins 
chip realizable today packaging technology 
aside desirable implement separate chip network connecting computation tile peripheral memory tiles offer reasonably simple means supplying computation tiles chip network instructions 

stream algorithms section introduce stream algorithms set conditions enable increase efficiency increasing number tiles computational efficiency approaches asymptotically power consumed memory operations insignificant 
key strategy design stream algorithms recognize number memory tiles negligible compared number computation tiles memory tiles contribute useful computation tend consume relatively large amount energy 
call systolic algorithm memory accesses executed different tile computation decoupled systolic algorithm recognition decoupled access execute architecture 
impossible design efficient decoupled systolic algorithm small number tiles small problem size increase efficiency larger numbers tiles large problems 
emphasize observation formulating condition 
definition decoupling efficiency condition decoupled systolic algorithm network size number computation tiles number memory tiles 
say algorithm decoupling efficient informally decoupling efficiency expresses number memory tiles insignificant relative number computation tiles increase network size decoupling efficiency necessary condition amortize lack useful computation performed memory tiles power invested memory accesses 
example suppose implement algorithm computation tiles 
arrange memory tiles number negligible compared increasing network size resulting algorithm decoupling efficient 
decoupling efficient algorithm may choose example 
contrast design efficiently decoupled 
decoupled algorithms se independent particular architecture 
note dsa particularly suited executing algorithm multiple algorithms concurrently decoupling efficiency necessary sufficient condition guarantee high efficiency 
order formulate sufficient condition clarify notion computational efficiency means definition 

systolic algorithms originally designed systolic arrays hardware circuits including registers large local memories load store interface taken broader meaning time include accesses potentially large local memories 
hoffmann agarwal definition computational efficiency algorithm problem size number useful computational operations network size execution time number time steps area counted number tiles computational efficiency numerator equation consists number useful operations product denominator interpreted computational capacity dsa time period additional insight derive definition computational efficiency familiar notion speedup interpret denominator equation space time product vlsi model computation 
define area space normalized respect average area occupied computation memory tiles 
definition efficiency follows known relation efficiency speedup third line follows fact computational equals number time steps needed execute algorithm problem size single tile words algorithm problem size efficiency inversely proportional space time product stated definition 
practical purposes may relate problem size network size realvalued substituting computational efficiency follows 
equation obtain sufficient condition definition computation efficiency condition call algorithm problem size computationally efficient executed network size informally computation efficiency condition demands obtain computational efficiency asymptotically infinitely large network size problem size infinitely stream algorithms architecture larger network size 
note computation efficiency condition presents departure conventional parallel algorithm design scalability foremost goal 
particular past design parallel algorithms associated assumption able utilize larger numbers tiles increase problem size ideally tiles efficiently 
computation efficiency condition abandons criterion favor perspective real machines finite size 
loosely speaking prefer small machine efficiently large machine efficiently 
definitions computation efficiency condition decoupling efficiency condition define stream algorithm 
definition stream algorithm stream algorithm computation efficient decoupling efficient decoupled systolic algorithm 
definition stream algorithm suggests express efficiency stream algorithms product terms capturing computation efficiency capturing decoupling efficiency depending require term rational function representing average utilization computation tiles executing decoupled systolic algorithm 
stream algorithms developed far find assumes form rational function see table section term represents area efficiency decoupled systolic algorithm ratio computation tiles total number tiles stream algorithms listed table rational function form terms exhibit behavior characteristic stream algorithms product terms computational efficiency approaches large values illustrates general behavior efficiency function network size case top curve corresponds term approaches limit rapidly 
curve shows corresponding term bottom curve product terms 
term approaches rapidly efficiency curve curve dominating term 

write condition limit little condition quite strong remains open question weaker demand useful 
illustration efficiency hoffmann agarwal ideal network size function network size special case function represents average utilization computation tiles area efficiency 
sections discuss transformation matrix multiplication triangular solver convolution stream algorithms 
designs qualify stream algorithms fulfill decoupling efficiency computation efficiency conditions 
readers interested technical details algorithms analysis may skip sections 
matrix multiplication example stream algorithm consider dense matrix multiplication 
matrices wish compute matrix compute element row column product matrix inner product row column partitioning interested problems size start partitioning problem smaller independent subproblems 
dominating subproblems execute systolic dsa maximum computational efficiency 
matrix multiplication block recursive partitioning 
recurse rows columns matrices matrix matrix 
matrix multiplication partitioned homogeneous set subproblems 
decoupling stream algorithms architecture goal move memory accesses critical path decoupling computation memory accesses occur memory tiles computational operations computation tiles 
observing product element computed independently means equation 
addition equation allows stream entire rows entire columns computation tiles 
furthermore partition problem size size fit array computation tiles 
implement resulting subproblems systolic matrix multiplications illustrated rows flow left right columns top bottom array 
time steps systolic matrix multiplication matrices 
box represents computation tile 
values entering leaving generated array shown bold face 
shaded boxes mark completion inner product 
split data flow operands products top bottom rows 
computation tile row column computes product elements supply computation tiles proper data streams memory tiles store rows additional memory tiles store columns matrix multiplication computation tiles memory tiles 
illustrates data flow decoupled systolic matrix multiplication note memory tiles periphery determine schedule computations streaming combinations rows columns computation tiles 
compute streaming array 
second stream third product matrix streamed neighboring array consuming computation result compute order 
tiles chip altogether shall stored memory tiles may invest memory tiles total case conclude structure matrix multiplication decoupling efficient 
efficiency analysis analyze efficiency matrix multiplication show qualifies stream algorithm 
number multiply add operations multiplication matrices hoffmann agarwal data flow computation efficient matrix multiplication matrices computation tiles 
shaded boxes periphery mark memory tiles indicate completion inner product 
network size computation tiles memory tiles pipeline computation systolic matrix multiplications size times pipelining produces optimal tile utilization startup drain phases combined take time steps cf 
total number time steps required computation equation computational efficiency matrix multiplication parameter obtain efficiency 
consider product terms independently 
term approaches large values problem size larger network size hand term approaches large network sizes asymptotic behavior illustrated 
assume constant value find efficiency matrix multiplication increases increase network size approaches optimal computational efficiency asymptotically 
note fixed stream matrix multiplication requires time steps network computation tiles 
stream algorithms architecture discuss suitable network size dsa consider absolute numbers 
example systolic matrix multiplication maximum efficiency just infinitely large network 
hand relatively small value network size computationally efficient matrix multiplication problem size achieves efficiency 
larger problem sizes larger networks operate efficiency 
single dsa chip network size computational efficiency matrix multiplication problem sizes 
triangular solver triangular solver computes solution linear system equations assuming matrix triangular 
example lower triangular matrix finding solution straightforward computation known forward substitution interested triangular solvers building blocks algorithms including lu factorization 
particular interested lower triangular version finds matrix solution matrix representing right hand sides 
partitioning partition lower triangular system linear equations multiple right hand sides recursively equation 
matrices lower triangular 
hoffmann agarwal partitioned triangular form leads series smaller problems lower triangular solver compute solution lower triangular systems equations yielding solutions subsequently update matrices equations producing compute matrix subtraction equations computation tiles array 
save associated data movement executing subtraction memory tiles 
alternative simpler program 
matrices systems yields right hand sides lower triangular systems equations 
solving equations define recursive algorithm solving lower triangular system equation 
recursion reduces problem solving system linear equations smaller lower triangular systems linear equations plus matrix multiplications discussed section 
decoupling arrive decoupled design observe computations individual right hand sides linear system independent 
consider system right hand sides 
computation column depends elements elements column computations columns may performed independently 
depicts systolic algorithm lower triangular solver 
stream rows left right columns top bottom computation array columns stream bottom array 
tile row column computation array responsible computing element note due independence columns computation may permute columns arbitrarily provided preserve staggered data movement 
systolic design upper triangular solver reversing order rows stored memory tiles reversing order elements columns fed computation tiles 
illustrate systolic algorithm describing computation element time step 
tile receives element left computes intermediate result time step element tile receives left 
executing multiply add operation computes intermediate result time step tile receives stream algorithms architecture 
systolic lower triangular solver right hand sides 
left computes step element time available bottom array 
reducing problem size recursively subproblems fit array computation tiles need memory tiles periphery computation array means buffer matrices shows computation equations 
implied memory tiles store rows memory tiles columns respectively 
decoupled systolic solver require computation tiles memory tiles meeting decoupling efficiency condition matrix multiplication factorization triangular solver produce identical subproblems 
faced additional challenge finding efficient composition subproblems 
pipeline subproblems avoid idle cycles due data dependencies heterogeneity computations equations 
minimize loss cycles grouping independent computations hoffmann agarwal phases decoupled systolic lower triangular solver array computation tiles 
phase solve equation phase update equation 
matrix multiplication executed computation tiles matrix subtraction performed memory tiles receive individual result phase solve equation shapes matrix areas indicate rows columns enter computation array staggered fashion 
type pipelining switching group 
example group pipeline computations equations equations equations 
unfold recursion way base case subproblems find best schedule equivalent block iterative ordering subproblems 
efficiency analysis determining computation efficiency lower triangular solver equation 
number multiply add operations counting division multiply add operation 
mentioned crux efficient schedule order computations equations subsequent systolic algorithms overlapped 
matrix multiplication finding perfect overlap relatively easy systolic algorithm 
lower triangular solver illustrate search schedule corresponding efficiency analysis means example equation matrices consist blocks block dimension recall computations individual columns column blocks independent 
may sequence systolic computations rows maximize stream algorithms architecture lap 
partitioning computation column block consists solver computations update operation 
interleaving computations yields sequence equations 
consider column block example equation sequence operations 
solve second update third solve fourth update involves update operations 
fifth solve sixth update involving update operations 
solve observe increasing number solvers increases quadratically number update operations increases 
update operations little matrix multiplications may pipeline overlap possible resembling stream structured matrix multiplication 
block iterative schedule iterates row blocks 
row block schedule solver computations entire row maximal overlap 
solvers row block require time steps plus time steps starting draining pipeline 
compute overlap update operations associated update operations resulting update operations associated row operations require time steps plus time steps start drain pipeline 
may save time steps overlapping update operation solver computation solver computation row block update operation previous row block 
number time steps lower triangular solver blocks size find fixed total number time steps computation tiles 
equation computational efficiency lower triangular solver analogous equation matrix multiplication efficiency product terms depending indirectly problem size second depending network size problem reduces single systolic lower triangular solver obtain efficiency efficiency increases increase computational efficiency approaches optimal value solver requires memory tiles sides array computation tiles second term requires slightly larger network size achieve high achieve efficiency 
example large efficiency golden network size 

convolution convolution sequence output sequence hoffmann agarwal length sequence length produces length loss generality assume element partitioning partition convolution subproblems partitioning sum equation follows partitioning expresses convolution sum convolutions weight sequences intuitively partition weight sequence chunks length compute partial convolutions exploit associativity addition form sum partial convolutions convenient 
decoupling systolic design implement convolution design independent length sequence example chosen weight sequence sequence enter array left output sequence leaves array right 
computation tile responsible storing element weight sequence 
stream elements folds way left right array 
contrast sequence streams left right folding 
time step computation tiles multiply local value element arriving left add product intermediate value received left send new intermediate value right 
elements leave array right 
illustrate data movement discussing computation time step element enters tile stream algorithms architecture systolic convolution sequence input values length weights weights input sequence fed linear array computation tiles 
intermediate results shown corresponding tiles 
value represents intermediate value products left 
element computed equation 
sends tile time step receives resident tile computes intermediate value resident 
tile computes intermediate value available tile tile time step receives time step exits computation array 
tile left 
weight computes sends intermediate value time step values computes partitioning equation reduce convolution weight sequence length systolic convolutions match network size linear array computation tiles 
addition employ memory tile left array buffer sequences memory tile right array store intermediate values computation compute sum subproblems 
illustrates computation convolution linear processor array 
decoupled systolic convolution requires computation tiles memory tiles 
observe convolution decoupling efficient 
note organization requires networks computation tiles 
sacrificing asymptotically insignificant amount efficiency preload weights order reduce number needed networks supported decoupled systolic architecture 
efficiency analysis number multiply add operations convolution sequence weight sequence length linear network size computation tiles length memory tiles partition computation subproblems convolution sequence hoffmann agarwal stream convolution input sequence length weights linear array computation tiles memory tiles 
value executed times inner summation executed times 
note memory tile right performs addition accumulate results partial convolutions 
represents computation outer summation equation length weight sequence length subproblems overlap perfectly obvious 
account time steps convolution follows 
systolic convolutions linear array computation tiles pipeline perfectly 
systolic convolutions requires time steps stream sequence length array size processor executes multiply add operation time step 
due perfect overlap subsequent systolic convolutions time steps needed drain pipeline incurred 
number time steps linear network size consisting computation tiles memory tiles floating point efficiency convolution stream algorithms architecture assumption efficiency stream structured convolution approaches optimal value large values obtain efficiency equivalently stream convolution reduces systolic convolution computational efficiency note efficiency systolic convolution upper bound stream convolution upper bound provided sufficiently large 

related compare stream algorithms architecture prominent contributions arena systolic computing 
primary novelty rigorous simple definition stream algorithms asymptotically optimal mapping decoupled systolic algorithm decoupled systolic architecture dsa 
programmable tiled microarchitecture dsa numerous concepts invented earlier systolic architectures 
fact dsa particularly novel architecture 
embodies minimal selection architectural features motivated analysis stream algorithms allows execute stream algorithms flexible area efficient computationally efficient energy efficient manner 
addition dsa viewed machine emulating stream computations existing tiled microarchitectures 
roots dsa lie earliest systolic arrays inspired design philosophy expounded kung leiserson 
arrays lacked flexibility special purpose circuits particular application particular problem size 
specialized designs date ultra high performance digital signal processing example 
programmable systolic systems wavefront array processor psc warp saxpy matrix offered flexibility including independence problem size realize potential stream algorithms 
example kung advocated systolic communication means achieving high efficiency avoiding local memory accesses reports systolic algorithms show implementations local memory stream algorithmic versions 
simulation emulation multiple processing elements larger powerful processor parallel programming methodology choice allow problem size independence network embedding albeit cost potentially unbounded local memory requirements 
consequence local memory accesses simulated algorithms slower stream algorithms load store architectures typically factor consume significantly energy due memory accesses occupy larger silicon area dsa 
primary novelty stream algorithms lies efficiency definitions serve guiding methodology allowing qualify stream algorithms larger class decoupled systolic algorithms achieve efficiency asymptotically 
particular stream algorithms achieve higher efficiencies purely systolic designs 
case point consider computational efficiency systolic matrix multiplication special purpose systolic array matches problem size 
due startup drain phases staggered dataflow see hoffmann agarwal computational efficiency systolic matrix multiplication just equation accounts streaming result matrix array 
similarly systolic algorithms proposed past suffer bubbles data streams favor simple processor design 
definition stream algorithms prescribes large problem size compared network size decoupled systolic architecture 
due requirement stream matrix multiplication achieves computational efficiency 
course special purpose array achieve computational efficiency scenario multiple independent matrix multiplications overlapped interleaved 
complex applications phases different systolic algorithms may offer luxury identical independent subproblems 
example matrix multiplication systolic communication style proposed kung stores matrix operands local memory streams matrix processing elements 
programming style falls short exploiting full potential systolic communication 
second case point consider computational efficiency simulation uses larger processing element simulate multiple systolic processors uses local memory associated computational state 
best case values fit local memory remain subject loads stores local memory 
example simple single issue processing element requires loads store instruction multiply add operation depending quality register allocation 
consequence simulated version times slower stream algorithm local memory decouples memory accesses computation 
course complex processing element vliw architecture mask local memory accesses 
solution provide area energy efficiency dsa achieves high performance simple processor core local memories 
emulate dsa programmable tiled microarchitectures various degrees efficiency 
consequence design philosophy developed stream algorithms carries tiled architectures programmable systolic array processors 
example emulated dsa raw implementing stream algorithms raw assembly 
power supply raw chip memories software controlled stream algorithms energy efficient raw powering chip data memories 
raw area efficient canonical dsa offers potential operated higher clock frequencies absence local memory leads smaller tiles shorter wires 
design methodology stream algorithms includes application specific partitioning step 
partitioning new 
example context systolic arrays navarro discuss classify partitioning schemes spatial mappings temporal mappings 
notion spatial mapping coincides simulating multiple systolic processing elements powerful 
temporal mappings proposed closer stream algorithms partition systolic algorithms heterogeneous phases methodology differs centered idea transforming dense matrix operations band matrix operations 
temporal mappings introduced computationally efficient decoupling efficient sense defined section 
lack attention partition applications decoupling efficiency mind prevents systolic algorithms enjoying energy efficiency advantage stream algorithms 
noteworthy blocked matrix computations heart saxpy matrix programming philosophy type simulation spatial mapping decoupling efficient 

results stream algorithms architecture section summarize results related design stream algorithms practical results stream algorithms architecture 
experimental results emulating dsa existing tiled architecture demonstrating benefits stream algorithms design discipline 
second argue reasonable network size single chip dsa summary stream algorithms table summarizes stream algorithms developed far 
stream algorithms table lists number computation tiles memory tiles function network size execution time computational efficiency functions problem size note efficiency approaches definition algorithms qualify stream algorithms 
application triangular solver lu factorization cholesky fact 
qr factorization svd convolution dft vandermonde table summary stream algorithms 
show number computation tiles number memory tiles compare execution time computational efficiency problem size see details 
emulation results demonstrated performance benefits stream algorithms cycle accurate simulator raw architecture network size extended peripheral memory tiles local data memories 
table compares computational efficiency raw predicted efficiency decoupled systolic architecture table 
computational efficiencies raw fall slightly short predicted dsa smaller problem sizes number architectural reasons discussed 
large problem hoffmann agarwal sizes raw approaches efficiency dsa 
due small network size computational efficiency dominated factor equation 
application problem size tri 
solver lu fact 
qr fact 
convolution table computational efficiency stream algorithms 
compare efficiency raw microprocessor network size predicted efficiency decoupled systolic architecture 
case convolution report results chip size order implement single chip dsa choose particular network size section seen allows design bit architecture limited pin constraints today packaging technology 
section provide additional arguments favor argument analysis stream algorithms 
recall equation may express efficiency stream algorithm product terms equation term depends network size 
equation introduced algorithm specific parameter interpret number peripheral sides computation array data streams cross way memory tiles 
stream algorithms listed table find average number sides maximum value corresponds sides dimensional mesh computation tiles 
declare golden network size obtained term assumes mark words term approaches value reasonably large problem sizes stream algorithms achieve computational efficiency dsa network size contrast network size maximum computational efficiency stream algorithms fact efficiency level satisfactory right allowing argue constitutes network size single chip serve building block larger systems 
fat tree chips network size behave dsa exceeding golden network size golden network size resembles lower bound argument network size second argument provides upper bound 
interested expanding decoupled systolic stream algorithms architecture architecture serve general purpose multiprocessor may want run multiple applications concurrently 
worst case applications strictly sequential data intensive executed peripheral computation tiles memory tiles 
raises question largest sensible network size utilize inner computation tiles desire respectable computational efficiency 
back envelope estimate attempts answer question 
define variables network size array computation tiles number peripheral computation tiles number idling computation tiles maximum efficiency peripheral tiles perform fully utilized tiles idle 
inner illustrates array computation tiles peripheral computation tiles shaded gray 
working computation tiles 
remaining computation tiles shall idle 
number maximum effi ciency achievable set applications respect number computation tiles 
table right shows maximum efficiency function network size inversely proportional efficiency drops quickly increases 
obtain efficiency close addition able harvest peak performance entire chip 
losing factor certainly acceptable general scheduling problems considered near optimal reaching factor optimum 
consequently consider network size reasonable upper bound size single chip dsa 
decoupled systolic architecture peripheral computation tiles shaded gray 
table right shows maximum sustainable efficiency function utilizing peripheral tiles 
practice network size dsa chip fit area provided existing vlsi process 
estimates raw chip indicate dsa chip hoffmann agarwal built process 
process permit driving chip clock frequencies ghz dsa sustain performance nearly gflops stream algorithms 
level performance quite remarkable 
able shrink japan earth simulator occupies entire building power plant chips ibm blue gene supercomputer chips 
addition dsa enable unconventional high performance applications including type software radio hdtv execute real time 
emphasize reasonable size computation array peripheral memory architectural building block mega tile larger decoupled systolic architecture 
permits build chips area capacity accommodate array consist multiple arrays peripheral memory 
perfectly reasonable network size exceeds golden network size stream algorithms efficient increase network size 
studied feasibility highly efficient computation tiled microarchitectures 
result propose design stream algorithms architecture permits computational efficiency asymptotically large networks appropriate problem size 
consequence decoupled systolic architecture interprets stream algorithms energy efficient manner design 
transformed number regular applications stream algorithms 
examples sections summarized stream algorithms developed far table section 
design methodology stream algorithms led discover new ground 
instance best knowledge stream qr stream svd appear efficient parallel organizations algorithms existence date 
achieve highest efficiency levels developed application specific partitioning decoupling transformations comparable complexity needed design systolic arrays 
believe unreasonable expect conventional compiler technology generate competitive code advanced methods automatic parallelization targeting systolic arrays 
feel template code generation la fftw promising way compile stream algorithms 
pleasant side effect highly efficient computation predictability number execution cycles 
efficiency analysis stream algorithms allows predict computational performance case network problem size small approach optimal computational efficiency performance predictability turn enable reasonably accurate prediction power consumption 
performance indicator power consumption may employ circuit design decoupled systolic architecture supports varying clock frequencies trade power consumption computational performance 
programmable microprocessors contrast special purpose hardware allow differentiate quality service performance 
instance typical algorithms digital signal processing deliver higher quality audio video require larger number operations 
situation quality related computational performance power consumption clock frequency 
light physical relations emphasize desire schedule computations computational efficiency single chip microarchitectures 
stream algorithms architecture view decoupled systolic architecture extreme point design space tiled microarchitectures 
demonstrates programmable regular hardware structures utilized high performance computing energy efficient manner 
perspective decoupled systolic architecture serves measuring rod general purpose microarchitectures matches physical technological constraints single chip designs 
acknowledgments funded part raw project darpa nsf oxygen alliance mit lincoln laboratory lincoln scholars program 
express appreciation janice mcmahon bob bond support 
taylor kim miller greenwald hoffmann johnson 
lee lee ma frank amarasinghe agarwal raw microprocessor computational fabric software circuits general purpose programs ieee micro vol 
pp 
march april 
nagarajan burger design space evaluation grid processor architectures th annual international symposium microarchitecture pp 
dec 
hampton casper vector thread architecture st international symposium computer architecture nchen germany pp 
june 
swanson th international symposium microarchitecture san diego ca pp 
ieee computer society dec 
oliver rao crandall jones iv franklin chong multiple clock domain power aware tile embedded processor st international symposium computer architecture nchen germany pp 
june 
energy efficient processor system design 
phd thesis department electrical engineering computer science university california berkeley 
gonzalez horowitz energy dissipation general purpose microprocessors ieee journal solid state circuits vol 
pp 
sept 
kim taylor miller energy characterization tiled architecture processor chip networks acm symposium low power electronics design seoul korea pp 
aug 
smith decoupled access execute computer architectures acm transactions computer systems vol 
pp 
nov 
hoffmann agarwal kung leiserson algorithms vlsi processor arrays vlsi systems mead conway eds ch 
pp 
addison wesley 
navarro partitioning essential step mapping algorithms systolic array processors ieee computer vol 
pp 
july 
kung warp experience map computations parallel computer efficiently nd international conference supercomputing pp 
acm press 
leighton parallel algorithms architectures arrays trees hypercubes 
morgan kaufmann 
hoffmann agarwal stream algorithms architecture technical memo mit lcs tm laboratory computer science massachusetts institute technology mar 
hoffmann agarwal stream algorithm svd technical memo mit lcs tm computer science artificial intelligence laboratory massachusetts institute technology oct 
ho mai horowitz wires proceedings ieee vol 
pp 
apr 
micheli synthesis optimization digital circuits 
mcgraw hill 
wolfe high performance compilers parallel computing 
addison wesley 
rau fisher instruction level parallel processing history overview perspective journal supercomputing vol 
pp 
may 
kung systolic architectures ieee computer vol 
pp 
jan 
fortes wah systolic arrays concept implementation guest editors ieee computer vol 
pp 
july 
gross kung lam webb warp architecture implementation th annual symposium computer architecture pp 

henry joerg tightly coupled processor network interface th international conference architectural support programming languages operating systems pp 
acm press 
hillis connection machine 
cambridge ma mit press 
dally carter chang gurevich lee machine multicomputer th international symposium microarchitecture anne arbor mi pp 
ieee computer society dec 
leiserson fat trees universal networks hardware efficient supercomputing ieee transactions computers vol 
pp 
oct 
krishnamurthy collision model randomized routing fat tree networks technical memo mit lcs tm laboratory computer science massachusetts institute technology july 
stream algorithms architecture palacharla kessler evaluating stream buffers secondary cache replacement st international symposium computer architecture pp 
ieee computer society press 
zucker lee flynn hardware software cache prefetching techniques mpeg benchmarks ieee transactions circuits systems video technology vol 
pp 
aug 
thompson area time complexity vlsi th annual acm symposium theory computing pp 

kumar gupta analysis scalability parallel algorithms architectures survey th international conference supercomputing pp 
acm press 

kung gal arun wavefront array processor architecture language applications conference advanced research vlsi pp 
jan 
schreiber saxpy matrix general purpose systolic computer ieee computer vol 
pp 
july 
fisher kung architecture psc programmable systolic chip th international symposium computer architecture pp 
ieee computer society press 
gross hallaron anatomy parallel computing system 
cambridge ma mit press 
kung systolic communication international conference systolic arrays san diego ca pp 
may 
gross hinrichs hallaron stricker hasegawa communication styles parallel systems ieee computer vol 
pp 
dec 
koch leighton maggs rao rosenberg schwabe preserving fixed connection networks journal acm vol 
pp 
jan 
hoffmann stream algorithms architecture master thesis massachusetts institute technology cambridge ma june 
karp miller winograd organization computations uniform recurrence equations journal acm vol 
pp 
july 
rao kailath regular iterative algorithms implementation processor arrays proceedings ieee vol 
pp 
mar 
shang fortes time optimal linear schedules algorithms uniform dependencies ieee transactions computers vol 
pp 
june 
johnson fftw adaptive software architecture fft ieee international conference acoustics speech signal processing vol 
pp 


