greedy layer wise training deep networks bengio pascal dan hugo universit de montr montr qu bec iro umontreal ca complexity theory circuits strongly suggests deep architectures efficient exponentially shallow architectures terms computational elements required represent functions 
deep multi layer neural networks levels non linearities allowing compactly represent highly non linear highly varying functions 
clear train deep networks gradient optimization starting random initialization appears get stuck poor solutions 
hinton introduced greedy layer wise unsupervised learning algorithm deep belief networks dbn generative model layers hidden causal variables 
context optimization problem study algorithm empirically explore variants better understand success extend cases inputs continuous structure input distribution revealing variable predicted supervised task 
experiments confirm hypothesis greedy layer wise unsupervised training strategy helps optimization initializing weights region near local minimum giving rise internal distributed representations high level abstractions input bringing better generalization 
analyses bengio le roux bengio le cun modern nonparametric machine learning algorithms kernel machines support vector machines svms graph manifold semi supervised learning algorithms suggest fundamental limitations learning algorithms 
problem clear kernel approaches kernel local gaussian kernel converges constant increases 
analyses point difficulty learning highly varying functions functions large number variations domain interest require large number pieces represented piecewise linear approximation 
number pieces grow exponentially number factors variations input connected known curse dimensionality classical non parametric learning algorithms regression classification density estimation 
shapes pieces unrelated needs examples piece order generalize properly 
shapes related predicted non local learning algorithms potential generalize pieces covered training set 
ability necessary learning complex domains artificial intelligence tasks related vision language speech robotics 
kernel machines local kernel shallow architecture levels data dependent computational elements 
true feedforward neural networks single hidden layer svms number hidden units large bengio le roux vincent marcotte 
serious problem shallow architectures inefficient terms number computational units bases hidden units terms required examples bengio le cun 
way represent highly varying function compactly parameters composition non linearities deep architecture 
example parity function inputs requires examples parameters represented gaussian svm bengio parameters hidden layer neural network parameters units multi layer network log layers parameters recurrent neural network 
generally boolean functions function computes multiplication numbers bit representation expressible log layers combinatorial logic elements layer may require elements expressed layers utgoff bengio le cun 
representation concept requires exponential number elements shallow circuit number training examples required learn concept may impractical 
formal analyses computational complexity shallow circuits hastad allender 
point direction shallow circuits expressive deep ones 
believed difficult train deep multi layer neural networks 
empirically deep networks generally better worse neural networks hidden layers tesauro 
negative result reported machine learning literature 
reasonable explanation gradient optimization starting random initialization may get stuck near poor solutions 
approach explored success past constructively adding layers 
previously done supervised criterion stage fahlman lebiere 
hinton osindero teh introduced greedy layer wise unsupervised learning algorithm deep belief networks dbn generative model layers hidden causal variables 
training strategy networks may hold great promise principle help address problem training deep networks 
upper layers dbn supposed represent concepts explain input observation lower layers extract low level features learn simpler concepts build learn concepts 
strategy studied detail exploited machine learning 
hypothesize aspects strategy particularly important pre training layer time greedy way second unsupervised learning layer order preserve information input fine tuning network respect ultimate criterion interest 
extend dbns component layers restricted boltzmann machines rbm naturally handle continuous values input 
second perform experiments better understand advantage brought greedy layer wise unsupervised learning 
basic question answer approach helps solve difficult optimization problem 
dbns building blocks applying strategy auto encoders yielded similar results 
discuss problem occurs layer wise greedy unsupervised procedure input distribution revealing conditional distribution target variable input variable 
evaluate simple successful solution problem 
deep belief nets input hidden variables layer joint distribution 
conditional layers factorized conditional distributions computation probability sampling easy 
hinton 
considers hidden layer binary random vector elements biases unit layer weight matrix layer denote generative model layer follows 
restricted boltzmann machines top level prior restricted boltzmann machine rbm layer layer 
notation consider generic rbm input layer activations visible units hidden layer activations hidden units 
joint distribution eh normalization constant distribution vector biases visible units vector biases hidden units weight matrix layer 
minus argument exponential called energy function energy 
denote rbm parameters 
denote layer layer conditional distributions associated rbm joint distribution 
layer layer conditionals associated rbm factorize give rise vk bk hj cj 
gibbs markov chain log likelihood gradient rbm obtain estimator gradient log likelihood rbm consider gibbs markov chain visible units hidden units pair variables 
gibbs sampling rbm proceeds sampling denote vt th sample chain starting input observation rbm 
vk hk sample joint 
log likelihood value model rbm log log log energy log energy gradient respect log energy vk hk vk hk energy vk hk 
unbiased sample energy energy vk hk vk sample vk hk sample markov chain expectation easily computed hk vk factorizing 
idea contrastive divergence algorithm hinton take small typically 
pseudo code contrastive divergence training rbm binomial input hidden units appendix algorithm 
procedure called repeatedly sampled training distribution rbm 
decide may proxy training criterion reconstruction error log 
greedy layer wise training dbn greedy layer wise training algorithm proposed hinton train dbn layer time 
trains rbm takes empirical data input models 
denote posterior associated trained rbm recall observed input 
gives rise empirical distribution layer sampled data empirical distribution 
note level dbn rbm 
basic idea greedy layer wise strategy training top level rbm level dbn changes interpretation rbm parameters insert level dbn distribution rbm associated layers kept part dbn generative model 
rbm layers defined terms parameters rbm dbn defined terms parameters upper layers 
consequently rbm correspond dbn rbm top layer dbn 
rbm approximation posterior dbn 
samples empirical distribution converted stochastically samples distribution 
represented explicitly easy sample pick training example propagate stochastically gi gi level 
nice side benefit obtains approximation posterior hidden variables dbn levels input mean field propagation see gives fast deterministic approximation posteriors 
note consider layers dbn level top smaller dbn generates marginal distribution complete dbn 
motivation greedy procedure partial dbn levels starting level may provide better model rbm initially associated level 
greedy procedure justified variational bound hinton 
consequence bound inserting additional layer initialized appropriately units guarantee initial improvements training criterion layer fitting yield improvement training criterion previous layer likelihood respect 
greedy layer wise training algorithm dbns quite simple illustrated pseudo code algorithm appendix 
supervised fine tuning training stage possible fine tune parameters layers 
example hinton 
propose wake sleep algorithm hinton dayan frey neal continue unsupervised training 
hinton 
propose optionally mean field approximation posteriors gi replacing samples level bit wise mean field expected value bi 
propagation rules network deterministically computes internal representations functions network input unsupervised pre training layers dbn algorithm see appendix network optimized gradient descent respect deterministically computable training criterion depends representations 
example hinton fine tune deep auto encoder minimizing reconstruction error 
possible initialization layer traditional multi layer neural network gradient descent fine tune network respect supervised training criterion 
algorithm appendix contains pseudo code supervised fine tuning part global supervised learning algorithm 
note better results obtained fold larger learning rate supervised criterion squared error cross entropy updates contrastive divergence updates 
extension continuous valued inputs binary units introduced dbns hinton 
cheat handle continuous valued inputs scaling interval considering input continuous value probability binary random variable take value 
worked pixel gray levels may inappropriate kinds input variables 
previous continuous valued input include chen murray noise added sigmoidal units rbm forms special form diffusion network movellan williams 
concentrate simple extensions rbm framework energy function allowed range values changed 
linear energy exponential truncated exponential consider unit value rbm connected units layer 
obtained terms exponential contain grouped ya linear energy functions bias unit vector weights connecting unit units allow take value interval conditional density exp ya exp va exponential density dv parameter normalizing integral equals exists computing density computing expected value sampling easy 
alternatively closed interval applications interest unit hidden unit non linear expected value density truncated exponential 
simplicity consider case normalizing integral conditional expectation interesting exists exp exp sigmoidal saturating monotone non linearity sampling truncated exponential easily obtained uniform sample inverse cumulative conditional density log exp truncated truncated cases contrastive divergence updates form binomial units input value times output value updates depend derivative energy respect parameters 
sampling changed unit conditional density 
quadratic energy gaussian units obtain gaussian distributed units adds quadratic terms energy 
adding gives rise diagonal covariance matrix units layer yi continuous value gaussian unit positive parameter equal inverse variance yi 
classification error training set deep network pre training dbn partially supervised pre training dbn unsupervised pre training training classification error vs training iteration cotton price task deep network pre training dbn unsupervised pre training dbn partially supervised pre training 
illustrates optimization difficulty deep networks advantage partially supervised training 
abalone cotton train 
valid 
test 
train 
valid 
test 

deep network pre training 
logistic regression 
dbn binomial inputs unsupervised 
dbn binomial inputs partially supervised 
dbn gaussian inputs unsupervised 
dbn gaussian inputs partially supervised table mean squared prediction error abalone task classification error cotton task showing improvement gaussian units 
case variance unconditional mean depends inputs unit unit inputs inverse variance contrastive divergence updates easily obtained computing derivative energy respect parameters 
parameters linear terms energy function derivatives form input unit value times output unit value case binomial units 
quadratic parameter derivative simply dy gaussian units previously hidden units rbm binomial multinomial inputs applied information retrieval task welling rosen zvi hinton 
interest continuous valued inputs 
continuous valued hidden units introduced rbm units continuous values better deal representation input variables considered hidden layers replacement complementing binomial units past 
gaussian exponential hidden units weakness mean field propagation gaussian unit gives rise purely linear transformation 
linear hidden units multi layered network mean field propagation function maps inputs internal representations completely linear 
addition dbn containing gaussian units able model gaussian data 
hand combining gaussian types units interesting 
contrast gaussian exponential units conditional expectation truncated exponential units non linear fact involves sigmoidal form non linearity applied weighted sum inputs 
experiment experiment performed data sets uci repository abalone data set split training examples validation examples test examples financial data set 
real valued input variables representing averages returns squared returns binomial approximation inappropriate 
target variable month return cotton futures contract 
continuous input variables averages returns different time windows days 
training examples validation examples test examples 
dataset publicly available www iro umontreal ca lisa fin data 
table rows show improvements brought dbns gaussian inputs dbns binomial inputs binomial hidden units cases 
networks hidden layers 
hyper parameters selected validation set performance 
understanding layer wise strategy works reasonable explanation apparent success layer wise training strategy dbns unsupervised pre training helps mitigate difficult optimization problem deep networks better initializing weights layers 
experiments support clarify 
training layer auto encoder want verify layer wise greedy unsupervised pre training principle applied auto encoder rbm layer building block 
input vector xi 
layer weights matrix hidden biases column vector input biases column vector reconstruction probability bit pi vector probabilities 
training criterion layer average negative log likelihoods predicting 
example interpreted sequence bits sequence bit probabilities minimize reconstruction cross entropy xi log pi xi log pi 
report experimental results training criterion layer comparison contrastive divergence algorithm rbm 
pseudo code deep network obtained training layer auto encoder appendix algorithm 
question arises auto encoders comparison auto encoders fail learn useful representation number units strictly decreasing layer networks theoretically just learn identity perfectly minimize reconstruction error 
experiments suggest networks non decreasing layer sizes generalize 
due weight decay stochastic gradient descent preventing large weights optimization falls local minimum corresponds transformation input provides initialization supervised training net 
greedy layer wise supervised training reasonable question ask fact layer trained unsupervised way critical 
alternative algorithm supervised greedy layer wise train new hidden layer hidden layer hidden layer supervised neural network nn input output previously trained layers throw away output layer nn parameters hidden layer nn pre training initialization new top layer deep net map output previous layers hopefully better representation 
pseudo code deep network obtained training layer hidden layer supervised hidden layer neural network appendix algorithm 
experiment 
compared performance mnist digit classification task obtained algorithms dbn deep network layers initialized auto encoders described supervised greedy layer wise algorithm pre train layer deep network pre training random initialization shallow network hidden layer pre training 
final fine tuning done adding logistic regression layer top network training network stochastic gradient descent cross entropy respect target classification 
networks architecture inputs outputs hidden layers variable number hidden units selected validation set performance typically selected layer sizes 
shallow network single hidden layer 
weight decay hyper parameter optimized 
dbn slower train experiments performed longer training appropriately chosen sizes layers learning rates yield better results hinton unpublished reports error mnist test set 
experiment experiment train 
valid 
test train 
valid 
test dbn unsupervised pre training deep net auto associator pre training deep net supervised pre training deep net pre training shallow net pre training table classification error mnist training validation test sets best hyperparameters validation error pre training purely supervised purely unsupervised pre training 
experiment size top hidden layer set 
mnist differences statistically significant 
results table suggest auto encoding criterion yield performance comparable dbn layers tuned supervised fashion 
clearly show greedy unsupervised layer wise pre training gives better results standard way train deep network greedy pre training shallow network pre training deep networks tend perform worse shallow networks 
results suggest unsupervised greedy layer wise pre training perform significantly better purely supervised greedy layer wise pre training 
possible explanation greedy supervised procedure greedy learned hidden units representation may discard information target information captured easily hidden layer neural network captured composing hidden layers 
experiment troubling experiment results table networks greedy layer wise pre training perform perfectly training set appear contradict hypothesis main effect layer wise greedy strategy help optimization poor optimization expect poor training error 
possible explanation coherent initial hypothesis results captured hypothesis 
pre training lower layers initialized poorly allowing top layers learn training set perfectly output layer hidden layer form standard shallow fat neural network 
consider top layers deep network pre training presumably takes input better representation allows better generalization 
network pre training sees random transformation input preserves information input fit training set help generalize 
test hypothesis performed second series experiments constrain top hidden layer small hidden units 
experiment results table clearly confirm hypothesis 
pre training training error degrades significantly hidden units top hidden layer 
addition results obtained pre training extremely large variance indicating high sensitivity initial conditions 
results tables consistent hypothesis greedy layer wise procedure essentially helps better optimize deep networks probably initializing hidden layers represent meaningful representations input yields better generalization 
continuous training layers dbn layer wise training algorithm dbns appendix element dispense having decide number training iterations layer 
explicitly add layers time train layers simultaneously keeping greedy idea layer pre trained model input ignoring effect higher layers 
achieve sufficient insert line called layers stochastic hidden values propagated way 
experiments variant demonstrated works original algorithm 
advantage single stopping criterion network 
computation time slightly greater computations initially upper layers wasted lower layers converge decent representation time saved optimizing hyper parameters 
variant may appealing line training large data sets cycle back training data 
dealing uncooperative input distributions classification problems mnist classes separated structure input distribution naturally contains information target variable imagine supervised learning task input distribution unrelated regression problems interested studying problem prevalent 
example imagine task target noise gaussian particular relation settings expect unsupervised greedy layer wise pre training procedure help training deep supervised networks 
deal uncooperative input distributions propose train layer mixed training criterion combines unsupervised objective modeling reconstructing input supervised objective helping predict target 
simple algorithm adds updates hidden layer weights unsupervised algorithm contrastive divergence reconstruction error gradient updates gradient supervised prediction error temporary output layer greedy layer wise supervised training algorithm 
experiments appeared sufficient perform partial supervision layer predictive information target forced representation layer tends stay upper layers 
results table clearly show advantage partially supervised greedy training algorithm case financial dataset 
pseudo code partially supervising layer algorithm appendix 
motivated need develop training algorithms deep architectures representationally efficient shallow ones svms neural nets 
study deep belief networks applied supervised learning tasks principles explain performance yielded 
principal contributions 
extended dbns new ways naturally handle continuous valued inputs showing examples better predictive models obtained 
second performed experiments support hypothesis greedy unsupervised layer wise training strategy helps optimize deep networks suggest better generalization obtained strategy initializes upper layers better representations relevant highlevel abstractions 
experiments suggest general principle applied dbns obtained similar results layer initialized auto associator rbm 
important unsupervised component train layer fully supervised greedy layer wise strategy performed worse studied supervised tasks structure input distribution revealing conditional density case dbn unsupervised greedy layer wise strategy appears inadequate proposed simple fix partial supervision yield significant improvements 
allender 

circuit complexity dawn new millennium 
th annual conference foundations software technology theoretical computer science pp 

lecture notes computer science 
bengio le roux 

curse highly variable functions local kernel machines 
weiss sch lkopf platt 
eds advances neural information processing systems pp 

mit press cambridge ma 
bengio le cun 

scaling learning algorithms ai 
bottou chapelle decoste weston 
eds large scale kernel machines 
mit press 
bengio le roux vincent marcotte 

convex neural networks 
weiss sch lkopf platt 
eds advances neural information processing systems pp 

mit press cambridge ma 
chen murray 

continuous restricted boltzmann machine implementable training algorithm 
iee proceedings vision image signal processing 
fahlman lebiere 

cascade correlation learning architecture 
touretzky 
ed advances neural information processing systems pp 
denver morgan kaufmann san mateo 
hastad 

computational limitations small depth circuits 
mit press cambridge ma 
hinton osindero teh 

fast learning algorithm deep belief nets 
neural computation 
hinton 

training products experts minimizing contrastive divergence 
neural computation 
hinton dayan frey neal 

wake sleep algorithm unsupervised neural networks 
science 
hinton 

reducing dimensionality data neural networks 
science 


training mlps layer layer objective function internal representations 
neural networks 
movellan williams 

monte carlo em approach partially observable diffusion processes theory applications neural networks 
neural computation 
tesauro 

practical issues temporal difference learning 
machine learning 
utgoff 

layered learning 
neural computation 
welling rosen zvi hinton 

exponential family application information retrieval 
advances neural information processing systems vol 
cambridge ma 
mit press 
