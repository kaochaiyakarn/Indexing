support vector machines multiple instance learning stuart andrews ioannis tsochantaridis thomas hofmann department computer science brown university providence ri stu th cs brown edu presents new formulations multiple instance learning maximum margin problem 
proposed extensions support vector machine svm learning approach lead mixed integer quadratic programs solved heuristically 
generalization svms state art classification technique including non linear classification kernels available area largely dominated special purpose methods 
experimental results pharmaceutical data set applications automated image indexing document categorization 
multiple instance learning mil generalization supervised classification training class labels associated sets patterns bags individual patterns 
pattern may possess associated true label assumed pattern labels indirectly accessible labels attached bags 
law inheritance set receives particular label patterns set possesses label 
important case binary classification implies bag positive member patterns positive example 
mil differs general set learning problem set level classifier design induced pattern level classifier 
key challenge mil cope ambiguity knowing patterns positive bag actual positive examples ones 
mil setting numerous interesting applications 
prominent application classification molecules context drug design 
molecule represented bag possible conformations 
efficacy molecule tested experimentally way control individual conformations 
second application image indexing content image retrieval 
image viewed bag local image patches image regions 
annotating images far time consuming marking relevant image regions ability deal type weakly annotated data desirable 
consider problem text categorization apply mil setting 
usually documents contain relevant passage considered relevant respect particular cate gory topic class labels rarely available passage level commonly associated document 
formally applications share type label ambiguity opinion strong argument favor relevance mil setting 
approaches modify extend support vector machines svms deal mil problems 
approach explicitly treats pattern labels unobserved integer variables subjected constraints defined positive bag labels 
goal maximize usual pattern margin soft margin jointly hidden label variables linear kernelized discriminant function 
second approach generalizes notion margin bags aims maximizing bag margin directly 
appropriate cases mainly care classifying new test bags approach preferable goal derive accurate pattern level classifier 
case singleton bags methods identical reduce standard soft margin svm formulation 
algorithms mil problem 
methods related analytical results hypothesis classes consisting axis aligned rectangles 
similarly methods developed subsequently focused specially tailored machine learning algorithms compare favorably limiting case standard classification setting :10.1.1.16.5316
notable exception 
kernel approach suggested derives mi kernels bags kernel defined pattern level 
mi kernel approach treats mil problem merely representational problem strongly believe deeper conceptual modification svms outlined necessary 
share ultimate goal state ofthe art kernel classification methods available multiple instance learning 
multiple instance learning statistical pattern recognition usually assumed training set labeled patterns available pair xi yi generated independently unknown distribution 
goal induce classifier function patterns labels focus binary case 
multiple instance learning mil generalizes problem making significantly weaker assumptions labeling information 
patterns grouped bags label attached bag pattern 
formally set input patterns xn grouped bags bm xi index sets typically non overlapping 
bag bi associated label yi 
labels interpreted way yi yi pattern bag positive example 
hand yi pattern xi bi positive example underlying concept 
notice information provided label asymmetric sense negative bag label induces unique label pattern bag positive label 
general relation pattern labels yi bag labels yi expressed compactly yi maxi yi alternatively set linear constraints yi yi yi yi 
call discriminant function mi separating respect multiple instance data set sgn maxi xi yi bags bi holds 
large margin classifiers mil 
negative patterns denoted symbols positive bag patterns numbers encode bag membership 
left sketches mi svm solution right shows mi svm solution 
maximum pattern margin formulation mil omit svms refer reader excellent books topic 
mixed integer formulation mil generalized soft margin svm written follows primal form yi xi yi hold 
mi svm min yi min notice standard classification setting labels yi training patterns xi simply labels yi patterns xi belonging negative bag treated unknown integer variables 
mi svm maximizes soft margin criterion jointly possible label assignments hyperplanes 
illustrates idea separable case looking mi separating linear discriminant pattern positive bag positive halfspace patterns belonging negative bags negative halfspace 
time achieve maximal margin respect completed data set obtained labels patterns positive bags accordance eq 

similar approach pursued transductive inference 
case patterns labeled unlabeled 
unlabeled data points utilized refine decision boundary maximizing margin data points 
labeling unlabeled pattern carried independently transductive inference labels patterns positive bags coupled mil inequality constraints 
mi svm formulation leads mixed integer programming problem 
find optimal labeling optimal hyperplane 
conceptual level mixed integer formulation captures exactly mil recover unobserved pattern labels simultaneously find optimal discriminant 
poses computational challenge resulting mixed integer programming problem solved efficiently state art tools moderate size data sets 
optimization heuristic section 
maximum bag margin formulation mil alternative way applying maximum margin ideas mil setting extend notion margin individual patterns sets patterns 
natural define functional margin bag respect hyperplane yi max xi 
generalization reflects fact predictions bag labels take form yi xi 
notice positive bag margin defined margin positive pattern margin negative bag defined negative pattern 
difference formulations maximum margin problems illustrated 
pattern centered mi svm formulation margin pattern positive bag matters freedom set label variables maximize margin 
bag centered formulation pattern positive bag matters determine margin bag 
witness patterns identified relative position patterns positive bags respect classification boundary irrelevant 
notion bag margin define mil version soft margin classifier mi svm min yi max xi 
negative bags unfold max operation introducing inequality constraint pattern single slack variable constraints negative bag patterns yi read xi positive bags introduce selector variable denotes pattern selected positive witness bi 
result constraints xs arrive equivalent formulation min min yi xi yi xs 
formulation positive bag bi effectively represented single member pattern xi xs 
notice non witness patterns xi impact objective 
selector variables straightforward derive dual objective function similar standard svm wolfe dual 
major difference box constraints lagrange parameters modified compared standard svm solution gets yi yi 
influence bag bounded optimization heuristics shown formulations mi svm mi svm cast programs 
deriving optimization heuristics exploit fact initialize yi yi repeat compute svm solution data set imputed labels compute outputs fi xi xi positive bags set yi sgn fi yi positive bag bi yi compute arg maxi fi set yi imputed labels changed output pseudo code mi svm optimization heuristics synchronous update 
initialize xi xi positive bag bi repeat compute qp solution data set positive examples xi yi compute outputs fi xi xi positive bags set xi arg maxi fi yi selector variables changed output pseudo code mi svm optimization heuristics synchronous update 
integer variables hidden labels mi svm selector variables mi svm problem reduces qp solved exactly 
course derivations hold general kernel functions general scheme simple optimization heuristic may described follows 
alternate steps integer variables solve associated qp find optimal discriminant function ii discriminant update integer variables way locally minimizes objective 
step may involve update label variable yi single pattern mi svm update single selector variable mi svm simultaneous update integer variables 
integer variables essentially decoupled discriminant exception bag constraints mi svm done efficiently 
notice re initialize qp solver iteration previously solution usually result significant speed 
terms initialization optimization procedure suggest impute positive labels patterns positive bags initial configuration mi svm 
mi svm xi initialized centroid bag patterns 
summarize pseudo code descriptions algorithms utilized experiments 
possibilities refine heuristic strategy example starting different initial conditions branch bound techniques explore larger parts discrete part search space performing stochastic updates simulated annealing maintaining probabilities integer variables spirit deterministic annealing 
able achieve competitive results simpler optimization heuristics val dd mi nn iapr mi svm mi svm musk musk table accuracy results various methods musk data sets 
maximum margin formulation svm 
address algorithmic improvements 
experimental results performed experiments various data sets evaluate proposed techniques compare methods mil 
method implemented em diverse density em dd method competitive results reported musk benchmark musk data set musk data sets benchmark data sets virtually previous approaches described detail landmark 
data sets musk musk consist descriptions molecules multiple low energy conformations 
conformation represented dimensional feature vector derived surface properties 
musk contains average approximately conformation molecule musk average conformations bag 
averaged results fold cross validation runs summarized table 
svm results rbf kernel exp coarsely optimized 
musk musk data sets mi svm achieves competitive accuracy values 
mi svm outperforms mi svm musk significantly worse musk 
methods fail achieve performance best method iterative apr compare favorably approaches mil 
automatic image annotation generated new mil data sets image annotation task 
original data color images corel data set preprocessed segmented blobworld system 
representation image consists set segments blobs characterized color texture shape descriptors 
utilized different categories elephant fox tiger experiments 
case data sets positive negative example images 
randomly drawn pool photos animals 
due limited accuracy image segmentation relative small number region descriptors small training set size ends quite hard classification problem 
currently investigating alternative image description em dd indicate authors test data select optimal solution obtained multiple runs algorithm 
pseudo code formulation em dd di compute error th data fold fact dt di notation 
corrected version algorithm experiments obtained accuracy numbers em dd line previously published results 
iapr iterative axis parallel rectangle methods specifically designed optimized musk classification task superiority apr interpreted failure 
data set dims em dd mi svm mi svm category inst feat linear poly rbf linear poly rbf elephant fox tiger table classification accuracy different methods corel image data sets 
data set dims em dd mi svm mi svm category inst feat linear poly rbf linear poly rbf tst tst tst tst tst tst tst table classification accuracy different methods trec document categorization sets 
representations context applying mil content image retrieval automated image indexing hope achieve better absolute classification accuracies 
data sets legitimate comparative performance analysis 
results summarized table 
show mi svm mi svm achieve similar accuracy outperform em dd percent 
mi svm performed marginally better mi svm heuristic methods susceptible nearby local minima 
evidence effect observed experimentation updates described section varied number integer variables updated iteration 
text categorization generated mil data sets text categorization 
starting publicly available trec data set known ohsumed split documents passages overlapping windows maximal words 
original data set consists years selected medline articles 
worked data set training data trec filtering task consists approximately documents 
medline documents annotated mesh terms medical subject headings defining binary concept 
total number mesh terms trec 
currently performing larger scale evaluation mil techniques full data set report preliminary results smaller randomly subsampled data set 
categories pre test portion positive examples 
compared data sets representation extremely sparse high dimensional data interesting additional benchmark 
linear polynomial kernel functions generally known text categorization methods show improved performance em dd cases 
significant difference methods clearly evident text classification task 
novel approach multiple instance learning alternative generalizations maximum margin idea svm classification 
formulations lead hard mixed integer problems simple local optimization heuristics yield quite competitive results compared baseline approach 
conjecture better optimization techniques example avoid unfavorable local minima may improve classification accuracy 
ongoing extend experimental evaluation include larger scale problems 
far mil research problem concerned considered wider range data sets applications usually done able obtain results variety data sets 
strongly suspect mil methods optimized perform musk benchmark plan data sets experiments available public encourage empirical comparisons 
acknowledgments sponsored nsf itr award number iis 
auer 
learning multi instance examples empirical evaluation theoretical approach 
proc 
th international conf 
machine learning pages 
morgan kaufmann san francisco ca 
carson thomas belongie hellerstein malik 
blobworld system region image indexing retrieval 
proceedings third international conference visual information systems 
springer 
bennett 
optimization approaches semisupervised learning 
ferris mangasarian pang editors applications algorithms complementarity 
kluwer academic publishers boston 
dietterich lathrop lozano perez 
solving multiple instance problem axis parallel rectangles 
artificial intelligence 
rtner flach kowalczyk smola 
multi instance kernels 
proc 
th international conf 
machine learning 
morgan kaufmann san francisco ca 
joachims 
transductive inference text classification support vector machines 
proceedings th international conference machine learning pages 
morgan kaufmann san francisco ca 
long tan 
pac learning axis aligned rectangles respect product distributions multiple instance examples 
proc 
comp 
learning theory 
maron lozano rez 
framework multiple instance learning 
advances neural information processing systems volume 
mit press 
maron 
multiple instance learning natural scene classification 
proc 
th international conf 
machine learning pages 
morgan kaufmann san francisco ca 
ramon de raedt 
multi instance neural networks 
proceedings icml workshop attribute value relational learning 
sch lkopf smola 
learning kernels 
support vector machines regularization optimization 
mit press 
qi zhang sally goldman 
em dd improved multiple instance learning technique 
advances neural information processing systems volume 
mit press 
