accurate efficient replaying file system traces timothy wong erez zadok stony brook university appears proceedings fourth usenix conference file storage technologies fast replaying traces time honored method benchmarking stress testing debugging systems forensic analysis 
benefit replaying traces reproducibility exact set operations captured specific workload 
existing trace capture replay systems operate different levels network packets disk device drivers network file systems system calls 
system call replayers memory mapped operations replay workloads original speeds 
traces captured levels vital information available file system level 
designed implemented replayfs system replaying file system traces vfs level 
vfs appropriate level replaying file system traces operations reproduced manner relevant file system developers 
uniform vfs api traces replayed transparently existing file system different originally traced modifying existing file systems 
replayfs user level compiler prepares trace replayed efficiently kernel multiple kernel threads prefetch schedule replay file system operations precisely efficiently 
techniques allow replay intensive traces different speeds accelerate hardware trace captured originally 
trace replaying useful file system benchmarking stress testing debugging forensics 
reproducible way apply real life workloads file systems 
advantage traces ease distribution 
benchmarking synthetically generated workloads rarely represent real workloads 
compile benchmarks put little load file system scalable 
real workloads complicated artificially created ones traces real file system activity better test workloads 
represent actual file system workloads scaled needed 
addition scaling captured traces modified ways replayed 
example modification disk block locations help evaluate new disk layout policies 
trace replaying test target file system synthetic workload application reproducibility difficult 
common examples replaying tpc traces file systems running actual benchmark complicated requires database system 
replaying traces captured allows fair comparison file systems 
trace replaying accurate method prepare file system benchmarking aging 
synthetic benchmarks may predictable real life workloads exercise possible file system operation sequences 
replaying stress test file system practical conditions 
trace replaying allows selectively replaying portions trace 
useful narrow search problems debugging 
precise timing minimal influence system tested key requirements reproduce exact timing conditions 
replaying file system traces considered form fine grained versioning 
existing versioning file systems reproduce timing memory conditions related file system modifications 
trace replaying better choice forensic purposes 
replaying traces back forth time useful post mortem analysis attack 
file system traces captured replayed different logical levels system calls virtual file system vfs network level network file systems driver level 
easiest way collect replay file system traces recording system calls entirely user mode 
unfortunately method capture memory mapped file system operations 
significant problem decades ago nowadays applications perform large portion file system interactions memory mapped operations normal reads writes 
system call replayers non zero overheads allow replay high rates high performance applications spikes activity applications may issue requests low rates 
researchers captured file system activity vfs level linux windows nt 
replayed traces vfs level 
network tracers capture requests satisfied client side file system caches 
device driver tracers capture raw disk requests distinguish file system meta data events pathname related calls data related events 
fore network level driver level replaying comprehensive evaluation entire file system primarily suitable replaying level captured trace 
uses 
example network trace replaying suitable evaluation nfs servers driver level replayers useful evaluate physical disk layouts 
techniques lower overheads cpu time normally spent client side applications prepare events replaying network device driver replayers efficient able replay high rates accurately 
designed vfs level replayer call replayfs 
replays traces captured tracefs stackable file system 
traces preprocessed optimized user level trace compiler 
replayfs runs kernel directly directory file system mounted file systems 
replays requests form vfs api calls multiple kernel threads 
replayfs location kernel hierarchy allows combine performance benefits existing network driver level replayers ability replay memory mapped operations evaluate entire file systems 
memory mapped operations easily captured replayed vfs level part vfs api part system call api 
replayfs uses time normally spent context switching verifying user parameters copying prefetch schedule events 
addition replayfs saves time requests eliminating data copying kernel user buffers 
user mode tools replay highest possible rates spikes activity due overheads 
ironic exactly activity crucial replay accurately 
replayfs replay high rate traces spikes activity faster original programs generated exactly hardware 
example replayfs replay read operations times faster possible generate user level 
allows replayfs replay workload accurately original event rates 
rest organized follows 
section describes capturing replaying design 
section describes implementation 
evaluate system section 
describe related section 
conclude section discuss 
design main goal replayfs reproduce original file system workload accurately possible 
operations efficiently captured kernel part vfs api system call api 
logical replayfs replay traces level traces cap ext rename user process rename vfs rename virtual file system vfs tracefs rename lower file system traced tracefs tracefs trace ext ext user kernel tracefs stackable file system located lower file system vfs 
ext rename lower file system target replayfs ext replayfs trace ext kernel replayfs located directly lower file system 
stackable file system 
seen vfs lower file system 
tured 
shown tracefs stackable file system 
tracefs passes virtual file system vfs requests lower file system tree mounted file systems 
invoking lower operations returning tracefs logs input output values timing information associated requests 
replayfs logically located level tracefs right lower file system shown 
replayfs stackable file system file system 
reproduces behavior vfs trace capture time appears vfs lower file system 
replayfs operates similarly part vfs directly interacts lower file systems 
file system related requests interact os intricate ways concurrent threads locks synchronize access compete shared resources disks os may purge caches due file system activity reproduce original workload correctly necessary reproduce original timing requests side effects accurately 
simple file system event rates low 
high rates replayers overheads difficult replay traces accurately 
specifically request issuing process consists intervals user mode activity system time activity vfs tv lower file system event servicing time tfs 
call time necessary replayer prepare calling replayed event 
clearly timing rate reproduced correctly events issued close illustrated 
happen example trace generated original program legend system call replayer replayfs user time replayer overhead vfs time file system time tfs original time lower file system requests time consecutive file system events triggered original program system calls replayer replayfs 
example replayfs user mode replayer event overheads 
lower file system receives events time captured replayed replayfs tv system call replayer unable generate events time 
high performance application spike activity 
unfortunately situations frequent exacerbated typical replayers non negligible overheads 
existing system call replayers overheads ranging higher 
replayers overheads come need prefetch data manage threads invoke requests 
surprising existing user mode replayer replay maximum possible application rates reproduce peaks activity modes file system activity important benchmarking debugging 
increasing cpu speeds solves problem network file system trace replayers replayer tested file system run different hardware 
non network file system trace replayers run hardware 
hardware changes affect behavior lower file system replayer increase cpu speed decrease replayfs overheads tv tfs component 
may result different file system request interaction changing file system behavior 
replayfs replays traces directly lower file system 
operation overhead smaller tv smaller just illustrated bottom timeline 
replaying overheads replayfs overheads user mode replayer replayfs replay higher rates user mode replayer 
running kernel gives replayfs additional advantages described section resulting lower overheads 
allows replayfs replay high rate traces accurately system call replayer 
replayfs trace natural disparity raw traces traces 
trace captured tracer portable descriptive verbose offer information possible analysis 
trace needs specific system replayed terse possible minimize replaying overheads 
natural preprocess raw traces replaying shown 
preprocessing traces allows perform tasks user level adding complexity kernel components 
call user mode program conversion optimization tracefs raw traces trace compiler call resulting trace replayfs trace 
trace compiler uses existing tracefs library 
new trace parsers added preprocess traces captured different tools different levels different oss 
trace compiler splits raw tracefs trace components optimize run time replayfs operation 
component different typical access pattern size purpose 
kernel level tracefs user level kernel level trace compiler replayfs raw trace replayfs trace captured raw traces compiled replayfs traces replaying 
replayfs trace component called commands 
sequence vfs operations associated timestamp process id parameters expected return value return object pointer 
runtime commands sequentially scanned replayed time 
sequence commands sequentially prefetched demand runtime 
execution command actual return value compared return value captured original trace 
replaying terminated expected actual return values match 
second component called resource allocation table rat 
tracefs replayfs operate vfs objects locations memory known advance objects shared commands added level indirection refer commands parameters return values 
commands contain offsets rat associated vfs objects memory buffers 
replayfs populates rat entries run time trace compiler creates commands referencing rat entries trace compile time 
tracefs captures memory addresses vfs objects related captured operations 
vfs ob jects memory address trace capture share rat entry 
rat accessed randomly reading writing offsets program elements rat kept memory entire replaying process 
store integer parameters command stream 
allows decrease size rat avoid unnecessary pointer dereferencing 
purpose rat counting 
particular count regular vfs object may different replayfs count object 
example happens object nonzero count time replaying process started 
counts release vfs objects properly completion replaying 
third replayfs trace component memory buffers necessary replay trace 
include file names buffers written point time 
buffers usually accessed sequentially may accessed times replaying process 
usually largest component replayfs trace 
replaying memory buffers accessed reading information read disk discarded 
outline properties replayfs trace components table 
component access memory read write commands sequent 
demand read rat random read write buffers random demand read table replayfs trace components properties 
shows example replayfs trace fragment 
example dentry linux vfs directory entry object rat entry referenced output object lookup operation input parameter create operation 
foo bar file name example buffer 
replayfs trace generation trace compiler performs optimizations 
keep rat memory entire replaying process 
trace compiler reuses rat entries possible 
example trace compiler reuses memory buffer entry point time stores file pointer entry required point 
minimize amount prefetching memory buffers trace compiler scans compares 
memory buffers read buffers exactly contents may removed trace 
data prefetching commands buffers components replayfs trace loaded memory demand 
data read patterns known apply commands lookup create ms pid param return ms pid param expect expect return dentry string dentry count count count foo bar dentry dentry rat example replayfs trace 
commands resource allocation table rat index values 
rat points directly shared objects memory 
standard prefetching algorithms 
chosen fixed horizon algorithm works best replayfs trace fetched dedicated disk :10.1.1.118.6273
optimize prefetching low cpu usage 
theoretically shown fixed horizon similar algorithms optimal prefetching process bound 
assume dedicated disk prefetching replayfs trace prefetching process bound 
commands buffers replayfs trace components located separate disks decrease contention 
rat component memory interfere prefetching components 
additional advantage fixed horizon algorithm small memory consumption 
note information buffers require prefetching extracted commands 
prefetch commands stream earlier necessary keep replaying commands 
threads scheduling replayfs issues requests lower file system behalf different threads different threads generated requests original trace 
necessary accurately reproduce properly exercise lower file system case resource contention disk head repositioning locks semaphores replay timing properly lower operations block 
excessive number threads may hurt performance replayfs reuses threads possible 
particular trace compiler optimizes commands stream file system traces contain information thread creation termination times 
similar rat entries reuse trace compiler reuses processes 
program spawns thread termination spawns replayfs automatically thread replay operations invoked 
minimize scheduling related overheads replayfs create master thread manage threads 
example thread running traced data generated single process 
important note scheduling overheads replaying approximately trace capture time 
conditions necessary replay traces efficiently hardware trace capture 
standard event timers precision ms 
increase event replaying precision pre spin technique commonly event timers set ms actual event time 
thread spins loop constantly checking current time desired event time reached 
natural way lower cpu load pre spinning time productive activity 
call technique productive pre spin 
replayfs uses move thread run queue time actual event time operation replayed different thread 
thread woken immediately just put run queue 
way cpu cycles effectively spent moving process run queue spinning 
zero copying data main advantages kernel replayers user mode replayers ability avoid copying unnecessary data kernel user boundary 
data pages just read need copied separate user mode buffer 
data read trace replaying interest replaying tools 
desired checksums sufficient data verification purposes 
copying read byte data data page set page accessed bit 
easy way user mode program read data avoid copying user space 
direct allows programs avoid extra data copying usually processed differently file system level replaying inaccurate normal read write requests replayed direct requests 
avoiding data copying difficult write operations 
kernel mode replayers access low level file system primitives 
example data page belongs trace file simply moved target file just changing pointers 
writing data copying elimi nated 
elimination unnecessary data copying reduces cpu memory usage replayfs 
note replayers direct fetching data disk copy data twice copy kernel user space buffers load trace data copy data kernel issue write request 
file system caches file system page caches may different state replaying traces capturing 
times desirable replay trace reproducing original cache state precisely useful example replaying trace different hardware conditions benchmarking 
debugging forensics desirable reproduce lower file system behavior close original possible 
replayfs supports replaying modes dealing read operations 
reads performed current cache state 
particular replayfs calls captured buffer read operations 
case non cached data pages result calls page read operations 
second reads performed original cache state 
reads invoked page level pages cache tracing 
third reads replayed 
useful recreation resulting disk state fast possible 
directory entries may released dentry cache replaying process stay trace capture 
result inconsistency rat entries actual 
avoid situation force stayed cache capture stay cache replaying process increase dentry counter time looked decrease released original trace 
asynchronous file system activity file system activity performed asynchronously background thread 
replaying asynchronous activity complicated intertwined file system internals 
example access time updates may supported file system replaying supported original 
replayfs replays activity indirectly meta data information updated time trace data lower file system write corresponding changes stable storage 
way replaying process exercises lower file system enforcing restrictions related originally traced file system 
initial file system state simplest case trace captured starting empty file system replayed empty file system 
traces usually contain operations files file system objects existed tracing process started 
file system prepared replaying may 
convenient prepare file system information contained trace 
best way prepare lower file system 
full restoration initial file system state trace replaying precise file system algorithms different performance different file system states 
example directory sizes may influence performance lookup readdir operations files directory show trace 
existing systems capture restore snapshots replayfs 
implementation file system trace precisely replayed captured perturbing behavior lower file system 
performed optimizations tracefs 
traditionally stackable file systems buffer data twice 
allows keep modified encrypted compressed unmodified data memory time save considerable amounts cpu time tracefs modify data pages 
double caching provide benefits page cache size effectively half original size 
data copied layer unnecessarily consuming cpu resources 
unfortunately linux vfs architecture imposes constraints sharing data pages lower upper layers complicated 
particular data page vfs object belongs single inode uses information inode time 
applied solution 
specifically data pages belong upper inode assigned lower level inodes short duration lower level page operations 
tested resulting tracefs stackable file system single cpu multi cpu machines compile intensive workloads 
addition cpu time memory savings optimization allowed reduce tracefs source size lines 
cases replayfs need original data buffers replaying read operations 
data verification purposes md checksum sufficient 
added new tracefs option instructs capture data buffers writing operations reads 
allowed reduce tracefs trace sizes tracefs system time overheads 
trace compiler 
trace compiler optimized performance 
intermediate data sets commonly larger amount available memory 
added hash data structures avoid repeatedly scanning data reduce usage 
compare buffers comparing md checksums 
allows save cpu time md checksums calculated buffer 
trace compiler consists lines code 
replayfs kernel module 
trace compiler prepares data replaying replayfs relatively small simple 
consists thread management timing control trace prefetching eviction operation invocation vfs resource management components 
replayfs source lines long 
replayfs supports accelerated playback fixed factor replaying fast possible 
replayfs tracefs implemented loadable kernel modules 
ported tracefs linux kernel tracefs replayfs linux kernels 
evaluation conducted benchmarks ghz pentium machine gb ram 
system disk gb rpm western digital ide formatted ext 
addition machine maxtor atlas rpm gb ultra scsi disks formatted ext 
scsi disks storing traces replayfs traces disk running test workloads replaying 
lower file systems benchmark run purge file system caches 
ran test times student distribution compute confidence intervals mean elapsed system user wait times 
wait time elapsed time cpu time consists process scheduling affect 
case confidence intervals mean 
test machine running core linux distribution vanilla kernel 
evaluation tools workloads created additional statistics module evaluation purposes module records timeline statistics ext timing deviation figures replayfs 
module uses proc interface export data user level analysis plotting 
statistics module stores resulting information static array effects file system operation querying time incrementing value output array 
corresponding overheads negligible measured cpu time experiments ran 
am utils build 
building am utils cpu intensive benchmark 
am utils contains lines code files 
build process begins running small configuration tests detect system features 
builds shared library binaries scripts documentation total new files new directories 
compile cpu intensive contains fair mix file system operations 
instrumented ext file system uses writes operations reads open operations close operations remaining operations mix readdir lookup am utils activity uniform bursts activity separated intervals high cpu activity related user mode computations 
allows analyze replaying precision visually 
compilation process heavily uses memory mapped operations 
postmark 
postmark simulates operation electronic mail servers 
performs series file system operations appends file reads creations deletions 
benchmark uses little cpu intensive 
configured postmark create files bytes size perform transactions 
selected create delete read write operations equal probability 
postmark particular configuration stresses replayfs heavy load intensive operations 

small micro benchmark evaluate replayfs cpu time consumption 
spawns threads concurrently read kb buffers cached data system call 
experiment performed read operations 
compare results stateof art system call replayer 
micro benchmark allowed demonstrate benefits zero copying replaying 
memory overheads memory consumed replayers effectively reduces file system cache sizes affect behavior lower file system 
compiled binary module sizes negligible 
kb replayfs module kb statistics module 
user mode trace compiler reduces trace size generating variable length program elements eliminating duplicate data buffers 
table shows characteristics raw compiled traces compilation times 
see original am utils trace size reduced postmark 
recall rat entirely kept memory size small traces 
program buffers trace com ponents prefetched demand 
separate disk storing traces 
reduced contention allowed prefetch minimal amount data necessary replay trace time 
addition direct access page cache allowed promptly evict pages accessed near 
result memory prefetching storing traces exceeded mb experiments 
means replayfs memory overheads available memory test machine time benchmark runs 
am utils postmark raw trace mb mb mb commands mb mb mb rat mb mb bytes buffers mb mb bytes compilation time minutes table size compilation time traces 
timing precision replaying standard os timers usually low resolution 
example standard linux timers resolution millisecond larger microsecond typical duration file system operation result applied pre spin technique described section bring timing accuracy microsecond scale 
shows cumulative distribution function cdf operation invocation timing errors 
naturally timing errors postmark run pre spin distributed equally millisecond events triggered poor millisecond resolution 
see pre spinning dra fraction operations timing error microseconds postmark pre spin postmark pre spin am utils pre spin am pre spin cumulative distribution functions event invocation error rates replaying experiments 
closer curve upper left corner better average accuracy timing error microseconds time seconds am utils pre spin am utils pre spin dependence average invocation time error elapsed time am utils trace replaying 
matically decreases error values 
error distributions differ different workloads 
figures clarify behavior 
shows average timing error second am utils trace replaying 
shows corresponding file system operation counts recorded instrumented ext 
see clear correlation replaying event rates related average error 
reason correlation events spaced apart replayed high accuracy events invoked close invoked accurately cpu time overheads 

cdf invocation errors easily hide real replaying problems 
example cdf captured slow operation rate workload may indicate operations replayed high precision 
shows information inferred tool behave medium high rates 
second timers accuracy important file system activity replayers believed 
timer resolution contributes event invocation errors low event rates timing precision necessary 
hand events separated long intervals file system activity long durations average event influence 
hand file system operations invoked close especially invoked different processes overlap interfere 
desirable replay precisely possible 
case timer resolution small impact resulting timing quality 
overheads replayers add operations define timing precision discussed sec tion 
see comparing traces am utils replayed different timer resolutions 
easily see pre spinning improvement effects visible micro second resolution shows improvements prominently 
see timer resolution cases small discrepancies peaks activity replayed captured traces 
may assume increase cpu speed solve timing precision problem 
case network packet network file system replayers replayer target system run different machines 
case non network file system replayers execute machine tested file system 
faster cpu replayed operations execute faster corresponding interactions change portion cpu time spent servicing file system requests decrease requests different processes overlap processes compete locks disk heads 
cpu time consumption system call replaying tools run user mode invoke system calls invoked original user programs 
usually user level replayers cpu time overheads higher user activity intervals original trace 
replayfs runs kernel avoids wasting time operations required cross user kernel boundary 
consider system call 
invoked vfs converts file descriptor number kernel file structure checks parameters validity correspondence file type verifies buffer writing 
replayfs benefits optimizations kernel mode vfs objects readily available replayfs need looked replayfs operates vfs objects directly file structure argument taken directly looking rat parameters file access modes checked trace capture skipped memory buffers passed user space replayfs allocate directly having verify 
shows times related execution original program replaying trace replayfs full speed 
replayfs bar shows skipping vfs operations described allows replayfs replay trace faster original program generated hardware 
replayfs avoid copying data user mode buffers kernel pages 
replayfs bar demonstrates number operations time seconds replayfs pre spin tracefs prepare write open permission number operations time seconds replayfs pre spin tracefs prepare write open permission counts file system operations seen lower ext file system replaying am utils traces pre spin timer enhancement left pre spin enhancement right 
see clear difference seconds scale spite fact timing micro second scale better pre spin configuration 
cases replayfs tracefs curves overlap indistinguishable 
small timing discrepancies correlated peaks activity 
show operations highest peaks activity operations highest timing errors observed 
show release commit write operations graphs closely resemble shapes open write operations respectively 
elapsed time sec replayfs replayfs wait time lower read system time system time user time prefetch system call replayer elapsed times program trace replayed replayfs copying data just read replayfs elapsed time trace replayed replayfs skips data copying replayfs time necessary read trace data fast possible prefetch estimated elapsed time system call replayer user time overhead system call replayer replayfs replay original trace times faster original program generated hardware 
see data copying reduce execution file system read operation cpu cycles average 
case trace prefetching data necessary bytes null terminated file name 
created modified version program call prefetch sequentially reads data file fast possible 
took prefetch seconds average read program trace component 
prefetch bar shows seconds spent waiting completion 
means replaying process bound replayfs prefetches traces asynchronously 
decrease replayfs cpu overheads may replayfs bound replaying trace replayfs reach physical limitations hardware disks point replaying sped 
case problem resolved replacing disk drive controller tested file system replayfs traces located different physical drives 
compared replayfs state art system call replayer 
unfortunately availability public especially comparison purposes limited evaluate 
overheads workload reported 
rightmost bar represents extrapolated timing result visual comparison purposes created bar adding overhead elapsed time user time original program time 
note actual overhead value important fact overhead positive 
cause overhead positive user level replayers replay traces rate original program issue requests 
having low negative overheads replayfs results reproduction original timing conditions 
despite existing discrepancies original traces replayed ones seen replayed original figures overlap operations peaks activity 
existing system call replaying tools match trace closely inherent overheads 
designed system overheads lower overheads published systems replay traces system calls 
precisely similar systems run user level higher overheads turn imposes greater limitations ability replay traces accurately 
benefit replayfs low overheads ability replay original traces faster speeds hardware 
described replay read intensive traces times faster original time 
addition replayed am utils trace accelerated mode 
able replay second long am utils trace seconds reproducing memory mapped disk state changing operations 
represents speedup orders magnitude 
related trace capture replaying decades describe representative set papers related file system activity tracing replaying 
capturing traces 
describe tracers level abstraction capture file system activity system call level tracers virtual file system level tracers network tracers driver level tracers 
discuss order network level tracers capture file system information level abstraction higher driver level lower vfs level 
common tool capture system calls 
uses ptrace system call capture sequence system calls invoked application associated parameter values 
showed special measures taken collect file system traces distributed environments long intervals time minimize volume generated transferred data 
problem missed memory mapped operations system call traces long recognized 
roselli show decades ago memory mapped operations common normal reads writes 
collected traces virtual file system level linux windows nt traces include memory mapped operations 
network packet traces collected specialized devices software tools tcpdump 
specialized tools capture preprocess network file system related packets 
network file system traces contain information requests satisfied caches contain information multiple hosts 
driver level traces contain requests satisfied caches 
useful disk layout information needs collected minimizing trace size 
trace replaying 
similar capturing traces replaying traces performed logical levels 
usually traces replayed level captured 
way changes timing operations mix minimized 
simplicity authors replay kernel level traces user level 
simple replay system calls contain necessary information parameters 
existing system call replayers designed specifically replay file system activity 
replay system call traces user level 
evaluation showed slowdown replaying traces high rates authors claimed accurate performance data replaying mode available main focus authors capturing traces 
network traffic replayers operate user mode replay arbitrary network traces 
network file system trace replaying conceptually similar ordinary network packet trace replaying 
knowledge network file system protocol details allows replayers reorder packets faster replaying 
replayers tracers run dedicated machines separate tested servers 
network file system trace replaying intrusive replaying capturing method 
replaying patterns disk driver level allows evaluation elevator algorithms driver subsystems lower overheads little complexity 
allows evaluation disk layouts 
example driver level replayer measure effects journal file relocation disk 
particular case system call level replaying appropriate physical file location disk easily controlled user level 
capture replay traces different logical levels 
example drive pro cesses driver level traces replays system call level evaluate power consumption 
unrelated file system operations removed preprocessing phase speed replaying process 
replayed network file system traces disk simulators benchmarking 
network traces suitable purpose captured caches minimally disturb workload 
file system state versioning 
file system trace replaying considered form fine grained versioning 
replaying reproduce version file system state including possible state abnormalities caused timing conditions 
property useful forensics post attack investigation debugging purposes 
emulate aging file system running actual benchmarks 
replaying file system activity replayers may recreate pre tracing file system image 
important accuracy file layouts age file system affect behavior significantly 
authors opted extrapolate original file system state information gleaned trace 
technique disadvantages 
full path name information required trace data identify exact directories files accessed 
second files accessed trace known system files affected file system layout age 
third trace capture techniques omit information vital replaying accurately 
example nfs trace replayer sees nfs write protocol message tell file written existed 
belief best method restore pre tracing file system state 
data prefetching 
data prefetching common technique decrease application latency increase performance 
access patterns normally inferred history past accesses hints provided applications 
access patterns known advance simple approaches possible 
data aggressively read advance overwriting prefetched data 
second data read just time avoid stalls 
cao showed approaches times worse optimal solution 
algorithms simple implementations 
tip system uses version second algorithm called fixed horizon :10.1.1.118.6273
sophisticated reverse aggressive algorithm near optimal performance difficult implement 
algorithm attempt combine best algorithms simplicity prefetching performance 
timing inaccuracy 
existing system call replayers suffer timing precision problems peak load reproduction problems degree reasons user mode replayers high memory cpu overheads due redundant data copying user kernel buffers 
page eviction completely controlled user level prefetching policies harder enforce 
interface help somewhat control data page eviction 
kernels preemptive long execution paths including interrupt handlers 
replaying processes preempted tasks 
partially solved instructing scheduler treat replaying process real time process 
standard timer interfaces exposed user level precise 
authors investigated problem came similar sufficient setup timer early busy wait timer expires 
metric evaluate replaying precision papers average difference actual event time traced event time 
example better kernel timers interface resulted typical microsecond difference times improvement compared replayer measures 
trace replaying offers number advantages file system benchmarking debugging forensics 
effective accurate file system traces captured replayed close possible file system code 
existing systems capture file system traces network file system level client side cached aggregated events translate protocol messages system call traces popular memory mapped reads writes level traces omit important meta data file system events involve file names 
problems exacerbated traces captured level replayed information loss results 
demonstrated previously believed accuracy replaying high rate traces limited overheads replayers precision timers 
file systems run kernel userlevel file system replayers suffer overheads affect accuracy significantly 
user mode replayers produce excessive number context switches data copies user kernel boundary 
existing replayers inaccurate unable replay file system traces high event rates 
designed developed evaluated new replaying system called replayfs replays file system traces immediately file systems inside kernel 
carefully chose actions best done offline user level trace compiler online runtime kernel replayfs module 
replaying close actual file system distinct benefits capture replay file system operations including important memory mapped operations resulting accurate replaying 
second access important internal kernel caches allowed avoid unnecessary data copying reduce number context switches optimize trace data prefetching 
third precise control thread scheduling allowing oft wasted pre spin periods productively technique call productive pre spin 
kernel mode replayer assisted user mode trace compiler takes portable traces generated tracefs produces binary trace suitable executing kernel 
trace compiler carefully partitions data distinct groups different access patterns allowed apply optimizations aimed improving performance commands read sequentially resource allocation table rat determines memory resources replaying phase 
particular rat allows reuse resources replay time longer needed discarding buffers bulk data pages file names accessed randomly need basis 
partitioning possibility evict cached data pages directly allowed reduce memory usage considerably experiments replayfs consumed mb kernel memory modern systems 
replayfs replay traces faster known user level system handle replaying traces spikes activity high rates events 
fact optimizations replayfs replay traces captured hardware faster original program produced trace 
commands executed different threads may issued original order 
example thread waiting long request threads may continue execution dependency requests 
useful stress testing certain benchmarking modes 
commands synchronized points threads depend 
example original trace shows thread read file second thread wrote file ordering preserved trace replaying 
modifying existing trace compiler add thread synchronization commands commands replayfs trace component 
workloads may result different behavior file system operations order preserved 
example threads concurrently writing file may create different output file due kernel preemption 
policies allow replayfs differentiate real replaying errors tolerable mismatches return values due race conditions 
large body existing traces captured past decades different systems different levels 
unfortunately traces replayed lack tools 
currently developing user mode translators convert traces formats portable format 
carefully analyzed vfs interfaces linux freebsd solaris windows xp 
despite significant internal implementation differences remarkably similar functionality 
primarily file system interfaces evolved time cooperate best apis posix system call standard 
plan port replayfs operating systems 
acknowledgments acknowledge jordan hoch charles wright help different stages replayfs design development testing 
fsl members support productive environment 
partially possible nsf awards eia career ccr hp intel gifts numbers 
anderson swaminathan 
toolkit flexible high fidelity benchmarking 
proceedings third usenix conference file storage technologies fast pages san francisco ca march april 
wright zadok 
tracefs file system trace 
proceedings third usenix conference file storage technologies fast pages san francisco ca march april 
belady 
study replacement algorithms virtual storage computers 
ibm systems journal 
blaze 
nfs tracing passive network monitoring 
proceedings usenix winter conference san francisco ca january 
cao felten karlin li 
study integrated prefetching caching strategies 
proceedings acm sigmetrics conference measurement modeling computer systems pages ottawa canada may 
chen patterson 
new approach performance evaluation self scaling benchmarks predicted performance 
proceedings acm sigmetrics international conference measurement modeling computer systems pages seattle wa may 

tornado novel input replay tool 
proceedings international conference parallel distributed processing techniques applications pdpta volume pages las vegas nevada june 
ellard seltzer 
new nfs tracing tools techniques system analysis 
proceedings annual usenix conference large installation systems administration san diego ca october 
feng goel feng walpole 
high performance packet replay engine 
proceedings acm sigcomm workshop models methods tools reproducible network research pages karlsruhe germany 
goel snow walpole 
supporting time sensitive applications general purpose operating systems 
proceedings fifth symposium operating system design implementation osdi pages boston ma december 
lbnl network research group 
tcp dump libpcap site 
www tcpdump org february 
heidemann popek 
performance cache coherence stackable filing 
proceedings fifteenth acm symposium operating systems principles sosp pages copper mountain resort december 
hitz lau malcolm 
file system design nfs file server appliance 
proceedings usenix winter technical conference pages san francisco ca january 
howard kazar menees nichols satyanarayanan sidebotham west 
scale performance distributed file system 
acm transactions computer systems february 
rai zadok 
increasing distributed storage survivability stackable raid file system 
proceedings ieee acm workshop cluster security conjunction fifth ieee acm international symposium cluster computing grid ccgrid cardiff uk may won best award 
katcher 
postmark new filesystem benchmark 
technical report tr network appliance 
www com tech library html 
karlin 
near optimal parallel prefetching caching 
proceedings th ieee symposium foundations computer science pages october 
tomkins patterson bershad cao felten gibson karlin li 
trace driven comparison algorithms parallel prefetching caching 
proceedings second symposium operating systems design implementation osdi pages seattle wa october 
satyanarayanan 
long term distributed file tracing implementation experience 
technical report cmu cs carnegie mellon university pittsburgh pa 
reddy wright zadok 
versatile user oriented versioning file system 
proceedings third usenix conference file storage technologies fast pages san francisco ca march april 
ousterhout 
threads bad idea purposes 
invited talk usenix technical conference january 
home net threads ppt 
ousterhout costa harrison kunze kupfer thompson 
trace driven analysis unix bsd file system 
proceedings tenth acm symposium operating system principles pages orcas island wa december patterson gibson zelenka :10.1.1.118.6273
informed prefetching caching 
proceedings th acm symposium operating system principles sosp pages copper mountain resort december 
peek flinn 
drive fast accurate evaluation storage power management 
proceedings annual usenix technical conference pages anaheim ca april 
williams zadok 
am utils user manual edition july 
www am utils org 
peterson burns 
ext cow time shifting file system regulatory compliance 
trans 
storage 
arpaci dusseau arpaci 
analysis evolution journaling file systems 
proceedings annual usenix technical conference anaheim ca may 
wright zadok 
improving application performance system call composition 
technical report fsl computer science department stony brook university june 
www fsl cs sunysb edu docs cosy perf 
roselli lorch anderson 
comparison file system workloads 
proc 
annual usenix technical conference pages san diego ca june 
ruemmler wilkes 
unix disk access patterns 
proceedings winter usenix technical conference pages san diego ca january 
smith seltzer 
file system aging increasing relevance file system benchmarks 
proceedings acm sigmetrics international conference measurement modeling computer systems pages seattle wa june 
craig soules garth goodson john strunk gregory ganger 
metadata efficiency versioning file systems 
proceedings second usenix conference file storage technologies fast pages san francisco ca march 
developers 
february 
sourceforge net 
tomkins patterson gibson 
informed multi process prefetching caching 
proceedings acm sigmetrics conference measurement modeling computer systems pages seattle wa june 
transaction processing performance council 
transaction processing performance council 
www tpc org 
merchant alvarez 
mems storage disk arrays 
proceedings second usenix conference file storage technologies fast pages san francisco ca march 
vogels 
file system usage windows nt 
proceedings th acm symposium operating systems principles pages charleston sc december 
software home page 
www 
nl 
wright dave gupta krishnan zadok 
versatility unix semantics fan unification file system 
technical report fsl computer science department stony brook university october 
www fsl cs sunysb 
edu docs tr pdf 
zadok 
fist language stackable file systems 
proc 
annual usenix technical conference pages san diego ca june 
zhang yu krishnamurthy wang 
configuring scheduling eager writing disk array transaction processing workload 
proceedings usenix conference file storage technologies fast pages monterey ca january 
zhu chen chiueh ellard 
scalable accurate trace replay file server evaluation 
technical report tr computer science department stony brook university december 
www cs sunysb edu tr tr pdf 
