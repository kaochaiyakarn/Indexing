journal machine learning research submitted revised published probabilistic non linear principal component analysis gaussian process latent variable models neil lawrence neil dcs shef ac uk department computer science university sheffield regent court portobello street sheffield dp editor hyv rinen summarising high dimensional data set low dimensional embedding standard approach exploring structure 
provide overview existing techniques discovering embeddings 
introduce novel probabilistic interpretation principal component analysis pca term dual probabilistic pca 
model additional advantage linear mappings embedded space easily gaussian processes 
refer model gaussian process latent variable model gp lvm 
analysis gp lvm objective function relate model popular spectral techniques kernel pca multidimensional scaling 
review practical algorithm gp context large data sets develop handle discrete valued data missing attributes 
demonstrate model range real world artificially generated data sets 
keywords gaussian processes latent variable models principal component analysis spectral methods unsupervised learning visualisation 
machine learning split categories supervised learning data set split inputs outputs reinforcement learning typically reward associated achieving set goal unsupervised learning objective understand structure data set 
approach unsupervised learning represent data lower dimensional embedded space probabilistic model variables associated space known latent variables 
focus methods represent data latent embedded shall terms interchangeably space 
approach inspired probabilistic latent variable models 
roots previously proposed approaches density networks mackay multi layer perceptron mlp provide mapping latent projections observed data prior distribution placed latent space latent space posterior distribution approximated sampling 
density networks mlp perform mapping bishop 
replaced mlp radial basis function rbf network aim decreasing training time model 
model evolved bishop generative topographic mapping gtm latent space sampled uniform grid importance sampling reinterpreted fitting mixture model expectation maximisation em algorithm 
neil lawrence 
lawrence allows points latent space laid uniform grid sampled 
grid layout shared self organising map som kohonen bishop 
argued gtm provides principled alternative self organising map 
models outlined typically designed embed data set dimensions rely importance sampling grid points latent space achieve embedding causes problems dimensionality latent space increases 
point representations latent space useful allow non linear models point easy propagate non linear mapping data space 
non linear mappings designed address weaknesses visualising data sets arise standard statistical tools rely linear mappings principal component analysis pca factor analysis fa linear mapping may possible reflect structure data low dimensional embedding 
principal component analysis seeks lower dimensional sub space typically represented orthonormal basis projected variance data maximised 
dimensional sub space sought projections may visualised may necessary include latent dimensions capture variability hopefully means necessarily structure data 
principal component analysis latent variable model representation tipping bishop strongly related factor analysis fa :10.1.1.33.4726
linear gaussian latent variable models fa allows richer noise model pca non linear factor analysis see honkela valpola 
naturally statisticians constrained linear methods visualising data section shall briefly review multidimensional scaling related techniques rely proximity data 
multidimensional scaling kernel pca mentioned visualisation techniques rely learning mapping latent space embedded space data space 
section briefly review methods proximity data obtain visualisation embedding 
broadly speaking methods variants enhancements technique known multidimensional scaling mds 
methods observing data directly information data set summarised matrix similarities dissimilarities 
examples include distance matrices dissimilarity matrix kernel matrices similarity matrix 
method review provides answers questions 

proximity matrix compiled 

embedding developed proximity matrix 
variants multidimensional scaling mardia appear focus second question 
classical mds torgerson eigendecomposition centred similarity matrix performed 
viewed minimising particular stress function distances visualised space matched data space 
attempting preserve distances known metric mds non metric mds ordering distances preserved 

sampling techniques latent points random positions 

data form distance dissimilarity matrix simple conversion may performed obtain similarity matrix 
probabilistic non linear pca strong connections mds kernel pca sch lkopf formalised williams 
kernel pca provides answer question suggestion proximity data provided positive semi definite mercer kernel computed data kernel implies existence non linear mapping data space latent space recall gtm density networks perform non linear mapping opposite direction 
existence function important allows data points training set mapped position latent space re solving eigenvalue problem 
kernel pca mds methods obvious project back latent space data space known pre image problem 
clear handle missing data proximity data matrix normally computed consistently particular attribute available 
sammon mappings sammon attempt match embedded distances points distances observed space form mds 
suffer weakness mds projection data points original data set computationally demanding despite name provide explicit mapping data latent space 
lack mapping addressed algorithm lowe tipping version suggested mds tipping 
importance focussed forming proximity matrix includes isomap tenenbaum approximation geodesic distance spectral clustering see shi malik proximity data derived graph 
table summarised properties algorithms models 
included model subject gaussian process latent variable model gp lvm 
remainder introduce gp lvm latent variable model perspective 
gp lvm belongs class methods density networks gtm connections classical mds kernel pca 
particular section show approaches share objective function 
section cover algorithmic issues arise model 
framework gp lvm developed straightforward modify approach data gaussian noise model appropriate binary ordinal discussed section 
handling missing data attributes straightforward section 
algorithm characteristics explored empirically section 
gaussian process latent variable models gaussian process latent variable model 
shall see model strongly related approaches outlined 
point representation latent space gtm density networks minimise objective function related classical mds kernel pca see section 
starting point novel probabilistic interpretation principal component analysis 
introduces mercer kernels sch lkopf smola chapter 
missing data mean missing attributes normally computing proximity data matrix 
proximity data methods missing data mean elements missing proximity matrix discuss case 
lawrence proximity non linear probabilistic convex pca fa kernel pca mds sammon mapping spectral clustering density networks gtm gp lvm table overview relationship algorithms 
indicates algorithm exhibits property indicates interpretation algorithm exhibits associated property 
characteristics algorithm proximity method proximity data 
method lead mapping embedded data space 
method lead mapping data embedded space 
non linear method allow non linear embeddings 
probabilistic method probabilistic 
convex algorithms considered convex unique solution local optima occur 
refer dual probabilistic principal component analysis 
dual probabilistic principal component analysis turns special case general class models refer gp 
latent variable models typically specify latent variable model relating set latent variables set observed variables set parameters 
model defined probabilistically latent variables marginalised parameters maximising likelihood 
consider alternative approach latent variables optimising parameters marginalise parameters optimise latent variables 
show approaches equivalent particular choice gaussian likelihood prior approaches lead probabilistic formulation principal component analysis pca 
section review standard derivation probabilistic pca tipping bishop show alternative probabilistic formulation may arrived see appendix :10.1.1.33.4726
probabilistic pca probabilistic pca ppca latent variable model maximum likelihood solution parameters solving eigenvalue problem data covariance matrix probabilistic non linear pca tipping bishop :10.1.1.33.4726
assume set centred dimensional data yn denote dimensional latent variable associated data point xn 
relationship latent variable data point linear noise added yn matrix specifies linear relationship latent space data space noise values taken independent sample spherical gaussian distribution mean zero covariance likelihood data point written yn xn yn 
obtain marginal likelihood integrate latent variables yn yn xn xn dxn requires specify prior distribution xn 
probabilistic pca appropriate prior unit covariance zero mean gaussian distribution xn xn 
marginal likelihood data point analytically marginalisation yn yn ww advantage independence data points likelihood full data set yn 
parameters maximisation 
tipping bishop showed analytic solution maximisation 
solution achieved matrix spans principal sub space data 
model interpretation probabilistic version pca 
latent variables optimising parameters maximum likelihood standard approach fitting latent variable models 
section introduce alternative approach 
optimising parameters latent variables suggest dual approach parameters optimising respect latent variables particular choice prior distribution probabilistic model turn equivalent pca 

notation denote gaussian distribution mean covariance 
lawrence probabilistic pca latent variable optimisation bayesian framework parameters viewed random variables 
bayesian methodology requires suitable choice prior proceeds treat parameters latent variables 
simple choice prior conjugate spherical gaussian distribution wi wi ith row matrix unfortunately marginalisation xn intractable 
wish proceed turning approximate methods faced choice marginalise 
natural choice marginalise typically larger dimension practice turns approaches equivalent 
marginalisation straightforward due choice conjugate prior 
resulting marginalised likelihood takes form represent dth column xx 
look optimise respect latent variables 
expected duality optimisation similar tipping bishop 
objective function log likelihood dn ln ln tr yy xx gradients respect may magnus yy dk fixed point gradients zero appendix show values maximise likelihood 
possible marginalise parameters latent variables analytically bayes factors perform model selection see example bishop 

matrix larger dimension features data points 
probabilistic non linear pca matrix columns eigenvectors diagonal matrix jth element eigenvalue associated jth eigenvector arbitrary rotation matrix 
follows assume eigenvalues ordered magnitude largest placed 
note eigenvalue problem developed easily shown equivalent solved pca see appendix formulation pca manner key step development kernel pca sch lkopf matrix inner products replaced kernel see tipping concise overview derivation 
probabilistic pca model shares underlying structure tipping bishop differs optimise marginalise marginalise optimise 
gaussian processes gaussian processes hagan williams class probabilistic models specify distributions function spaces 
function infinite dimensional object distribution function space considered focussing points function instantiated 
gaussian processes distribution instantiations taken gaussian 
modelling gaussian processes consists specifying gaussian process prior 
usually gaussian distribution parameterised mean covariance 
case gaussian processes mean covariance functions space process operates 
typically mean function taken zero covariance function necessarily constrained produce positive definite matrices 
consider simple gaussian process prior space functions fundamentally linear corrupted gaussian noise variance covariance function kernel prior xi xj xi vectors space inputs function kronecker delta 
inputs taken embedding matrix covariance function evaluated points recover covariance matrix form xx element ith row jth column 
recognised covariance associated factor marginal likelihood dual probabilistic pca 
marginal likelihood dual probabilistic pca product independent gaussian processes 
principal component analysis optimising parameters input positions gaussian process prior distribution linear covariance function dimension gaussian process latent variable models dual interpretation probabilistic pca described points new class models consist gaussian process mappings latent space observed data space dual 
positive definite constraint implies covariance functions valid mercer kernels 
common refer covariance function kernel 
shall terms interchangeably 
lawrence probabilistic pca special case output dimensions priori assumed linear independent identically distributed 
assumptions obtain new probabilistic models 
independence broken example allowing arbitrary rotation data matrix identically distributed assumption broken allowing different covariance functions output dimension 
focus third assumption linearity 
replacing inner product kernel covariance function allows non linear functions obtain non linear latent variable model 
due close relationship linear model interpretation probabilistic pca model interpreted non linear probabilistic version pca 
proximity data gp lvm indicated gp lvm connections proximity data methods kernel pca classical mds 
connections unifying objective function embraces models 
section briefly introduce objective function 
unifying objective function classical mds kernel pca rely proximity data similarity matrices 
denote matrix similarities methods case positive definite similarity measures matrix interpreted covariance covariance function 
cross entropy gaussian gaussian process marginal likelihood lnn dz ln ln tr 
substitute yy see scaling identical 
xx minimising respect leads solution appendix form matrix columns eigenvectors specific case optimisation identical dual probabilistic pca 
general case kernel function simply positive definite matrix similarities kernel pca classical mds recovered 
note entropy constant may subtract objective function affecting optimisation respect resulting objective function kullback leibler divergence kullback leibler gaussians kl ln dz ln ln tr sk 
simple example idea allow different noise distributions output direction 
probabilistic model underlying factor analysis allows flexibility see example tipping bishop 

analysis follows extended positive semi definite adding diagonal term considering limit 

mds literature referred principal ordinate analysis 
probabilistic non linear pca appropriate choice valid objective function pca kernel pca classical mds gp lvm 
kernel pca non linear kernel linear kernel 
gp lvm linear kernel non linear kernel 
practice means gp lvm harder optimise solving eigenvalue problem longer sufficient gp lvm maintains probabilistic interpretation kernel pca doesn 
methods overlap inner product matrices outlined 
note similarity measure form inner product kernel objective function longer interpretation likelihood 
approach probabilistic interpretation multidimensional scaling refer reader mackay oh raftery details probabilistic mds methods 
note reversing kullback leibler divergence kullback leibler divergence asymmetric measure distribution divergence natural consider effect reversing role distributions expectations distribution governed governed special case reversed kl divergence similar original matrices replaced inverses 
new objective function ln ln tr ks minimum eigenvalue problem retained eigenvalues smallest largest 
respect model leads minor component analysis 

fitting non linear gp lvm saw previous section pca interpreted gaussian process maps points points data space 
positions points latent space determined maximising process likelihood respect natural consider alternative gp introducing covariance functions allow non linear processes 
resulting models general eigenvalue problem 
optimisation non linear model previous section saw linear kernel closed form solution obtained arbitrary rotation matrix 
typically non linear kernels closed form solution multiple local optima 
wide choice non linear covariance functions reviewed section 
particular kernel gp lvm note gradients respect latent points gradient respect kernel yy dk combining chain rule 
computation straightforward xn independent kernel choice require gradient kernel respect lawrence latent points computed 
gradients may combination non linear optimiser obtain latent variable representation data 
furthermore gradients respect parameters kernel matrix may computed jointly optimise kernel parameters 
log likelihood highly non linear function embeddings parameters 
forced turn gradient optimisation objective function 
scaled conjugate gradient ller approach optimisation implicitly considers second order information scale parameter regulate positive hessian point 
scaled conjugate gradient scg experiments 
illustration gp lvm scg illustrate simple gaussian process latent variable model turn multi phase oil flow data bishop james 
twelve dimensional data set containing data known classes corresponding phase flow oil pipeline stratified annular homogeneous 
bishop 
see section data demonstrate gtm algorithm 
data set artificially generated known lie lower dimensional manifold 
sub sampled version data containing data points demonstrate fitting gp lvm simple radial basis function rbf kernel 
saw section seeking lower dimensional embedding pca equivalent gp lvm model linear kernel xi xj xi xj element ith row jth column kernel matrix kronecker delta function 
comparison visualised data set approaches mentioned 
show principal components data 
shows visualisation obtained gp lvm rbf kernel xi xj rbf exp xi xi bias white obtain visualisation log likelihood optimised jointly respect latent positions kernel parameters bias white rbf 
kernel initialised pca set kernel parameters initialised rbf white bias exp 
note redundancy representation scale matrix value 
redundancy removed penalising log likelihood half sum squares element implies seeking map solution gaussian prior xn 
likelihood rbf kernel optimised scaled conjugate gradient see www dcs shef ac uk neil code 

multiplying likelihood prior leads joint distribution data points latent points 
function joint distribution proportional posterior distribution maximising joint distribution equivalent seeking map solution 
probabilistic non linear pca visualisation oil data pca linear gp lvm gp lvm uses rbf kernel non metric mds kruskal stress metric mds sammon mapping gtm kernel pca 
red crosses green circles blue plus signs represent stratified annular homogeneous flows respectively 
plot indicate precision manifold expressed data space latent point 
lawrence method pca gp lvm non metric mds metric mds gtm kernel pca errors table errors different methods latent space nearest neighbour classification latent space 
gtm kernel pca asterisks result shown best obtained method range different parameterisations 
provide visualisations data range algorithms reviewed 
show result non metric mds stress criterion kruskal 
shows result metric mds criterion sammon 
objectively evaluate quality visualisations classified data point class nearest neighbour dimensional latent space supplied method 
errors classification table 
gtm kernel pca selection parameters required 
gtm varied size latent grid number hidden nodes rbf network varied 
best result obtained latent grid nodes rbf network shown 
note characteristic effect gtm visualisation arises layout latent points 
kernel pca rbf kernel varied kernel width 
best result obtained kernel width associated visualisation shown 
gradient optimisation rbf gp lvm latent space shows results clearly superior terms separation different flow phases achieved linear pca model 
gp lvm approach leads number errors smallest approaches 
additionally gaussian process perform mapping means express uncertainty positions points data space 
formulation gp lvm level uncertainty shared dimensions may visualised latent space 
visualising uncertainty recall likelihood product separate gaussian processes 
chose retain implicit assumption pca priori dimension identically distributed assuming processes shared covariance kernel function sharing covariance function leads posteriori shared level uncertainty process 
possible different covariance functions dimension may necessary data attributes different characteristics constrained model implemented allows visualise uncertainty latent space preferred empirical 
simple example 
scaled gp lvm scale parameter associated dimension data 
probabilistic non linear pca studies 
subsequently uncertainty visualised varying intensity background pixels 
lighter pixel higher precision mapping 
computational complexity quality results quick analysis algorithmic complexity shows gradient step requires inverse kernel matrix see operation rendering algorithm impractical data sets interest 
section show practical algorithm may developed circumvents problem maximising sparse approximation 

practical algorithm gp far shown pca viewed probabilistically perspectives involves integrating latent variables second optimising 
perspective develop non linear probabilistic version pca 
unfortunately optimisation problem faced non linear high dimensional nq interdependent parameters consider parameters kernel 
section describe approximation relies forced sparsification model 
resulting computational advantages visualisation large numbers data points practical 
base approach informative vector machine algorithm lawrence 
see section machinery added advantage allowing extend non linear pca model non gaussian noise models 
sparsification kernel methods may sped sparsification representing data set subset points known active set 
remaining points denoted informative vector machine ivm selects points sequentially reduction posterior process entropy induce implementation details ivm algorithm lawrence 

consequence enforced sparsification optimisation points active set proceeds quicker optimisation full set latent variables likelihood active set yi ki exp tr optimised respect kernel parameters xi gradient evaluations costing prohibitive arise full model 
dominant cost asymptotically active selection 
approaches constraining data direction kernel allowing data dimension kernel somewhat analogous difference probabilistic pca output data shares variance factor analysis data dimension maintains variance 
lawrence algorithm algorithm visualisation gp lvm 
require size active set number iterations initialise pca 
iterations 
select new active set ivm algorithm 
optimise respect parameters optionally latent positions xi scaled conjugate gradients 
select new active set 
point active set optimise respect scaled conjugate gradients 
latent variable optimisation interested visualising points data set significant speed advantage selecting active set need optimise inactive points 
fortunately active set selection allows optimise points independently fixed active set individual data points longer interdependent 
standard result gaussian processes see williams point inactive set shown project data space gaussian distribution mean ji ki ki denotes kernel matrix developed active set ki rows jth column variance xj jk ki gradients respect depend data independently optimise likelihood respect corresponding full set xj optimised pass data 
active set process repeated 
algorithm summarises order implemented steps 
active set selected kernel parameters active set positions optimised 
active set re selected latent positions points active set optimised 
iteration perform active set selections choice active set dependent kernel parameters latent point positions 
note data sets may necessary optimise xi active set regularly 

fixed variance output dimensions consequence sharing kernel output discussed section 

alternative noise models probabilistic non linear pca gaussian process latent variable model 
far considered gp lvm particular case gaussian noise dimension variance section consider extensions noise model 
firstly reformulate gaussian process contains additional latent variable fn yn fn df 
far considering case yn fn fni straightforward realise slightly general case variance dependent data point output dimension yn fn fni ni 
approach different noise models approximate gaussian noise model form see csat minka 
noise models consider independent dimensions giving approximations form yn fn fni fni fni ni lawrence approximation noise model leads gaussian approximation posterior distribution vector constructed stacking columns constructed stacking columns matrix fn covariance matrix block diagonal structure 
shown see csat minka parameters approximation ni ni ni ni fni ni ni nth diagonal element fni ni ni ni 
fni df 
prevent cluttering notation indicated approximation typically formed sequential manner parameters change data points incorporated 
approach approximating posterior distribution known assumed density filtering see maybeck chapter minka chapter 
missing values applications attributes missing particular data points 
ability handle missing values principled way desirable characteristic algorithm 
motivation probabilistic interpretation pca resulting algorithm handle missing data principled manner 
characteristic gaussian process latent variable model shares 
contrasted kernel pca handling missing values straightforward 
formalism described different noise models straightforward handle missing attribute 
corresponding variance set infinity ni 

special case gaussian noise fixed variance spherical noise shared kernels data dimension find blocks equal 
leads computational memory savings 
kernels different general noise models blocks equal 

results probabilistic non linear pca section range empirical evaluations different data sets explores different characteristics gp lvm 
note visualisation algorithms fitting problem long latent space lower dimensionality data space 
consequence integration mapping latent data space 
far briefly considered different kernel covariance functions proceeding reconsider introduce kernels experiments follow 
kernels gaussian process covariance function developed positive definite kernel new kernels formed adding kernels 
experiments principally different kernel functions 
linear kernel briefly discussed linear kernel simply matrix inner products klin xi xj introduced lin process variance controls scale output functions 
rbf kernel popular rbf kernel leads smooth functions fall away zero regions data 
inverse width parameter 
mlp kernel xi xj rbf exp xi xi mlp kernel williams derived considering multi layer perceptron mlp infinite number hidden units xi xj mlp sin wx wx xi call weight variance bias variance interpretations variances prior distributions neural network model 
covariance function leads smooth functions important characteristic differentiates rbf kernel outside regions data lies functions fall zero tend remain value 
noise term lawrence experiments section white noise term 
white noise process kernel form xi xj white kronecker delta zero takes value 
note white noise kernel redundant parameters noise model example gaussian noise model leaving white noise term setting yin fin yin fin white equivalent including white noise kernel setting yin fin lim yin fin experiments preferred include noise term kernel noise level white jointly optimised kernel parameters latent point positions 
parameter constraints initialisation kernels mentioned far parameters need constrained positive 
experiments implemented ln exp note transformed parameter parameter see consistent initialisation parameters experiments 
lin rbf mlp overview experiments experiments follow algorithm iterations active set size 
experiments run shot basis experiment run setting random seed values 
remainder section structured follows firstly section revisit oil data introduced section revised algorithm allows efficiently visualise data points 
comparing sparse algorithm gtm pca include full gp lvm model 
different algorithms explore quality visualisation terms ease different flow regimes separated embedded space 
section turn higher dimension data set hand written digits 
compare gtm pca sparse gp lvm algorithm seeing different digits separated latent space 
preceding data sets gaussian noise model final experiment noise model concerns issues initialisation 
data sets simple ground truth algorithm hopes recover 
section consider swiss roll data tenenbaum 

data ground truth known turns pca initialise gp lvm ground truth recovered initialising probabilistic non linear pca model pca sparse gp lvm rbf gp lvm rbf sparse gp lvm mlp gtm errors table number errors nearest neighbour classification latent space full oil data set points 
isomap known give ground truth recover probabilistic representation data 
section move non gaussian data sets 
consider binary data set handwritten 
compare binary model gaussian model show binary model effective reconstructing pixels obscured model 
oil flow data section return twelve dimensional oil data set introduced section 
visualise data points 
data set interested evaluating different things effect different non linear kernels effect sparse gp lvm algorithm relative full model 
visualisations data sparse gp lvm algorithm rbf mlp kernels respectively 
show data visualised non sparse gp lvm algorithm recreated visualisation bishop uses gtm algorithm 
considered nearest neighbour classifier latent space quantify quality visualisations 
note appears degradation quality gp lvm model associated sparsification comparision full gp lvm algorithm gtm sparse gp lvm performs worse 
handwritten digits oil flow data twelve attributes twelve dimensions structure data set visualised resorting displaying embedded spaces data sets greater dimensionality 
popular data set visualisation algorithms handwritten digits 
followed hinton roweis visualisation sub set digits digit greyscale version usps digit data set 
rbf mlp kernel 
visualising gp lvm visualisations gtm pca 
oil data looked objective assessment quality visualisation evaluation errors nearest neighbour classifier latent space 
performance benefits associated non linear visualisations apparent oil data table 
sparse gp lvm outperformed gtm algorithm criterion 
comparision full gp lvm model data set currently practical 
lawrence full oil flow data set visualised rbf sparse gp lvm mlp sparse gp lvm 
probabilistic non linear pca full gp lvm algorithm rbf kernel oil flow data 
gtm latent points laid grid rbf nodes 
lawrence digit images visualised latent space 
represented red crosses green circles blue pluses cyan stars magenta squares 
visualisation rbf kernel 
visualisation mlp kernel 
probabilistic non linear pca digit images visualised latent space 
red crosses green circles blue pluses cyan stars magenta squares 
visualisation gtm algorithm 
visualisation pca 
lawrence model pca sparse gp lvm rbf sparse gp lvm mlp gtm errors table errors nearest neighbour classification latent space digit data 
initialisation model experiments described pca initialise positions points latent space data sets pca provide poor initialisation causing gp lvm caught local minima 
show result modelling swiss roll data set tenenbaum data available line 
data true structure known manifold dimensional square twisted spiral dimensions living dimensional space 
follow roweis saul colour show position sheet 
gp lvm initialised pca stuck optimum recover true embedded space 
initialising isomap algorithm able recover underlying structure provide probabilistic description data gp lvm 
way combine strengths different approaches isomap related proximity data algorithms provide unique solution recover structure manifold data lies gp lvm provides underlying probabilistic model easy way compute mapping latent observed space 
due probabilistic nature gp lvm compare resulting models log likelihood 
log likelihood isomap initialised model factor smaller pca initialised model demonstrating advantage isomap initialisation data set 
missing data non gaussian noise models examples far gaussian noise models 
cases data continuous gaussian noise model longer appropriate 
non gaussian linear latent trait models proposed tipping section adf approach described section explore non gaussian data sets gp lvm models non gaussian noise models 
visualisation binary data example follow tipping visualising binary handwritten 
show visualisations data set derived usps cedar cd rom 
data contains examples examples taken complete data set digits hinton 

visualisations rbf kernel combination gaussian prior latent space visualisations different noise models 
gaussian noise model bernoulli noise model 
certainly differences visualisations wish objective assessment qualities embedded spaces 
turned test set containing digits 
digit test set removed pixel values 
digit model position embedded space probabilistic non linear pca effect poor initialisation 
gp lvm initialised pca 
log likelihood resulting model gp lvm initialised isomap 
log likelihood resulting model 
reconstruction method pixel error rate gp lvm bernoulli noise gp lvm gaussian noise assume pixels ink table pixel reconstruction error rates 
optimised 
missing pixels filled mapping embedded data space 
note local minima embedded space optimised embedded space location times different starting positions selected largest likelihood 
know original pixel values compute pixel reconstruction error rate 
rates summarised table 
results shown bernoulli noise model gaussian noise model baseline approach simply assume missing pixels contain ink 
hoped approaches considerably outperform baseline approach 
note bernoulli noise model leads far better results gaussian noise model 
illustrate type mistakes show randomly sampled results 
test digit original digit image showing pixels removed reconstruction methods outlined 
note gp lvm reconstructions particularly bernoulli noise model mistakes resulting image looks handwritten 
lawrence images visualised latent space 
visualisation gaussian noise model 
visualisation bernoulli noise model 
probabilistic non linear pca randomly sampled examples test data problem 
top row test images data set second row pixels removed test images shown red third row reconstruction assumes missing pixels ink fourth row reconstruction gaussian gp lvm fifth row reconstruction binary noise model 

discussion gaussian process latent variable model non linear probabilistic extension pca 
experiments show gp lvm viable alternative nonlinear visualisation approaches small data sets 
reviewed practical algorithm fitting gp lvm lawrence large data sets noted associated degradation performance method 
gp lvm model extended principled manner take account missing data binary data 
advantage explicitly modelling data type shown missing data problem handwritten digits 
computing likelihood test data key advantage gp lvm probabilistic 
likelihood associated training data 
model viewed non parametric density estimator size grows proportionally size introduces particular problems interested computing likelihood previously unseen test data point 
traditional probabilistic pca model new data point likelihood marginal distribution ww easily computed 
likelihood previously unseen test data set straightforward compute 
gp lvm likelihood takes different form 
new datum associated latent variable 
likelihood special case variances output direction constant ki ki column vector developed computing elements kernel matrix active set new point 
variance ki 
lawrence determine likelihood new point find map solution new latent point 
likelihood approximated computing probability observed data distribution projecting map solution back data space 
posterior multi modal respect solution necessarily unique 
ideal world integrate latent space determine marginal likelihood problem multiple modes arise 
practice may necessary seek modes random restarts latent space likelihood strongly peaked modes large difference magnitude largest modes approximate solution largest mode 
cases may necessary turn sampling methods evaluate likelihood 

new class models probabilistic modelling visualisation high dimensional data 
provided theoretical models proving principal component analysis special case 
showed general objective function kullback leibler divergence connects models proximity data methods kernel pca multidimensional scaling 
analysis objective function expected provide deeper insights behaviour algorithms 
real world data sets showed visualisations provided model placed related data points close 
demonstrated empirically model performed traditionally difficult domains involve missing discrete data high dimensions 
approach related density networks generative topographic mapping models provide non linear mapping embedded space observed space 
cases embedded space treated latent variable problems propagating distributions non linear mapping avoided point representations data latent space 
novel characteristic gp lvm visualise uncertainty manifold defined data space 
acknowledgments aaron hertzmann collaborators ongoing access style inverse kinematics amos pointing gp lvm fails swiss roll data pca initialisation michael tipping discussions visualisation techniques 
appendix probabilistic interpretations pca standard probabilistic interpretation pca tipping bishop involves likelihood taken gaussian yn xn yn xn yn probabilistic non linear pca graphical representation standard probabilistic pca model dual representation leads probabilistic interpretation pca :10.1.1.33.4726
nodes shaded represent different treatments 
black shaded nodes optimised white shaded nodes marginalised grey shaded nodes observed variables 
prior distribution latent variables taken gaussian xn xn marginalised recover marginal likelihood data yn yn yn ww 
structure model shown graphically 
dual representation probabilistic pca involves integrating maximising respect xn specifying prior distribution yn xn dw 
wi wi ith row matrix integrating obtain marginalised likelihood dn exp tr yy xt xt structure model shown 
note substituting dn exp tr lawrence highlights greater extent duality 
optimisation clearly highly related optimisation 
tipping bishop showed optimise section review optimisation generalise slightly applies symmetric matrix inner product matrix yy derivation covers kernel pca multidimensional scaling cases outlined section 
appendix optimisation dual pca kpca mds objective functions maximising equivalent minimising ln ln tr yy derivation follows holds regardless form applies objective function outlined section 
needn constrained form outlined objective function kernel pca positive definite kernel 
gradient likelihood respect sk setting equation zero pre multiplying gives xx substitute singular value decomposition giving su right multiplying sides note solution invariant rearrangement su diagonal solved eigenvalue problem eigenvectors eigenvalues 
implies elements diagonal retained eigenvalues li 
natural follow question possible eigenvalues vector pairs retained 
convenience ignore previously defined ordering eigenvalues terms magnitude assume keep eigenvalues 
note probabilistic non linear pca eigenvectors full kl divergence kl ln ln tr ln ln ln tr ln ln fact differentiating respect setting result zero obtain fixed point equation gives substituted back leads kl ln ln recognised difference log ratio arithmetic geometric means discarded eigenvalues 
difference zero discarded eigenvalues constant arithmetic geometric means equal positive 
difference minimised ensuring eigenvalues discard adjacent terms magnitude 
eigenvalues discard 
note retained eigenvalues larger li complex 
way true discard smallest eigenvalues retaining force eigenvalue negative 
appendix equivalence eigenvalue problems section review equivalence eigenvalue problems associated ppca 
eigenvalue problem form premultiplying gives yy 
yy eigenvectors yy see previous section matrix yy matrix orthonormal 
post multiplying sides gives yu recognised form eigenvalue problem associated ppca eigenvectors eigenvalues 
lawrence david 
latent variable models factor analysis 
charles griffin london 
alexander 
statistical factor analysis related methods 
wiley new york 
christopher bishop 
bayesian pca 
michael kearns sara solla david cohn editors advances neural information processing systems volume pages cambridge ma 
mit press 
christopher bishop james 
analysis multiphase flows dual energy gamma neural networks 
nuclear instruments methods physics research 
christopher bishop marcus svens christopher williams 
fast em algorithm latent variable density models 
touretzky michael mozer hasselmo editors advances neural information processing systems volume pages 
mit press 
christopher bishop marcus svens christopher williams 
gtm principled alternative self organizing map 
advances neural information processing systems volume pages 
mit press 
christopher bishop marcus svens christopher williams 
gtm generative topographic mapping 
neural computation 
csat 
gaussian processes iterative sparse approximations 
phd thesis aston university 
keith steven martin aaron hertzmann zoran popovic 
style inverse kinematics 
acm transactions graphics siggraph 
geoffrey hinton peter dayan brendan frey radford neal 
wake sleep algorithm unsupervised neural networks 
science 
geoffrey hinton sam roweis 
stochastic neighbor embedding 
sue becker sebastian thrun klaus obermayer editors advances neural information processing systems volume pages cambridge ma 
mit press 
antti honkela valpola 
unsupervised variational bayesian learning nonlinear models 
lawrence saul yair weiss editors advances neural information processing systems volume pages cambridge ma 
mit press 
teuvo kohonen 
self organizing map 
proceedings ieee 
joseph kruskal 
multidimensional scaling optimizing goodness fit nonmetric hypothesis 
psychometrika 
solomon kullback richard leibler 
information sufficiency 
annals mathematical statistics 
probabilistic non linear pca neil lawrence 
gaussian process models visualisation high dimensional data 
sebastian thrun lawrence saul bernhard sch lkopf editors advances neural information processing systems volume pages cambridge ma 
mit press 
neil lawrence matthias seeger ralf herbrich 
fast sparse gaussian process methods informative vector machine 
sue becker sebastian thrun klaus obermayer editors advances neural information processing systems volume pages cambridge ma 
mit press 
david lowe michael tipping 
feed forward neural networks topographic mappings exploratory data analysis 
neural computing applications 
david mackay 
probabilistic model multidimensional scaling proximity preference data 
marketing sciences 
david mackay 
bayesian neural networks density networks 
nuclear instruments methods physics research 
jan magnus heinz 
matrix differential calculus applications statistics econometrics 
john wiley sons chichester west sussex nd edition 
mardia john kent john bibby 
multivariate analysis 
academic press london 
peter maybeck 
stochastic models estimation control volume volume mathematics science engineering 
academic press new york ny 
isbn 
thomas minka 
family algorithms approximate bayesian inference 
phd thesis massachusetts institute technology 
martin ller 
scaled conjugate gradient algorithm fast supervised learning 
neural networks 
man oh adrian raftery 
bayesian multidimensional scaling choice dimension 
journal american statistical association 
anthony hagan 
bayesian numerical analysis 
jos bernardo james berger phillip dawid adrian smith editors bayesian statistics pages valencia 
oxford university press 
sam roweis lawrence saul 
nonlinear dimensionality reduction locally linear embedding 
science 
john sammon 
nonlinear mapping data structure analysis 
ieee transactions computers 
bernhard sch lkopf alexander smola klaus robert ller 
nonlinear component analysis kernel eigenvalue problem 
neural computation 
bernhard sch lkopf alexander smola 
learning kernels 
mit press 
lawrence jianbo shi jitendra malik 
normalized cuts image segmentation 
ieee transactions pattern analysis machine intelligence 
joshua tenenbaum virginia de silva john langford 
global geometric framework nonlinear dimensionality reduction 
science 
michael tipping 
topographic mappings feed forward neural networks 
phd thesis aston university aston street birmingham 
michael tipping 
probabilistic visualisation high dimensional binary data 
michael kearns sara solla david cohn editors advances neural information processing systems volume pages cambridge ma 
mit press 
michael tipping 
sparse kernel principal component analysis 
todd leen thomas dietterich volker tresp editors advances neural information processing systems volume pages cambridge ma 
mit press 
michael tipping christopher bishop 
probabilistic principal component analysis 
journal royal statistical society 
warren torgerson 
multidimensional scaling theory method 
psychometrika 
christopher williams 
computing infinite networks 
michael mozer michael jordan thomas petsche editors advances neural information processing systems volume cambridge ma 
mit press 
christopher williams 
prediction gaussian processes linear regression linear prediction 
michael jordan editor learning graphical models volume series behavioural social sciences dordrecht netherlands 
kluwer 
christopher williams 
connection kernel pca metric multidimensional scaling 
todd leen thomas dietterich volker tresp editors advances neural information processing systems volume pages cambridge ma 
mit press 

