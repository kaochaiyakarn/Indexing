scalable exploration physical database design arnd christian nig microsoft research microsoft com physical database design critical performance large scale dbms 
corresponding automated design tuning tools need select best physical design large set candidate designs quickly 
large workloads evaluating cost query workload candidate scale 
overcome novel comparison primitive evaluates fraction workload provides accurate estimate likelihood selecting correctly 
show primitive construct accurate scalable selection procedures 
furthermore address issue ensuring estimates conservative highly skewed cost distributions 
proposed techniques evaluated prototype implementation inside commercial physical design tool 
performance applications running enterprise database systems depends crucially physical database design chosen 
enable exploration potential designs today commercial database systems incorporated apis allow analysis :10.1.1.126.6699
take input query database configuration return optimizer estimated cost executing configuration 
interface key building tools exploratory analysis automated recommendation physical database designs 
problem physical design tuning defined follows physical design tool receives representative query workload wl constraints configuration space explore input outputs configuration executing wl possible cost measured optimizer cost model determine best configuration number candidate configurations enumerated evaluated analysis 
representative workload typically obtained tracing queries execute production system tools ibm query sql server profiler representative period time 
note queries include update select statements 
formulation models trade improved performance select queries maintenance costs additional indexes views 
stanford university stanford edu large number sql statements may execute time straightforward approach comparing configurations repeatedly invoking query optimizer query wl configuration tractable 
experience commercial physical design tool shows large part tool overhead arises repeated optimizer calls evaluate large numbers configuration query combinations 
research tools focused minimizing overhead evaluating carefully chosen configurations 
approach orthogonal focuses reducing number queries optimizer calls issued 
topic configurations enumerate studied extensively comment 
current commercial tools address issue large workloads compressing front initially selecting subset queries tuning smaller set 
approaches offer guarantees compression affects likelihood choosing right configuration 
guarantees important due significant overhead changing physical design performance impact bad database design 
problem study efficiently comparing configurations large workloads 
solving problem crucial scalability automated physical design tuning tools 
provide probabilistic guarantees likelihood correctly choosing best configuration large workload set candidate configurations 
approach sample workload statistical inference techniques compute probability selecting correctly 
resulting probabilistic comparison primitive fast interactive exploratory analysis configuration space allowing db administrator quickly find promising candidates full evaluation core comparison primitive inside automated physical design tool providing scalability locally decisions probabilistic guarantees accuracy comparison 
depending search strategy extended guarantees quality final result 
key challenges probabilistic approach twofold 
accuracy estimation depends critically variance estimator 
challenge pick estimator little variance possible 
second sampling techniques rely applicability central limit theorem clt derive confidence statements estimates sample variance estimate true variance underlying distribution 
unfortunately assumptions may valid scenario 
need techniques determine applicability clt workload set configurations 
contributions propose new probabilistic comparison primitive input workload wl set configurations target probability outputs configuration lowest optimizer estimated cost executing wl probability target probability 
works incrementally sampling queries original workload computing probability selecting best configuration new sample stopping target probability reached 
salient contributions derive probabilistic guarantees likelihood selecting best configuration section 
propose modified sampling scheme significantly reduces estimator variance leveraging fact query costs exhibit stability configurations section 
show reduce estimator variance stratified sampling scheme leverages commonality queries section 
describe novel technique address problem highly skewed distributions sample may representative distribution clt may apply sample size section 
remainder organized follows section review related 
section give formal description problem introduce necessary notation 
section describe sampling schemes estimate probability selecting correct configuration 
section show reduce variances stratification combine parts efficient algorithm 
section describe validate assumptions probabilistic guarantees described earlier 
evaluate techniques experimentally section 
related techniques developed related field statistical selection ranking concerned probabilistic ranking systems experimental setups series measurements system 
statistical ranking techniques typically aimed comparing systems individual measurements distributed normal distribution 
clearly case scenario 
incorporate non normal distributions statistical selection techniques batching suggested initially generate large number measurements transform raw data batch means 
batch sizes chosen large individual batch means approximately independent normally distributed 
procedures type need produce number normally distributed estimates configuration require large number initial measurements batch sizes measurements common efficiency gain due sampling scenario 
workload compression techniques compute compact representation large workload submitting physical design tool 
approaches heuristics sense means assessing impact compression configuration selection consequently physical design tuning 
poses workload compression clustering problem distance function models maximum difference cost queries arbitrary configurations 
distance function optimizer cost estimate clear approximation holds complex queries 
compresses workload selecting queries order current costs user specifiable percentage total workload cost reached 
computationally simple approach may lead significant reduction tuning quality workloads queries templates generally expensive remaining queries templates considered tuning 
making user specifiable alleviate problem user way assessing impact tuning quality 
serious drawback approaches adapt number queries retained space configurations considered 
consequently may result compression conservative resulting excessive numbers optimizer calls coarse resulting inferior physical designs 
problem statement section give formal definition problem addressed 
definition identical informal section addition parameters wl target probability introduce additional parameter describes minimum difference cost configurations care detect 
specifying value helps avoid scenarios algorithm samples large fraction workload comparing configurations nearly identical costs 
accuracy configurations necessary cases detecting large differences matters 
instance comparing new candidate configuration current overhead changing physical database design justified new configuration significantly better 
notation cost denote cost executing query configuration similarly total estimated cost executing set queries qn configuration cost qn cost qi set queries includes entire workload referred cost configuration 
interest simple notation simplifying assumption overhead making single optimizer call constant queries remainder term cost describe optimizer estimated query costs 
phrase sample query denote process obtaining query text workload table file evaluating cost query optimizer 
configuration evaluate query clear context 
probability event denoted pr conditional probability event event pr 
configuration selection problem problem formulation set physical database configurations ck workload wl qn target probability sensitivity parameter select configuration ci probability correct selection pr cs larger pr cs pr cost wl ci cost wl cj making minimal number optimizer calls 
approach solving configuration selection problem repeatedly sample queries wl evaluate pr cs terminate target probability reached 
describe estimate pr cs section estimates stratification workload construct algorithm configuration selection problem section 
discuss incorporate differences optimization costs queries section 
sampling techniques section sampling schemes derive pr cs estimates describe straightforward approach called independent sampling section describe delta sampling exploits properties query costs come accurate estimator section 
independent sampling independent sampling base sampling scheme estimate differences costs pairs configurations 
define unbiased estimator xi cost wl ci 
estimator obtained sampling set queries sli wl calculated xi sli sli cost ci xi mean sli random variables scaled total number queries workload 
variance underlying cost distribution cost wl ci cost ql ci simple random sample size estimate xi variance estimate var xi 
shorthand 
choose better configurations cl cj random variable xl xj unbiased estimator true difference costs cost wl cl cost wl cj 
large sample assumptions standardized random variable xl xj sl sll normally distributed mean variance clt 
assess probability making correct selection choosing cl cj write pr csl 
decision procedure choose configurations pick configuration cl estimated cost xl smallest 
case probability making incorrect selection corresponds event xl xj 
pr csl pr xl xj pr xl xj pr xl xj pr sll sll compute probability appropriate lookup tables 
true variances cost distributions known sample variances sli cost ci sli cost ci sli sli unbiased estimators true variances place pr csl 
consequently accurate estimation pr csl depends having sampled sufficiently values central limit theorem clt applicable able estimate accurately possible verify conditions sample single large outlier value may dominate variance skew cost distribution 
describe additional domain knowledge cost distributions verify section 
practice standard rule thumb sample nmin queries pr cs computed normality cases results clt applicable dependence sample variances means pr cs may estimated 
number configurations greater derive probability correct selection pr cs pairwise selection probabilities pr csl 
selection procedure choose configuration ci smallest estimate xi 
consider case ci chosen best configuration ck 
ci incorrect choice xi xj 
derive trivial bound pr cs bonferroni inequality pr cs pr csi 
delta sampling delta sampling variation independent sampling leverages stability rankings query costs configurations obtain significant reduction variance resulting estimator 
delta sampling pick single set queries sl wl workload estimate difference total cost configurations cl cj xl sl sl cost cl cost cj 
selection procedure pick configuration cl xl 
give bounds probability correct selection define xl sl sl variance distri bution cost qi cl cost qi cj cost differences cl cj 
derive pr csl case independent sampling 
lower bound pr cs equation case multiple configurations applies 
explain delta sampling typically outperforms independent sampling compare equation equation 
difference estimator variance delta sampling independent sampling depends difference terms sl equation sll equation weighted difference respective variances 
known covariance distributions query costs workload evaluated configurations cl cj 
delta sampling exploits follows 
large workloads typically holds ranking queries costs cl cj ranks individual query vary configurations especially configurations similar costs correspond difficult selection problems 
means cases cost query qi evaluated cl higher average cost queries cl cost evaluated cj higher average query cj vice versa average costs 
example multi join queries typically expensive single value lookups matter physical design 
covariance defined cost qi cl cost wl cl cost wl cj cost qi cj queries qi property holds add positive term summation formula signs factors equal 
consequently property holds sufficiently queries covariance positive making number optimizer calls roughly equally distributed configurations independent sampling delta sampling result tighter estimates pr cs 
verify experimentally section 
workload stratification possible reduce estimator variances stratified sampling 
stratified sampling generalization uniform sampling workload partitioned disjunct strata wl wl 
strata exhibit high variances contribute samples 
introduce stratification partitioning wl increasingly fine strata sampling progresses 
resulting algorithm configuration selection problem shown algorithm 
algorithm samples wl pr cs greater 
sample check stratification expected lower estimator variance implement 
describe detail section 
workload stratified choose stratum pick sample 
describe detail section 
heuristic large sampling configurations clearly optimal 
cl chosen configuration iteration pr csl negligible cj cj dropped iterations 
way configurations eliminated quickly ones contributing uncertainty remain pool 
study effects optimization section 
input workload wl configurations ck probability sensitivity output configuration probability pr cs cl pick pilot sample sll sll nmin 
note delta sampling sl repeat stratification beneficial create new strata sample additional queries stratum contains nmin queries section select query case independent sampling configuration ci evaluate section evaluate sections select xj best independent sampling best delta sampling sections pr cs return pr cs algorithm computing pr cs sampling preprocessing workloads large query strings fit memory write query strings database table contains query id template known query signature skeleton 
queries template identical constant bindings parameters 
template information acquired workload collection tool capable recording template information parsing queries generally requires small fraction overhead necessary optimize 
obtain random sample size table computing random permutation query ids single scan reading queries corresponding ids memory 
approach trivially extends stratified sampling 
progressive stratification strata wlh land configuration ci stratified estimates xi case independent sampling calculated sum weighted estimates stratum wlh 
variance cost distribution wlh evaluated configuration ci si wlh 
wlh number queries sampled wlh nh 
denoted variance estimator xi cost ci independent sampling var xi si wlh nh nh wlh 
pick stratification allocation sample sizes nl minimizes equation 
delta sampling variances estimators xi obtained analogously 
picking stratification initially consider case independent sampling different stratification wl xi 
discuss stratify wl configuration ci 
costs queries share template typically exhibit smaller variance costs entire workload possible get estimate average cost queries sharing template sample queries 
consequently consider queries template grouped stratum average costs template estimate strata variances mean fine grained stratification single stratum template necessarily best way stratify 
fine stratification may result small variance large sample sizes comes price order estimate stratified sampling valid number samples nh taken stratum wlh sufficiently large estimator cost configuration stratum normal 
choosing compute stratum variances resulting stratification st wl sample allocation nt nl equation 
variances estimate number samples necessary achieve pr cs set configurations omit details algorithm due space constraints essentially involves computing target variances xi estimator xi variances substituted formulas section give target probability 
stratification st initial sample allocation nt configuration ci denote minimum number samples required achieve target variance assumption underlying remain constant samples ci st nt 
estimate compare different stratification schemes 
choose stratification wl configuration ci follows sampling maintain average cost queries ci template estimate seen small number queries template 
algorithm starts single stratum st wl 
sample ci evaluates ignore finite population correction number computed log operations combining binary search neyman allocation 
change stratification queries configuration 
done iterating set possible new st templates resulting splitting existing strata pth expensive template computing samples ci st nt 
chose nt initially stratum contains maximum number queries sampled nmin 
scalability add single stratum step 
note stratification context survey sampling approximate query processing chosen stratification result complex optimization problem incur negligible computational overhead changing stratification 
details shown algorithm 
input strata wl configuration ci output stratum split wls wl wl templates store wl wl 
initialization min sam samples ci wl nl wl set expected number samples samples ci wl nl nmin order query templates average cost 
split point pm consider split wl wl templates 
compute sam samples ci wl wl wl max nmin wl max nmin wl nl sam min sam sam update target variances 
return wls template information 
algorithm computing optimum stratification overhead templates wl possible split points strata 
split point evaluate corresponding value samples requires log operations 
consequently entire algorithm runs log time 
number different templates typically orders magnitude smaller size entire workload resulting overhead negligible compared overhead optimizing single query 
note execute algorithm configuration ci sample chosen estimates sample variances remaining configura tions stay unchanged 
stratification delta sampling delta sampling problem finding stratification complicated fact delta sampling samples set queries configurations stratification scheme needs reconcile variances multiple random variables xi consequence consider reduction average variance xi comparing stratification schemes 
furthermore ordering templates xi may vary resulting orderings consider computing new candidate stratification 
reasons tractability consider single ranking xi 
picking sample consider independent sampling stratification ideally pick combination query configuration evaluate pr cs maximized 
heuristic approach attempting minimize sum estimator variances 
case independent sampling means increasing sample size var xi minimized 
don know effect query sample means estimate change sum variances assuming remain unchanged 
delta sampling sampled query evaluated configuration sample selection trivial 
workload stratified similar approach independent sampling choosing configuration stratum resulting biggest estimated improvement sum variances 
delta sampling stratification query stratum resulting biggest estimated improvement sum variances estimators chosen evaluated configuration 
approach assumes optimization times constant different optimization times template modeled computing average overhead configuration stratum pair selecting maximizing variance reduction relative expected overhead 
applicability clt discussed section estimation pr cs relies fact sampled sufficiently queries central limit theorem apply ii sample variances accurate estimators true variances assumptions hold practice potential impact choosing inferior database design desirable able validate 
follows scenario physical database design know total number queries workload obtain upper lower bounds individual costs 
compute upper bounds skew variance underlying distribution verify assumptions ii 
section describe obtain bounds costs queries sampled section describe bounds compute upper bounds skew variance 
deriving cost bounds queries problem bounding true cost arbitrary database queries studied extensively known hard 
context physical database design seek configuration best optimizer estimated cost bounds true selectivity query expressions 
simplification allows utilize knowledge space physical database designs optimizer problem tractable essential physical design tuning tuning tool influence optimizer decisions run time needs keep recommendations sync optimizer behavior 
consider issue obtaining bounds select queries 
context automated physical design tools possible determine called base configuration consisting indexes views configurations enumerated tuning process 
optimizer behaved adding index view base configuration improve optimizer estimated cost select query 
consequently cost select query base configuration gives upper bound cost configuration enumerated tuning 
lower bounds costs select query obtained reverse method cost query configuration containing indexes views may useful automated physical design tools known components suggest set structures query may benefit ultimately number structures may large 
overcome possible techniques described 
query optimizer instrumented additional code outputs individual access path considered optimization query corresponding index view optimal support access path 
reduces number relevant indexes views significantly guess access paths may relevant ii complex sql query may executed extremely large number different ways optimizer reasons scalability considers subset 
order bound costs update statements including insert delete statements standard approach splitting complex update statement select update part statement date set separated select ii update top set estimated selectivity 
select part query handled outlined 
bound cost update part observations number different update templates occurring tends orders magnitude smaller workload size cost models currently query optimizers cost pure update statement grows selectivity 
bound cost update statements specific template configuration optimizer cost estimate queries largest smallest selectivity means optimizer calls template configuration bounding cost select queries approach scales number different templates generally orders magnitude smaller size workload 
note automated physical design tuning tools issue single optimizer call queries wl derive initial costs require second optimizer call query report generation 
techniques simple conservative cost bounds tend resulting confidence bounds clt converge quickly 
exhaustive discussion bounding techniques sql queries focus interesting research challenge 
verifying validity estimators presence bounds costs queries part sample allows compute upper bound place en max estimated value pr cs conservative 
computing max maximization problem constraints defined cost intervals 
problem statement bounding variance set variables vn representing query costs vi bounded lowi vi highi compute max max vn lowi vi highi vi vi 
known problem np hard solve exactly approximate arbitrary 
best known algorithm problem requires operations practical workload sizes consider 
propose novel algorithm approximating max 
basic idea solve problem values vi multiples appropriately chosen factor bound difference solution obtained way ll refer max solution max unconstrained vi 
notation describing rounded values 
rewriting equation get max max vn vi vi 
lowi vi highi define maxv maximum value constraint low 
consequently solution max maximization problem multiples form maxv low find solution examining possible values equation 
considering multi ples round boundaries vi clos est multiples low lowi high highi 
vi take high low distinct values 
consequently average values take different values 
limitation key idea making approximation algorithm scalable values typically increase slowly number combinations different highi lowi values 
compute possible values maxv recurrence maxv max maxv low low 
ifi computation requires steps compute solution constrained maximization problem equation steps 
performance optimizations possible nd central moment distribution global maximum compact box attained boundary point take maximum minimum value 
consequently need check cases recurrence 
furthermore minimize number steps algorithm traversing values vi increasing order computing maxv 
accuracy approximation difference approximate solution max true optimum max bounded follows existence set vari ables vn lowi vi highi implies exists set multiples low high vi vi 
consequently difference value value vi equation sets values vi similarly difference vi equation vi existence solution max equation implies existence set multiples low high difference max variance bounded vi 
similarly existence solution max optimization problem multiples implies existence set variables vn lowi vi highi difference max variance vn bounded 
compute solution max optimization problem multiples implies solution unconstrained problem max implies solution max equation exists larger max 
overhead demonstrate scalability approximation algorithm measured overhead computing max tpc workload queries various values pentium ghz pc table 
time max sec 
sec 
sec 
table overhead approximating max verifying applicability clt nd assumption modeling pr cs sample size sufficiently large clt apply 
stated far sufficiently large minimum sample size nmin picked 
general result area cochran rule states populations marked positive skewness population contains significant outliers sample size satisfy fisher measure skew 
assumption disturbance moment higher rd negligible additional conditions sampling fraction shown condition guarantees confidence statement wrong time xi standardized statistic zn pr zn pr zn 
derivation additional results type studied 
details scope modification cochran rule proposed robust population sizes consider 
order verify condition need able compute upper bound problem statement bounding skew set variables vn vi bounded lowi vi highi compute max max vn lowi vi highi vi vi vi vi case variance maximization best knowledge complexity maximizing known 
approach problem approximation scheme similar max size constraints omit full description algorithm 
quick convergence clt rough bounds derived previous section allow formulate practical constraints sample size 
example highly skewed query costs vary multiple degrees magnitude query tpc workload described section satisfying equation required sample query tpc workload samples queries needed 
experiments section evaluate efficiency sampling techniques described sections 
show estimation pr cs construct scalable accurate comparison primitive large numbers configurations 
compare approach existing experimentally 
setup experiments run pentium ghz pc cost model sql server optimizer 
evaluated techniques databases synthetic real life 
synthetic database follows tpc schema generated frequency attribute values follows zipf distribution skew parameter 
total data size gb 
workload consisting queries generated standard tool 
real life database database running crm application tables size gb 
obtained workload database trace tool resulting workload contains queries inserts updates deletes 
evaluating sampling techniques section evaluate efficiency different sampling techniques described independent delta sampling progressive stratification 
experiment tpc workload consider problem choosing configurations significant difference cost respective sets physical design structures index contains number views 
note wl solving problem exactly requires optimizer calls 
experiment run sampling scheme sample size output selected configuration 
process repeated times resulting monte carlo simulation compute true probability correct selection number samples 
results shown 
see probability correct selection sample size independent sampling delta sampling independent sampling stratification delta sampling stratification monte carlo simulation pr cs sampling approach configuration selection efficient number optimizer calls required compute configuration costs exactly suffice select correct configuration near certainty 
delta sampling outperforms independent sampling significantly small sample sizes adding progressive stratification little difference small sample sizes 
experiment illustrates issues initially picking fine stratification workload 
run experiment sampling schemes having partitioned workload strata query template 
fine stratification small sample sizes estimates stratum normal probability correct selection significantly lower 
large sample sizes accuracy fine stratification comparable progressive stratification scheme 
hold true number different experimental setups 
experiment uses workload configurations significantly harder distinguish difference cost share significant number design structures configurations index 
delta sampling outperforms independent sampling bigger margin configurations share large number objects resulting higher covariance cost distributions 
larger sample sizes stratification significantly improves accuracy independent sampling 
conducted experiments real life database configurations difficult compare difference cost little overlap physical design structures 
consequently advantage delta sampling pronounced 
furthermore workload contains relatively large number distinct templates 
means rarely estimates avg 
cost templates progressive stratification cases 
probability correct selection probability correct selection sample size progressive vs fine stratification sample size independent sampling prog 
stratification delta sampling prog 
stratification independent sampling simple stratification delta sampling simple stratification independent sampling delta sampling independent sampling prog 
stratification delta sampling prog 
stratification monte carlo simulation pr cs experiments multiple configurations section evaluate accuracy scalability comparison primitive pr cs estimates large numbers configurations collected commercial physical design tool 
run algorithm determine best configuration probability delta sampling progressive stratification sampling technique 
experiments sample variances estimate order guard oscillation pr cs estimates accept pr cs condition holds consecutive samples 
additional optimization sampling configurations cj contribution uncertainty pr cs negligible case pr csl 
compare primitive alternative methods identical number samples sampling stratification sampling number queries stratum 
methods report resulting probability selecting best configuration true pr cs maximum difference cost best configuration selected method max 

allows assess worst case impact alternative techniques 
experiment carried times tpc table cases crm comparison database table primitive 
delta sampling performs significantly better alternatives 
true pr cs resulting selection probability correct selection sample size independent sampling delta sampling independent sampling prog 
stratification delta sampling prog 
stratification monte carlo simulation pr cs method delta sampling true pr cs max 
strat 
true pr cs max 
equal alloc 
true pr cs max 
table results tpcd workload primitive matches target probability closely exceeds demonstrating capability chose sample size effectively match accuracy requirement 
comparison workload compression section compare existing workload compression approach key evaluation criteria scalability quality adaptivity 
queries selected order costs current configuration prespecified percentage total workload cost selected 
obviously method scales 
regarding quality technique fails query templates contain expensive queries 
illustrate generated query tpc workload tool 
capture queries corresponding tpc query templates 
consequently tuning compressed workload fails yield design structures beneficial remaining templates 
show tuned different random samples size compressed workload improvement entire workload resulting tuning sample twice improvement resulting tuning compressed workload 
evaluate quality resulting measured difference improvement tuning delta sample compressed workload size tpc workload experiment approaches performed comparably 
regarding scalability requires wl complex distance computations preprocessing step high pr cs crm workload due requirement pr cs hold consecutive samples causes sampling easy selection problems 
method delta sampling true pr cs max 
strat 
true pr cs max 
equal alloc 
true pr cs max 
table results crm workload clustering problem 
contrast overhead executing algorithms negligible necessary counters measurements maintained incrementally constant cost 
biggest difference technique concerns adaptivity 
depend initial guess sensitivity parameter 
maximum allowable increase estimated running time queries discarded percentage total cost retained 
clear set parameter correctly deterministic techniques allow formulation probabilistic guarantees similar ones sensitivity parameter set front configurations space account 
observed experiments fraction workload required accurate selection varies significantly different sets candidate configurations 
choosing sensitivity parameter incorrectly significant impact tuning quality speed 
algorithm contrast offers principled way adjusting sample size online 
outlook scalable comparison primitive solving configuration selection problem providing accurate estimates probability correct selection 
primitive crucial importance scalable exploration space physical database designs serve core component automated physical design tools 
described novel scheme verify validity central limit theorem sample variances compute pr cs 
techniques limited problem domain clt estimator required bounds individual values underlying distribution obtained 
benefitted tremendously insightful comments vivek narasayya surajit chaudhuri sanjay agrawal amin 
agrawal chaudhuri database tuning advisor microsoft sql server 
proc 
th vldb conf toronto canada 
bruno chaudhuri 
automatic physical database tuning relaxation approach 
acm sigmod conf 
casella berger 
statistical interference 
duxbury 
chaudhuri das robust optimization approach answering aggregate queries 
proc 
acm sigmod conf 
chaudhuri gupta compressing sql workloads 
proc 
acm sigmod conf 
chaudhuri nig continuous monitoring framework relational database engines 
proc 
th cde conf 
chaudhuri narasayya 
efficient cost driven index selection tool microsoft sql server 
proc 
rd vldb conf athens greece 
chaudhuri narasayya :10.1.1.126.6699
index analysis utility 
proc 
acm sigmod conf seattle wa usa 
cochran 
sampling techniques 
wiley 
das automatic sql tuning oracle 
proc 
th vldb conf 
ginzburg computing variance interval data np hard 
acm sigact news vol 
pages 
ginzburg exact bounds finite populations interval data 
technical report cs university texas el paso 
finkelstein physical database design relational databases 
acm tods 
ioannidis christodoulakis 
optimal histograms limiting worst case error propagation size join results 
acm tods 

kim nelson 
selecting best system theory methods 
proc 
winter simulation conf pages 
computing higher central moments interval data 
technical report cs university texas el paso 
steiger wilson 
improved batching confidence interval construction steady state simulation 
proc 
winter simulation conf 
lohman 
leo db learning optimizer 
proc 
th conference large databases 
smith cochran rule simple random sampling 
journal royal statistical society pages 
rao db design advisor integrated automatic physical database design 
proc 
th vldb conf canada 
