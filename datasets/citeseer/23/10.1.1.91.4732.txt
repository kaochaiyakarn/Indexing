contents distance metric learning comprehensive survey liu yang advisor rong jin department computer science engineering michigan state university may supervised global distance metric learning pairwise constraints 
global distance metric learning convex programming 
probabilistic approach global distance metric learning 
supervised local distance metric learning local adaptive distance metric learning 
problem setting 
methodology 
local feature relevance 
local linear discriminative analysis 
locally adaptive feature relevance analysis 
adaptive kernel metric nearest neighbor classification 
local adaptive distance metric learning svm 
discussions 
neighborhood components analysis 
relevant component analysis 
brief review rca 
kernel relevant component analysis 
incremental learning 
unsupervised distance metric learning problem setting 
linear methods 
nonlinear methods 
lle locally linear embedding 
isomap 
laplacian eigenmaps 
connections isomap lle laplacian eigenmaps spectral clustering 
unified framework dimension reduction algorithms 
solution 
solution 
sample extensions 
distance metric learning svm review svm 
large margin nearest neighbor distance metric learning 
objective function 
reformulation sdp 
cast kernel margin maximization sdp problem 
definition margin 
cast sdp problem 
apply hard margin 
apply soft margin 
kernel methods distance metrics learning review kernel methods 
kernel alignment 
kernel alignment sdp 
learning idealized kernel 
ideal kernel 
idealized kernel 
formulation distance metric learning 
primal dual 

machine learning algorithms nearest neighbor knn heavily rely distance metric input data patterns 
distance metric learning learn distance metric input space data collection pair similar dissimilar points preserves distance relation training data 
years studies demonstrated empirically theoretically learned metric significantly improve performance classification clustering retrieval tasks 
surveys field distance metric learning principle perspective includes broad selection 
particular distance metric learning reviewed different learning conditions supervised learning versus unsupervised learning learning global sense versus local sense distance matrix linear kernel versus nonlinear kernel 
addition discusses number techniques central distance metric learning including convex programming positive semi definite programming kernel learning dimension reduction nearest neighbor large margin classification graph approaches 
learning distance metric feature space crucial real world application 
distance metrics important computer vision tasks image classification content image retrieval 
example retrieval quality content image retrieval cbir systems known highly dependant criterion define similarity images motivated significant research learning distance metrics training data 
distance metrics critical image classification applications 
instance nearest neighbor knn classifier key identify set labeled images closest test image space visual features involving estimation distance metric 
previous shown distance metrics significantly benefit knn classification accuracy compared standard euclidean distance 
considerable research distance metric learning past years 
depending availability training examples algorithms distance metric learning divided categories supervised distance metric learning unsupervised distance metric learning 
supervised learning algorithms training examples class labels training examples supervised distance metric learning cast pairwise constraints equivalence constraints pairs data points belong classes constraints pairs data points belong different classes 
supervised distance metric learning divided categories global distance metric learning local distance metric learning 
learns distance metric global sense satisfy pairwise constraints simultaneously 
second approach learn distance metric local setting satisfy local pairwise constraints 
particularly useful information retrieval knn classifiers methods influenced data instances close test query examples 
section section devoted review supervised distance metric learning 
existing unsupervised distance metric learning methods section 
section discuss maximum margin distance metric learning approaches 
kernel methods distance metrics summarized section 
supervised global distance metric learning approaches category attempt learn metrics keep data points classes close separating data points different classes far apart 
representative category formulates distance metric learning constrained convex programming problem 
learns global distance metric minimizes distance data pairs equivalence constraints subject constraint data pairs constraints separated 
section organized 
start pairwise constraints 
review framework global distance metric learning 
probabilistic framework global distance metric learning represented section 
pairwise constraints typical supervised learning training example annotated class label label information distance metric learning usually specified form pairwise constraints data equivalence constraints state pair semantically similar close learned metric constraints indicate points semantically dissimilar near learned metric 
learning algorithms try find distance metric keeps data pairs equivalence constraints close separating constraints 
features weights adjusted adaptively test point reflect importance features determining class label test point 
distance function information geometry learned labeled examples reflect geometric relationship labeled examples 
distance metric explicitly learned minimize distance data points equivalence constraints maximize distance data points constraints 
xn collection data points number samples collection 
xi data vector number features 
set equivalence constraints denoted xi xj xi xj belong class set constraints denoted xi xj xi xj belong different classes distance metric denoted matrix distance data points expressed global distance metric learning convex programming equivalence constraints constraints formulated problem metric learning convex programming problem min xi xj xi xj xi xj xi xj note positive semi definitive constraint needed ensure negative distance data points triangle inequality 
problem falls category convex programming may solved efficiently reasons fall special class convex programming quadratic programming semi definite programming 
result solved generic approach unable take advantage special structure problem 
second pointed number parameters quadratic number features 
property difficult scale large number features 
disadvantage unable estimate probability data points share class 
algorithm extended nonlinear case kernels 
authors dual reduce computation complexity original optimization problem 
discuss kernel version section 
probabilistic approach global distance metric learning computation complexity original optimization problem simplify calculation probabilistic framework global distance metric set 
idea assume logistic regression model estimating probability data points xi xj share class pr yi xi xj yi exp yi xi xj xi xj xi xj parameter threshold 
data points xi xj class label distance xi xj threshold 
log likelihood equivalence constraints constraints written lg log pr log pr log exp xi xj xi xj xi xj log exp xi xj maximum likelihood estimation cast problem distance metric learning optimization problem min rm lg difficulty solving lies positive semi definitive constraint 
simplify computation model matrix eigenspace training instances xs 

xn include training instances constraints include pairwise correlation features 
vi top eigenvectors matrix assume linear combination top eigenvectors 

non negative weights linear combination 
parametric form written pr yi xi xj exp yi kwk xi xj xi xj log likelihood function xi xj xi xj log log exp exp kw kw optimization problem simplified form min 
notice optimization problem convex programming problem solved directly newton method 
furthermore framework allows incorporation unlabeled data 
matrix constructed labeled data unlabeled data 
supervised local distance metric learning local adaptive distance metric learning addition general purpose algorithms distance metric learning papers approaches learn appropriate distance metrics knn classifier 
specifically approaches tried find feature weights adapted individual test examples 
refer approaches local adaptive distance metric learning 
problem setting consider discrimination problem classes training data samples 
training dataset xn known class labels denoted yn xi yi 
cj represent set training samples class goal predict class label testing sample predictor vector 
assume data generated unknown distribution 
testing sample task reduces estimating class posterior essentially speaking nearest neighbor approach assumes small 
neighborhood denotes number points 
simplest estimate indicator function xi yi xi indicator function returns input argument true 
assumption smoothness neighborhood hold input observations approach class boundaries dimensionality large 
consequently modified local neighborhood posterior probabilities approximately constant need produced locally adaptive metric techniques nearest neighbor classification setting 
methodology nearest neighbor method classifies frequent class neighbors training set 
extremely flexible method assumption training data 
furthermore nearest neighbor method supported theoretical argument asymptotic error rate nearest neighbor twice bayes error rate independent distance metric 
refer detailed discussion knn 
case finite samples high dimensional space curse dimensionality hurt nearest neighbor rule 
crucial assumption knn approach class conditional probabilities local nearest neighbor constant 
assumption relaxed assuming conditional probabilities neighborhood test examples smooth slow changing function 
necessarily true 
instance area close decision boundary classes expect class labels change dramatically range short distance 
order preserve smoothness neighborhood terms class conditional probability distance change label tends large data points having inconsistent labeling query point excluded neighbor query example 
squeeze distance include points neighborhood query point share class labels query point words goal adaptive feature relevance learning obtain neighborhood testing point high consistency assigning class labels 
gives cases may cause inconsistency class conditional probabilities local neighborhood 
case data sparseness caused curse dimension second case bumpy class distribution local neighborhood query point close decision boundary 
research motivated cases significant done learning local distance metrics adaptive query point modified spacial resolution 
case local adaptive distance metric learning essentially determines feature relevance query 
resulting neighborhood elongated relevant feature dimensions influential ones 
consequently dimension low feature relevance value eventually eliminated similar feature selection adapted query point 
second case discriminative function learned algorithms svm learn distance metric increase spatial resolution decision surface decrease spatial resolution 
specifically direction perpendicular decision boundary class labels change dramatically distance elongated exclude points inconsistent class labels neighborhood query point direction decision boundary class labels change distance shrunk 
concept local feature relevance originated learns flexible metrics capture local feature relevance 
specifically uses recursive partitioning strategy adaptively shrink shape rectangular neighborhood test point 
learning approach combines strength knn method recursive partitioning method 
proposes adaptive nearest neighbor classification method local linear discriminative analysis lda 
specifically lda analysis applied query data point weighting training examples distance query point 
discriminative directions identified local lda distance measurement 
interestingly illustrated relationship chi square statistics local lda local lda viewed approximation chi square statistics assume mixture gaussian distribution class 
similar idea proposed discriminative direction computed line joining training examples different classes close test example 
developed unified theory adaptive metric encompasses strength 
inspired similar observation developed kernel version adaptive local distance measurement 
distance metric learning method improve svm increases spatial resolution decision surface riemannian geometry 
parallel done computes local flexible metric svms 
rest section review concept local feature relevance defined 
reveal details 
introduce unified framework local adaptive distance metric learning kernel extension 
review algorithm adaptive local distance metric svm 
local feature relevance concept local feature relevance introduced 
motivation feature relevance estimation comes need exploit differential relevance input measurement variables class assignment 
briefly review idea 
squares estimate predicting just expected value joint probability density ef dx 
restriction xi squares prediction xi xi dx 
xi represents probability density distribution input variables ith variable xi xi dx xi dirac delta function property dx 
improvement squared prediction error associated knowing value ith input variable xi ef xi xi xi ef xi clearly assumptions pi xi aixi easily calculated 
case xi xi xi dxi average value ith input variable 
general suppose fi xi get fi 
reflects influence ith input variable variation point xi consider arbitrary point zn dimensional feature space 
measure relevance ith input variable xi variation zi zk independent xi depends xi generalized definition local feature relevance changing conditioning single point subregion includes specifically define dz dx function input argument true 
measures relevance ith dimension variation region 
local linear discriminative analysis posterior probabilities neighborhood homogenous modifies neighborhood distance metric estimation called local linear discriminative analysis 
estimated distance metric shrinks neighborhoods directions orthogonal local decision boundaries parallel boundaries 
discovers elegant connection local lda chi squared distance true estimated posterior justify proposed metric computing neighborhood 
details 
briefly review standard linear discriminant analysis lda classification procedure classes 
discriminative feature transform lda finds eigenvectors matrix sw sb 
sb denotes class covariance matrix covariance matrix class means sw denotes class covariance matrix weighted sum covariance matrices class 
captures compactness class sb represents separation class means 
principle eigenvectors keep data points classes close separate data points different classes far apart 
form transform matrix st stacking principle eigenvectors discriminative features computed original input patterns test example 
standard lda proposed localize sb sw iterative procedure initializes distance metric identical matrix euclidean distance metric 
step calculates sb sw points neighborhood testing point measured distance metric 
second step estimated sb sw update distance metric follows sw sw sw sw sb sw steps computing local lda updating local distance metric iterated alternatively converges 
note essentially comes equation sw sw sw sw sw sb sw introduced prevent neighborhood infinitely long complement sphered space 
denote projection sb sphered space sw 
essentially sw obtain projection distance sphered space sb local neighborhood discloses consistency class centroids 
consequently metric defined shrinks neighborhood directions local centroids differs neighborhood directions class centroids close 
sense goal consistent general goal local adaptive distance metric learning 
furthermore justifies proposed metric showing item approximates chi squared distance true estimated posterior test point 
represent neighborhood test point 
true probability class point chi square distance true estimation approximate estimation posterior class probabilities location expressed assume class conditional density gaussian distribution mean uj covariance matrix class 
order taylor approximation pr point pr pr pr pr 
approximation simplified pr approximated distance metric sw pr pr sb pr metric written exact form 
assuming sphered space obtained 
locally adaptive feature relevance analysis computes distance true estimated posteriors 
consider neighbor testing sample 
chi squared distance tell extent ith dimension relied predicting pr 
achieved computing expectation pr conditioned ith dimension estimating relevance ith dimension ability predicting class posterior probabilities locally 
specifically defines measure feature relevance testing point ri follows ri pr pr xi zi pr xi zi pr xi zi conditional expectation pr calculated pr xi zi pr xi zi pr xi zi dx indicated definition feature relevance ri measures distance pr conditional expectation pr location closer pr xi zi pr information ith dimension provides predicting class posterior probabilities locally furthermore defined expected relevance value ith feature averaging ri vicinity query point ri ri denotes neighborhood metric 
similarly small ri implies class posterior test example approximated ith dimension vicinity 
definition ri relative relevance ith feature defined wi ri rl ri max rj ri 
parameter set corresponds linear quadratic weighting 
define relative relevance exponential weighting wi exp cri exp crl chosen adjust influence ri relative relevance wi 
compared polynomial weighting exponential weighting sensitive changes local feature relevance 
distance computed wi xi yi weights wi enable neighborhood important feature dimensions influential ones 
adaptive kernel metric nearest neighbor classification extension proposed new kernel kernel nearest neighbor classification 
particular proposed kernel adjusts rbf kernel introducing weights local consistency class labels labeling uncertainty 
mapping defined sample testing sample positive function assuming corresponding kernel distance weight function defined pr pr jm jm arg max pr jm 
distance written follows pr jm pr jm pr jm computational efficiency rbf kernel replaced taylor expansion second order 
exp simplification leads weighted combination chi squared distance mahalanobis distance 
pr jm pr jm pr jm term expression zero reduces distance mahalanobis distance weighted 
interesting note different roles weighting functions play distance measure 
particular measures consistency class labels vicinity smaller consistency class labels neighborhood 
measures labeling uncertainty smaller uncertain class label 
product measures labeling uncertainty label consistency neighborhood 
essentially sample risk defined defined represent class consistency 
defined defined measure degree uncertainty labeling maximum likelihood class 
shortage chi squared distance adopted ignores direction variation pr pr 
disclosed information captured 
shows dilation contraction distance due variation proportional square root chi squared distance 
direction variation pr pr decide degree mahalanobis distance modified drive neighborhood modified distance measure closer homogeneous class posterior probabilities say determine dilation contraction 
pr jm pr jm pr jm represents sign pr jm pr jm 
local adaptive distance metric learning svm proposed technique computes locally flexible distance metric svms 
similar weights features relevance class conditional probabilities query example 
feature relevance computed chi square distance measures feature relevance discriminative directions identified svms 
specifically decision function constructed svms determine discriminative direction neighborhood query example 
identified direction provide local feature weighting scheme 
furthermore learned distance metric fed back svms increase performance maximum margin 
empirically combining strength local adaptive metric learning local discriminative direction obtained svms improves classification performance svm 
distance direction parallel decision boundary distance direction perpendicular decision boundary class conditional probabilities resulting neighbor tend constant 
recall svms classify patterns classification function ns xi ns number support vectors 
kernel function defined mapping input space higher dimensional feature space 
class label test example determined sign classification function decision boundary determined equation 
gradient vector nd df computed point level curve gives perpendicular direction decision boundary input space vector nd identifies orientation input space projected training data separated locally neighborhood 
information define local measure feature relevance 
review definition feature relevance measurement local discriminative direction 
consider query point close decision boundary 
closest point boundary 
class conditional probabilities change significantly direction nd need increase distance measurement direction close nd order exclude points different class labels query example 
distance reduced direction away nd class labels direction tends uniform 
follow measure feature relevance defined define ri nd nd nd nd nd ui zero vector ith element 
similar previous discussion weights ri dimensions informative dimensions critical prediction class labels 
listed detailed steps local flexible metric classification svms computer approximated closest point boundary computer gradient vector nd df set feature relevance values ri nd 
estimate distance boundary follows compute feature relevance min si si wi exp ari 
exp ari sis nonzero support vectors variable defined averaged minimum distance nonzero support vectors entire training dataset computed follows ns xk min xk si si note factor determine query example close decision boundary 
specifically larger closer query example decision boundary 
resulting 
wm knn classification query point 
discussions provide discussions local adaptive distance metric learning approaches reviewed 
define relevance measure features driven different motivation 
identify dimensions expected variation class posterior probability maximized aims find dimension minimize difference true class probability distribution query example estimation training examples neighborhood query example 
major difference 
metric computed approximates weighted chi squared distance series expansion assumption class densities gaussian covariance matrix practical real world application 
assumptions approximates weighted chi squared distance taylor expansion 
neighborhood components analysis neighborhood component analysis nca algorithm proposed learns mahalanobis distance metric knn classifier maximizing leave cross validation 
briefly review central idea nca 
labeled data set denoted 
xn cn 
ensure learned distance matrix symmetric positive semi definite assumes distance metric form matrix 
parametric form guarantees distance data points positive fact ax ay ax ay 
point xi soft neighbor xi defined pij probability xj selected neighbor xi shares class label xi 
probability pi defined pij exp axi exp axi set points share class xi denoted ci ci cj 
probability classifying xi correctly expressed pi ci pij expected number correctly classified points pi 
order derivative respect pi pi xi xk xi xk pi xi xj xi xj cj average classification accuracy suggests leave oneout cross validation objective function log ci pi nca drawbacks nca suffers scalability problem objective function differentiated distance matrix number parameters quadratic number features 
updating distance matrix intractable large dimensionality 
gradient ascent algorithm proposed nca guarantee converge local maxima 
nca tends overfit training data number training examples insufficient learning distance metric 
happens data points represented high dimensional space 
relevant component analysis rca developed unsupervised learning equivalence relations 
founded information theoretic basis closed form expressions data rca algorithm simple efficient algorithm learning full ranked mahalanobis distance metric 
constructs mahalanobis distance metric weighted sum class covariance matrices 
similar pca linear discriminant analysis depend second order statistics 
number studies shown success learning distance metric learning rca 
section give brief review rca followed application distance metric learning 
discuss kernel version rca study 
brief review rca relevant component analysis rca applies global linear transformation assign large weights relevant dimensions low weights irrelevant dimensions 
relevant dimensions estimated 
rca defined subset points known belong unknown class 
detailed steps rca mean points compute covariance matrix centered data points 
points contains points xji nj mean mj covariance matrix rca computed nj xji mj xji mj apply whitening transformation associated covariance matrix original data points xnew wx 
inverse mahalanobis distance 
information maximization constraints formulates problem rca constrained optimization problem 
follows information theoretic criterion proposed 
searches transformation input patterns maximizes mutual information transformed data points rn suitable constraints 
accordingly set xl transformed set points xl rm goal find function family maximizes 
addition transformation function required keep data points close 
consequently problem cast optimization problem max nj yji denotes mean data points jth transforma tion total number points threshold constant 
transformation function deterministic uncertainty regarding maximizing mutual information equivalent maximizing entropy 
jacobian transformation py dy px dx 
expressed terms follows log dy log dx log term depends transformation jacobian 
jacobian linear transformation ax 
written max nj xji mj denote mahalanobis distance matrix positive definite log 
way written log max nj xji mj solving lagrangian gives solution rca average covariance matrix dimension feature space 
apply rca distance metric learning addresses problem learning distance metrics side information rca algorithm simple efficient algorithm learning full ranked mahalanobis metric 
demonstrates permission singular mahalanobis matrix rca top fisher linear discriminant optimal dimensionality reduction algorithm criterion 
related rca reviewed details sections 
kernel relevant component analysis similar kernel pca shows rca kernelized elegant matrix manipulations 
experiments shows significant improvements achieved linear version especially nonlinearities needed 
presents learning algorithm incremental setting 
assume cj comprises nj data points xj xj nj 
data point xj denote dimensional pattern cj ij matrix diag 
patterns stacked matrix xj xj xj nj 
written matrix form nj xj xj ij nj nj nj nj ij nj rn symmetric block diagonal positive semi definite 
plays similar role centering matrix pca 
note singular solve problem regularizer introduced small positive constant 
leads expression woodbury formula bc ca ca inverse computed efficiently follows xh xt xh extend rca kernelized version compute dot product patterns xh xt xh nonlinear mapping kernel function dot product calculated kt kh ky xi xj ij kernel matrix defined patterns kx xj nj ky yj nj incremental learning equation requires computing kh new computationally expensive 
presents efficient learning algorithm incremental updating having recompute inverse large matrix 
defining value written 
woodbury formula xh kh xt set processed denoted containing na patterns new containing nb patterns 
corresponding matrices xa rd na xb rd nb respectively 
decomposed kab ab ha hb kab axb bxb ha corresponds processed hb nb nb vector 
denote za fact ca get kh za kt ca kt kt ab denote ya kh ya hb complexity algorithm fully discussed 
total computational complexity nb nan 
fact nb na complexity naive approach na nb 
conclusively kernelized rca extends ability rca produce nonlinear transforms input space 
incremental update procedure allows kernel rca transform computed efficiently adaptive environment 
unsupervised distance metric learning unsupervised distance metric learning called manifold learning main idea learn underlying low dimensional manifold geometric relationships distance observed data preserved 
deep connection unsupervised distance metric learning dimension reduction 
dimension reduction approach essentially learn distance metric label information 
example classical dimension reduction approach principle component analysis pca viewed special distance metric 
specifically principle eigenvectors ui covariance matrix construct distance metric distance measured 
essential connection emphasis section review dimension reduction 
problem setting dimension reduction problem data set xn find set points yn yi represents counterpart xi 
convenience presentation denote matrix xn correspondingly matrix yn 
consider special case 
xn manifold embedded rm 
dimension reduction algorithms proposed 
table summarizes dimension reduction algorithms linear nonlinear global local 
start discussion linear dimension reduction approach including principle component analysis pca multiple dimension scaling mds followed discussion nonlinear approaches including isomap local linear embedding lle laplacian 
section unified framework dimension reduction algorithms 
linear nonlinear global pca mds isomap local llp lle laplacian table algorithms dimension reduction linear methods task dimensionality reduction find small number features represent large number observed dimensions 
classical linear algorithms includes principle component analysis pca multidimensional scaling mds 
principal component analysis pca finds subspace best preserves variance data 
formally speaking find orthonormal basis um maximize variance yi xi 
xi centered mean input space var var xx xx var 
assume pca pca eig pca eigen matrix containing eigenvectors columns pca diagonal matrix diagonal elements corresponding eigenvalues 
matrix maximizes composed eigenvectors denoted pca cor responding eigenvalues denoted pca consequently pca projections rank pca multidimensional scaling mds finds rank projection best preserves inter point distance dissimilarity pairwise distance matrix detailed steps calculate xt centering matrix rm vector 
spectrally decompose vmds mds vmds vmds mds eig 
mds mds rank projections closet vm mds vmds mds top eigenvectors eigenvalues respectively 
relations pca mds revealed equations pca xv mds pca mds pca pca mds conclusively euclidean case mds differs pca starting calculating starting useful euclidean dissimilarity function data 
pca euclidean mds simple efficient guarantee optimize criterions 
linear approaches find nonlinear structure data 
motivates series nonlinear dimension reduction methods 
nonlinear methods significant research done nonlinear dimension reduction 
follow categorization scheme divide algorithms categories 
embedding methods embedding algorithms divided global local 
global algorithm isomap assumes isometric properties preserved observation space intrinsic embedding space affine sense 
assumption isomap finds subspace best preserves geodesic interpoint distances 
extensions conformal mappings discussed 
isomap tries preserve geodesic distance pair data points locally linear embedding lle laplacian focus preservation local neighbor structure 
extension locality preserving projection lpp finds linear projective maps optimally preserve neighborhood structure data set 
optimal linear approximation eigen functions laplace beltrami operator manifold 
laplace beltrami operator self adjoint elliptic differential operator defined exterior derivative df xi dxi written coordinate chart adjoint respect inner product 
mutual information methods group methods assumes mutual information measurement differences probability distribution observed space embedded space 
related includes stochastic nearest neighborhood sne manifold charting 
projection methods group methods geometrically intuitive 
goal find principal surfaces passing middle data example principal curves 
difficulty lies generalize global variable arc length parameter higher dimensional surface 
generative methods group methods adopts generative topology models hypothesizes observed data generated evenly spaced low dimensional latent nodes 
mapping relationship observation space latent space modeled 
em expectation maximization algorithms generative models easily fall local minimum slow convergence rates 
survey emphasize nonlinear embedding methods including isomap lle laplacian 
review methods discuss underlying connections 
lle locally linear embedding locally linear embedding lle algorithm local method establish mapping relationship observed data corresponding low dimensional data 
principle lle algorithm preserve local order relation data embedding space intrinsic space 
sample observation space linearly weighted average neighbors 
basic lle algorithm local covering numbers described follows find nearest neighbor xi dataset 
assuming data point neighbors lie locally linear patch manifold local geometry patches characterized linear coefficients reconstruct data point neighbors 
assumption reconstruction error measured adding squared distance data points reconstructions xi weight matrix wij summarizes contribution jth data point ith reconstruction 
weighted matrix obtained solving square problem 
constraints regarding wij data point xi 
wij xj neighbor xi 
constraints important solution constraints characterizes intrinsic geometric properties neighborhood invariant rotations rescalings translations data point neighbors 
construct approximation matrix 
choose wij minimizing xi condition wij xi 
construct neighbor preserving mapping 
idea reconstruction weights wij reflect intrinsic geometric properties data invariant linear transformation high dimensional coordinates neighborhood global internal coordinates low dimensional manifold 
expectation wij reconstruct ith data point dimensions reconstruct embedded manifold coordinated dimensions 
equivalent approximate nonlinear manifold point xi linear hyperplane passes neighbors xi xik 
minimizing cost function yi argmin get argmin 
constraints 
yi enforcing objective invariant translation avoid degenerate solution number local covering set 
reduced eigenvector decomposition argmin problem argmin minimize cost function need compute embedding eigen analysis symmetric positive matrix 
lowest eigenvalues taken smallest eigenvector discarded considering constraint terms 
high dimensional xi rm mapped swiss roll data set illustrating isomap exploits geodesic paths nonlinear dimensionality reduction 
low dimensional vector yi yi represents global internal coordinates manifold 
disadvantage lle related improvement lle algorithm requires recomputing eigen spectrum entire matrix test example computationally expensive 
corresponding improvement 
approximation theorem continuous function closed bounded interval uniformly approximated interval polynomials degree accuracy 
theorem mla algorithm uses gaussian rbf kernel approximate relationship 
testing point internal coordinates manifold computed ik xi xi exp xi complete data 
isomap computed challenge nonlinear dimension reduction illustrated example data lying dimensional swiss roll 
leftmost picture shows arbitrary points circled nonlinear manifold euclidean distance high dimensional input space length dashed line may accurately reflect intrinsic similarity measured geodesic distance low dimensional manifold length solid curve 
points far apart underlying manifold measured geodesic distances length shortest path may appear deceptively close high dimensional input space measured straight line euclidean distance 
geodesic distance capable revealing true low dimensional geometry manifold 
pca mds see euclidean structure fail detect intrinsic dimensionality 
address challenge propose approach named isomap exploit eigen analysis nonlinear embedding 
detailed steps isomap algorithm follows set neighbor relations manifold pairwise euclidean distance dx input space realized fixed neighbor size fixed distance range 
identified neighbor relations represented weighted graph data points weight dx assigned corresponding edges 
picture middle shows neighborhood graph constructed step allows approximation solid segments true geodesic path computed efficiently step shortest path estimate pairwise geodesic distance dm manifold geodesic distance defined distance shortest path dg graph neighboring points input distance approximation geodesic distance far away points geodesic distance adding sequence short hops neighboring points shortest paths graph edges connecting neighboring data points 
apply mds matrix geodesic distance dg dg dg construct embedding data dimensional euclidean space best preserve estimated intrinsic geometry manifold 
rightmost picture shows dimensional embedding recovered isomap step best preserves shortest path distances neighborhood graph overlaid 
straight lines embedding represent simpler cleaner approximations true geodesic paths corresponding graph paths curves 
laplacian eigenmaps laplacian eigenmaps method locality preserving nonlinear dimension reduction method 
eigenmaps provided eigenvectors graph laplacian eigenfunctions laplace beltrami operator manifold 
map generated discrete approximation continuous map 
specifically data set xn construct weighted graph nodes point set edges connecting neighboring points 
eigenvectors graph laplacian construct embedding map input space embedded manifold 
detailed steps listed constructing adjacency graph 
nodes xi xj connected close 
closeness criterions xi xj nearest neighbor node node connected edge node nearest neighbors node node nearest neighbor node weighting edges 
rigid weighting soft weighting 
rigid weighting wij nodes connected wij 
soft weighting nodes connected wij wij 
need chosen carefully 
calculate 
connected graph graph connected go connected components solve generalized eigen decomposition problem lf df 
diagonal weight matrix entries column sums dii wji laplacian matrix 
eigenvalues fk corresponding eigenvectors 
leaving zero eigenvalue top eigenvectors obtain embedded mapping xi fm justification simplest case reveals scheme laplacian preserve locality dimension reduction process suggests simplest case 
dataset xn connected weighted graph 
imagine mapping weighted graph line get corresponding map yn connected points stay close possible 
case minimizing objective function yi yj wij ensures xi xj close yi yj close 
precisely wij large means xi xj close minimization objective function force yi yj small means yi yj close 
yi yj wij simplest case shows written ly minimization problem reduced argmin ly dy 
constraint dy prevent minimization problem go trivial solution scaling factors dii represents importance vertex second constraint remove translation invariance sure solution eigenvectors smallest nonzero eigenvalue 
generalization problem embedding graph line generalized problem embedding graph dimensional euclidean space 
suppose ym ith row dimension representation xi 
minimizing reduced find wij argmin yt tracey dy ly dimensional case constraint dy prevents collapse subspace dimension 
solution optimization problem provided matrix eigenvectors corresponding lowest nonzero eigenvalues generalized eigenvalue problem ly dy 
connections isomap lle laplacian eigenmaps spectral clustering important observation process dimensionality reduction preserves locality yields solution clustering 
essentially local approaches lle laplacian eigenmaps attempts preserve neighborhood information interpreted soft clustering data 
local approaches dimension reduction deemed natural clustering data 
solutions lle laplacian eigenmaps closely tied spectral clustering 
differently global strategy isomap attempts approximate geodesic distances manifold 
strong connection lle laplacian eigenmaps 
denote semi definite matrix lle 
deemed operators acting functions defined data points 
proved certain conditions ef tors coincide consequently lle attempts minimize reduced find eigenfunctions essentially find eigenfunctions iterated laplacian unified framework dimension reduction algorithms unsupervised learning algorithms eigendecomposition obtaining lower dimensional embedding data lying non linear manifold 
inspired unify algorithms common framework computation embedding training points obtained principal eigenvectors symmetric matrix 
data set xn xi rm affinity matrix constructed denoted si ith row sum affinity matrix si normalizing si gives compute largest positive eigenvalues eigenvector vt embedding xi alternative solutions solution embedding xi ei eit ei ej best approximation hij squared error sense 
mds multi dimensional scaling isomap belong category 
multi dimensional scaling mds multi dimensional scaling distance affinity matrix computed pair training examples 
normalize convert distances equivalent dot products double centering hij hij hij si sj embedding eit example xi 
sk isomap simply generalizes mds non linear manifolds 
replaces euclidean distance approximation geodesic distance manifold 
computes mij squared geodesic distance applies matrix distance dot product transformation mds 
similar mds embedding vti 
solution embedding xi yi th element th element vt spectral clustering laplacian lle belong category 
spectral clustering spectral clustering normalization done way hij hij principal eigenvectors computed 
clustering purpose means applied new unit norm coordinates obtained embedding vti 
laplacian laplacian algorithm laplacian operator approximated heat kernel simple minded matrix element vertices connected zero 
laplacian method solve generalized eigenproblem vj eigenvalues eigenvectors vj diagonal matrix entries si 
bottom eigenvectors smallest embedding 
local linear embedding lle seeks embedding preserve local geometry neighborhood data point 
method local predictive matrix computed subject wij wij xj nearest neighbor xi 
minimizing xi gives 
lowest eigenvectors smallest zero eigenvalue vt get embedding vti vt sample extensions section reviewed unsupervised learning algorithms eigendecomposition provide embedding training points direct extension testing examples short recomputing eigenvectors 
extension unsupervised learning algorithms spectral embedding data mds spectral clustering laplacian eigenmaps isomap lle 
extension allows apply trained model testing points having recompute eigenvectors 
distance metric learning svm achieve generalization classification setting distance metric achieve high consistency neighborhood maintain large margin boundaries different classes 
incorporate maximization margin process distance metric learning :10.1.1.117.5831
simplify computation svms reformulated semidefinite programming problem sdp 
provides discussion systematically applying sdp methods solve kernelized margin maximization problem 
section give brief review svm 
describe large margin nearest neighbor distance metric learning methods 
sdp methods provided solve kernel version margin maximization problem summarized 
review svm consider binary classification problem 
labeled dataset xi yi class label yi 
support vector machines require solution optimization problem min yi xi 
training vectors xi mapped higher dimensional space function 
svm finds linear separating hyperplane maximal margin higher dimensional space 
penalty parameter error term 
dual problem max xi xj iyi kernel trick xi xj xi xj 
defined hypercube lagrange coefficients 
goal learn set parameters maximize objective function 
obtained classifier sign xi computed data 
svm desirable properties address 
bounding issue maximum margin property different form traditional methods aim minimizing empirical risk svms aim minimizing upper bound generalization error 
specifically svms learn trained machine satisfy maximum margin property decision boundary represents maximum minimum distance closest training point 
defining expected risk expectation test error training machine dp empirical risk mean error rate measured training set remp bounds remp yi xi remp conf represents accuracy attained particular training set conf represents ability machine learn training set error 
items drive bias variance generalization error respectively 
tradeoff remp conf get best generalization error 
goal svm minimize upper bound generalization error 
linearly separable case svm computes hyperplane minimum distance closest training point nonseparable case svm looks hyperplane maximizes margin minimizes upper bound error simultaneously 
sparseness representation svms sparseness representation decision function 
precisely training examples lie far away hyperplane receive zero weight participate specification 
training examples lie close decision boundary called support vectors receive nonzero weights 
majority training examples safely ignored svms classify new examples efficiently 
small number support vectors indicates classes separated 
large margin nearest neighbor distance metric learning learns distance metric knn classification setting semidefinite programming :10.1.1.117.5831
learned distance metric enforces nearest neighbors belong class examples different classes separated large margin 
algorithm assumptions distribution data 
objective function training set labeled samples corresponding class labels xi yi yij indicate label yi yj match 
class label information euclidean distance nearest neighbor approach specify target neighbors input xi 
ij indicate xj target neighbor xi 
yij ij fixed binary matrices training 
goal learn linear transformation optimizes nearest neighbor classification 
transformation compute squared distance xi xj xi xj hand large distances input target neighbors penalized small distances input differently labeled inputs penalized 
consequently cost function ij xi xj ij xi xj xi xl ij ijl max denotes standard hinge loss constant 
term penalizes large pairwise distances similarly labeled examples 
second term cost function incorporates idea margin 
particular input xi hinge loss incurred differently labeled inputs distances exceed distance input xi target neighbors absolute unit distance exceed contribution cost function 
cost function favors distance metrics differently labeled inputs maintain large margin distance threaten invade neighborhoods 
similarity cost function method svms fold cost functions term penalizes norm linear transformation distance metric incurs hinge loss examples violate condition unit margin 
svms hinge loss triggered examples near decision boundary similarly hinge loss triggered differently labeled examples invade neighborhoods 
reformulation sdp efficiently compute global minimum reformulated sdp problem rewritten xi xj xi xj xi xj mahalanobis distance metric induced linear transformation way term linear second item slack variables ij introduced pairs labeled differently :10.1.1.117.5831
resulting sdp min ij xi xj xi xj ij ijl ij 
xi xj xi xj xi xl xi xl ijl ijl 
reviewed section purposes margin approach context local adaptive distance metric learning comparable :10.1.1.117.5831
suggests ijl decision boundaries svms induce locally adaptive distance metric knn classification 
specifically testing point decision function constructed svms determine discriminant direction neighborhood 
direction local feature weighting scheme constructed keep class conditional probabilities neighbor consistent 
comparing may find hinge distance metric learning large margin involve training svms need train svms place :10.1.1.117.5831
sense efficient :10.1.1.117.5831
cast kernel margin maximization sdp problem margin considered criterion measure separation labeled data 
give brief review hard margin soft margin kernelized margin maximization problem 
sdp methods solve kernelized margin maximization problem proposed reviewed detail 
definition margin hard margin labeled sample set sl xn yn maximal margin classifier geometric margin realized finding hyperplane solves optimization problem min yi xi maps xi high dimensional space 
corresponds distance convex hulls smallest convex sets contain data class 
lagrangian dual problem max vector ones 
defined gij xi xj 
soft margin hard margin solution exists labeled sample linearly separable feature space 
soft margin defined nonlinearly separable labeled sample set sl 
consider norm norm soft margins 
section ws wss represent cost functions norm soft margin norm soft margin respectively 
norm soft margin labeled sample set sl xn yn norm soft margin classifier geometric margin realized solving optimization problem min yi xi corresponding lagrangian dual problem ws max norm soft margin labeled sample set sl xn yn norm soft margin classifier geometric margin realized solving optimization problem min yi xi corresponding lagrangian dual problem ws max difference norm soft margin norm soft margin fold 
error tolerance item different norm norm norm 
second upper bound lagrange multiplier introduced directly norm soft margin indirectly item norm soft margin 
cast sdp problem semidefinite programming adopted minimize generalized performance measure wc max training data kernel matrix positive semidefinite cone constraint trace min wc ktr trace show realize optimization problem semidefinite programming framework 
proposition wc max convex max convex objective function 
constraints convex 
convex optimization problem 
theorem labeled sample set set labels denoted kernel matrix optimizes solving convex optimization problem min trace ktr intr 
theorem labeled sample set set labels denoted kernel matrix iki ki initial guess optimizes additional constraint solving convex optimization problem considering dual solution max trace ki ri 
apply hard margin ct ki tr ri maximizing margin hard margin svms respect kernel matrix realized special case semidefinite programming framework derived theorem min ktr trace ktr ktr 
theorem linearly separable labeled sample set set labels denoted kernel matrix optimizes solving problem min trace ktr 
cast qcqp specific case ki rank matrices ki viv vi orthonormal normalized eigenvectors initial kernel matrix semidefinite program reduces qcqp max ct 
vi diag fact ki viv vt vi ij iki ri ki tr applying theorem 
benefits provided restriction iki fold positive weights yields smaller set kernel matrices second general sdp reduced qcqp 
third constraint result improved numerical stability prevents algorithm large weights opposite sign cancel 
theorem linearly separable labeled sample set set labels denoted kernel matrix iki optimizes additional constrain solving convex optimization problem considering dual solution max ct ri ki tr 
rm trace ki ri 
proved applying theorem 
apply soft margin apply norm soft margin case nonlinearly separable data consider norm soft margin cost function 
goal optimize quantity respect kernel matrix min ws ktr trace convex optimization problem 
theorem labeled sample set set labels denoted kernel matrix optimizes solving convex optimization problem max trace ktr 
ws ktr wc ktr 
formula obtained applying theorem 
adding additional constraint linear combination fixed kernel matrices leads sdp min trace iki iki iki tr te 
case ki viv vi orthonormal sdp reduces qcqp theorem similar hard margin case max vi diag vi 
ct 
apply norm soft margin case nonlinearly separable data consider norm soft margin cost function 
optimize quantity respect kernel matrix min ws ktr trace convex optimization problem restated follows 
theorem labeled sample set set labels denoted kernel matrix optimizes solving convex optimization problem min trace ktr intr 
ws ktr ktr 
formulas obtained applying theorem 
restricted linear combination fixed kernel matrices obtain min trace iki iki iki tr intr 
case rank matrices ki viv vi orthonormal obtain qcqp max ct kernel methods distance metrics learning decade witnessed substantial developments kernel learning methods :10.1.1.11.2062
appropriate kernel choice generate dramatic improvements classification accuracy kernel learning attracted significant interest 
early area assumed parametric forms kernels learned optimal parameters kernel functions minimizing classification error 
parametric kernels method brief description main ideas kernel feature spaces gives kernel learning algorithms 
emphasis survey non parametric approaches allow positive kernel matrix 
representative category includes kernel alignment semi definite programming approach 
give review kernel methods 
details provided 
consider learning idealized kernel extension 
review kernel methods provide general idea kernel methods 
detailed 
kernel learning algorithms attempts embed data hilbert space searching linear relations space 
performed implicitly embedding specifies inner product pair points giving coordinates explicitly 
advantages kernel methods come fact inner product embedding space computed easily coordinates points 
suppose dataset samples map embedding space 
kernel matrix defined xi xj kij xi xj xi xj way completely determines relative positions points embedding space 
proposition positive definite symmetric matrix kernel matrix inner product matrix embedding space 
conversely kernel matrix symmetric positive definite 
kernel alignment kernel alignment measure similarity kernel functions kernel target function originally introduced 
quantity designed capture degree agreement kernel learning task 
proved kernel alignment sharply concentrated expected values 
unlabeled sample set xi xi denote inner product kernel matrices kernel kernel xi xj xi xj ki kernel matrix sample kernel ki 
alignment kernel kernel respect sample defined label vector yi sample known yy considered target kernel xi xj yi yj xi xj yi yj 
alignment kernel respect sample set considered quality measure kernel yy yy yy concentration generalization concentration means alignment dependent training set say probability empirical estimate deviating mean bounded exponentially decaying function deviation 
suggests alignment training set optimized expected keep high alignment performance test set 
details proof concentration generalization kernel alignment 
kernel alignment sdp concept kernel alignment defined considers optimizing alignment set labels kernel matrix sdp transductive setting 
shows class linear combinations fixed kernel matrices problem cast sdp 
suppose transduction setting labeled training set unlabeled testing set tnt nt 
corresponding kernel matrix ktr ktr tr kij xi xj nt 
consistent general goal transductive learning label unlabeled data specific goal setting learn optimal mixed block ktr optimal testing data block kt optimizing cost function training data block ktr 
optimization problem training testing data blocks entangled optimizing embedding training data entries results automatic tuning testing data entries 
capacity search space possible kernel matrices controlled prevent overfitting achieve generalization test data 
provide general optimization problem kernel matrix restricted convex subset positive semidefinite cone 
consider specific example search space lies intersection low dimensional linear subspace positive semidefinite cone 
case kernel matrix restricted positive semidefinite cone 
denote set positive semidefinite matrices 
kernel matrix maximally aligned label vector solving optimization problem equivalent max yy trace 
max ktr yy trace 
introducing matrix satisfies trace equivalently written max ktr yy trace matrix 
case belongs set positive semidefinite matrices bounded trace expressed linear combination fixed kernel matrices set kg parameters constrained non negative 
way subset satisfying iki trace set kg guess kernel matrix linear gaussian polynomial kernels different kernel parameter values 
rank matrices ki viv vi set orthogonal vectors 
consequently task turns optimizing weights linear combination obtained kernel matrices 
see optimization problem significantly reduced computational complexity 
adding additional constraint leads max ktr yy iki 
written standard form sdp similar way 
max iki tr yy trace iki iki 
ik specific case ki rank matrices ki viv vi semidefinite program reduces qcqp vi vi 
max learning idealized kernel derived non parametric kernel methods distance metric learning 
intuition obtaining ideal kernel function infeasible classification problem ideal kernel matrix defined training patterns provided label information 
kernel making similar ideal kernel matrix 
formulate distance metric learning problem searches suitable linear transform kernel induced feature space 
challenging issue generalize patterns outside training dataset 
ideal kernel kernel defines pairwise similarity patterns 
class problem training set xn yn xi 
ideally patterns considered similar iff belong class 
introduced concept ideal kernel 
ij xi xj xi xj xi xj uses alignment access quality kernel 
definition kernel alignment alignment kernel sample ki corresponding kernel matrix kernel ki 
ideal kernel target alignment measure similarity kernel target kernel making similar ideal kernel 
high alignment training dataset obtained expect high alignment test dataset means high alignment implies generalization performance resulting classifier 
idealized kernel kernel means similar ideal kernel simple way modify parameter determined 
class problem leads kij kij yi yj kij yi yj valid kernels idealized kernel proposition assume alignment idealized kernel greater original kernel number positive negative examples training set respectively 
long aligned right direction slightly wrongly small negative number idealized kernel increased alignment 
extending classes take form block diagonal ll ll 
ll nc ni number patterns belonging ith class ll ni ni ni matrix ones 
valid kernels higher alignment cp formulation distance metric learning look linear kernel case 
assume original inner product pattern xi xj input space defined sij kij mm positive semi definite matrix 
corresponding squared distance ij xi xj xi xj 
changing squared distance changed kii kjj kij ij yi yj ij yi yj modify inner product sij sij xj am am 
ai set useful directions input space 
consequently corresponding distance metric ij xi xj aa xi xj aat positive semi definite valid distance metric 
search matrix dij yi yj ij ij yi yj word patterns different classes pulled apart amount equal modified distance metric class may get close 
consistent slogan reducing intra class variability increasing inter class variability 
extend formulation case similarity information available denote sets containing similar dissimilar pairs 
modified ij xi xj ij xi xj primal dual matrix performs projection set useful features 
set ideally small means small rank aa desirable rank rank aa 
assume eigen decomposition aa rank aa rank 
simplify problem approximated euclidean norm aa 
leads primal problem min ij aat cs ns xi xj ij ij ij xi xj ij ij ij xi xj ij 
ij cd ij nd xi xj ns nd number pairs respectively cs cd non negative adjustable parameters 
dual problem max ij xi xj xi xj xk xl xi xj xk xl xi xj xk xl ij xi xj xi xj ij kl xi xj xk xl ij cd xi xj cs ns ij cd nd xi xj ij kl xi xj xk xl ij kl xi xj xk xl xi xj xi xj ij xi xj xi xj ij lagrangian multipliers 
standard quadratic programming qp problem ns nd variables independent dimension suffer local optimum 
comparable distance metric learning algorithm learns full distance matrix bases similarity information 
reviewed section 
full matrix method leads convex programming problem variables 
involves iterative procedure comprising projection eigen decomposition 
high costly compared idealized kernel learning method 
may notice inner products required formulation 
replace feature map corresponding original kernel applying kernel trick idealized kernel xa xb ij kai ij kai xi xj xi xj distance metric xa xb xa xb xa xb ij kai ij kai survey provides comprehensive review problems algorithms distance metric learning look particularly interesting principle point view 
categorize disparate issues distance metric learning 
category summarize existing disclose essential connections strengths weaknesses 
listed categories addressed supervised distance metric learning contains supervised global distance metric learning local adaptive supervised distance metric learning neighborhood components analysis relevant component analysis 
unsupervised distance metric learning covering linear pca mds nonlinear embedding methods isomap lle laplacian eigenmaps 
unify algorithms common framework embedding computation 
maximum margin distance metric learning approaches including large margin nearest neighbor distance metric learning methods sdp methods solve kernelized margin maximization problem 
kernel methods distance metrics covering kernel alignment sdp approaches extension learning idealized kernel 
significant done field range problems requiring practical feasible solution truly optimal addressed 
listed 
unsupervised distance metric learning 
dimension reduction explored unsupervised distance metric learning general problem dimension reduction 
dimension reduction purposes clustering maximizing margin 
currently general principle framework set learn distance metric pairwise constraints side information 
going local principle manner 
previous research distance metric learning mainly focused finding linear distance metric optimizes data compactness separability global sense bringing data points classes close separating data points different classes 
global distance metric may realistic optimization goals conflict classes exhibit multi modal data distributions 
focusing local compactness effective way avoid conflict enhance metric quality 
local adaptive methods group trying go local find feature weights adapted individual test examples 
local distance metrics hand crafted unable take full advantage available training data 
related nca suffers scalability problem 
consequently need principle approaches learn local distance metrics 
learn explicit nonlinear distance metric local sense 
current nonlinear dimension reduction approaches learn explicit nonlinear metric 
interesting question ask learn explicit nonlinear distance metric local pairwise constraints optimizing local compactness local separability 
efficiency issue 
recall global distance metric learning method neighborhood component analysis nca large margin nearest neighbor classifier :10.1.1.117.5831
formulate distance metric learning constrained convex programming problem attempt learn complete distance metrics training data 
computationally expensive prone overfitting especially applied tasks limited number training samples feature dimension large online medical image retrieval 
tough problem distance metric learning algorithms simplify learning problem improving performance training samples larger dimensions 
conclusively essential technique machine learning research distance metric learning problem addressed far 
done interesting questions remaining 
king 
ma li zhang learning semantic space user relevance feedback image retrieval ieee trans 
circuits systems video technology vol 


ma 
zhang learning image manifold retrieval proc 
acm multimedia 
li 
zhang tong zhang manifold ranking image retrieval proc 
acm multimedia 
muller pun squire learning user behavior image retrieval application market basket analysis international journal computer vision vol 

yan hauptmann jin negative pseudo relevance feedback content video retrieval proc 
acm multimedia 
hastie tibshirani discriminant adaptive nearest neighbor classification ieee pattern analysis machine intelligence vol 

gunopulos adaptive nearest neighbor classification support vector machines proc 
nips 
peng dai adaptive kernel metric nearest neighbor classification proc 
international conference pattern recognition 
goldberger roweis hinton neighbourhood components analysis proc 
nips 
weinberger saul distance metric learning large margin nearest neighbor classification proc :10.1.1.117.5831
nips weiss sch lkopf platt eds 
cambridge ma mit press pp 

xing ng jordan russell distance metric learning application clustering side information proc 
nips 
lebanon flexible metric nearest neighbor classification proc 
uncertainty artificial intelligence 
bar hillel hertz weinshall learning distance functions equivalence relations proc 
international conference machine learning 
vandenberghe boyd semidefinite programming siam review vol 

gill murray wright practical optimization 
academic press 
zhang kwok yeung parametric distance metric learning label information proc 
international joint conference artificial intelligence 
kwok tsang learning idealized kernels proc 
international conference machine learning 
friedman flexible metric nearest neighbor classification stanford university statistics department tech 
rep 
zhang tang kwok applying neighborhood consistency fast clustering kernel density estimation proc 
computer vision pattern recognition pp 

duda hart pattern classification scene analysis 
new york wiley 
cover hart nearest neighbor pattern classification ieee transaction information theory pp 

cover rates nearest neighbor procedures hawaii inter 
conf 
system sciences western periodicals honolulu pp 

mclachlan discriminant analysis statistical pattern recognition 
new york wiley 
peng gunopulos locally adaptive metric nearest neighbor ieee pattern analysis machine intelligence vol 
pp 

large margin nearest neighbor classifiers proceedings th international conference artificial natural neural networks 
london uk springer verlag pp 

short new nearest distance measure proc 
international conference pattern recognition pp 

short optimal measure nearest neighbor classification ieee transaction information theory pp 

atkeson schaal locally weighted learning ai review 
hertz weinshall pavel adjustment learning relevant component analysis proc 
european conference computer vision 
london uk springer verlag pp 

tsang pak ming cheung kernel relevant component analysis distance metric learning proc 
international joint conference neural networks 
linsker application principle maximum information preservation linear systems proc 
nips pp 

tenenbaum de silva langford global geometric framework nonlinear dimensionality reduction science vol 
pp 

saul roweis think globally fit locally unsupervised learning low dimensional manifolds journal machine learning research vol 
pp 

belkin niyogi laplacian eigenmaps dimensionality reduction data representation neural computation vol 
pp 

cox cox multidimensional scaling london chapman hall 
zhang li wang manifold learning applications recognition intelligent multimedia processing soft computing 
springer verlag heidelberg 
de silva tenenbaum unsupervised learning curved manifolds msri workshop nonlinear estimation classification 
new york springer verlag 
roweis nonlinear dimensionality reduction locally linear embedding science vol 
pp 

niyogi locality preserving projections proc 
nips 
hinton roweis stochastic neighbor embedding pp 

brand charting manifold proc 
nips vol 
pp 

hastie stuetzle curves journal american statistical association pp 

gl zeger learning design principal curves ieee pattern analysis machine intelligence pp 

bishop williams gtm generative topographic mapping neural computation vol 
pp 

chang ghosh unified model probabilistic principal surfaces ieee pattern analysis machine intelligence vol 
pp 

smola williamson mika sch lkopf regularized principal manifolds th european conference computational learning theory pp 

bengio vincent roux extensions lle isomap mds eigenmaps spectral clustering proc 
nips 
cambridge ma mit press 
ng weiss spectral clustering analysis algorithm proc 
nips 
lanckriet cristianini bartlett ghaoui jordan learning kernel matrix semidefinite programming journal machine learning research vol 
pp 

cortes vapnik support vector network machine learning pp 

boser vapnik training algorithm optimal margin classifiers fifth annual workshop computational learning theory 
sch lkopf smola learning kernels :10.1.1.11.2062
mit press 
sch lkopf statistical learning kernel methods 
data fusion perception technical report 
redmond wa springer 
muller mika sch lkopf kernel learning algorithms ieee neural networks vol 
pp 

cristianini shawe taylor elisseeff kandola kernel target alignment nips pp 

cristianini shawe taylor support vector machines kernel learning methods 
cambridge university press 
sch lkopf smola learning kernels 
cambridge ma mit press 

