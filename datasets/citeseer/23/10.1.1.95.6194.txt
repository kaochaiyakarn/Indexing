topographic processing relational data barbara hammer alexander fabrice rossi marc university technology department informatics hammer tu de projet axis inria le chesnay cedex france fabrice rossi org pattern recognition group stricker de keywords relational data batch clustering visualization som ng batch optimization schemes self organizing map som neural gas ng modified called median variants allow transfer methods arbitrary distance measures standard euclidean metric 
principle particularly suitable complex applications data compared means problem specific possibly discrete metrics protein sequences compared fasta blast 
median variants allow continuous update prototype locations capacity restricted particular small data sets 
contribution consider relational dual batch optimization formulated terms pairwise distances application distance matrices known euclidean embedding possible 
som direct visualization data means underlying euclidean hyperbolic lattice structure 
ng pairwise distances prototypes computed data matrix subsequent mapping means multidimensional scaling applied 
algorithms evaluated experiments 
self organizing map proposed kohonen constitutes popular methods visual data inspection clustering highly flexible easily applied data interest 
alternative methods exists neural gas martinetz focuses aspect clustering serve inspection tool combination subsequent visualization multidimensional scaling 
original online variants som ng proposed euclidean data 
extensions general situations inevitable euclidean distance measure appropriate applicable case discrete data 
variety extensions som proposed deal situation including statistical variants variants built generative models encoder decoder frameworks 
proposals common original intuitive version som directly obtained case standard euclidean data modifications necessary 
batch optimization schemes extended general proximity data means generalized median 
restricts prototype locations data points maintaining standard batch optimization principle far possible 
theoretical background method convergence proof provided median variants interpreted consecutive optimization cost function ng resp 
som respect assignments prototype locations 
median clustering similar standard batch variants relies cost function 
restricts prototype locations data space severe loss accuracy expected particular sparsely covered input space continuous update takes place 
propose relational variants som ng allow continuous update prototypes case distance matrix 
versions optimize cost function standard batch versions convergence guaranteed 
relational clustering quite known simple means clustering fuzzy variants thereof 
article example provides reliable costly solution optimize cost function relational means means statistical physics 
som ng online kernelized variants proposed related relational clustering kernel induces general metric vice versa 
proposals provide fast batch optimization scheme require similarities dissimilarities 
embed relational clustering general framework means cost function convergence guaranteed extensions supervision easily integrated 
focus article lies usefulness methods inspection low dimensional visualization relational data characterized pairwise distances 
purpose standard som euclidean hyperbolic lattice proposed 
alternatively derive formulas describe pairwise distances prototypes relational models ng structures fixed prior lattice embedded low dimensional euclidean hyperbolic space 
depending complexity data hyperbolic space better suited cluster visualize hierarchical structures close connections data 
introduce standard batch optimization ng som extend models relational data 
discuss possibility incorporate supervision visualization demonstrate applicability experiments bioinformatics 
relational topographic maps classical neural topographic maps consider vectorial data distributed underlying distribution euclidean plane 
goal clustering distribute prototypes 
faithfully data 
new data point assigned winner prototype smallest distance clusters data space receptive fields prototypes 
different popular variants neural clustering proposed learn prototype locations training data 
assume number prototypes fixed neural gas ng introduced cost function eng ki ki rank prototypes sorted distances exp scales neighborhood cooperation neighborhood range constant ki neglect simplicity 
classical ng optimized online means stochastic gradient descent 
fixed training set alternative fast batch optimization scheme offered algorithm turn computes ranks treated hidden variables cost function optimum prototype locations init repeat compute ranks ki compute new prototype locations ki ki means ng preprocessing step data mining visualization followed subsequent projection methods multidimensional scaling 
self organizing map som proposed kohonen uses fixed usually low dimensional regular lattice structure determines neighborhood cooperation 
restriction induce topological mismatches data topology match prior lattice 
dimensional regular lattice chosen benefit apart clustering direct visualization data results representation data regular lattice space 
som constitutes direct data inspection visualization method 
som possess cost function slight variation thereof proposed heskes 
cost function denotes neighborhood structure induced lattice exp scales neighborhood degree gaussian function 
index refers slightly altered winner notation neuron winner average distance minimum 
neurons arranged graph structure defines topology rectangular hexagonal tessellation euclidean plane resp 
hyperbolic grid dimensional hyperbolic plane allowing dense connection prototypes exponentially increasing number neighbors 
cases function denotes length path connecting prototypes number lattice structure 
original som optimized online fashion 
fixed training data batch optimization possible subsequently optimizing assignments prototype locations init repeat compute winner assignments minimizing compute new prototypes shown batch optimization schemes converge finite number steps local optimum cost function provided data points located borders receptive fields final prototype locations 
case convergence guaranteed final solution lie border basins attraction 
relational data relational data embedded vector space pairwise similarities dissimilarities available 
assume training data 
means pairwise distances dij assume stems unknown distance measure find possibly high dimensional euclidean points dij note notation includes possibly nonlinear mapping feature map corresponding embedding euclidean space 
embedding known directly optimize cost functions embedding space 
median clustering restricts prototype locations data points determines prototype locations iterative step way corresponding part cost function assumed fixed assignments minimum 
values determined extensive search turning linear complexity quadratic number training data 
procedure severe drawback discrete adaptation steps performed result usually worse compared standard som ng euclidean setting 
relational learning overcomes problem 
key observation consists fact optimum prototype locations expressed linear combination data points 
unknown distances expressed terms known values dij 
precisely assume exist points dij assume prototypes expressed terms data points ij ij 
dij ij constitutes distance matrix ij coefficients 
fact substitute terms batch optimization schemes 
optimum solu tions find equality ij batch ng batch som introduced ij ki ki ng ij som 
allows reformulate batch optimization schemes terms relational data 
obtain algorithm init ij ij repeat compute compute optimum assignments values ij ki ng ij som compute ij ij ij prototype locations computed indirectly means coefficients ij 
initialization done setting initial prototype locations random data points realized random selection rows distance matrix 
new data point isometrically embedded euclidean space pairwise distances dj corresponding distance winner determined equality denotes vector distances dj quantization error expressed terms values dij substituting interestingly formula optimum assignments batch optimization derive relational dual cost functions algorithms 
relational dual ng som obtain ll ki ki dll ki ll dll note relational learning gives exactly results standard batch optimization provided relations stem euclidean metric 
convergence guaranteed case holds standard batch versions 
distance matrix come euclidean metric equality longer hold terms negative 
case correct distance matrix spread transform sufficiently large equals entry identity 
alternatively directly apply optimization scheme converges general symmetric nonsingular matrix value dual cost function shown relational ng 
supervision possibility include information available important get meaningful results unsupervised learning 
help prevent garbage garbage problem unsupervised learning discussed 
assume additional label information available accounted 
labels embedded assume label attached denoted equip prototype label adapted learning 
euclidean case basic idea consists substitution standard euclidean distance mixture takes similarity labels account 
controls influence labels 
procedure proposed euclidean clustering 
principles relational clustering 
discrete euclidean settings 
cost functions related batch optimization follows eng ij ki ki denotes rank neuron measured distances change computation rank accompanied adaptation prototype labels batch optimization way cost function som denotes generalization winner notation proposed heskes supervised setting minimizes batch optimization uses winner extends updates 
shown procedures converge finite number steps 
relational learning possible substituting distances identity ij optimum assignments holds extensions 
computation yields algorithm dissimilarity data class ha class hb class class gg gp class mapping non euclidean protein dataset relational som hyperbolic grid structure 
init ij ij repeat compute distances id compute assignments ij values compute ij ij ij compute prototype labels ij version identical euclidean version euclidean distance matrix procedure converges finite number steps 
note vanishing neighborhood size final prototype labels coincide averaged label taken receptive field prototype 
rapid learning improve classification result setting prototype labels averaged label receptive fields training 
note prototype locations affected label information pure unsupervised learning posterior labeling 
supervised setting prototypes forced follow borders class information 
visualization discussed data clustered trained prototypes 
obvious able create visualization extracted information 
low dimensional euclidean som direct visualization embedding data points respective positions winner lattice space 
hyperbolic som allows embedding points means poincar disk model embeds hyperbolic space non isometrically unit disk focus attention put point mapped center disk fish eye effect results 
moving focus allows browse map 
ng direct visualization easily reached subsequent embedding prototypes dimensional euclidean plane means class ha class hb class class gg gp class mapping non euclidean protein dataset relational non metric multidimensional scaling 
distance preserving projection techniques prototypes multidimensional scaling 
pairwise distances ij prototypes possibly nonlinearly preprocessed ij appropriate weighting function model finds dimensional projections pairwise distances ij stress function ij ij ij similar objective function minimized 
apply techniques pairwise distance prototypes needs computed 
assume identity il optimum prototypes assume pairwise distances dij accumulated matrix 
find denotes vector il experiments protein classification evolutionary distance globin proteins determined alignment described 
samples originate different protein families hemoglobin hemoglobin distinguish classes proposed ha hb gg gp 
table shows class distribution dataset 
training neurons relational batch ng relational hyperbolic som grid standard relational som respectively 
number neurons derived hyperbolic grid depth cf 

neighborhood range annealed starting number neurons experiments 
results reported table gained repeated class 
count percentage ha hb gg gp table class statistics protein dataset fold stratified cross validation averaged repetitions epochs run 
supervision included cost function mixing parameter 
results reported svm versus rest encoding setting gives integrated clustering model 
depending choice kernel reports errors approximately add leave error 
result comparable results due different error measure 
nearest neighbor classifier yields accuracy setting nearest neighbor larger worse comparable results 
continuous updates improve results median clustering 
projections relational som hyperbolic grid structure relational non metric multidimensional scaling kruskal normalized stress criterion shown 
neurons depicted majority vote 
obviously neurons arrange associated class clear dimensional representation data set obtained 
chromosome images copenhagen chromosomes database benchmark 
set human nuclear chromosomes classes resp 
sex chromosome considered represented grey levels images transferred strings representing profile chromosome thickness silhouettes 
edit distance typical distance measure strings different length described 
application distances strings computed standard edit distance substitution costs signed difference entries insertion deletion costs 
algorithms tested repeated fold stratified cross validation neurons relational relational corresponding rings grid standard relational som 
results median relational standard relational batch batch relational hyperbolic ng ng som som accuracy protein data set mean stddev accuracy copenhagen chromosome database mean stddev table classification accuracy protein data set copenhagen chromosome database respectively posterior labeling 
visualization dataset relational som hyperbolic grid structure 
mean accuracy repetitions method epochs run cf 

supervision incorporated mixing parameter 
improvement observed 
data barley important crop plant gene expression analysis temporal development barley seeds important issue derivation key metabolic processes different stages growth 
extensive gene expression measurements time points day zero day steps days carried cdna technology 
high signal noise ratios high reproducibility independently taken experimental series led selection genes genes available 
data matrix genes time points considered 
common gene expression analysis log transformed final expression values considered 
analysis available gene expression time series required identify groups commonly regulated genes may temporal impact 
clustering helps extract candidate genes responsible triggering events example influence cell wall degradation factors lateral tissue nutrition subsequent accumulation processes 
fig 
shows arrangement obtained training neurons epochs data transformation pearson correlation expression patterns distance measure better accounts principled shape described 
obviously map organizes data evolution regulation genes time 
due hyperbolic structure map clearly sep opposite shapes preserves data topology 
extension som ng relational data offers intuitive simple possibility project data distance matrix low dimensional space preserving data topology 
method directly cost function som ng respectively opening way extensions supervision demonstrated experiments hyperbolic mds euclidean data 
alternative proposals relational visualization maintains simplicity original som ng 
drawback proposed method complexity training scales number data original som ng 
complexity drastically reduced exact methods introduced article low dimensional approximations vectors encode prototypes proposed approach 
james anderson hyperbolic geometry second edition springer 
guez rossi el fast algorithm implementation dissimilarity self organizing maps neural networks 
cottrell hammer villmann batch median neural gas neural networks 
graepel herbrich obermayer classification pairwise proximity data jordan kearns solla eds nips vol 
mit press 
graepel obermayer stochastic selforganizing map proximity data neural computation 
learning distance substitution kernels pattern recognition proc 
th dagm symposium 
hammer relational neural gas appear ki 
hammer 
villmann supervised batch neural gas proceedings conference artificial neural networks pattern recognition ed springer pages 
bezdek 
means relational fuzzy clustering 
pattern recognition 
davenport bezdek 
relational duals means algorithms 
pattern recognition 
heskes 
self organizing maps vector quantization mixture modeling 
ieee transactions neural networks 
hofmann buhmann multidimensional scaling data clustering advances neural information processing systems 
juan vidal normalized edit distances efficient nn search technique fast accurate string classification icpr vol 
kaski roos discriminative clustering yeast stress response bioinformatics computational intelligence paradigms jain schweizer eds pages springer 
kaski oja nen trustworthiness metrics visualizing similarity gene expression bmc bioinformatics 
kruskal multidimensional scaling optimizing goodness fit nonmetric hypothesis 
psychometrika 
kohonen self organized formation topologically correct feature maps biological cybernetics 
kohonen large self organizing maps data neural networks 
phillip granum quantitative analysis digitized banded human chromosomes clinical genetics 
martinetz schulten neural gas network vector quantization application time series prediction ieee transactions neural networks 
quantifying local reliability sequence alignment protein engineering 
bunke edit distance kernel functions structural pattern classification pattern recognition 
qin kernel neural gas algorithms application cluster analysis icpr vol pp 
ritter self organizing maps non euclidean spaces kohonen maps eds oja kaski 
rossi hammer accelerating relational clustering algorithms sparse prototype representation submitted 
ller ritter hyperbolic topographic mapping proximity data proc 
iasted international conference artifical intelligence applications acta press 
sammon jr 
nonlinear mapping data structure analysis 
ieee transactions computers 
seo obermayer self organizing maps clustering methods matrix data neural networks 
villmann hammer generalized relevance lvq correlation measures gene expression analysis neurocomputing 
tino sun generative probabilistic approach visualizing sets symbolic sequences 
proceedings tenth acm sigkdd international conference knowledge discovery data mining kdd eds kohavi gehrke dumouchel ghosh 
pp 
acm press 
walter mds new approach interactive visualization multidimensional scaling hyperbolic space 
information systems 
yin equivalence kernel maps self organising mixture density network neural networks 

