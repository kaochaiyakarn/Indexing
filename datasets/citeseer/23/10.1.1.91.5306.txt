esann proceedings european symposium artificial neural networks bruges belgium april side publi isbn 
spiral recurrent neural network online learning gao rudolf hans peter kriegel university munich germany siemens ag corporate technology germany 
autonomous self sensor networks require sensor nodes certain degree intelligence 
elementary component intelligence ability learn online predicting sensor values 
consider recurrent neural network rnn models trained extended kalman filter algorithm real time recurrent learning rtrl teacher forcing 
compared performance conventional neural network architectures spiral recurrent neural networks spiral rnn novel rnn architecture combining trainable hidden recurrent layer echo state property echo state neural networks esn 
novel rnn architecture shows stable performance faster convergence 
sensor networks going deployed control diagnosis applications expects solutions low effort installation configuration operational maintenance 
plug play requirements imply autonomous behavior forbid extensive parameter tuning 
requires certain degree self organization little priori knowledge environment 
kind intelligence achieved self learning systems 
important component self learning system prediction models sensor values properties able learn online manner low computational complexity low price embedded systems stable 
predicted values associated errors basic ingredients diagnostic solutions efficient energy management battery powered sensor nodes 
recurrent networks rnn motivated successful application offline learning tasks 
time delay neural networks tdnn basically feed forward neural networks disadvantage prefixed number historic inputs 
shift register tdnn recurrent neural network rnn models recurrent couplings 
enable rnns embed theoretically infinite history recurrent hidden layer 
examples simple recurrent networks srn elman echo state neural networks esn long short time memory networks lstm block diagonal recurrent neural network 
srns show stability problems mainly due unconstrained hidden matrix require large recurrent network layers increasing computational complexity tend generalize shown result 
esann proceedings european symposium artificial neural networks bruges belgium april side publi isbn 
best knowledge aware investigations involving lstm networks online learning tasks considering met stability problems online learning lstm networks 
order overcome aforementioned deficiencies novel architecture called spiral recurrent neural network spiral rnn introduced 
organized follows section consider environment model question generic online learning algorithm 
spiral rnns introduced section 
section describe simulation experiments results 
draw section 
online learning aforementioned requirements imply generic online learning rule 
rule derived bayes rule leads extended kalman filter 
learning system maintain update model environment allowing predict observations 
typical ansatz model describing generic dynamical system wt wt st st wt xt xt st wt eq holds assume stationary dynamics environment static model parameters wt random fluctuations obeying gaussian statistics qt covariance matrix qt 
eq describes dynamic evolution hidden state vector depends previous state st wt 
eq assume current observation st wt 
measurement noise satisfying rt covariance matrix 
extended kalman filter ekf algorithm obtain update scheme model parameters st st wt xt ot st wt pt ht ht rt wt wt pth xt ot pt represents covariance matrix wt priori covariance matrix wt satisfying pt qt ht represents gradient output wt notice indexing st wt ht dot wt st wt ot st st st st wt st wt ot wt st wt st st st wt esann proceedings european symposium artificial neural networks bruges belgium april side publi isbn 
data xt input eq corresponds teacher forcing 
expression multiplication term eq gradient calculated real time recurrent learning rtrl teacher forcing 
instability problems may occur norm eigenvalues jacobian matrix st st exceeds may lead unlimited amplification gradient components 
recurrent neural network jacobian matrix closely related hidden weight matrix 
spiral recurrent neural network similar rnn models spiral rnns satisfy model equations st win xt xt andb bias vectors matrices win wout represent synaptic connections layers activation functions 
order avoid instability problems online learning parametrize recurrent hidden matrix eigenvalue spectrum bounded 
suitable choice dimensional data shown 
neurons lying circle fig clockwise nearest neighbor links weight 
similarly clockwise nearest neighbor links share weight longer range links 





hidden layer spiral rnn dimensional data fig 
sample structure hidden layer dimensional data part connections coming nodes shown 
spiral rnn dimensional data isolated similar structure groups hidden nodes 
hidden layer neurons hidden weight matrix representation tanh esann proceedings european symposium artificial neural networks bruges belgium april side publi isbn 
fixed value 
representation introduces unconstrained parameters hidden matrix realize fading memory feature maximum absolute eigenvalue bounded tanh dimensional data recurrent hidden layer repeated times input output neurons fully connected hidden neurons fig 
hidden weight matrix spiral rnns multi dimensional case block diagonal matrix maximum absolute eigenvalue ddimensional data maximum absolute eigenvalues eq dimension max max 
parametrization spiral rnns trainable hidden weight matrix pre definable bound eigenvalue spectrum 
experimental settings examine generalization ability networks comparisons run srn esn spiral rnn time series spike time series period period contains zeros spike value chaotic lorentz time series parameters generated 
time series corrupted uniformly distributed noise range 
training networks teacher forcing rtrl ekf algorithm introduced section 
note esn model fixed hidden weights st wt eq equals zero rtrl gradient calculation applied 
covariance matrix rt measurement noise filtered estimate model error filter coefficient 
models roughly number model parameters determining computational cost mainly due ekf training 
randomly initialized weights networks lack priori information 
network hidden output activation functions set hyperbolic tangent function tanh linear map respectively 
esn hidden matrix scaled maximum absolute eigenvalue max equal sparsity set 
spiral rnns setting theoretically ensure max experiments shown better performance setting 
due nonlinear tanh activation function fact limit eq holds 
time intervals long term prediction tests performed previous output input step 
maximum prediction step spike time series lorentz time series 
normalized square error xt calculated step prediction target xt variance data xt step iterative prediction error ipe defined xt 
esann proceedings european symposium artificial neural networks bruges belgium april side publi isbn 
multidimensional data ipe value averaged dimensions 
section take ipe criterion comparisons 
result discussion tables show averaged ipe values respective standard deviation simulations different prediction steps nd column time interval 
best solution condition row marked bold font 
test spike time series comparison results networks trainable synaptic weights listed table 
esn shows performance short term prediction falls back long term prediction 
indicates poor generalization behavior 
recognize spike step prediction 
spiral rnn shows performance long term prediction step prediction 
spiral rnn requires spike periods converge 
note due instability problems srn model failed provide useful output simulations 
spiral esn srn pred 
mean std 
mean std 
mean std 
mean std 
table ipe comparisons spike time series weights 
note ipe values tend small characteristics spike time series 
test lorentz time series chaotic time series lorentz time series excellent examples testing models ability generalization 
results shown table confirm impression tests spike time series spiral rnns converge fast generalize show excellent performance long term prediction tasks 
srn improve test interval test interval converges fast spiral rnn 
esn unable accurate predictions small network size trainable weights 
similar results achieved simulations trainable parameters 
comparisons performed dimensional chaotic time series extension lorentz data 
spiral rnn achieved better results models 
esann proceedings european symposium artificial neural networks bruges belgium april side publi isbn 
benchmark experiments spiral rnn successfully employed predicting quasi periodical trajectories computer mouse moved user 
spiral esn srn pred 
mean std 
mean std 
mean std 
mean std 
table ipe comparisons lorentz time series weights considered rnns applications self organizing sensor networks 
focus autonomous online learning required stable performance fast convergence cheap computation 
introduced spiral rnn model order overcome drawbacks traditional models combine advantageous features trainable recurrent hidden layer srns constraint eigenvalue spectrum recurrent weight matrix 
novel architecture proven provide stable accurate results fast convergence 
suitable candidate online learning tasks embedded systems 
zimmermann schaefer ch 
dynamical consistent recurrent neural networks 
int 
joint conference neural networks ijcnn 
rumelhart mcclelland 
parallel distributed processing explorations microstructure cognition 
cambridge mass mit press 
jeffrey elman 
finding structure time 
cognitive science 
herbert jaeger 
adaptive nonlinear system identification echo state networks 
advances neural information processing systems 
hochreiter schmidhuber 
long short term memory 
neural comput nov 

stable learning algorithm block diagonal recurrent neural networks part algorithm 
ieee international joint conference neural networks 
lewis 
optimal estimation stochastic control theory 
wiley interscience publication 
isbn 
williams zipser 
learning algorithm continually running fully recurrent neural networks 
neural 

