conditional random fields predict pitch accents conversational speech michelle gregory linguistics department university buffalo buffalo ny buffalo edu detection prosodic characteristics important aspect speech synthesis speech recognition 
correct placement pitch accents aids natural sounding speech automatic detection accents contribute better recognition better textual understanding 
investigate probabilistic contextual phonological factors influence pitch accent placement natural conversational speech sequence labeling setting 
introduce conditional random fields crfs pitch accent prediction task order incorporate factors efficiently sequence model 
demonstrate usefulness incremental effect factors sequence model performing experiments hand labeled data switchboard corpus 
model outperforms baseline previous models pitch accent prediction switchboard corpus 
suprasegmental features speech relay critical information conversation 
major natural sounding speech synthesis identification implementation prosodic characteristics 
difficulty task lies fact prosodic cues absolute relative individual speakers gender dialect discourse context local context phonological environment factors 
especially true pitch accent acoustic cues word prominent utterance 
example word fundamental frequency hz quite prominent male speaker typical female speaker 
likewise accent utterance jon leaving critical determining answer question leaving jon leaving jon doing jon leaving 
accurate pitch accent prediction lies successful combination con altun department computer science brown university providence ri altun cs brown edu textual variables possible 
syntactic information part speech proven successful predictor accentuation hirschberg pan hirschberg 
general function words accented content words 
various measures word informativeness information content ic word pan mckeown collocational strength context pan hirschberg proven useful models pitch accent 
open topic conversational speech accent unpredictable 
part speech informativeness word capture aspects accentuation see example taken switchboard function word gets accented accented words uppercase strong objections 
accent influenced aspects rhythm timing 
length words number phones normalized duration affect likelihood accented 
additionally immediately surrounding words bear pitch accent affect likelihood accentuation 
words word typically accented may surrounding words bear pitch accent 
phrase boundaries play role accentuation 
word intonational phrases ip accented word ip tends accented 
short accented words ip independent 
previous pitch accent prediction neglected dependency labels 
different machine learning techniques decision trees hirschberg rule induction systems pan mckeown bagging sun boosting sun scenario accent word predicted independently 
exception line research hidden markov models hmm pitch accent prediction pan mckeown 
pan mckeown demonstrate effectiveness sequence model rule induction system rip treats label independently showing hmms outperform ripper variables 
hmms predominant formalism model label sequences 
major shortcomings 
trained non discriminatively maximum likelihood estimation model joint probability observation label sequences 
require questionable independence assumptions achieve efficient inference learning 
variables hidden markov models pitch accent prediction limited part speech frequency pan mckeown 
discriminative learning methods maximum entropy markov models mccallum projection markov models punyakanok roth conditional random fields lafferty sequence adaboost altun sequence perceptron collins hidden markov support vector machines altun maximum margin markov networks taskar overcome limitations hmms 
methods crfs common technique nlp successfully applied part speech tagging lafferty named entity recognition collins shallow parsing sha pereira mccallum 
goal study better identify words string text bear pitch accent 
contribution fold employing new predictors utilizing discriminative model 
combine advantages probabilistic syntactic phonological predictors advantages modeling pitch accent sequence labeling setting crfs lafferty 
rest organized follows section introduce crfs 
describe corpus variables section section 
experimental setup report results section 
discuss results section conclude section 
conditional random fields crfs considered generalization logistic regression label sequences 
define conditional probability distribution label sequence observation sequence 
denotes sentence length 
denotes label sequence corresponding pitch accent prediction word binary label denoting accented 
crfs specify linear discriminative function parameterized feature representation observation label sequence 
model assumed stationary feature representation partitioned respect positions sequence linearly combined respect importance feature denoted discriminative function stated equation conditional probability normalization constant computed summing possible label sequences observation sequence extract types features sequence pair 
current label information observation sequence part speech tag word window centered word currently labeled current word pitch accented part speech tag previous word noun 

current label neighbors label features capture inter label dependencies current word pitch accented previous word accented 
crfs condition observation sequence efficiently employ feature representations incorporate overlapping features multiple interacting features long range dependencies observations opposed hmms generate observation sequences 
limit order markov model features encode inter label dependencies 
information encode observation label dependencies explained detail section 
crfs objective function log loss model parameters respect training set function defined negative sum conditional probabilities training label sequence yi observation sequence xi xi yi 

crfs known overfit especially noisy data regularized 
overcome problem penalize objective function adding gaussian prior term proportional squared norm suggested johnson 
loss function log yi xi xi yi log xi constant 
lafferty 
proposed modification improved iterative scaling parameter estimation crfs 
gradient methods efficient minimizing equation minka sha pereira 
conjugate gradient descent method optimize objective function 
gradients computed equation ep xi xi yi expectation respect possible label sequences observation sequence xi computed forward backward algorithm 
observation sequence best label sequence arg max parameter vector minimizes 
best label sequence identified performing viterbi algorithm 
corpus data study taken switchboard corpus godfrey consists telephone conversations adult speakers approximately words 
participants male female represented major dialects american english 
portion corpus phonetically greenberg segmented speech boundaries turn boundaries pauses ms sides 
fragments contained words average 
additionally word coded probabilistic contextual information word frequency conditional probabilities rate speech canonical pronunciation morgan 
dataset analysis study consists hour database comprised utterances words 
utterances hand coded pitch accent intonational phrase brakes 
pitch accent coding utterances hand labeled accents boundaries tilt intonational model taylor 
model characterized series intonational events accents boundaries 
labelers instructed duration amplitude pausing information changes identify events 
general labelers followed basic conventions coding taylor 
tilt coding scheme simplified 
accents coded major minor rare level accents breaks rising falling 
agreement tilt coding reported 
cu coding simplified coding scheme accent types conflated major breaks coded 
accent break coding pair wise agreement coders kappa difference expected agreement actual agreement 
variables label predicting binary distinction accented 
variables prediction fall main categories syntactic probabilistic variables include word frequency collocation measures phonological variables capture aspects rhythm timing affect accentuation 
syntactic variables syntactic category classification hand generated part speech pos function noun verb includes adjectives adverbs table gives percentage accented items pos 
tested categorization distinct part speech classes results improve report way classification 
accented function verb noun table percentage accented items pos 
variable definition example unigram log wi bigram log wi wi rev bigram log wi wi rid joint log wi wi rev joint log wi wi table definition probabilistic variables 
probabilistic variables line research incorporates information content word collocation measures pan mckeown pan hirschberg included number probabilistic variables 
probabilistic variables unigram frequency predictability word preceding word bigram predictability word word reverse bigram joint probability word preceding joint joint probability word word reverse joint 
table provides definition high probability examples corpus emphasized word current target 
note probabilistic variables log scale 
values probabilities obtained entire words table presents spearman rank correlation coefficient probabilistic measures accent conover 
values indicate strong correlation accents probabilistic variables 
probability increases chance accent decreases 
note values significant level 
created combined part speech unigram frequency variable order variable corresponds variable pan current implementation crf takes categorical variables experiments probabilistic variables binned equal categories 
tried bins produced similar results report binned categories 
computed correlations pitch accent original variables binned variables similar 
variables spearman unigram bigram reverse bigram joint reverse joint table spearman correlation values probabilistic measures 
mckeown 
phonological variables category predictors phonological variables concern aspects rhythm timing utterance 
main sources variables computed solely string text textual require sort acoustic information acoustic 
sun demonstrated number phones syllable number syllables word position word sentence useful predictors syllables get accented 
sun concerned predicting accented syllables variables apply word level targets 
textual phonological features included number syllables word number phones citation form transcribed form 
position sentence position word utterance fragments necessarily correspond sentences database 
utterance length 
list textual features number canonical syllables number canonical phones number transcribed phones length utterance number words position word utterance main purpose study better predict words string text receive accent 
far predictors ones easily computed string text 
included variables affect likelihood word accented require acoustic data 
best knowledge features acoustic models pitch accent prediction 
features include duration word speech rate intonational phrase boundaries 
nature corpus disfluencies 
feature sig canonical syllables canonical phones transcribed phones utt length utt position duration speech rate pause filled pause ip boundary table significance phonological features pitch accent prediction 
included pauses filled pauses predictors 
list acoustic features log duration milliseconds normalized number canonical phones binned equal categories 
log speech rate calculated strings speech bounded side pauses ms greater binned equal categories 
pause binary distinction word followed period silence 
filled pause binary distinction word followed filled pause uh um 
ip boundary table indicates features significantly affect presence pitch accent 
certainly variables independent crfs incorporate variables pitch accent prediction model advantage making dependencies labels 
surrounding information sun shown values immediately preceding target predictors value target 
experimented effects surrounding values varying window size observation label feature extraction described section 
window size values word labelled incorporated model 
window size values previous words current word incorporated model 
window size captures values current word previous words words 
experiments results experiments run fold crossvalidation 
viterbi decoding find sequence report performance terms label accuracy 
ran experiments varying window sizes 
baseline simply assigns common label achieves 
previous research demonstrated part speech frequency combination reliable predictors pitch accent 
test crf model experiment ran comparison hmm crf just combination part speech unigram 
hmm score referred hmm pos unigram table crf model referred crf pos unigram table performed significantly better 
note pan mckeown reported accuracy hmm model 
difference due different corpora case 
spontaneous speech limited domain sense speech discharge orders doctors medical facility 
corpus open domain conversational speech 
order capture aspects ic collocational strength word second experiment ran part speech plus probabilistic variables referred crf pos prob table 
model accuracy improved model pos unigram values 
third experiment wanted know tts applications purely textual input aided addition timing rhythm variables gleaned text string 
included textual features described section addition probabilistic syntactic features referred crf pos prob txt table 
accuracy improved 
final experiment added acoustic variable resulting variables described section referred crf table 
get increase accuracy window size 
larger windows resulted minor increases performance model summarized table 
best accuracy features window size 
model variables baseline hmm pos unigram crf pos unigram crf pos prob crf pos prob txt crf table test accuracy pitch accent prediction various variables window sizes 
discussion pitch accent prediction difficult task number different speakers topics utterance fragments production corpus increase difficulty 
fact function words accented indicates models pitch accent rely part speech unigram frequency fair corpus 
model pitch accent captures factors influence accentuation 
addition adding probabilistic variables phonological factors sequence model captures interdependence accents phrase 
distinct natures corpora difficult compare results earlier models 
experiment hmm pos unigram vs crf pos unigram shown crf model achieves better performance hmm model features 
real strength crfs comes ability incorporate different sources information efficiently demonstrated experiments 
test directly probabilistic measures collocation measures task information content ic pan mckeown mutual information pan hirschberg 
measures encompass similar information 
example ic additive inverse unigram measure ic log mutual information measure collocational strength unigram bigram joint probabilities 
model includes joint probability unigram probabilities wi wi comparable includes mutual information 
just likelihood word accented influenced silence ip boundary collocational strength target word word captured reverse bigram reverse joint factor 
pos unigram bigram joint probabilities shown crfs outperform hmms probabilistic variables increase accuracy model include pos unigram compared 
tasks pitch accent predicted solely string text addition acoustic data shown adding aspects rhythm timing aids identification accent targets 
number words utterance utterance word falls long number syllables number phones affect accentuation 
addition variables improved model nearly 
results suggest accent prediction models textual information improved addition variables 
trying provide complete model accentuation acoustic information study tested acoustic variables tested 
nature corpus allowed investigate role disfluencies widely variable durations speech rate accentuation 
especially speech rate duration surrounding silence predictors pitch accent 
addition predictors slightly improved model 
acoustic features sensitive individual speakers 
corpus different speakers varying ages dialects 
variables useful controls individual speaker differences 
really test usefulness variables combine acoustic features demonstrated predictors pitch accent sun 
crfs new measures collocational strength new phonological factors capture aspects rhythm timing model pitch accent prediction 
crfs theoretical advantage incorporating factors principled efficient way 
demonstrated crfs outperform hmms experimentally 
demonstrated usefulness new probabilistic variables phonological variables 
results mainly implications textual prediction accents tts applications useful automatic speech recognition tasks automatic transcription multi speaker meetings 
near incorporate reliable acoustic information controlling individual speaker difference apply different discriminative sequence labeling techniques pitch accent prediction task 
partially funded career award iis 
mark johnson idea project dan jurafsky alan bell cynthia jason helpful comments help database 
altun hofmann johnson 

discriminative learning label sequences boosting 
proc 
advances neural information processing systems 
altun tsochantaridis hofmann 

hidden markov support vector machines 
proc 
th international conference machine learning 
collins 

discriminative training methods hidden markov models theory experiments perceptron algorithms 
proc 
empirical methods natural language processing 
riccardi rose 

prosody recognition speech utterances acoustic linguistic models prosodic events 
proc 
eurospeech 
conover 

practical nonparametric statistics 
wiley new york nd edition 
morgan 

effects speaking rate word frequency conversational ations 
speech communication 
godfrey mcdaniel 

switchboard telephone speech corpus research 
proc 
international conference acoustics speech signal processing 
greenberg ellis 

insights spoken language gleaned phonetic switchboard corpus 
proc 
international conference spoken language 
hirschberg 

pitch accent context predicting intonational prominence text 
artificial intelligence 
johnson geman canon chi riezler 

estimators stochastic unification grammars 
proc 
acl association computational linguistics 
lafferty mccallum pereira 

conditional random fields probabilistic models segmenting labeling sequence data 
proc 
th international conference machine learning 
mccallum freitag pereira 

maximum entropy markov models information extraction segmentation 
proc 
th international conference machine learning 
mccallum 

efficiently inducing features conditional random fields 
proc 
uncertainty intelligence 
minka 

algorithms maximumlikelihood logistic regression 
technical report cmu department statistics tr 
pan hirschberg 

modeling local context pitch accent prediction 
proc 
acl association computational linguistics 
pan mckeown 

word informativeness automatic pitch accent modeling 
proc 
joint sigdat conference emnlp vlc 
punyakanok roth 

classifiers sequential inference 
proc 
advances neural information processing systems 
sha pereira 

shallow parsing conditional random fields 
proc 
human language technology 
sun 

pitch accent prediction ensemble machine learning 
proc 
international conference spoken language processing 
taskar guestrin koller 

markov networks 
proc 
advances neural information processing systems 
taylor 

analysis synthesis intonation tilt model 
journal acoustical society america 
stemmer 

perceptually automatic prosody labeling enriched unit selection improve concatenative text speech synthesis 
volume pages 
