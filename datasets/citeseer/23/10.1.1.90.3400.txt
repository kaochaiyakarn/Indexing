computer science artificial intelligence laboratory technical report mit csail tr june approximate correspondences high dimensions trevor darrell massachusetts institute technology cambridge ma usa www csail mit edu approximate correspondences high dimensions trevor darrell computer science artificial intelligence laboratory massachusetts institute technology cambridge ma trevor csail mit edu pyramid intersection efficient method computing approximate partial matching sets feature vectors 
introduce novel pyramid embedding hierarchy non uniformly shaped bins takes advantage underlying structure feature space remains accurate sets high dimensional feature vectors 
matching similarity computed linear time forms mercer kernel 
show matching correspondence field may extracted small increase computational cost 
previous matching approximation algorithms suffer distortion factors increase linearly feature dimension demonstrate approach maintain constant accuracy feature dimension increases 
kernel discriminative classifier approach achieves improved object recognition results state art set kernel 
single data object described set feature vectors useful consider matching correspondence sets elements order measure similarity recover alignment parts 
example computer vision images represented collections local part descriptions extracted regions patches recognition algorithms rely establishing correspondence parts images quantify similarity objects localize object image :10.1.1.14.4931
likewise text processing document may represented bag word feature vectors example latent semantic analysis recover word meaning subspace project occurrence count vectors word 
relationship documents may judged terms matching sets local meaning features 
critical challenge compute correspondences feature sets efficient way 
optimal correspondences minimize matching cost require cubic time compute quickly prohibitive sizeable sets processing realistic large data sets impractical 
due optimal matching complexity researchers developed approximation algorithms compute close solutions fraction computational cost 
previous approximations suffer distortion factors increase linearly dimension features fail take advantage structure feature space 
new algorithm computing approximate partial matching point sets remain accurate sets high dimensional feature vectors benefits advantage underlying structure feature space 
main idea derive hierarchical data dependent decomposition feature space encode feature sets multi resolution histograms non uniformly shaped bins 
histograms pyramids matching cost efficiently calculated counting number features intersect bin weighting match counts geometric estimates inter feature dis 
method allows partial matchings means input sets varying numbers features outlier features larger set ignored penalty matching cost 
matching score computed time linear number features set forms mercer kernel suitable existing kernel algorithms 
demonstrate previous set matching approximations including original pyramid match algorithm described proposed approach maintain consistent accuracy dimension features sets increases 
approach generally applicable situations high dimensional features necessary may improve performance 
show data dependent hierarchical decomposition feature space results accurate correspondence fields previous approximation method uses uniform decomposition 
matching measure kernel discriminative classifier achieve improved object recognition results state art set kernel benchmark data set 
related previous matching approximation methods considered hierarchical decomposition feature space reduce matching complexity suffer distortion factors scale linearly feature dimension 
show alleviate decline accuracy high dimensional data tuning hierarchical decomposition particular structure data structure exists 
build pyramid match algorithm partial matching approximation uses histogram intersection efficiently count matches implicitly formed bin structures 
contrast data dependent non uniform bins precise weighting scheme results matchings consistently accurate structured high dimensional data 
idea partitioning feature space vector quantization vq fairly widely practice vision literature particular vq establish vocabulary prototypical image features textons visual words 
spatial pyramids shown effective matching quantized features :10.1.1.81.1372
authors shown hierarchical quantization image features provides scalable means indexing large feature vocabulary 
actual tree structure employed similar constructed authors interested matching single features independently access inverted file approach computes approximate correspondences sets features 
note similar distinction problem addressing approximate correspondence matching sets problem efficiently identifying approximate exact nearest neighbor feature vectors trees goal correspondence sets vectors single vector independently matched nearby vector 
approach main contribution new efficient approximate bipartite matching method measures correspondence similarity unordered variable sized sets vectors optionally extract explicit correspondence field 
previous approaches method uses feature distribution aid approximation efficiently produce accurate correspondences high dimensional feature sets 
call algorithm pyramid match histogram pyramids defined vocabulary structure feature space pyramids count implicit matches 
basic idea partition feature space pyramid non uniformly shaped regions distribution provided corpus feature vectors 
point sets encoded multi resolution histograms determined pyramid efficient intersection computation histogram pyramids yields approximate matching score original sets 
implicit matching version method estimates inter feature distances respective distances bin centers 
produce explicit correspondence field sets simply pyramid construct divide conquer optimal matching computation 
uniform bins vocabulary guided bins carve feature space uniformly shaped partitions left vocabulary structure feature space determine partitions right 
result bins better concentrated decomposing space features cluster particularly high dimensional feature spaces 
figures depict grid boundaries resolution levels feature space 
left plot contains coarser resolution level right plot contains finer 
features small points red bin centers larger black points blue lines denote bin boundaries 
experiments show proposed algorithm practice provides approximation optimal partial matching orders magnitude faster compute 
preliminaries consider feature space dimensional vectors point sets algorithm matches come input space contains sets feature vectors drawn xm xi value may vary instances sets please note text terms feature vector point interchangeably refer elements set 
partial matching point sets assignment maps points smaller set subset points larger equal sized set 
point sets partial matching xm pairs point unique point permutation indices specified specifies point matched xi cost partial matching sum distances matched points xi 
xi optimal partial matching uses assignment minimizes cost argmin 
matching wish efficiently approximate 
section describe algorithm approximates cost describe small increase computational cost extract explicit correspondences approximate 
building vocabulary guided pyramids step generate structure vocabulary guided pyramid define bin placement multi resolution histograms matching 
time process performed matching takes place 
bins pyramid follow feature distribution concentrate partitions features fall 
accomplish perform hierarchical clustering sample representative feature vectors randomly select example feature vectors feature type interest form representative feature corpus perform hierarchical means clustering euclidean distance build pyramid tree 
hierarchical clustering techniques agglomerative clustering possible change operation method 
unsupervised clustering process parameters number levels tree branching factor initial corpus features clustered top level groups group membership determined voronoi partitioning feature corpus cluster centers 
clustering repeated recursively times groups filling tree total levels containing ki bins nodes level levels counted children root leaves 
bins irregularly shaped sized boundaries determined voronoi cells surrounding cluster centers 
see 
bin vocabulary guided pyramid record diameter maximal inter feature distance points initial feature corpus assigned 
constructed vocabulary guided pyramid embed point sets multiresolution histograms 
point placement histogram pyramid determined comparing appropriate bin centers pyramid levels 
histogram count incremented bin choices point nearest terms distance function cluster initial corpus 
push point tree continue increment finer level counts branch bin center chosen level 
point assigned top level clusters assigned children recursively 
amounts total kl distances computed point pyramid bin centers 
bin structure vocabulary guided pyramid point set mapped pyramid hi hl hi hi ki dimensional histogram associated level vocabulary guided pyramid zi entries hi entry histogram triple giving bin index bin count bin points maximal distance bin center respectively 
storing vocabulary guided pyramid requires space kl dimensional feature vectors cluster centers 
point set histogram stored sparsely meaning ml nonzero bin counts maintained encode entire pyramid set features 
important point store kl counts point set hi represented triples having 
achieve sparse implementation follows vector set pushed tree described 
level record triple describing nonzero entry current bin 
vector pi pj denotes indices clusters traversed root far denotes count bin initially denotes distance computed inserted point current bin center 
reaching leaf level dimensional path vector indicating bins chosen level path vector uniquely identifies bin pyramid 
initially input set features yields total ml triples nonzero entry level point 
lists entries sorted index vectors triple collapsed list sorted nonzero entries unique indices entries index replaced single entry index summed counts maximum distance sorting done linear time integer sorting algorithms 
maintaining maximum distance point bin bin center allow efficiently estimate inter point distances time matching described section 
vocabulary guided pyramid match point sets pyramid encodings efficiently compute approximate matching score simple weighted intersection measure 
vocabulary guided pyramid provides partitioning feature space multiple resolutions direct matching 
basic intuition start collecting groups matched points bottom pyramid increasingly larger partitions 
way consider matching closest points leaves climb higher level clusters pyramid allow increasingly points matched 
define number new matches bin count minimum number points input sets contributes bin minus number matches counted child bins 
weighted sum counts yields approximate matching score 
nij denote element jth bin entry histogram hi ch nij denote element hth child bin entry similarly dij refer element triple 
point sets compute matching score pyramids follows wij min nij nij min ch nij ch nij outer sum loops levels pyramids second sum loops bins level innermost sum loops children bin 
min term reflects number points current bin second min term tallies number matches counted finer resolutions child bins 
note leaf nodes children sum zero 
matches new leaves 
matching scores normalized size input sets order favor larger sets 
number new matches calculated bin weighted wij estimate distance points contained bin 
vocabulary guided pyramid match alternatives distance estimate weights diameters pyramid bins precise weights maximal distances points bin center 
option conservative estimate actual inter point distances bin corpus features built pyramid representative feature space 
similarity measure choice advantage providing guaranteed mercer kernel see eliminates need store distance entry triples 
hand produce input specific distance estimates 
second option estimate distance points bin sum stored maximal center distances input set wij dij dij 
fact true upper bound furthest points 
just encode pyramids sparsely derive means compute intersections eqn 
traversing entire pyramid tree 
sparse lists hi hi sorted bin indices obtain minimum counts linear time number nonzero entries moving pointers lists processing entries share index making time required compute approximate matching pyramids ml 
key aspect method obtain measure matching quality point sets computing pair wise distances features savings sub optimal greedy matchings 
exploit fact points placement pyramid reflects distance 
inter feature distances computed kl distances need insert point pyramid small time cost amortized time re pyramid embedding approximate matching different point set 
suggested idea histogram intersection count implicit matches multiresolution grid 
bins constructed uniformly partition space bin diameters exponentially increase levels intersections weighted entire level 
contrast developed pyramid embedding partitions distribution features weighting schemes allow precise approximations inter feature costs 
show section proposed vocabulary guided pyramid match remains accurate efficient high dimensional feature spaces method limited practice relatively low dimensional features 
increased accuracy method provides complexity trade offs versus method require computing distances place points bins uniform shape size allows points placed directly division bin size 
hand sorting bin indices proposed method lower complexity integer values range branch factor typically smaller feature aspect ratio bounds range 
addition show section cost extracting matching cost function weights set distance estimates similarity measure weights set function inverse distance estimates 
explicit correspondence field pyramid high dimensions approaches cubic cost optimal measure remains linear proposed approach assuming features uniformly distributed 
approximation compare sets vectors case presence low cost correspondences indicates similarity nearest neighbor retrieval 
employ measure kernel function structured inputs 
mercer theorem kernel corresponds inner product feature space 
re write eqn 
wij pij min nij nij pij refers weight associated parent bin jth node level min operation kernels closed summation scaling positive constant vocabulary guided pyramid match mercer kernel wij pij 
inequality holds child bin receives similarity weight greater parent bin child bin distance estimate parent 
case weighting option wij set inversely proportional diameter bin vocabulary guided pyramid 
holds definition hierarchical clustering diameter subset points equal diameter points 
guarantee second weighting option 
addition scalar matching scores optionally extract explicit correspondence fields pyramid 
case vocabulary guided pyramid decomposes required matching computation hierarchy smaller matchings 
encountering bin nonzero intersection optimal matching computed features sets fall particular bin 
points bin matching flagged matched may take part subsequent matchings coarser resolutions pyramid 
results section provide results empirically demonstrate matching accuracy efficiency real data compare alternative approach uniform partitioning feature space 
addition directly evaluating matching scores correspondence fields show method leads improved object recognition performance kernel discriminative classifier 
approximate matching scores experiments extracted local sift features images eth database producing unordered set vectors example 
case space sift image features 
randomly sampled features images build vocabulary guided pyramid images test matching 
order test varying feature dimensions training features establish pca subspace project features varying numbers bases 
feature dimension build vocabulary guided pyramid encode point sets pyramids compute pair wise matching scores method optimal cost matching 
distance computation require mercer kernel employed second weighting option 
measure approximating optimal matching find ranking induce highly correlated ranking produced optimal matching data 
words images sorted similarly method 
spearman rank correlation coefficient provides quantitative measure evaluate difference rank corresponding ordinal values assigned measures 
spearman rank correlation optimal match ranking quality feature dimensions uniform bin pyramid vocabulary guided pyramid feature dimension optimal rankings optimal rankings rank correlations uniform bins pyramid match rankings rank correlations vocabulary guided pyramid match rankings optimal rankings optimal rankings rank correlations uniform bins pyramid match rankings rank correlations vocabulary guided pyramid match rankings comparison optimal approximate matching rankings image data 
left set rankings produced vocabulary guided pyramid match blue diamonds consistently accurate increasing feature dimensions accuracy alternative approach uniform bins red circles degrades linearly feature dimension 
right example rankings approximations 
left plot shows spearman correlation scores optimal measure method approximation varying feature dimensions pairwise matching scores test sets 
vocabulary guided pyramid match remains consistently accurate correlations high feature dimensions accuracy approximation uniform bins degrades rapidly dimensions 
plots right display actual ranks computed approximations dimensions summarized left plot 
red diagonals denote optimal performance approximate rankings identical optimal ones higher spearman correlations points clustered tightly diagonal 
low dimensional features methods perform fairly comparably full features vocabulary guided pyramid match far superior rightmost column 
optimal measure requires match approximation faster match 
pyramid matching gradation bin sizes pyramid levels pyramid capture distinct groups points match bins 
points sets equidistant bin placement allow match near points finest resolutions gradually add matches distant coarser resolutions 
low dimensions uniform data dependent bins achieve 
high dimensions uniform bin placement exponentially increasing bin diameters fail capture gradation features different point sets close match share bins bins large match 
matching score approximately number points weighted single bin size 
contrast method tailors feature space partitions distribution data high dimensions gradual increase bin size levels effect obtain discriminating implicit matches 
confirms intuition image data 
approximate correspondence fields image data ran explicit matching variant method compared correspondences features induced matching produced globally optimal measure 
comparison applied variant pyramids uniform bins 
notation section measure error approximate matching sum errors link field xi 
number new matches formed vocabulary guided bins uniform bins pyramid level number new matches formed vocabulary guided bins uniform bins pyramid level number new matches formed vocabulary guided bins uniform bins pyramid level number new matches formed vocabulary guided bins uniform bins pyramid level number new matches formed vocabulary guided bins uniform bins pyramid level number new matches formed pyramid level uniform dashed red solid blue bins sets increasing feature dimensions 
points represent mean counts level matches bars denote standard deviations 
low dimensions partition styles gradually collect matches pyramid 
higher dimensions uniform partitioning points sharing bin pyramid level 
contrast vocabulary guided bins accrue new matches consistently spread levels high dimensions decomposition tailored points cluster feature space 
mean error match error approximate correspondence fields uniform bins random uniform bins optimal vocab guided bins random vocab guided bins optimal feature dimension mean time match log scale computation time optimal uniform bins random uniform bins optimal vocab guided bins random vocab guided bins optimal feature dimension comparison correspondence field errors left associated computation times right 
best viewed color 
note errors level methods due pca variance dimensions vectors 
compares correspondence field error computation times vocabulary guided uniform pyramids 
approximation variations tested optimal assignment computed points bin random assignment 
left plot shows mean error match method right plot shows corresponding mean time required compute matches 
computation times expect optimal matching orders magnitude expensive approximations 
random assignment variation approximations negligible costs simply choose combination points bin 
important note high dimensions time required uniform bin pyramid optimal bin matching approaches time required optimal matching 
occurs similar reasons poorer matching score accuracy exhibited uniform bins left plot points match certain level pyramid help divide conquer computation high dimensions optimal matching entirety computed 
contrast expense vocabulary guided pyramid matching remains steady low high dimensions data dependent pyramids better divide matching labor natural segments feature space 
similar reasons errors comparable optimal bin variation vocabulary guided uniform bins 
vocabulary guided bins divide computation done inexpensively uniform bins divide computation poorly compute accurately 
likewise error uniform bins bin random assignment high lowest dimensions red line left plot large number points randomly assigned 
contrast pyramid bins result similar errors points bin matched optimally randomly indicating tuning bins data distribution pyramid achieves suitable breakdown computation high dimensions 
pyramid matching method mean recognition rate class time match vocabulary guided bins uniform bins realizing improvements recognition experimented proposed measure discriminative classifier object recognition task 
trained svm matching kernel recognize categories caltech benchmark data set 
trained images class tested remaining images 
extracted features harris maximally stable extremal region detectors sift descriptor 
generated features pca 
form mercer kernel weights set bin diameter aij wij aij set automatically mean distance sample features training set 
table shows improvements state art set matching kernel 
results show method accurate requires minor additional computation 
near perfect performance data set comparable reached literature real significance result distinguishes achieved vocabulary guided pyramid embedding opposed uniform histograms particularly high dimensional features 
addition optimal matching requires match cost method 
introduced linear time method compute matching point sets takes advantage underlying structure feature space remains consistently accurate efficient high dimensional inputs real image data 
results demonstrate strength approximation empirically compare directly alternative state art approximation successfully mercer kernel object recognition task 
commented potential applications vision text fact generic matching measure applied meaningful compare sets correspondence 
agarwal varadarajan 
near linear algorithm euclidean bipartite matching 
symposium computational geometry 
belongie malik puzicha 
shape matching object recognition shape contexts 
ieee trans 
pattern analysis machine intelligence april 
berg berg malik 
shape matching object recognition low distortion correspondences 
proceedings ieee conference computer vision pattern recognition san diego ca june 
charikar 
similarity estimation techniques rounding algorithms 
proceedings th annual acm symposium theory computing 
darrell 
pyramid match kernel discriminative classification sets image features 
proceedings ieee international conference computer vision beijing china oct 
indyk 
fast image retrieval embeddings 
rd international workshop statistical computational theories vision nice france oct 
landauer foltz laham 
lsa 
discourse processes 
lazebnik schmid ponce :10.1.1.81.1372
bags features spatial pyramid matching recognizing natural scene categories 
proceedings ieee conference computer vision pattern recognition new york city ny june 
leung malik 
recognizing surfaces dimensional textons 
proceedings ieee international conference computer vision corfu greece sept 
lowe 
distinctive image features scale invariant keypoints 
international journal computer vision jan 
matas urban 
robust wide baseline stereo maximally stable extremal regions 
british machine vision conference cardiff uk sept 
mikolajczyk schmid 
indexing scale invariant interest points 
proceedings ieee international conference computer vision vancouver canada july 

scalable recognition vocabulary tree 
proceedings ieee conference computer vision pattern recognition new york city ny june 
verri 
building kernels binary strings image matching 
ieee trans 
image processing feb 
shawe taylor cristianini 
kernel methods pattern analysis 
cambridge univ press 
zisserman 
video google text retrieval approach object matching videos 
proceedings ieee international conference computer vision nice oct 
