extending learning general adaptive multi agent systems gerald tesauro ibm thomas watson research center skyline drive hawthorne ny usa tesauro watson ibm com multi agent extensions learning require knowledge agents payoffs functions assume game theoretic play times agents 
proposes fundamentally different approach dubbed hyper learning values mixed strategies base actions learned agents strategies estimated observed actions bayesian inference 
hyper may effective different types adaptive agents persistently dynamic 
certain broad categories adaptation argued hyper may converge exact optimal time varying policies 
tests rock scissors hyper learns significantly exploit infinitesimal gradient ascent iga player policy hill climber phc player 
preliminary analysis hyper 
question agents may adapt strategic behavior interacting arbitrarily adapting agents major challenge machine learning multi agent systems research 
game theory provides calculation nash equilibrium strategies limited practical due hidden imperfect state information computational intractability 
trial error learning develop strategies trying actions number environmental states observing actions combination actions agents lead high cumulative reward 
highly effective single learner stationary environment algorithms learning able learn optimal policies line model environment 
straight shelf rl algorithms learning problematic learn deterministic policies mixed strategies generally needed environment generally non stationary due adaptation agents 
multi agent extensions learning published 
littman developed convergent algorithm player zero sum games 
hu wellman algorithm player general sum games convergence clarified bowling 
littman developed convergent agent friend learning algorithm combining cooperative learning adversarial learning 
extend normal function state action pairs function states joint actions agents 
algorithms number strong assumptions facilitate convergence proofs may realistic practice 
include agents payoffs fully observable agents learning algorithm learning agents strategies derivable game theoretic analysis current functions 
particular agents employ non gametheoretic nonstationary strategies learned functions accurately represent expected payoffs obtained playing agents associated greedy policies correspond best play agents 
aim develop general practical extensions learning avoiding assumptions 
multi agent environment modeled repeated stochastic game agents actions observable payoffs 
agents assumed learn forms learning algorithms unknown strategies may asymptotically non stationary 
learning proposed estimate agents current strategies observation game theoretic analysis 
considerations lead new algorithm section called hyper learning key idea learn value joint mixed strategies joint base actions 
section discusses effects function approximation exploration agents strategy dynamics hyper convergence 
section presents bayesian inference method estimating agents strategies applying version bayes rule observed action sequence 
section discusses implementation details hyper simple rock scissors test domain 
test results algorithms learning mixed strategies infinitesimal gradient ascent iga policy hill climbing phc 
preliminary results hyper vs discussed 
concluding remarks section 
general hyper formulation agent normal learning finite mdp repeatedly observes state chooses legal action observes immediate reward transition new state learning equation discount parameter appropriate learning rate schedule 
suitable method exploring state action pairs learning guaranteed converge optimal value function associated greedy policy optimal policy multi agent generalization mdp called stochastic game agent chooses action ai state payoffs ri agent state transitions functions joint actions agents 
important special class stochastic games matrix games payoffs functions joint actions 
choosing best action state agent task stochastic game choose best mixed strategy xi xi expected mixed strategy agents 
xi denotes set probabilities summing selecting ni ni legal actions state space possible mixed strategies continuous ni dimensional unit simplex choosing best mixed strategy clearly complex choosing best base action 
consider extensions learning stochastic games 
agent needs learn mixed strategy may depend mixed strategies agents obvious idea function evaluate entire mixed strategies base actions include state description observation estimate agents current mixed strategy 
forms basis proposed hyper learning algorithm formulated follows 
notational simplicity denote hyper learner current mixed strategy denote estimated joint mixed strategy agents referred opponents 
time agent generates base action observes payoff new state new estimated opponent strategy hyper function adjusted max greedy policy associated hyper function defined convergence hyper learning function approximation arg max hyper function continuous mixed strategies expect require sort function approximation scheme 
establishing convergence learning function approximation substantially difficult normal table finite mdp number known counterexamples 
particular finite discretization may cause loss mdp markov property 
function approximation schemes enable learning continuous spaces 
discretization scheme finite difference reinforcement learning provably converges optimal value function underlying continuous mdp 
employs simple uniform grid discretization mixed strategies hyper agent opponents 
attempt prove convergence scheme 
certain types opponent dynamics described plausible conjecture finite difference rl implementation hyper provably convergent 
exploration convergence normal learning requires visiting state action pair infinitely 
clearest way achieve simulation exploring starts training consists episodes starting randomly selected state action pair 
real environments may feasible may utilize policy randomized exploration greedy policies 
ensure visited states action tried infinitely guarantee states visited infinitely mdp ergodicity property 
result expect trained function exactly match ideal optimal mdp difference expected payoffs respective policies vanishingly small 
considerations apply equally hyper learning 
exploring starts states agent opponent mixed strategies guarantee sufficient exploration state action space 
exploring starts agent greedy exploration obtain sufficient exploration mixed strategy space 
opponents similar exploration situation equivalent normal qlearning stochastic game states visited infinitely cost expected payoff vanishingly small 
opponents explore effect reduction effective state space explored hyper agent effective state stochastic game state plus opponent strategy state 
negligible effect agent long run expected payoff relative policy learned opponent exploration 
opponent strategy dynamics opponent strategies governed arbitrarily complicated dynamical rules hyper learning converge arbitrary opponents 
broad categories identified convergence achievable 
simple example stationary opponent strategy constant 
case stochastic game obviously reduces equivalent mdp stationary state transitions stationary payoffs appropriate conditions exploration learning rates hyper clearly converge optimal value function 
important broad class dynamics consists opponent strategies evolve fixed history independent rule depending actions hyper player yt yt 
reasonable approximation player games individual negligible market impact player influence player occurs global summarization function 
cases relevant population strategy representation need express global summarizations averages details player 
example replicator dynamics model evolutionary game theory strategy grows decays population fitness relative population average fitness 
leads history independent order differential equation population average strategy 
models hyper learner faces effective mdp effective state undergoes stationary transitions hyper able converge 
final interesting class dynamics occurs opponent accurately estimate hyper strategy adapts strategy fixed history independent rule yt yt xt 
occur players required announce mixed strategies hyper player voluntarily announces strategy 
example infinitesimal gradient ascent iga model agent uses knowledge current strategy pair small change strategy direction gradient immediate payoff 
type model reduces mdp stationary history independent transitions effective state depending 
note claims reduction mdp depend hyper learner able accurately estimate opponent mixed strategy hyper learner face pomdp situation standard convergence proofs apply 
opponent strategy estimation consider estimation opponent strategies history base actions 
approach model consider class explicit dynamical models opponent strategy choose model best fits observed data 
difficult aspects approach class possible dynamical models may need extraordinarily large known danger infinite regress opponent models model attempts take account model alternative approach studied model free strategy estimation 
keeping spirit learning learns state valuations explicitly modeling dynamics underlying state transitions 
simple method section known exponential moving average ema technique 
maintains moving average opponent strategy updating observed action ua ua unit vector representation base action ema assumes observations informative older observations give accurate estimates significant strategy changes take place time scales 
bayesian strategy estimation principled model free alternative ema 
assume discrete set possible values uniform grid 
probability history observed actions computed bayes rule follows prior probability state sum extends strategy grid points 
conditional probability history strategy decomposed product individual action probabilities assuming conditional independence individual actions 
actions history equally informative regardless age may write ya corresponds naive bayes equal weighting observed actions 
reasonable assume actions informative 
way implement bayesian context exponent weights wk increase 
normalization factor write wk linear schedule wk weights intuitively obvious truncation history observations ensures weights positive 
implementation results examine performance hyper learning simple player matrix game rock scissors 
uniform grid discretization size represent mixed strategy component probabilities giving simplex grid size player mixed strategy entire hyper table size 
simulations simplicity constant learning rate 
hyper bayes formulation different opponent estimation schemes hyper learning omniscient perfect knowledge opponent strategy ema equation bayesian equations uniform prior 
equations modified bayesian case allow distribution opponent states probabilities 
corresponding equations max arg max technical note regarding equation improve tractability algorithm approximation hyper table updates performed updated distribution 
rock scissors results examine hyper training online iga player 
apart possible state observability discretization issues hyper principle able converge type opponent 
order conform original implicit assumptions underlying iga iga player allowed omniscient knowledge hyper player mixed strategy time step 
policies players greedy apart resets uniform random values time steps 
shows smoothed plot online bellman error hyper player average reward time step function training time 
exhibits hyper vs iga online bellman error time steps omniscient ema bayes hyper vs iga avg 
reward time step omniscient ema bayes time steps results hyper learning vs iga player rock scissors different opponent state estimation methods omniscient ema bayes indicated 
random strategy restarts occur time steps 
left plot shows smoothed online bellman error 
right plot shows average hyper reward time step 
asymptotic iga trajectory iga rock prob iga prob reward time steps trajectory iga mixed strategy hyper strategy starting single exploring start 
dots show hyper player cumulative rescaled reward 
progress convergence suggested substantially reduced bellman error substantial positive average reward time step 
estimation methods bayes reached lowest bellman error long time scales 
probably updates elements hyper table time step techniques update single element 
bayes far worst average reward start learning asymptotically clearly outperforms ema comes close matching performance obtained omniscient knowledge opponent state 
part hyper advantage comes exploiting transient behavior starting random initial condition 
addition hyper exploits asymptotic behavior iga shown 
plot shows initial transient lasts time steps 
hyper policy causes iga cycle different rock different probabilities preventing iga reaching nash mixed strategy 
profit hyper cycling positive average shown rising cumulative hyper reward 
observed cycling positive profitability reminiscent algorithm called phc play phc player 
interesting difference phc uses explicit model opponent behavior model needed hyper learner 
hyper vs phc online bellman error omniscient ema bayes hyper vs phc avg 
reward time step time steps omniscient ema bayes results hyper vs phc rock scissors 
left plot shows smoothed online bellman error 
right plot shows average hyper reward time step 
hyper vs phc player 
phc simple adaptive strategy actions rewards 
maintains table values base actions time step adjusts mixed strategy small step greedy policy current function 
phc strategy history dependent reduction mdp possible hyper learner 
hyper exhibit substantial reduction bellman error significantly exploits phc terms average reward shown 
phc ignores opponent state weak competitive player fact worse average reward iga 
interesting note bayesian estimation clearly outperforms ema estimation surprisingly outperforms omniscient state knowledge 
understood focus ongoing research 
hyper omniscient vs online bellman error hyper bayes vs online bellman error smoothed online bellman error hyper vs 
left plot uses omniscient state estimation right plot uses bayesian estimation 
examine preliminary data hyper vs 
average reward plots uninteresting expect player average reward close zero 
online bellman error shown interesting 
surprisingly plots noisy achieve asymptotic errors low lower iga phc 
hyper play history dependent argue mdp equivalence 
possible players greedy policies simultaneously stationary enabling optimize 
examining actual play converge nash point appear cycle small number grid points roughly zero average reward cycle players 
conceivably hyper converged cyclic nash equilibrium repeated game certainly nice outcome self play learning repeated game 
hyper learning appears versatile general purpose published multi agent extension learning date 
grid discretization scales badly function approximators may practical 
tantalizing early results rock scissors tests published adaptive opponents 
research topic progress 
vastly research needed develop satisfactory theoretical analysis approach understanding kinds realistic environments versions algorithm successfully deployed environments 
significant improvements opponent state estimation easy obtain 
principled methods setting recency weights achievable example proposes method training optimal weight values observed data 
time series prediction data mining methods result substantially better estimators 
model estimators advantageous reasonable basis modeling opponents dynamical behavior 
author michael littman helpful discussions rish insights bayesian state estimation michael bowling assistance implementing phc algorithm 
bowling 
convergence problems general sum multiagent reinforcement learning 
proceedings icml pages 
bowling veloso 
multiagent learning variable learning rate 
artificial intelligence 

chang kaelbling 
playing believing role beliefs multi agent learning 
proceedings nips 
mit press 
hong hosking natarajan 
multiplicative adjustment class probability educating naive bayes 
technical report rc ibm research 
hu wellman 
multiagent reinforcement learning theoretical framework algorithm 
proceedings icml pages 
morgan kaufmann 
kearns mansour 
efficient nash computation large population games bounded influence 
proceedings uai pages 
littman 
markov games framework multi agent reinforcement learning 
proceedings icml pages 
morgan kaufmann 
littman 
friend foe learning general sum games 
proceedings icml 
morgan kaufmann 

convergent reinforcement learning algorithm continuous case finite difference method 
proceedings ijcai pages 
morgan kaufman 
singh kearns mansour 
nash convergence gradient dynamics general sum games 
proceedings uai pages 
morgan kaufman 
smart kaelbling 
practical reinforcement learning continuous spaces 
proceedings icml pages 
uther veloso 
tree discretization continuous state space reinforcement learning 
proceedings aaai pages 
watkins 
learning delayed rewards 
phd thesis cambridge university 
weibull 
evolutionary game theory 
mit press 
