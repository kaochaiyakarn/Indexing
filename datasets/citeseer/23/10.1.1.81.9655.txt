neural information processing letters reviews vol january review blind source separation independent component analysis review choi department computer science university science technology san dong nam gu korea mail ac kr andrzej cichocki riken brain science institute japan warsaw university technology poland mail cia bsp brian riken go jp min park soo young lee department biosystems department electrical engineering computer science chung moon soul center korea advanced institute science technology dong gu korea mail kaist ac kr submitted october blind source separation bss independent component analysis ica generally wide class unsupervised learning algorithms potential applications areas engineering neuroscience 
trend bss consider problems framework matrix factorization general signals decomposition probabilistic generative tree structured graphical models exploit priori knowledge true nature structure latent hidden variables sources spatio temporal decorrelation statistical independence sparseness smoothness lowest complexity sense best predictability 
possible goal decomposition considered estimation sources necessary statistically independent parameters mixing system generally finding new reduced hierarchical structured representation observed sensor data interpreted physically meaningful coding blind source estimation 
key issue find transformation coding linear nonlinear true physical meaning interpretation 
review bss ica including various algorithms static dynamic models applications 
mainly consists parts bss algorithms static models instantaneous mixtures extension bss ica incorporating sparseness non negativity constraints bss algorithms dynamic models convolutive mixtures 
keywords independent component analysis blind source separation information theory feature extraction independent component analysis review choi cichocki park lee meg eeg signals brain sources 
general model illustrating blind source separation bss models exploited noninvasive multi sensor recording brain activity eeg meg 
assumed scalp sensors electrodes picks superposition neuronal brain sources non brain sources related movements eyes muscles 
objective identify individual signals coming different areas brain 

fairly general blind signal separation bss problem referred blind signal decomposition blind source extraction bse formulated follows see 
observe records sensor signals 
xm discrete time means transpose vector unknown mimo multiple input multiple output mixing filtering system 
objective find inverse system termed reconstruction system neural network adaptive inverse system exists stable order estimate primary source signals 
sn specific properties 
estimation performed basis output signals 
yn sensor signals 
preferably inverse unmixing system adaptive way tracking capability non stationary environments 
estimating source signals directly convenient identify unknown mixing filtering system inverse system exist especially system overcomplete number observations number source signals estimate source signals implicitly exploiting priori information mixing system applying suitable optimization procedure 
problems separating extracting original source waveforms sensor array knowing transmission channel characteristics sources expressed briefly number related bss blind signal decomposition problems independent component analysis ica extensions topographic ica multidimensional ica kernel ica tree dependent component analysis subband decomposition ica sparse component analysis sca sparse pca spca non negative matrix factorization nmf smooth component analysis parallel factor analysis time frequency component analyzer multichannel blind deconvolution :10.1.1.107.3613
appears magical blind source separation estimating original source signals knowing parameters mixing filtering processes 
difficult imagine estimate 
fact priori knowledge possible uniquely estimate original source signals 
usually estimate certain 
mathematical terms ambiguities expressed arbitrary scaling permutation delay estimated source signals 
preserve waveforms original sources 
severe limitations great number applications limitations essential relevant information source signals contained temporal waveforms time frequency patterns source signals usually amplitudes order arranged output system 
dynamical models guarantee estimated extracted signals exactly waveforms source signals requirements neural information processing letters reviews vol january mixing whitening separation 
block diagrams illustrating linear blind source separation blind identification problem general schema optional whitening detailed model 
overcomplete problem separating matrix may exist cases attempt identify mixing matrix estimate sources exploiting priori knowledge sparsity independence unknown sources 
relaxed extent extracted waveforms distorted filtered convolved versions primary source signals see 
mixing filtering processes unknown input sources sj may different mathematical physical models depending specific applications :10.1.1.107.3613
linear bss models simplest forms expressed algebraically specific problems matrix factorization observation called sensor data matrix 
perform matrix factorization number available samples number observations number sources represents unknown basis data matrix mixing matrix depending applications unknown matrix representing errors noise matrix 
contains corresponding latent hidden components give contribution basis vectors 
usually latent components represent unknown source signals specific statistical properties temporal structures 
matrices usually clear physical meanings 
example rows matrix represent sources sparse possible sca statistically mutually independent possible ica 
required estimated components piecewise smooth take non negative values nmf values specific constraints 
decompositions matrix factorizations provide exact reconstruction data shall consider decompositions approximative nature 
fact problems signal image processing expressed terms matrix factorization 
different cost functions imposed constraints may lead different types matrix factorization 
signal processing applications data matrix 
represented vectors 
set discrete time instants multiple measurements recordings compact aggregated matrix equation written independent component analysis review choi cichocki park lee non stationarity time varying variances mutual independence non gaussianity ica time frequency spectral spatial diversities temporal structure linear predictability non whiteness 
basic approaches blind source separation 
approach exploits priori knowledge specific properties source signals 
vector form system linear equations 

xm vector observed signals discrete time instant 
sn vector components time instant formulated problems related closely linear inverse problem generally solving large ill conditioned system linear equations overdetermined underdetermined depending applications necessary estimate reliably vectors cases identify matrix noisy data 
systems equations contaminated noise errors problem finding optimal robust respect noise solution arises 
wide classes extrapolation reconstruction estimation approximation interpolation inverse problems converted minimum norm problems solving underdetermined systems linear equations 
generally speaking signal processing applications overdetermined system linear equations describes filtering enhancement deconvolution identification problems underdetermined case describes inverse extrapolation problems 
general number source signals unknown 
assumed sensor vector available necessary design feed forward recurrent neural network associated adaptive learning algorithm enables estimation sources identification mixing matrix separating matrix tracking abilities 
bss obtained finding full rank linear transformation separating matrix means defined pseudo inverse output signal vector 
yn contains components independent possible measured information theoretic cost function kullback leibler divergence criteria sparseness smoothness linear predictability :10.1.1.56.3619
different source separation algorithms available principles summarized fundamental approaches see popular approach exploits cost function measure signals statistical independence non gaussianity sparseness 
original sources assumed statistically independent temporal structure higher order statistics hos essential implicitly explicitly solve bss problem 
case method allow gaussian source 
sources temporal structures source non vanishing temporal correlation restrictive conditions statistical independence second order statistics sos sufficient estimate mixing matrix sources 
line methods developed :10.1.1.52.5680
note sos methods allow separation sources identical power spectra shapes 
independent identically distributed sources 
data represented time domain complex frequency time frequency domain index may different meaning 
neural information processing letters reviews vol january 
fundamental procedures implemented exploited bss ica efficient decomposition extraction signals 
third approach exploits non stationarity ns properties second order statistics sos 
mainly interested second order non stationarity sense source variances vary time 
non stationarity taken account shown simple decorrelation technique able wide class source signals perform bss task 
contrast approaches non stationarity information methods allow separation colored gaussian sources identical power spectra shapes 
allow separation sources identical non stationarity properties 
works non stationary source separation 
methods exploit temporal structure sources mainly second order correlations non stationarity sources lead simplest scenario second order statistics bss methods 
contrast bss methods hos second order statistics methods infer probability distributions sources nonlinear activation score functions see sections 
fourth approach exploits various diversities signals typically time frequency spectral time coherence time frequency diversities generally joint space time frequency stf diversity 
approach leads concept time frequency component analyzer 
decomposes signal specific components time frequency domain computes time frequency representations individual components 
usually components interpreted localized sparse structured signals time frequency plain spectrogram 
words components estimated analyzing time frequency distribution observed signals 
provides elegant promising solution suppression artifacts interference masking filtering undesired components 
sophisticated advanced approaches combinations integration mentioned approaches hos sos ns stf space time frequency diversity order separate extract sources various statistical properties reduce influence noise undesirable interferences 
mentioned bss methods belongs wide class unsupervised learning algorithms 
unsupervised learning techniques try discover structure underlying data set extraction meaningful features finding useful representations data 
data interpreted different ways knowledge needed determine features properties represent true latent hidden components 
example pca finds low dimensional representation data captures variance 
hand sca tries explain data mixture sparse components usually time frequency domain nmf seeks explain data parts localized additive representations non negativity constraints 
bss algorithms ica sca nmf std techniques considered pure mathematical formulas powerful mechanical procedures illusion left user machinery optimally implemented 
successful efficient tools strongly depends priori knowledge common sense appropriate preprocessing postprocessing tools 
words preprocessing data postprocessing models expertise truly needed order extract reliable significant physiologically meaningful components 
typical preprocessing tools include principal component analysis pca factor analysis fa model reduction whitening filtering fast fourier transform fft time frequency representation tfr sparsification wavelets package diversities mean usually different characteristics features signals 
independent component analysis review choi cichocki park lee transformation data see 
postprocessing tools includes deflation reconstruction cleaning original raw data removing undesirable components noise artifacts 
hand assumed linear mixing models valid approximately original sources signals specified statistical properties 
problem blind source separation bss received wide attention various fields signal analysis processing speech image biomedical signals eeg meg fmri pet especially signal extraction enhancement denoising model reduction classification problems :10.1.1.107.3613
real time applications may need special analogue digital chips developed 

blind source separation spatio temporal decorrelation non stationarity temporal spatial spatio temporal play important roles eeg meg data analysis 
techniques second order statistics sos 
basis modern subspace methods spectrum analysis array processing preprocessing stage order improve convergence properties adaptive systems eliminate redundancy reduce noise 
spatial decorrelation considered necessary sufficient condition stronger stochastic independence criteria 
bss ica tasks usually somewhat easier posed ill conditioned subsequent separating unmixing system described orthogonal matrix real valued signals unitary matrix complex valued signals weights 
furthermore spatio temporal time delayed decorrelation identify mixing matrix perform blind source separation colored sources certain weak conditions 
robust orthogonalization whitening whitening data sphering important pre processing step variety bss methods 
conventional whitening exploits equal time correlation matrix data effect additive noise removed 
idea new whitening method lies utilizing time delayed correlation matrices sensitive white noise 
new whitening method named robust whitening motivated fact sensitive white noise 
somewhat different huber robust statistics 
time delayed correlation matrix observation data form rx ars 
easily see transformation rx whiten data effect noise vector 
reduces noise effect project data signal subspace contrast conventional whitening transformation rx 
source separation methods employed robust whitening transformation 
general matrix rx positive definite whitening transformation rx may valid time lag 
idea robust whitening consider linear combination time delayed correlation matrices cx im rx 
literally space time 
spatio temporal data spatial location temporal time related components 
neural information processing letters reviews vol january proper choice results positive definite matrix cx extended matrix pencil method 
method find set coefficients matrix cx positive definite 
matrix cx eigen decomposition 
um cx diagonal matrix diagonal elements principal eigenvalues cx 

un 
robust whitening transformation matrix project data dimensional signal subspace carrying whitening 
denote whitened dimensional data qx transformation bs qv whitened data sense im unitary mixture sources additive noise bb algorithm outline robust whitening 
estimate time delayed correlation matrices construct mj matrix compute singular value decomposition svd 
mj mj orthogonal matrices nonzero entries position 
zeros 
number sources detected inspecting singular values 
define ui ith column vector matrix 

compute 
choose initial 
compute un 


compute schur decomposition check positive definite 
positive definite algorithm terminated 
go step 
independent component analysis review choi cichocki park lee 
choose eigenvector corresponding smallest eigenvalue update replacing ju ut ut 
ju go step 
loop terminated finite number steps see proof 

compute cx perform eigenvalue decomposition cx im cx contains eigenvectors associated principal singular values 

robust whitening transformation performed 
qx note case equal number sources sensors steps necessary 
simply 
algorithm properties algorithm belongs group second order statistics spatio temporal decorrelation sos std algorithms 
provides identical similar decomposition raw data known popular algorithms 
class algorithms classified referred ica algorithms 
algorithms exploit implicitly explicitly statistical independence 
contrast standard higher order statistics ica algorithms able estimate colored gaussian distributed sources performance estimation original sources usually better sources temporal structure 
algorithm similarity standard pca 
main difference employs pca times cascade way separate steps step standard pca applied whitening sphering data second step svd pca applied time delayed covariance matrix pre whitened data 
mathematically algorithm stage procedure step apply standard robust sphering linear transformation qx standard covariance matrix rxx xt vector observed data time instant pre whitened data svd applied time delayed covariance matrix rx xt diagonal matrix decreasing singular values orthogonal matrices left right singular vectors 
unmixing separating matrix estimated 
main advantage algorithm comparison bss ica algorithms allows automatically order components due application svd singular value decomposition 
fact components ordered decreasing values singular values time delayed covariance matrix 
words algorithm exploit simple principle estimated components tends complex precisely better linear predictability mixture sources 
emphasized components estimated uniquely defined consistently ranked 
consistent ranking due fact singular values ordered decreasing order 
real world data probability singular values achieve exactly value small ordering consistent unique 
neural information processing letters reviews vol january disadvantage algorithm relatively sensitive additive noise algorithm exploits time delayed covariance matrix 
robust current trend ica bss investigate average eigen structure large set data matrices formed functions available data typically covariance cumulant matrices different time delays 
words objective extract reliable information estimation sources mixing matrix eigen structure possibly large set data matrices 
practice finite number samples signals corrupted noise data matrices exactly share 
furthermore noted determining eigen structure basis data matrices leads usually poor unsatisfactory results matrices usually arbitrary choice may degenerate eigenvalues leads loss information contained data matrices 
statistical point view order provide robustness accuracy necessary consider average eigen structure account simultaneously possibly large set data matrices 
average eigen structure easily implemented linear combination time delayed covariance matrices applying standard evd svd 
alternative approach evd svd apply approximate joint diagonalization procedure 
objective procedure find orthogonal matrix set matrices rx pi 
rx pi data matrices example time delayed covariance matrices rx pi pi cumulant matrices di diagonal real represent additive errors noise matrix small possible 
arbitrary matrices rx pi problem overdetermined generally find exact diagonalizing matrix important advantage joint approximate diagonalization numerically efficient algorithms exist computation including jacobi techniques sided sided alternating squares als parallel factor analysis subspace fitting techniques employing efficient gauss newton optimization 
idea implemented robust algorithm briefly outlined follows 
perform robust orthogonalization similar algorithm 

estimate set covariance matrices rx pi pi rx pi preselected set time lags 
pl band pass filters bi 

perform rx pi estimate orthogonal matrix available numerical algorithm 

estimate source signals mixing matrix main advantage algorithm robustness respect additive noise number covariance matrices sufficiently large typically 
incorporating non stationarity second order non stationary source separation algorithm illustrated 
set matrices form said pencil 
frequently encounter case symmetric symmetric positive definite 
pencils variety referred symmetric definite pencils 
independent component analysis review choi cichocki park lee theorem pp 
symmetric definite exists nonsingular matrix 
un ui ir ui 
diag 
diag 

apparent theorem symmetric symmetric positive definite generalized eigenvector valid solution distinct 
unfortunately symmetric definite pencil considered numerical instability problem calculation generalized eigenvectors 
explain construct symmetric definite pencil 
consider time delayed correlation matrices rx rx nonzero time lags 
requirement symmetry replace rx rx defined rx rx 
pencil symmetric pencil 
general matrix positive definite 
consider linear combination time delayed correlation matrices im 
set coefficients chosen way symmetric matrix positive definite 
simple way finite step global convergence algorithm 
method referred extended matrix pencil method summarized 
algorithm outline extended matrix pencil method 
compute time lag calculate matrix im method 

find generalized eigenvector matrix pencil satisfies 
unmixing matrix 
consider case sources second order non stationary non vanishing temporal correlations 
follows assumptions kr am kr 
practice kr computed samples rth time windowed data frame rx kr nr nr kr rx kr kr nr set data points rth time windowed frame nr number data points nr 
straightforward see extended matrix pencil method applied case non stationary sources 
algorithm outline extended matrix pencil method non stationary case neural information processing letters reviews vol january 
partition observation data non overlapping blocks 

compute time lag data points 

calculate matrix im method data points 

find generalized eigenvector matrix pencil satisfies 
unmixing matrix 
remarks method employed matrices rx estimate unmixing matrix 
order improve statistical efficiency employ joint approximate diagonalization method case jade 
joint approximate diagonalization method finds unitary transformation jointly matrices symmetric positive definite 
method joint approximate diagonalization 
sense includes special case sources stationary 
algorithm summarized 
algorithm outline 
robust whitening method described section 
applied obtain whitened vector qx 
robust whitening step available data points 

divide whitened data non overlapping blocks calculate kr 

words time windowed data frame compute different time delayed correlation matrices 

find unitary joint kr joint approximate diagonalization method satisfies set diagonal matrices 
unmixing matrix computed kr pham developed joint approximate diagonalization method non unitary joint hermitian positive matrices computed way similar classical jacobi method 
second order non stationarity exploited noise free data considered 
extended pham cardoso method generalizes method 
advantage extended pham cardoso fact require whitening step joint approximate diagonalization method finds non unitary joint 
requires set matrices diagonalized hermitian positive definite need find linear combination time delayed correlation matrices positive definite data frame increase computational complexity 
algorithm outline extended pham cardoso 
divide data non overlapping blocks calculate kr 

independent component analysis review choi cichocki park lee 
data frame compute cr kr method 
note cr symmetric positive definite 

find non unitary joint cr joint approximate diagonalization method satisfies set diagonal matrices 

unmixing matrix computed 
blind source extraction linear predictability adaptive band pass filters main approaches solve problem blind separation deconvolution 
approach mentioned briefly previous section simultaneously separate sources 
second extract sources sequentially blind fashion separating simultaneously 
applications large number sensors electrodes sensors microphones transducers available source signals subjects interest 
example modern eeg meg devices observe typically sensor signals source signals interesting rest considered interfering noise 
example cocktail party problem usually essential extract voices specific persons separate source signals speakers available mixing form array microphones 
applications essential develop apply reliable robust effective learning algorithms enable extract small number source signals potentially interesting contain useful information 
blind source extraction bse approach may advantages simultaneous blind separation deconvolution 
interesting signals need extracted 
example source signals mixed large number noise terms may extract specific signals possess desired statistical properties 
signals extracted specified order statistical features source signals order determined absolute values generalized normalized kurtosis 
blind extraction sources considered generalization sequential extraction principal components decorrelated output signals extracted decreasing order variances 
available learning algorithms bse purely local global stable typically biologically plausible 
different models criteria 
criterion higher order statistics hos assumes sources mutually statistically independent non gaussian gaussian 
independence criteria measures non gaussianity 
second criterion concept linear predictability assumes source signals temporal structure sources colored different autocorrelation functions equivalently different spectra shapes 
approach exploit temporal structure signals statistical independence 
intuitively speaking source signals sj complexity mixed sensor signals xj 
words degree temporal predictability source signal higher equal mixture 
example waveforms mixture sine waves different frequencies complex predictable original sine waves 
means applying standard linear predictor model minimizing mean squared error measure predictability separate extract signals different temporal structures 
precisely minimizing error maximize measure temporal predictability recovered signal 
neural information processing letters reviews vol january ar model sources mixing unknown robust blind extraction sn learning algorithm yj 
block diagram illustrating implementation learning algorithm blind extraction temporally correlated source 
worth note criteria bse temporal linear predictability non gaussianity kurtosis may lead different results 
temporal predictability forces extracted signal smooth possibly complex non gaussianity measure forces extracted signals independent possible sparse representation sources positive kurtosis 
assume temporally correlated source signals modelled autoregressive processes ar see sj sj sj aj sj aj sj unknown innovative processes 
practice ar model extended general models auto regressive moving average arma model hidden markov model hmm :10.1.1.107.3613
ill conditioned problems mixing matrix ill conditioned source signals different amplitudes apply optional preprocessing sensor signals form qx decorrelation matrix ensuring auto correlation matrix rx identity matrix 
model temporal structures source signals consider linear processing unit adaptive filter transfer function estimates aj illustrated 
assume simplicity want extract source signal sj available sensor vector 
purpose employ single processing unit described see xi 


pz transfer function corresponding fir filter 
independent component analysis review choi cichocki park lee xm 
neural network structure single extraction unit linear predictor 
noted fir filter sparse representation 
particular single processing unit delay parameters 
processing unit outputs estimates extracted source signals represents linear prediction error estimation innovation passing output signal fir filter 
objective estimate optimal values vectors way processing unit successfully extracts sources 
achieved global vector defined wt contains nonzero element th row cj arbitrary nonzero scaling factor 
purpose reformulate problem minimization cost function 
main motivation applying cost function assumption primary source signals signals interest temporal structures modelled autoregressive model 
ar model source signals filter output represented py defined error estimator innovation source sj 
mean squared error achieves minimum positive scaling constant sj 
holds 
consider processing unit shown 
associated cost function evaluated follows rx rx rx rx estimators true values correlation cross correlation matrices rx rx respectively 
order estimate vectors evaluate gradients cost function equalize zero follows solving matrix equations obtain simple iterative algorithm rx rx 
rx matrices estimated parameters obtained previous iteration step 
order avoid trivial solution normalize vector unit length iteration step ensures 
worth note derivation matrices assumed independent vector estimated previous iteration step 
phase neural information processing letters reviews vol january xm wj wj cj bandpass filter yi yi 
conceptual model single processing unit extraction sources adaptive band pass filter 
procedure similar expectation maximization em scheme freeze correlation cross correlation matrices learn parameters processing unit ii freeze learn new statistics matrices estimated source signal go back repeat 
phase algorithm extracts source signal phase ii learns statistics source 
algorithm considerably simplified 
noted order avoid inversion autocorrelation matrix rx iteration step perform standard standard pca preprocessing step normalize sensor signals unit variance 
cases rx algorithm simplified rx 
rx rx interesting note algorithm formulated equivalent form 
follows algorithm similar power method finding eigenvector associated maximal eigenvalue matrix rx px xt 
observation suggests necessary minimize cost function respect parameters choose arbitrary set largest eigenvalue unique single 
generally eigenvalues generalized covariance matrix rx distinct extract sources simultaneously estimating principal eigenvectors rx 
noisy data linear predictor band pass filter parallel way processing units bank band pass filters fixed adjustable center frequency band pass bandwidth 
approach illustrated 
minimizing cost function wj subject constraint wj obtain line learning rule data wj yj wj yj wj wj yj yj bj yj xt bj 
algorithm extract source successfully cross covariance matrix unique single maximum eigenvalue 
proposed algorithm insensitive white noise arbitrary distributed zero mean noise bandwidth band pass filter 
processing unit able extract filtered independent component analysis review choi cichocki park lee noise version source signal narrow band signal 
summarizing method employed adaptive band pass filter advantages method need deflation procedure 
processing unit extract desired narrow band sources sequentially adjusting center frequency bandwidth band pass filter 
parallel extraction arbitrary group sources possible employing band pass filters different characteristics 
algorithm computationally simple efficient 
proposed algorithm robust additive noise white narrow band colored noise 
contrast methods covariance matrix noise need estimated modelled 

independent component analysis ica ica defined follows ica random vector obtained finding full rank separating transformation matrix output signal vector 
yn components estimated independent possible evaluated information theoretic cost function minima kullback leibler divergence maximization cumulants :10.1.1.56.3619
independence random variables general concept decorrelation 
roughly speaking say random variables yi yj statistically independent knowledge values yi provides information values yj 
mathematically independence yi yj expressed relationship yi yj yi yj denotes probability density function pdf random variable words signals independent joint pdf factorized 
independent signals zero mean generalized covariance matrix yi yj different odd nonlinear activation functions tanh super gaussian sources non singular diagonal matrix 
yn yn covariances yi yj zero 
noted odd probability density function zero mean source signal terms form yi yi equal zero 
true general condition statistical independence signals vanishing high order cross cumulants 
diagonalization principle expressed fg diagonal positive definite matrix typically pre multiplying equation separating matrix obtain suggest iterative multiplicative learning algorithm fg fg neural information processing letters reviews vol january equation represents symmetric orthogonalization keep algorithm stable 
algorithm simple fast need data 
fact wide class algorithms ica expressed general form see table matrix take different forms example suitably chosen nonlinearities yn yn 
assuming prior knowledge source distributions pi yi estimate maximum likelihood ml log det natural gradient descent increase likelihood get log pi yi 
fn yn entry wise nonlinear score function defined fi yi yi log pi yi pi yi yi alternatively signals corrupted additive gaussian noise higher order matrix cumulants 
illustrative example consider cost function measure independence log det yi notations cq denotes order cumulants signal yi cp denotes cross cumulant matrix elements ij cum yi yi 
yi yj yj 
yj 
term assures determinant global matrix approach zero 
including term avoid trivial solution yi second terms force output signals far possible gaussianity higher order cumulants natural measure non gaussianity vanish gaussian signals 
shown cost function derive equivariant robust respect gaussian noise algorithm sq sq sign diag sq 
noted ica perform blind source separation enable estimate true sources statistically independent non gaussian possibly 
subband decomposition independent component analysis sd ica despite success standard ica applications basic assumptions ica may hold kind signals caution taken standard ica analyze real world problems especially biomedical signal processing 
fact definition standard ica algorithms able estimate statistically dependent original sources independence assumption violated 
section natural extension generalization ica called subband decomposition ica sd ica relaxes considerably assumption regarding mutual independence primarily sources 
key idea approach assumption unknown wide band source signals dependent narrow band sub components independent 
words assume unknown source modelled represented sum linear combinations narrow band sub signals sub components si si si sik 
independent component analysis review choi cichocki park lee table 
basic equivariant adaptive learning algorithms ica 
algorithms require 

learning algorithm 
gt cichocki unbehauen diagonal matrix non negative elements ii gt cruces cichocki 
yt yi yi yi bell sejnowski ii yi yi ii amari cichocki yang choi cichocki amari 
yt yt cardoso laheld 
yt yt pajunen 
yt ii yi yi ii ii yi hyv rinen oja 
yt choi cichocki amari ii amari cichocki 
sq yi yj cum yi yj 
yj cruces cichocki 
exp 
cichocki neural information processing letters reviews vol january 
bank band pass filters employed preprocessing stage sd ica typical frequency bands 
sensor signal employ identical set filters 
example simplest case source signals modelled decomposed low highfrequency sub components si sil 

practice high frequency sub components mutually independent 
case high pass filter hpf extract high frequency sub components apply standard ica algorithm preprocessed sensor observed signals 
implemented concepts software extensively tested experimental data 
basic concept subband decomposition ica divide sensor signal spectra subbands treat individually purpose hand 
subband signals ranked processed independently 
assume certain set sub components independent 
provided frequency subbands sub components say sij mutually independent temporally decorrelated easily estimate mixing separating system condition subbands identified priori knowledge detected self adaptive process 
purpose simply apply standard ica algorithm available raw sensor data suitably pre processed subband filtered sensor signals 
explanation summarized follows 
sd ica subband decomposition ica formulated task estimation separating matrix estimating mixing matrix basis suitable subband decomposition sensor signals applying classical ica raw sensor data preselected subbands source sub components independent 
applying standard ica bss algorithm specific subbands raw sensor data obtain sequence separating matrices 
separating matrix original data separating matrix preprocessing sensor data xj th subband 
order identify subbands corresponding source sub components independent propose compute global mixing separating matrices jw estimating separating matrix th subband 
sub components mutually independent subbands say subband subband global matrix jw jq generalized permutation matrix nonzero dominated element row column 
follows simple observation case matrices represent inverses mixing matrix neglecting nonessential scaling permutation ambiguities 
way blindly identify essential information frequency subbands source sub components independent easily identify correctly mixing matrix 
furthermore concept estimate blindly performance index compare performance various ica algorithms especially large scale problems 
preprocessing stage linear transforms especially sophisticated methods independent component analysis review choi cichocki park lee mixture mixture mixture mixture mixture mixture output output output output output output output output output output output output 
example observed overlapped images 
reconstructed original images sd ica subband decomposition preprocessing 
block transforms multirate subband filter bank wavelet transforms applied 
extend generalize concept performing decomposition sensor signals composite time frequency domain frequency subbands 
naturally leads concept wavelets packets subband hierarchical trees block transform packets 
preprocessing techniques implemented 
simulation illustrative example experiment human faces persons mixed random generated ill conditioned mixing matrix assumed unknown 
mixing images shown linear superposition strongly correlated faces classical ica algorithm failed separate 
order reconstruct original images applied preprocessing stage subbands filters observed images 
method described identified subbands sub components completely independent 
preprocessed mixed images applied standard natural gradient ica learning algorithm 
estimated original images shown 
interesting note original images reconstructed perfectly strongly dependent 
principle applied real world eeg meg fmri signals 
neural information processing letters reviews vol january ica bss ica bss ica bss ica bss wl 
bank filters employed preprocessing stage investigating validity reliability ica bss algorithms 
subbands overlapped complex subbands forms 
furthermore coefficients transfer functions fir filters suitably designed randomly chosen 

validity ica bss algorithms real world data fundamental question bss problem obtained results specific bss ica algorithm reliable represent inherent properties model data just random purely mathematical decomposition data physical meaning 
fact bss algorithms stochastic nature results may somewhat different different runs algorithm 
results obtained single run single set data bss algorithm interpreted reserve reliability estimated sources analyzed investigating spread obtained estimates runs 
analysis performed example resampling bootstrapping method available data randomly changed producing surrogate data sets original data 
specific ica bss algorithm run times bootstrapped samples somewhat different 
alternative approach called developed running specific bss algorithm times various different initial conditions parameters visualizing clustering structure estimated sources components signal subspace 
order estimate algorithmic reliability suggested run bss algorithm times different initial conditions assessing components run 
purpose estimated components clustered classified 
reliable components corresponds small separated clusters rest components unreliable components usually belong cluster 
worth note concept msd ica described previous section extended easily general flexible multi dimensional models checking validity reliability ica generally bss algorithms number sensors equal larger number unknown sources see 
model bank stable filters transfer functions hi example set fir finite impulse response filters 
parameters coefficients fir filters randomly generated 
case proposed method similarity resampling bootstrap approach proposed 
similarly msd ica run bss algorithm sufficiently large number filters generate set separating matrices 
alternatively set estimated mix matrices 

step estimate global mixing separating matrices pw performance blind separation characterized single performance index referred amari performance index refer blind performance index know true mixing matrix bp ii gij maxi gij gij maxj gij max gij ij th element matrix 
cases able achieve perfect separation set matrices extended data bootstrapped initial conditions changed run 
independent component analysis review choi cichocki park lee sources able extract sources 
cases global performance index define local performance index gij bp ii maxj gij alternatively performance index component ei arccos gii gij performance index bp ii ei specific index bands close zero means high probability component successfully extracted 
order asses significant components estimated components clustered mutual similarities 
similarities searched time domain frequency domain 
natural measure similarity estimated components absolute value mutual correlation coefficients rij elements similarity matrix rxx 
rxx xx covariance matrix observations assumption covariance matrix sources rss ss diagonal matrix separating matrices normalized unit length vectors 
ica inherently duality 
considering data matrix row assumed time course attribute ica decomposition produces independent time courses 
hand regarding data matrix form ica decomposition leads independent patterns instance images fmri arrays dna microarray data 
standard ica considered treated temporal ica tica 
dual decomposition regarding known spatial ica 
combining ideas leads spatio temporal ica 
variations ica investigated 
spatial ica spatio temporal ica shown useful fmri image analysis gene expression data analysis 
suppose singular value decomposition svd min 
ud temporal ica temporal ica finds set independent time courses corresponding set dual uncon strained spatial patterns 
embodies assumption row vector consists linear combination independent sequences st st rn set independent temporal sequences length rn associated mixing matrix 
unmixing leads recover dual patterns associated independent time courses calculating uw consequence uw spatial ica spatial ica seeks set independent spatial patterns ss corresponding set dual unconstrained time courses 
embodies assumption row vector composed linear combination independent spatial patterns ss rn contains set independent patterns rn encoding variable matrix mixing matrix 
neural information processing letters reviews vol january define permuted version definition dual time courses rn associated independent patterns computed column vector corresponds temporal mode 
spatio temporal ica linear decomposition enforces independence constraints space find set independent spatial patterns tica embodies independence constraints time seek set independent time courses 
spatio temporal ica finds linear decomposition maximizing degree independence space time necessarily producing independence space time 
fact allows trade independence arrays independence time courses 
finds decomposition st ss rn contains set independent dimensional patterns st rn set independent temporal sequences length diagonal scaling matrix 
exist mixing matrices ss st relation implies leads st uw 
linear transforms jointly optimizing objective functions associated tica 
objective function form infomax criteria log likelihood functions defines relative weighting spatial independence temporal independence 
details 

sparse component analysis sparse signal representations sparse component analysis sca sparse signals representations ssr arise scientific problems especially wish represent signals interest small sparse number basis signals larger set signals called dictionary 
problems arise applications electro magnetic inverse problems eeg meg feature extraction filtering wavelet denoising time frequency representation neural speech coding spectral estimation direction arrival estimation failure diagnosis speed processing 
opposite ica mixing matrix source signals estimated simultaneously sca usually multi stage procedure 
stage need find suitable linear transformation guarantee sources transformed domain sufficiently sparse 
typically represent observed data domain wavelets package 
step estimate columns ai mixing matrix sophisticated hierarchical clustering technique 
step difficult challenging task requires identify precisely intersections hyperplanes observed data located 
step estimate sparse sources example modified robust linear programming lp quadratic programming qp semi definite programming sdp optimization 
big advantage sca ability reconstruct original sources number observations sensors smaller number sources certain weak conditions 
independent component analysis review choi cichocki park lee state subset selection sub problem follows find optimal subset columns matrix denote ar equivalently er er represents residual error vector norm threshold 
problem consists estimating sparse vector correct optimal sparsity profile sparsity index detection number sources 
usually interest sparsest unique representation necessary find solution having smallest possible number nonzero components 
problem reformulated robust optimization problem sj usually suitably chosen function measures sparsity vector noted sparsity measure need necessary norm notation 
example apply shannon gauss renyi entropy normalized kurtosis measure sparsity 
standard form lp norm 
especially quasi norm attract lot attention ensures sparsest representation unfortunately formulated problem lp norm difficult especially np hard large scale problem numerically 
reason basis pursuit bp standard linear programming lp 
practice due noise uncertainty measurement errors system linear underdetermined equations satisfied precisely prescribed tolerance sense 
practical point view statistical point view convenient quite natural replace exact constraints constraint choice lq norm depends distribution noise specific applications 
noisy uncertain data flexible robust cost function comparison standard problem referred extended basis pursuit denoising ebp ebp jq possible basic choices lq sparsity criteria example uniform laplacian distributed noise choose chebyshev norm norm 
basic choices lq minimum quasi norm atomic decomposition related matching pursuit mp algorithm basis pursuit denoising ridge regression 
optimal choice norms depends distribution noise sparse components 
example noisy components robust norms huber function defined si si si si si si epsilon norm defined sj sj max sj 
practical importance ebp approach comparison standard lp bp approach ep bd allows treating presence noise errors due 
ebp approach possibility adjust sparsity profile adjust number nonzero components tuning parameter 
contrast lp approach option 
furthermore method applied overcomplete models number sources larger number sensors 
practical importance extended quadratic programming approach contrast linear programming standard basis pursuit approach qp allows treating presence noise errors due 
practice presence noise true model 

non negative matrix factorization sparse coding non negativity constraints blind separation independent sources non negativity constraints applications computer tomography biomedical image processing non negative constraints imposed entries aij mixing matrix estimated source signals sj neural information processing letters reviews vol january 
authors suggested decomposition observation non negative factors non negative matrix factorization nmf able produce useful meaningful representation real world data especially image analysis hyperspectral data processing biological modeling sparse coding :10.1.1.18.9018
section simple practical technique estimation non negative independent sources entries mixing matrix standard ica approach suitable postprocessing 
words show simple modifications existing ica bss algorithms able satisfy nonnegativity constraints sources simultaneously impose sparse independent possible 
loss generality assume sources non negative sj sj cj assume zero mean sub component sj mutually statistically independent furthermore may assume necessary entries nonsingular mixing matrix nonnegative aij optionally columns mixing matrix normalized vectors norm equals unity 
propose stage procedure 
stage apply standard ica bss algorithm zero mean pre processed sensor signals constraints order estimate separating matrix arbitrary scaling permutation estimate waveforms original sources projecting nonzero mean raw sensor signals sj estimated separating matrix 
noted global mixing unmixing matrix defined successful extraction sources generalized permutation matrix containing nonzero negative positive element row column estimated source stage non negative non positive time instant 
second stage order recover original waveform sources correct sign estimated non positive sources inverted multiplied 
noted procedure valid arbitrary nonsingular mixing matrix positive negative elements 
original mixing matrix non negative entries order identify corresponding vectors estimating matrix multiplied factor 
way estimate original sources blindly identify mixing matrix satisfying non negativity constraints 
furthermore necessary redefine follows kj kj ij sj sj ij 
transformation new estimated mixing matrix column sums equal vector unchanged 
known procedures biased white gaussian noise 
second stage easily identify mixing matrix noted global mixing matrix generalized matrix containing nonzero negative positive element row column estimated sources summarizing simple explanation follows necessary develop special kind algorithms bss non negativity constraints see example :10.1.1.18.9018
standard ica algorithm batch line applied zero mean signals waveforms original sources desired mixing matrix non negativity constraints estimated exploiting basic properties assumed model strong assumption original sources mutually independent 
non negative matrix factorization multiplicative algorithms method standard ica approach previous section enables estimate mixing matrix non negative components sj sjk assumption original sources independent 
nmf non negative matrix factorization introduced lee seung called pmf positive matrix factorization introduced assume explicitly implicitly sparseness mutual statistical independence components usually provides sparse decomposition 
nmf wide applications spectroscopy chemometrics environmental science matrices clear physical meanings normalization imposed example matrix columns normalized unit length 
noted non negative sources sj sj cj non independent zero mean sub components sj independent dc constant sub components cj dependent 
due reason refer problem non negative blind source separation non negative ica 
independent component analysis review choi cichocki park lee nmf decomposes data matrix product matrices having non negative elements 
results reduced representation original data 
reduced data set feature linear combination original attribute set 
nmf low computational cost ability deal dense sparse data sets 
nmf method designed capture alternative structures inherent data possibly provide biological insight 
lee seung introduced nmf modern formulation method decompose images 
example context nmf yielded decomposition human faces parts reminiscent features lips eyes nose contrast factorization methods ica pca image data yielded components obvious visual interpretation 
applied text nmf gave interesting evidence differentiating meanings word depending context semantic polysemy 
attempt employ nmf extract hidden interesting components spectra spectrograms eeg data 
nmf allow negative entries matrix factors model 
matrix factorization non negativity constraints permit combination multiple basis signals represent original signals 
additive combinations allowed nonzero elements positive 
decomposition subtractions occur 
reasons non negativity constraints compatible intuitive notion combining components form signal image nmf learns parts representation 
original application nmf focused grouping elements images parts matrix take dual viewpoint focusing primarily grouping samples components representing matrix section overview adaptive algorithms nmf 
consider cost function optimal gaussian distributed noise ik ik aij sj sjk mixing matrix unknown corresponding basis matrix previous sections matrix composed unknown non negative sources observable data matrix rows mixtures sources 
assume matrices non negative 
observable mixture matrix estimates unknown matrices optimization approach 
rank nmf algorithms group available data classes clusters components 
key open issue find rank decomposes data meaningful components 
typically chosen mn 
general nmf algorithms may may converge meaningful solutions run depending random initial conditions kind algorithm 
clustering classes strong expect sample assignment clusters vary little run run 
nmf pure algebraic factorization shown rank increases method may uncover structure substructures robustness evaluated ran algorithm gradually increasing fact nmf may reveal hierarchical structure exists force structure data sca ica 
nmf may advantages exposing meaningful components discover fine substructures 
gradient descent approach cost function respect elements aij obtain aij aij aij ij ij ij ij aij analogously assuming elements aij fixed obtain additive update rule elements sjk sjk sjk sjk jk jk jk jk sjk sjk sj xik xi 
additive learning rules ensure automatically non negativity neural information processing letters reviews vol january constraints called exponential gradient eo sj sj exp sj non negativity constraints automatically preserved elements exponential gradients positive 
multiplicative learning rules eo typically lead faster convergence additive updates solution optimization sparse containing large number zero elements 
lee seung proposed choose specific learning rates leads simple multiplicative update rules ij aij ass ij sjk ij jk aij aij sjk sjk ij ij jk jk guarantee positivity constraints assuming initial conditions positive local convergence 
learning rules provides usually sparse non negative representation data guarantee sparsest possible solution necessary representation contain highest possible number zero elements 
solutions unique algorithms may stuck local minima 
usually better performance sense probability stuck local minima algorithm 
standard nmf auxiliary constraints provides sparseness component achieve control sparsity imposing additional constraints natural non negativity constraints 
fact mentioned incorporate sparsity constraints ways 
example order ensure sparse non negative solution cost function modified quite general form follows aij sjk ij aij sj sjk regularization parameters control tradeoff sparsity respectively accuracy nmf suitably chosen function measure sparsity 
order achieve sparse representation usually chose simply sj sj sj sj log sj constraints sj 
note treat matrices symmetric way 
gradient descent approach norm constraints multiplicative learning rules modified follows aij aij sjk sjk ij ij jk jk jk note multiplicative learning rules set values aij sjk exactly zero practice enforce introducing threshold constraints sik equals zero small positive value actual value sik threshold determines noise floor 
independent component analysis review choi cichocki park lee nonlinear operator max introduced ensure non negativity constraints 
alternative modification bee proposed hoyer expressed somewhat general form aij aij sjk sjk ij ij jk jk sparse nmf procedure implemented follows 
algorithm outline non negative matrix factorization sparsity constraints 
initialize elements random non negative values 
example choose random constrained non negative squares normalize column unit norm 
set 
update matrix multiplicative learning rule 

alternatively sparseness constraints imposed 
force small values approximately zero values smaller set zero small value 
normalize column unit norm 

update matrix modified multiplicative learning rule 

force small values approximately zero 

iterate back step till convergence achieved 
alternative cost function intrinsically ensures non negativity constraints related poisson likelihood functional kullback leibler divergence xik xij log xik ik aij sik ik optional additional terms introduced order impose sparseness components 
nonnegative coefficients control sparsity profiles matrix respectively 
minimization cost function leads multiplicative learning rules ij aij sjk sjk sjk xik ik aij ij ij ij ik aij xik ik updates update matrices alternatively 
noted need update matrices 
updating row need update corresponding column need row column corresponding matrices occurring learning rules 
due physical constraints order achieve unique solution necessary usually normalize iteration columns rows unity fixed norm 
neural information processing letters reviews vol january nmf multiplicative algorithm closely related smart simultaneous multiplicative algebraic reconstruction technique developed analyzed byrne sjk sjk exp sjk ij log xik aij xik th row normalized matrix unity norm columns 
local nmf algorithm standard nmf models impose constraints described able reveal local features data order learn local features alternative cost function proposed xik xij log xik ik vii ik vik ss non negative coefficients 
cost additional terms introduced order impose constraints basis vectors aj orthogonal possible order minimize redundancies 
accomplished minimizing terms minimize number basis components column aj required represent words wish basis vectors contains nonzero elements possible 
accomplished minimizing uii 
retain components give information variance 
equivalent maximizing vii 
multiplicative update rules local nmf take form sjk xik ij aij sjk aij ij ij aij ik sjk xik ik noted multiplicative learning rules ensures non negativity matrices initial matrices non negative 
usually start form arbitrary non negative matrices example elements matrices uniformly distributed 
iteration continued rms error change negligible small say 
algorithms gradient descent approach guarantee achieve local minima 
address limitation repeat procedure times starting form different initial matrices 
nmf factorizations leading lowest rms error analysis 
essential feature nmf approach reduces data set full data space lower dimensional nmf space determined rank typically mn 
utility nmf estimating latent hidden components clusters classes eeg data represented frequency time frequency domain stems non negativity constraints facilitates detection sharp boundaries classes 
components typically sparse localized relatively independent natural signals decomposition suitable flexible promising interpretation 
despite promising features nmf limitation due non uniqueness solutions difficulties chose optimal dimensions matrices interpretation components 
independent component analysis review choi cichocki park lee summary nmf powerful technique extracting clustering classifying latent components 
nmf generalized multilayer generative network leads multiplicative propagation learning 
challenge remains provide meaningful physiological interpretation nmf discovered latent components classes components structures true sources completely unknown 
application nmf pet image analysis interesting application nmf dynamic pet image analysis appeared 
performed pet scans dogs rest stress 
scans acquired exact scanner cti knoxville usa intrinsic resolution mm fwhm full width half maximum images contiguous planes thickness mm simultaneously longitudinal field view cm 
administration transmission scanning performed ge rod sources attenuation tion 
dynamic emission scans sec sec sec initiated simultaneously injection images reconstructed means filtered back projection algorithms matrices size mm initial eighteen frames minutes pet images analysis 
dynamic pet images re oriented short axis re sampled produce cm thick slices order increase signal noise ratio 
cardiac regions masked remove extra cardiac components reduce quantity data burden computation 
resulting masked images dimension pixel pixel plane frame reformulated frame pixel data matrix 
row matrix corresponds basis image represent cardiac component 
shows basis images obtained nmf 
cardiac components right ventricle left ventricle myocardium successfully extracted 
column vector matrix represent time activity curve tac useful calculate blood flow estimation 
shows tac peaks right ventricle left ventricle dispersion left ventricle 
counts pixel sec right ventricle left ventricle myocardium time seconds 
basis images time activity curves computed nmf shown 
eeg meg applications open problems great challenge neurophysiology non asses physiological changes occurring different parts brain 
activations modelled measured neuronal brain source signals indicate function malfunction various physiological sub systems 
extract relevant information diagnosis therapy expert knowledge medicine neuroscience statistical signal processing required 
understand human neurophysiology currently rely types non invasive neuroimaging techniques 
techniques include eeg meg neural information processing letters reviews vol january 
brain source signals extremely weak non stationary signals distorted noise interference going activity brain 
mutually superimposed low passed filtered eeg meg recording systems see 
classical signal analysis tools adaptive supervised filtering parametric non parametric spectral estimation time frequency analysis higher order statistics intelligent blind signal processing techniques preprocessing noise artifact reduction enhancement detection estimation neuronal brain source signals 
years great interest applying high density array eeg systems analyze patterns imaging human brain eeg desirable property excellent time resolution 
property combined systems eye tracking emg systems relatively low cost attractive investigating higher cognitive mechanisms brain opens unique window investigate dynamics human brain functions able follow changes neural activity millisecond time scale 
comparison functional imaging modalities pet functional magnetic resonance imaging fmri limited temporal resolution time scales order best second physiological signal noise considerations 
determining active regions brain eeg meg measurements scalp important problem 
accurate reliable solution problem give information higher brain functions patient specific cortical activity 
estimating location distribution electric current sources brain eeg meg recording ill posed problem unique solution solution depend continuously data 
ill posedness problem distortion sensor signals large noise sources finding correct solution challenging analytic computational problem 
knows positions orientations sources brain calculate patterns electric potentials magnetic fields surface head 
called forward problem 
patterns electric potential magnetic fields scalp level needs calculate locations orientations sources 
called inverse problem 
inverse problems notoriously difficult solve forward problems 
case electric potentials magnetic fields scalp surface unique solution problem 
hope additional information available constrain infinite set possible solutions single unique solution 
intelligent blind source separation 
eeg electrode montage acts kind spatial filters cortical brain activity bss procedure considered spatial filter attempt cancel effect superposition various brain activities estimated components represent physiologically different processes 
bss related methods spca promising approaches elimination artifacts noise eeg meg data enhancement neuronal brain sources 
fact applications ica bss techniques successfully applied remove artifacts noise including background brain activity electrical activity heart eye blink muscle activity environmental noise efficiently 
methods require manual detection classification interference components estimation cross correlation independent components signals corresponding specific artifacts 
important problem automatically detect extract eliminate noise artifacts 
relevant problem enhance extract classify brain sources 
conceptual model elimination noise undesirable components multi sensory data depicted 
bss performed suitably chosen robust respect noise algorithm linear transformation sensory data wx vector represents specific components sparse smooth spatio temporally decorrelated statistically independent components 
projection interesting useful components spatio temporal decorrelated independent activation maps yj back sensors electrodes 
corrected cleaned sensor signals obtained linear transformation pseudo inverse unmixing matrix vector obtained vector removal undesirable components replacing zeros 
entries estimated attenuation matrix indicate strongly electrode picks individual component 
back projection significant components allows remove artifacts noise enhance eeg data 
cases estimated components filtered smoothed order identify significant components 
addition denoising artifacts removal bss techniques decompose eeg meg data individual components representing independent component analysis review choi cichocki park lee sensor signals xm sensor signals xm demixing system bss ica hard switches inverse system reconstructed sensors signals demixing system bss ica optional nonlinear adaptive filters naf expert decision naf naf line switching decision blocks inverse system reconstructed sensors signals 
basic models removing undesirable components noise artifacts enhancing multi sensory eeg meg data expert decision hard switches auxiliary nonlinear adaptive filters smooth components hard switches 
estimated components normalized ranked ordered clustered order identify significant physiological meaningful sources artifacts 
neural information processing letters reviews vol january 
conceptual model sequential blind sources extraction 
stage different criterion 
physiologically distinct process brain source 
main idea apply localization imaging methods components turn 
decomposition usually underlying assumption sparsity statistical independence activation different cell assemblies involved 
alternative criteria decomposition spatio temporal decorrelation temporal predictability smoothness components 
bss general bsp approaches promising methods blind extraction useful signals eeg meg data 
eeg meg data decomposed useful signal noise subspaces standard techniques pca spca factor analysis fa standard filtering 
apply bss algorithms decompose observed signals signal subspace specific components 
bss approaches enable project component localized brain source activation map skull level 
activation map apply eeg meg source localization procedure looking single dipole brain source map 
localizing multiple sources independently dramatically reduce computational complexity increase likelihood efficiently converging correct reliable solution 
biggest strength bss approach offers variety powerful efficient algorithms able estimate various kind sources sparse independent spatio temporally decorrelated smooth 
algorithms tica able automatically rank order component complexity sparseness measures 
algorithms robust respect noise sons 
cases recommended algorithms cascade multiple parallel mode order extract components various features statistical properties 
real world scenario latent hidden components brain sources various complex properties features 
words true unknown sources seldom sparse statistically independent spatio temporally decorrelated 
apply single technique ica sca std usually fail extract hidden components 
need apply fusion strategy combination criteria associated algorithms extract desired sources 
may apply possible approaches 
promising approach sequential blind extraction see extract components stage applying different criterion statistical independence sparseness 
smoothness 
way extract sequentially different components various properties 
alternative approach suitable preprocessing perform simultaneously parallel way bss methods ica sca std 
estimated components normalized ranked clustered independent component analysis review choi cichocki park lee 
parallel model employing fusion strategy bss algorithms estimation physiologically meaningful event related brain sources 
reliability estimated sources components analyzed investigating spread obtained components trials possibly subjects 
usually useful significant components corresponds small separated clusters rest components unreliable components usually belong cluster 
compared similarity measures see 
furthermore components back projected scalp level brain sources localized basis clusters sub components 
way basis priori knowledge information external stimuli event related brain sources identify components electrophysiological meaning specific 
summary blind source separation generalized component analysis bss gca algorithms allows 
extract remove artifacts noise raw eeg meg data 

recover neuronal brain sources activated cortex especially auditory visual somatosensory olfactory cortex 

improve signal noise ratio snr evoked potentials ep especially aep sep 
improve spatial resolution eeg reduce level subjectivity involved brain source localization 

extract features hidden brain patterns classify 
applications bss show special promise areas non invasive human brain imaging techniques delineate neural processes underlie human cognition motor functions 
approaches lead interesting exciting new ways investigating analyzing brain data develop new hypotheses neural assemblies communicate process information 
extensive potentially promising research area 
techniques methods remain validated experimentally obtain full gain approach 
fundamental problems system real properties get information 
valuable information observed data noise interference 
observed sensor data transformed features characterizing brain sources reasonable way 

feature extraction speech signals ica finding statistically efficient representations speech natural sounds 
ica finds linear transform multivariate data minimizes mutual information data 
ica neural information processing letters reviews vol january frequency hz time ms 
contour lines wigner ville distributions learned basis vectors 
contour line represents locus half maximum peak amplitude 
function computational algorithm sensory information processing redundancy input signals reduced 
addition learn efficient representations set sparseness constraint distribution 
sparse distribution small percentage informative values nonzero values tails values zero encode decode data small number coefficients 
learned speech features basis vectors localized time frequency 
time frequency analysis basis vectors shows property similar critical bandwidth human auditory system shown 
order obtain complex speech features ica computational model developed 
generating speech features inner hair cells existing model cochlea applied ica algorithm topology preserving mapping 
shows learned speech features features represent complex signal characteristics auditory cortex onset offset frequency modulation time 

convolutive source separation problem formulation section formulate general model mixing involves convolution time delays 
consider set unknown independent components sn components si zero mean mutually independent 
independent components transmitted channels mixed give observations xi 
mixtures linear combinations delayed filtered versions independent components 
expressed xi lm aij sj aij denotes mixing filter coefficient 
task estimate independent components observations resort priori knowledge mixing system 
ica instantaneous mixtures ica convolutive mixtures needs certain assumptions independent components approximate distributions statistics 
furthermore ica convolutive mixtures indeterminacy estimated independent components permutation independent component analysis review choi cichocki park lee 
spectra learned complex speech features 
neural information processing letters reviews vol january 
feedforward network ica convolutive mixtures 

feedback network ica convolutive mixtures 
arbitrary filtering algorithms attempt estimated signals temporally whitened 
whitening may degrade outputs applications separation natural signals avoided forcing constraints post processing 
order perform ica convolutive mixtures methods try apply algorithms mixtures time domain frequency domain 
addition papers proposed methods filter banks subbands 
architectures feedforward vs feedback 
time domain methods obtain independent components observations consider types networks time domain 
feedforward architecture expressed yi la wij xj adaptive filters wij force outputs yi reproduce original independent components si 
illustrates feedforward network 
hand feedback architecture constructed inverse system expressed la yi wii xi la wij yj 
architecture consists different filter coefficients zero delay weights direct filters wii weights direct filters wii weights feedback cross filters wij feedback network shown 
advantage feedforward system learn general inverse system approximate solution ica phase mixing systems 
phase filter expressed product minimum phase filter pass filter 
minimum phase filter poles zeros inside unit circle pass filter represents time delay phase filter unit frequency magnitude response 
inverse phase filter product independent component analysis review choi cichocki park lee inverse minimum phase filter stable causal filter inverse pass filter 
resulting filter non causal stable filter 
imposing appropriate time delay feedforward system realized inverting phase mixing system 
distortion estimated independent components considered 
model situation transform domain independent components mixed give observations follows upper cases denote transforms corresponding lower cases 
ica convolutive mixtures indeterminacy arbitrary filtering estimated independent components distorted filtering depending ica algorithm 
original independent components signals recovered temporal whitening 
independent components including natural signals case methods estimate innovation processes independent components build post processing filter try artificially color signal 
desired results may observation obtain absence interfering source result affected distortion mixing process 
including whitening filters separating structure method proposed directly extracted colored components step 
feedback architecture result obtained forcing direct filters wii scaling factors 
note ica achieved addition mixing system stable inverses 
basic algorithms ica algorithms convolutive mixtures time domain various instantaneous mixtures complexity architectures perform ica 
go major algorithms infomax algorithm decorrelation algorithm 
pass outputs architectures bounded nonlinear functions approximate cumulative density functions cdfs original independent components give yi 
outputs yi desired independent components yi follow uniform density largest entropy distributions bounded variables 
infomax algorithm performs ica convolutive mixtures maximizing entropy yi 
feedforward architecture infomax algorithm provides learning rules adaptive filter coefficients follows pi yi yi wij fi yi xj fi yi pi yi matrix composed zero delay weights denote set estimated independent components observation vector respectively 
addition fi called score function pi yi denotes pdf yi 
learning rules feedback architecture wii wii fi yi xi wii fi yi xi wij fi yi yj 
hand second order statistics ica convolutive mixtures original signals non stationary 
non negative cost function log det diag log det denotes time averaging operator bth local analysis block number local analysis blocks 
note cost function takes minimum value second order cross correlation neural information processing letters reviews vol january zero 
gradient learning rule obtained minimizing cost function respect adaptive parameters 
applying natural gradient ica networks ordinary gradient provided popular learning algorithms various optimization frameworks 
parameter space euclidean cases 
cases steepest direction function ordinary gradient natural gradient :10.1.1.31.9597
commonly known natural gradient improves convergence speed significantly 
applying natural gradient infomax algorithm learning rule feedforward architecture derived pi yi yi fi yi pi yi matrix composed pth delay filter coefficients la 
unfortunately natural gradient learning rule shows update depends outputs 
addition involves intensive computation compute la time step 
practically algorithm modified imposing la sample delay remove non causal terms reusing past results 
modification algorithm approximated la la la 
generally natural gradient transform domain denotes ordinary gradient 
addition natural gradient applied feedback architecture 
flow 
frequency domain methods mixing environment quite complex filters ica network may require thousands taps appropriately invert mixing 
cases time domain methods large computational load compute convolution long filters amounts update filter coefficients 
methods implemented frequency domain fft order decrease computational load convolution operation time domain performed element wise multiplication frequency domain 
learning rules simply formulated frequency domain fir matrix algebra extends algebra scalar matrices algebra matrices filters polynomials 
example natural gradient infomax rule feedforward architecture expressed fft fft denotes matrix composed filters feedforward architecture frequency domain 
fast implementation adaptive filters frequency domain achieved employing overlap save block technique 
learning rule just efficient implementation fft time domain algorithm 
real sense frequency domain method means performing ica instantaneous mixtures frequency bins 
note convolutive mixtures expressed 
vectors frequency components mixtures independent components frequency respectively 
denotes matrix containing elements frequency transforms independent component analysis review choi cichocki park lee short time fourier transform inverse short time fourier transform 
frequency domain method ica convolutive mixtures 
mixing filters frequency reason convolutive mixtures represented set instantaneous mixtures frequency domain 
independent components recovered applying ica instantaneous mixtures frequency bin transforming results time domain shown 
denotes unmixing matrix frequency basic algorithms various ica algorithms instantaneous mixtures applied ica convolutive mixtures frequency domain 
algorithms go popular algorithms ica convolutive mixtures 
algorithms traditionally categorized groups 
takes aspects higher order statistics account explicitly implicitly nonlinear functions outputs 
prevailing methods group infomax algorithm considered 
algorithm derived manner explained time domain method 
applying natural gradient infomax learning rule frequency bin 
contrary time domain method input signals complex numbers 
order deal data score function changed 
worth noting infomax algorithm provides formulation maximum likelihood estimation negentropy maximization algorithm minimizing mutual information 
order perform ica computing higher order statistics explicitly fourth order cross cumulants usually considered 
zero mean random variables xi xj xk xl cross cumulant defined cum xi xj xk xl xixj 
cross cumulants independent signals zero obtain desired independent components minimizing cost function fourth order cross cumulants jointly diagonalizing cross cumulant tensor 
known diagonalization simple cross covariance matrix provides just decorrelated components contain sufficient information estimate independent components 
ica algorithms consider mutual information higher order statistics 
ica performed second order statistics adding covariances time lags original independent components time dependencies 
simple method time delayed decorrelation algorithm follows covariance matrix rxx time lag rxx neural information processing letters reviews vol january denotes mixing matrix diagonal matrix 
covariance matrices time lag time lag construct eigenvalue problem rxx rxx rxx rxx simplicity 
independent components obtained diagonalizing covariance matrices time 
addition number covariance matrices increased 
addition non stationarity independent components allows perform ica second order statistics 
independent components non stationary 
multiple different equations covariance matrices obtained different choices provide successful estimation independent components 
beamforming may combined frequency domain ica improve performance 
proposed system integrated frequency domain ica null beamforming estimated direction doa information 
resolving indeterminacy frequency domain ica algorithms short time fourier transform frequency domain ica algorithms regard convolutive mixtures set instantaneous mixtures 
ica algorithm instantaneous mixtures precisely estimates unmixing matrix frequency bin algorithm indeterminacy scaling permutation frequency bin 
indeterminacy may deteriorate performance ica algorithm 
permutation scaling problem resolved reconstruct desired independent components 
algorithms solve permutation scaling problem envelopes frequency spectra assuming independent components time varying statistical properties 
algorithm decomposition frequency spectra performed yi yi denotes ith element 
permutation problem solved envelopes frequency spectra vj positive constant vj denotes jth element 
definition similarity sim algorithm sorts frequency bins order weakness similarity independent components sim sim sim ff 
number frequency bins denotes normalized correlation estimated lk lk lk lk denotes length 
frequency bin smallest correlation independent components assigned specific outputs 
frequency bins independent component analysis review choi cichocki park lee fl sorted increasing order correlation independent components assigned outputs correlation envelopes frequency bins 
permutation arg max fl fl fl fj 
process repeated turns frequency bins covered 
known algorithm fix arbitrary permutations limit effective length unmixing filters estimating unmixing matrices sufficiently frequency bins 
constraint effective length links frequencies provides solution permutation problem restricting unmixing matrices continuous smooth frequency domain 
doa estimation solving permutation problem 
assuming linearly arranged closely spaced sensors plain wavefront reverberation frequency response mixing filter ajl approximated dj denote propagation velocity position sensor xj direction source sl respectively 
frequency response system expressed ik ajl exp dj cos ij ajk ij exp dj cos 
regarding variable provides pattern patterns estimate source directions align permutations 
method envelopes frequency spectra misalignment frequency bin may cause consecutive misalignments 
doa method fixes permutations frequency bin regardless frequency bins 
doa computed approximation mixing system doa method precise 
order exploit advantages methods method fixed permutations frequency bins confidence doa method sufficiently high decided permutations remaining frequency bins envelopes frequency spectra 
flow 
filter bank methods time domain methods regard convolutive mixtures results big complicated system try estimate unmixing network requires great parameters 
hand frequency domain methods decompose convolutive mixtures decomposed mixtures modelled instantaneous mixtures convolutive mixtures 
heavily environments performances frequency domain ica methods seriously degraded insufficiency data learn unmixing matrices large number frequency bins 
hand time domain ica methods adapt long unmixing filters usually show slow convergence especially colored inputs 
results frequency domain ica method regarded inputs time domain ica method order time domain ica remove residual components frequency domain ica 
combining ica methods compromise extreme cases give filter bank methods 
shows network filter bank method perform ica 
filter bank methods input mixtures splitted subband signals analysis filters 
resulting subband signal band limited subsampled 
input mixtures split subband signals subband covers somewhat broad frequency band 
subband signal subsampled neural information processing letters reviews vol january mixture mixture ica network ica network ica network 
network filter bank method perform ica 
result result decimation factor usually smaller length mixing filters 
subsampled signals convolutive mixtures effective mixing filters decrease decimation factor 
typical ica algorithm convolutive mixtures obtain independent components subsampled signals subband unmixing filter length shorter full band time domain methods 
output signal unmixing network expanded desired independent components reconstructed subband output signals synthesis filters fixing permutation scaling 
critically sampled filter banks analysis synthesis filter banks cross adaptive filters adjacent bands required compensate distortion caused aliasing spectral gaps required order aliasing 
cross adaptive filters introduce additional adaptive parameters may cause slow convergence speed poor performance 
hand spectral gaps distort reconstructed signals 
alias free property perfect reconstruction essential order filter banks side effects limit performance methods primarily apart capability ica algorithms subbands 
oversampled filter banks decimation factor smaller number analysis filters aliasing neglected filter having high attenuation 
oversampled filter bank implemented uniform complex valued filter bank 
filter bank analysis filters hl obtained real valued low pass prototype filter generalized discrete fourier transform hl lq lq lq length 
complex conjugate time reversed versions analysis filters selected synthesis filters fl hl lq 
prototype filter designed iterative squares algorithm cost function considers attenuation 
addition filter bank efficiently implemented employing polyphase representation analysis synthesis filters 
performing ica subbands resolving indeterminacy filter bank methods performs ica oversampled filter bank adaptive filter coefficients subband adjusted information subbands negligible aliasing filter bank 
subband signals convolutive mixtures instantaneous mixtures ica algorithm subband may basically time domain methods section 
perform ica complex valued filter banks subband signals complex valued data learning rules adaptive filter coefficients changed deal complex valued data 
infomax algorithm feedback network independent component analysis review choi cichocki park lee subband learning rules wii ii fi yi wii fi yi wij fi yi 
second order statistics estimating unmixing networks 
case time domain methods forcing direct filters scaling factors subband recovered outputs whitened 
filter bank methods unmixing network subband adapt filter coefficients network independently subbands 
filter bank methods permutation scaling problem frequency domain methods 
order fix problem algorithm applied filter bank methods necessary modifications 
algorithm frequency domain algorithm multiplies recovered independent component unmixing matrix frequency bin order ambiguity scaling 
filter bank methods ambiguity scaling avoided normalizing filters corresponding scaling factors direct filters fixing direct filters specific scales subband 
procedures follow algorithm frequency domain methods similar manner 
addition reported null initial value unmixing system relaxes permutation problem 

comparison methods length unmixing filter long time domain methods large computational load compute convolution long filters amounts update filter coefficients 
addition show slow convergence speed especially colored input signals speech signals 
computational load reduced frequency domain methods multiplication frequency bin replaces convolution operation time domain 
adaptation unmixing matrix interfere frequency domain methods improve convergence 
long frame size required cover long mixing filters 
maintain computational efficiency obtain data overlapped adjacent frames frame shift increase frame size increases 
number data frequency bin decreases 
causes insufficiency data learn unmixing matrices performance degraded 
addition performance ica algorithms frequency bin permutation scaling problem settled obtain desired outputs unmixing matrix adapted ica algorithms permutation scaling indeterminacy 
filter bank methods performance limitation frequency domain methods ica algorithms subband time domain methods 
addition computational complexity considerably reduced long adaptive filter length simplified ica network process decimated input signals subsampled rate subband 
filter bank methods appropriate parallel processing subband independently compute subband output signals adapt filter coefficients unmixing network subbands 
additionally methods able choose number subbands regardless complexity mixing environments improve convergence adaptive filter coefficients subband input signals whitened decimation time domain methods 
mixing environment complex frequency domain methods require great frequency bins large frame shift 
envelope frequency spectrum estimated exactly fix permutation 
number subbands filter bank methods usually smaller required number frequency bins frequency domain methods 
resolve permutation problem easily subband sufficiently broad band exactly estimate envelope 
example compared methods simulations blind separation speech mixtures 
real recorded speech data source signals 
signal second length khz sampling rate 
category chosen infomax algorithm learn adaptive parameters popularity simplicity 
known speech signal approximately follows laplacian distribution 
sgn score function 
mixed speech data room impulse responses speakers microphones measured normal office room shown 
experimental results compared terms signal interference ratio sir 
mixing unmixing neural information processing letters reviews vol january 
room impulse mixing system system sir defined ratio signal power interference power outputs sir db log 
yj si denotes jth output cascaded mixing unmixing system si active 
displayed learning curves methods blind source separation problem 
perform time domain method feedback network filter length taps 
frequency domain method frame size samples frame shift sixteenth frame size 
addition designed filter bank order perform filter bank method 
shows frequency response analysis filters uniform oversampled filter bank 
filter bank designed alias free decimation factor constructed prototype filter taps 
separation network subband feedback network number taps filter 
frequency domain method smaller methods 
frequency domain method performance limitation comes contradiction long reverberation covering insufficient learning data 
permutation problem severely degrades performances 
learning curves show filter bank method faster convergence speed time domain method colored signals decimation subband filter bank approach 
contrary frequency domain approach permutation problem successfully fixed filter bank approach 

remarks underdetermined problem papers tackled underdetermined case number independent components larger independent component analysis review choi cichocki park lee sir db filter bank approach time domain approach frequency domain approach sweeps 
learning curves methods blind source separation amplitude response db normalized frequency 
frequency response analysis filters uniform oversampled filter bank neural information processing letters reviews vol january mixtures 
problem challenging papers modelled mixtures delayed ones attenuation 
order get sparse signals employ linear transforms short time fourier transform stft assume independent component dominant point domain 
obtaining observations mixtures expressed sj dj 
jth source nonzero time frequency domain aj exp sj 
mixing parameters estimated dimensional histogram amplitude delay estimates determine number independent components mixing parameters 
independent component estimated applying corresponding time frequency mask mixture 
case number dominant independent components equal number observations point conventional ica methods estimate desired independent components 
addition making additional assumptions statistical properties independent components maximizing likelihood noisy mixing model provide estimation independent components model parameters 

discussion discussed briefly extensions modifications blind source separation decomposition algorithms spatio temporal decorrelation independent component analysis sparse component analysis non negative matrix factorization various criteria constraints imposed linear predictability smoothness mutual independence sparsity non negativity extracted components 
especially described generalization extension ica sd ica relaxes considerably condition independence original sources 
concepts cases able reconstruct recover original brain sources estimate mixing separating matrices original sources independent fact strongly correlated 
propose simple method checking validity true performance bss separation applying bank various frequency characteristics 
reviewed algorithms ica convolved mixtures 
methods divided categories time domain methods frequency domain methods filter bank methods 
gone known algorithms category 
addition compared advantages disadvantages algorithms categories 
acknowledgment choi 
park 
lee supported korean ministry science technology brain research program 
deville 
blind source separation convolutive mixtures hybrid approach colored sources 
proc 
int 
conf 
artificial natural neural networks pages june 
independent component analysis review choi cichocki park lee 
ahn choi 
oh 
multiplicative propagation 
proc 
int 
conf 
machine learning pages banff canada 
amari 
natural gradient works efficiently learning 
neural computation 
amari cichocki 
adaptive blind signal processing neural network approaches 
proceedings ieee 
amari cichocki yang 
new learning algorithm blind signal separation 
mozer david touretzky michael hasselmo editors advances neural information processing systems volume pages 
mit press cambridge ma 
amari douglas cichocki yang 
multichannel blind deconvolution equalization natural gradient 
ieee international workshop wireless communication pages 
amari hyv rinen 
lee 
lee sanchez 
blind signal separation independent component analysis 
neurocomputing 
amari cardoso 
blind source separation semi parametric statistical approach 
ieee trans 
signal processing dec 
ller 
amplitude modulation decorrelation convolutive blind source separation 
proc 
int 
conf 
ica bss pages june 
araki makino nishikawa 
subband blind source separation appropriate processing frequency band 
proc 
int 
conf 
ica bss pages april 
araki makino sawada 
underdetermined blind separation speech real environments sparseness ica 
proc 
ieee icassp pages 
araki makino nishikawa 
fundamental limitation frequency domain blind source separation convolved mixtures speech 
proc 
int 
conf 
ica bss pages december 
bach jordan 
independent components trees clusters 
journal machine learning research 

bae 
park 
lee 
blind signal separation independent component analysis 
neurocomputing 
rosca 
scalable non square blind source separation presence noise 
proc 
ieee icassp pages 
barros cichocki 
extraction specific signals temporal structure 
neural computation september 
bell sejnowski 
information maximization approach blind separation blind deconvolution 
neural computation nov 

de sources algorithme performances application des exp 
phd thesis enst telecom paris july 

cardoso 
moulines 
blind source separation technique second order statistics 
ieee trans 
signal processing february 
amin 
new approach blind source separation time frequency distributions 
proc 
spie 
neural information processing letters reviews vol january cichocki 
robust whitening procedure blind source separation context 
electronics letters nov 

underdetermined blind separation delayed sound sources frequency domain 
neurocomputing may 

cardoso 
jacobi angles simultaneous diagonalization 
siam mat 
anal 
appl 

cardoso 
infomax maximum likelihood blind source separation 
ieee signal processing letters 

cardoso laheld 
equivariant adaptive source separation 
ieee trans 
signal processing 

cardoso 
blind beamforming non gaussian signals 
iee proc 


cardoso 
jacobi angles simultaneous diagonalization 
siam journal mat 
anal 
appl january 
chang ding yau chan 
matrix pencil approach blind separation colored nonstationary signals 
ieee trans 
signal processing mar 

cho 
lee 
implementation infomax ica algorithm analog cmos circuits 
proc 
international conference independent component analysis blind signal separation pages 
canada 
choi amari cichocki 
natural gradient learning spatio temporal decorrelation recurrent network 
ieice trans 
fundamentals 
choi cichocki 
unsupervised hybrid network blind separation independent non gaussian source signals multipath environment 
journal communications networks 
choi cichocki 
blind separation nonstationary temporally correlated sources noisy mixtures 
proc 
ieee workshop neural networks signal processing pages sidney 
choi cichocki 
blind separation nonstationary sources noisy mixtures 
electronics letters apr 
choi cichocki amari 
blind equalization simo channels spatio temporal anti hebbian learning rule 
proc 
ieee workshop cambridge pages uk 
ieee press choi cichocki amari 
flexible independent component analysis 
proc 
ieee workshop pages cambridge uk 
choi cichocki amari 
equivariant nonstationary source separation 
neural networks 
choi cichocki 
second order nonstationary source separation 
journal vlsi signal processing august 
choi cichocki zhang amari 
approximate maximum likelihood source separation natural gradient 
ieice transactions fundamentals electronics communications computer sciences january 
cichocki 
blind signal processing methods analyzing multichannel brain signals 
international journal 
independent component analysis review choi cichocki park lee cichocki amari 
adaptive blind signal image processing 
john wiley new york 
new revised improved edition 
cichocki amari tanaka signal image processing www bsp brain riken go jp 
japan 
cichocki 
sources separation temporally correlated sources noisy data bank band pass filters 
third international conference independent component analysis signal separation ica pages san diego usa dec 
cichocki ski pope 
modified jutten algorithms blind separation sources 
digital signal processing april 
cichocki 
blind source separation algorithms matrix constraints 
ieice transactions fundamentals electronics communications computer sciences january 
cichocki 
efficient extraction evoked potentials combination wiener filtering subspace methods 
proceedings ieee international conference acoustics speech signal processing icassp volume pages salt lake city utah usa may 
ieee ieee 
cichocki li amari 
ica robust sparse signal representations 
proceedings ieee international symposium circuits systems iscas volume pages vancouver canada may 
cichocki 
blind signal extraction signals specified frequency band 
neural networks signal processing xii proceedings ieee signal processing society workshop pages martigny switzerland september 
ieee 
cichocki asada 
eeg filtering blind source separation improves detection alzheimer disease 
clinical 
cichocki 
line algorithm blind signal extraction arbitrarily distributed temporally correlated sources second order statistics 
neural processing letters august 
cichocki amari 
sequential blind signal extraction order specified stochastic properties 
electronics letters january 
cichocki unbehauen 
neural networks optimization signal processing 
john wiley sons new york 
new revised improved edition 
cichocki unbehauen 
robust neural networks line learning blind identification blind separation sources 
ieee trans 
circuits systems fundamentals theory applications nov 
cichocki unbehauen 
robust learning algorithm blind separation signals 
electronics letters august 
cichocki 
application ica automatic noise interference cancellation multisensory biomedical signals 
proceedings second international workshop ica bss ica pages helsinki finland june 
rabiner 
multirate digital signal processing 
prentice hall englewood cliffs nj 
cruces cichocki 
iterative inversion approach blind source separation 
ieee trans 
neural networks 
neural information processing letters reviews vol january cruces cichocki 
robust blind source separation algorithms cumulants 
neurocomputing december 
cruces cichocki 
combining blind source extraction joint approximate diagonalization thin algorithms ica 
proceedings th international symposium independent component analysis blind signal separation ica pages kyoto japan april 
riken ica 
cruces cichocki de lathauwer 
thin qr svd factorizations simultaneous blind signal extraction 
proceedings european signal processing conference eusipco pages vienna austria 
isbn 
delorme makeig 
open source toolbox analysis single trial eeg dynamics 
neuroscience methods 
donoho elad 
representation minimization 
proc 
national academy science march 
schuster 
blind separation convolutive mixtures application automatic speech noisy environment 
ieee trans 
signal processing october 
ferrara 
fast implementation lms adaptive filters 
ieee trans 
acoustics speech signal processing 

fully multiplicative group ica neural algorithm 
electronics letters 
cichocki 
sparse component analysis overcomplete mixtures improved basis pursuit method 
proceedings ieee international symposium circuits systems cas volume pages vancouver canada may 
theis cichocki 
blind source separation sparse component analysis overcomplete mixtures 
proceedings international conference acoustics speech signal processing icassp volume pages montreal canada may 
ieee signal processing society 
cichocki 
noise reduction brain evoked potentials third order correlations 
ieee transactions biomedical engineering 
cichocki 
second order statistics blind source separation bank subband filters 
digital signal processing 
vetterli 
adaptive filtering subbands critical sampling analysis experiments application acoustic echo cancellation 
ieee trans 
signal processing august 
girolami fyfe 
generalised independent component analysis unsupervised learning emergent properties 
proc 
ieee icnn volume pages june 
golub van loan 
matrix computations nd edition 
johns hopkins 
weiss stewart 
design near perfect reconstruction oversampled filter banks subband adaptive filters 
ieee trans 
circuits syst 
ii august 
haykin editor 
adaptive filter theory 
prentice hall new jersey nj 
haykin 
ed 
unsupervised adaptive filtering volume blind deconvolution volume 
john wiley sons february 
hyv rinen esposito 
validating independent components neuroimaging time series clustering visualization 
neuroimage 
independent component analysis review choi cichocki park lee hoyer hyv rinen 
independent component analysis applied feature extraction color stereo images 
network computation neural systems 
huang 
yen zhao 
subband adaptive decorrelation filtering channel speech separation 
ieee trans 
speech audio processing july 
hyv rinen karhunen oja 
independent component analysis 
john wiley new york 
morgan 
beamforming approach permutation alignment multichannel frequency domain blind speech separation 
proc 
ieee icassp pages may 
cichocki amari 
identification elimination artifacts meg signals efficient independent components analysis 
proc 
th th int 
conference pages sendai japan 


lee 
lee 
center frequency ordered speech feature extraction independent component analysis 
proceedings international conference neural information processing pages 
shanghai china 

jung 
lee 
temporal decorrelation feature parameters noise robust speech recognition 
ieee transactions speech audio processing 
jung humphries 
lee makeig mckeown sejnowski 
ica removes artifacts electroencephalographic recordings 
advances neural information processing systems nips 
jung makeig humphries 
lee mckeown sejnowski 
removing electroencephalographic artifacts blind source separation 
psychophysiology 
kim park kim lee choi 
fpga implementation ica algorithm blind signal separation active noise canceling 
ieee transactions neural networks 
kim choi 
independent arrays independent time gene expression data 
proc 
ieee int symp 
circuits systems kobe japan 
appear 
kim 
lee 
learning self organized topology preserving complex speech features primary auditory cortex 
neurocomputing 
press 
kreutz delgado murray rao 
lee sejnowski 
dictionary learning algorithms sparse representation 
neural computation february 
lambert 
multichannel blind deconvolution fir matrix algebra separation multipath mixtures 
phd thesis university southern california los angeles may 
lee seung 
learning parts objects non negative matrix factorization 
nature 
lee lee ahn 
application independent component analysis dynamic positron emission tomography 
prof iconip pages 

lee 
lee 
jung 
lee 
efficient speech feature extraction independent component analysis 
neural processing letters 
lee lee choi lee 
application non negative matrix factorization dynamic positron emission tomography 
proc 
ica pages san diego california 
lee lee choi park lee 
nonnegative matrix factorization dynamic images nuclear medicine 
ieee medical imaging conference 
neural information processing letters reviews vol january 
lee 
auditory pathway model vlsi implementation robust speech recognition real world noisy environment 
international joint conference neural networks signal processing china 

lee 
independent component analysis 
kluwer academic publishers boston ma 

lee bell lambert 
blind separation delayed convolved sources 
advances neural information processing systems pages cambridge ma 
mit press 

lee girolami bell sejnowski 
unifying information theoretic framework independent component analysis 
computers mathematics applications march 

lee sejnowski 
combining time delayed decorrelation ica solving cocktail party problem 
proc 
ieee icassp volume pages seattle wa 
lewicki 
efficient coding natural sounds 
nature neuroscience 
li cichocki amari 
analysis sparse representation blind source separation 
neural computation june 
li cichocki amari cao gu 
sparse representation applications blind source separation 
seventeenth annual conference neural information processing systems nips vancouver december 

linear modes gene expression determined independent component analysis 
bioinformatics 

ller 
resampling approach estimate stability dimensional multidimensional independent components 
neuroimage 
makeig delorme 
mining event related brain dynamics 
trends cognitive science 
makeig delorme townsend sejnowski 
electroencephalographic brain dynamics visual targets requiring manual responses 
biology page press 
matsuoka ohya kawamoto 
neural net blind separation nonstationary signals 
neural networks 
sosa yamaguchi 
decomposing eeg data space time frequency components parallel factor analysis 
neuroimage 
schuster 
separation mixture independent signals time delayed correlations 
physical review letters 
ller philips 
combining higher order statistics temporal information blind source separation noise 
proc 
ica pages france 
murata ikeda 
approach blind source separation temporal structure speech signals 
neurocomputing 
nishikawa shikano 
blind source separation multi stage ica combining frequency domain ica time domain ica 
proc 
ieee icassp volume pages may 
nishikawa shikano araki makino 
multistage ica blind source separation real acoustic convolutive mixture 
proc 
int 
conf 
ica bss pages april 
independent component analysis review choi cichocki park lee oppenheim schafer editors 
discrete time signal processing 
prentice hall new jersey nj 

park 
jung 
lee 
lee 
subband blind signal separation noisy speech recognition 
electronics letters 

park 
oh 
lee 
oversampled filter bank approach independent component analysis convolved mixtures 
proc 
joint int 
conf 
artificial neural networks neural information processing pages june 
park oh lee 
filter bank approach independent component analysis application adaptive noise cancelling 
neurocomputing 
parra mueller spence 
unmixing hyperspectral data 
advances neural information processing systems nips pages 
morgan kaufmann 
parra spence 
convolutive blind separation non stationary sources 
ieee trans 
speech audio processing may 
pearlmutter parra 
maximum likelihood blind source separation context sensitive generalization ica 
advances neural information processing systems pages cambridge ma 
mit press 
pham 
joint approximate diagonalization positive definite hermitian matrices 
technical report lmc imag university grenoble france 
pham 
cardoso 
blind separation instantaneous mixtures nonstationary sources 
proc 
ica pages helsinki finland 
plumbley 
algorithms nonnegative independent component 
ieee trans 
neural networks may 
rosca 
real time time frequency blind source separation 
proc 
int 
conf 
ica bss pages december 
rosca 
generalized sparse signal mixing model application noisy blind source separation 
proc 
ieee icassp pages 
du parra 
recovery constituent spectra non negative matrix factorization 
proceedings spie volume pages 
wavelets applications signal image processing 
takeda 
blind source separation combining frequency domain ica beamforming 
proc 
ieee icassp pages 
sawada araki makino 
polar coordinate activation function frequency domain blind source separation 
proc 
int 
conf 
ica bss pages december 
sawada araki makino 
robust precise method solving permutation problem frequency domain blind source separation 
ieee trans 
speech audio processing september 

information theoretic approaches source separation 
master thesis mit media arts sci 
dept mit boston june 

blind separation convolved mixtures frequency domain 
neurocomputing november 

blind source detection separation second order non stationarity 
proc 
ieee int conf 
acoustics speech signal processing pages 
neural information processing letters reviews vol january stone porter wilkinson 
spatiotemporal independent component analysis event related fmri data skewed probability density functions 
neuroimage 
stone 
blind source separation temporal predictability 
neural computation 
tanaka cichocki 
subband decomposition independent component analysis new performance criteria 
proceedings international conference acoustics speech signal processing icassp volume pages montreal canada may 
theis cichocki 
robust overcomplete matrix recovery sparse sources generalized hough transform 
proceedings th european symposium artificial neural networks esann pages bruges belgium april 
tong inouye liu 
finite step global convergence algorithm parameter estimation multichannel ma processes 
ieee trans 
signal processing oct 
tong 
liu 
soon 
huang 
indeterminacy identifiability blind identification 
ieee trans 
circuits systems may 
torkkola 
blind separation convolved sources information maximization 
proc 
ieee int 
workshop 

identification deconvolution multichannel linear non gaussian processes higher order statistics inverse filter criteria 
ieee trans 
signal processing march 
cichocki 
blind noise reduction multisensory signals ica subspace filtering application eeg analysis 
biological cybernetics april 
wee principe 
criterion bss simultaneous diagonalization time correlation matrices 
proc 
ieee int 
workshop pages 
weiss 
adaptive filtering oversampled subbands 
phd thesis signal processing division univ strathclyde glasgow may 
weiss stewart 
efficient implementations complex real valued filter banks comparative subband processing application adaptive filtering 
proc 
st int 
symp 
commun 
systems digital signal processing pages april 
weiss stewart 
steady state performance limitations subband adaptive filters 
ieee trans 
signal processing september 

wu principe 
simultaneous diagonalization frequency domain source separation 
proc 
int 
conf 
ica bss pages january 
yamada 
subband adaptive filter allowing maximally decimation 
ieee select 
areas commun september 


blind separation speech mixtures time frequency masking 
ieee trans 
signal processing july 
zhang cichocki amari 
multichannel blind deconvolution phase systems filter decomposition 
ieee transactions signal processing 
zhang cichocki amari 
self adaptive blind source separation activation functions adaptation 
ieee transactions neural networks 
pearlmutter 
blind source separation sparse representation 
advances neural information processing systems nips pages 
morgan kaufmann 
independent component analysis review choi cichocki park lee 
ller 
fast algorithm joint diagonalization non orthogonal transformations application blind source separation 
journal machine learning research july 
uller 

artifact reduction recordings time delayed second order correlations 
ieee trans 
biomedical engineering 
choi born seoul korea 
received degrees electrical engineering seoul national university korea respectively ph degree electrical engineering university notre dame indiana 
visiting assistant professor department electrical engineering university notre dame indiana fall semester 
laboratory artificial brain systems riken japan assistant professor school electrical electronics engineering national university 
associate professor computer science university science technology korea 
invited senior researcher laboratory advanced brain signal processing brain science institute riken japan 
primary research interests include statistical machine learning probabilistic graphical models bayesian learning representational learning computational biology independent component analysis 
dr choi ieee machine learning signal processing tc member currently ieee blind signal processing tc member 
andrzej cichocki born poland 
received sc 
honors ph doctorate dr sc 
degrees electrical engineering warsaw university technology poland respectively 
institute theory electrical engineering electrical measurements warsaw university technology full professor 
author books adaptive blind signal image processing john wiley professor shun ichi amari mos switched capacitor continuous time integrated circuits systems springer neural networks optimization signal processing wiley teubner verlag professor rolf unbehauen author papers 
books translated chinese languages 
spent university erlangen germany years alexander humboldt research fellow guest professor working area vlsi electronic circuits artificial neural networks optimization 
conducted realized successful research projects 
working team leader laboratory artificial brain systems frontier research program riken japan brain information processing group head laboratory advanced brain signal processing brain science institute riken japan 
member international scientific committees associated editor ieee transaction neural networks january 
current research interests include biomedical signal image processing especially blind signal image processing neural networks applications learning theory robust algorithms generalization extensions independent principal component analysis optimization problems nonlinear circuits systems theory applications 
min park received ph degrees electrical engineering computer science korea advanced institute science technology korea respectively 
currently working department biosystems korea advanced institute science technology post doc 
current research interests include theory applications independent component analysis blind signal separation adaptive noise canceling noise robust speech recognition 
neural information processing letters reviews vol january soo young lee graduated bs seoul national university ms korea advanced institute science technology ph polytechnic institute new york 
joined korea advanced institute science technology assistant professor professor department biosystems department electrical engineering computer science 
dr lee serving director brain science research center main research organization korean brain research program 
current research interests include modeling applications chip implementations human auditory pathways robust speech feature extraction binaural processing topdown attention 
research interests extended artificial brain human perception self development capability 

