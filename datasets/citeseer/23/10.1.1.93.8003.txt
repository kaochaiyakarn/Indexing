locally linear embedded eigenspace analysis ifp tr lea jan yun fu thomas huang beckman institute advanced science technology university illinois urbana champaign north mathews avenue urbana il huang ifp uiuc edu institute existing nonlinear local methods dimensionality reduction yield impressive results data embedding manifold visualization 
open problem define unified projection new data embedded subspace constructed training samples 
thinking globally fitting locally new linear embedding approach called locally embedded analysis lea dimensionality reduction feature selection 
lea viewed linear approximation locally linear embedding lle 
solving linear eigenspace problem closed form lea automatically learns local neighborhood characteristic discovers compact linear subspace optimally preserves intrinsic manifold structure 
new highdimensional data point lea finds corresponding low dimensional point subspace linear projection 
embedding process concentrates adjacent data points dense cluster effective discriminant analysis supervised classification unsupervised clustering 
test proposed lea algorithm benchmark databases 
experimental results show lea provides better data representation efficient dimensionality reduction classical linear methods 
computer vision pattern recognition problems large number sensory inputs images videos viewed intrinsically low dimensional data distributed high dimensional vector space 
order understand analyze multivariate data need reduce dimensionality find compact representations 
variance multivariate data faithfully represented set parameters data considered set geometrically related points lying smooth low dimensional manifold :10.1.1.14.8597
fundamental issue dimensionality reduction model geometry structure manifold produce faithful embedding data projection 
years large number approaches proposed constructing computing embedding 
categorize methods linearity 
nonlinear methods locally linear embedding lle laplacian eigenmaps isomap hessian lle semidefinite programming sde focus preserving geodesic distances reflect real geometry low dimensional manifold 
lle formulates manifold learning problem neighborhood preserving embedding learns global structure exploiting local symmetries linear reconstructions 
spectral decomposition graph laplacians laplacian finds approximation laplace beltrami operator defined manifold 
isomap extends classical mds computing pairwise distances geodesic space manifold 
sense isometry estimates hessian matrix sda considers local angles distances 
linear methods principal component analysis pca multidimensional scaling mds locality preserving projections lpp locality pursuit embedding lpe evidently effective observing euclidean structure :10.1.1.28.9072
pca finds embedding maximizes projected variance mds preserves pairwise distances data points embedding 
metric euclidean mds pca 
lpp lpe local approaches 
lpp computes embedding terms heat kernel nearest neighbor graph discovers essential manifold structure preserving local information 
lpe performs pca local nearest neighbor patches reveal tangent space structure manifold 
nonlinear embedding methods successfully applied standard data sets generated satisfying results dimensionality reduction manifold visualization 
approaches define input mapping solely training data space 
project new data embedded space open problem real word applications 
unclear reverse manifold input mapping defined 
linear embedding methods demonstrated computationally efficient suitable practical applications pattern classification visual recognition 
embedding approximated linear process statistical sense methods ignore geodesic structure true manifold 
linear methods reveal perfect geometric structure nonlinear manifold expect find better linear approximation nonlinear embedding lpp sufficiently effective deal practical problems computer vision machine learning 
locally linear embedded eigenspace analysis abbreviated locally embedded analysis lea linear dimensionality reduction feature selection 
inspired neighborhood preserving property geometric intuition lle propose linear eigenspace embedding strategy finds optimal projection minimize local reconstruction error 
approach builds objective function similar lle seeks linear input manifold mapping closed form 
result lea reveals novel perspectives constructing embedded low dimensional subspace 
algorithm weight matrix locally pairwise relation constructed nearest neighbor reconstruction 
embedded space built weight matrix 
learned subspace quadratic form constraint eigenspace analysis applied compute optimal basis eigenvectors data projection 
lea exhibits attractive properties dimensionality reduction feature selection 
lea exact closed form solution 
lea viewed linear approximation lle 
objective function lle lea formulates embedding problem linear eigenspace analysis 
advantage new high dimensional data point lea automatically finds corresponding low dimensional point manifold linear projection 
lea reasonable data representation method 
consider neighbors particular point lying certain subspace entire data space 
subspace property linear object class model prototype data represented linear combination 
lea imposes important property embedded space 
lea discriminant property supervised classification unsupervised clustering 
pattern classification theory classifier sufficiently smooth data points adequately close share label 
considering nearest neighbor classification small sample issues believe local structure important class distribution training data inadequate revealing faithful statistical class information 
lea directly holds local geometry concentrate nearest data points compact clusters pattern discriminant 
demonstrated lea outperforms pca lda visual recognition cases :10.1.1.28.9072
lea latent relation lpp sense linear subspace analysis 
methods try preserve local structure linear embedding absolutely different objective function mathematical derivation geometric sense 
main advantage lea automatically constructs weight matrix closed form solution learning intrinsic neighborhood structure lpp constructs laplacian graph heat kernel function needs tune kernel parameter practice 
sections review lle algorithm sec 
locally linear embedded eigenspace analysis lea sec discuss properties lea sec 
experimental results shown sec 
conclude sec 
locally linear embedding lle original locally linear embedding lle algorithm includes stages linear nearest neighbor reconstruction locally structure preserved embedding 
linear nearest neighbor reconstruction consider high dimensional data set elements denoted xn lle original data lie nonlinear manifold approximated local linear model 
geometric relationship high dimensional data described linear nearest neighbor reconstruction 
means data point represented linear combination nearest neighbors 
denote set xi nearest neighbors rd reconstruction error calculated cost function constrain xi calculated solving argmin 
reconstruction weight vector subject locally structure preserved embedding embedding stage representations intrinsic characteristics weights preserved maintain nonlinear manifold geometry lowdimensional space 
low dimensional data set denoted corresponds xi 
weights solved sec define embedding cost function yi reconstruction error minimized solving eigenvalue problem subject constrains yi yy ni yn identity matrix 
locally linear embedded eigenspace analysis nonlinear projection 
nonlinear projection nonlinear methods effectively reflect intrinsic geometric structure high dimensional data visible low dimensional subspace 
real world problems especially clustering visual recognition methods essential limitations nonlinear projection defined training data space 
novel input datum entire embedding procedure including global nearest neighbor search weight matrix calculation repeated 
nonlinear methods provide forward input manifold mapping reversible parametric mapping low dimensional space high dimensional space 
overcome limitations propose locally linear embedded eigenspace analysis embedding approximated nonparametric linear space projection optimally preserves local neighborhood structure 
linear space projection linear projection particularly attractive closedform solution computationally inexpensive 
problems excessive dimensionality linear projection maps high dimensional data lower dimensional subspace eigenspace analysis 
suppose mapping xn 
space projection defined transformation projection matrix denoted satisfies xi 
projection written single matrix equation xi respectively viewed columns matrix matrix original dimensional data set projected line 
obtain projection vector pn line project yn vector coordinates yi xi 
computing weight matrix closed form solve lea projection calculating weight matrix closedform 
weights constrained sum eq 
rewritten xi jl define local gram matrix gi xi gi xi xi gi xi xi column vector consists ones columns matrix contains xi nearest neighbors 
setting column vector wi rewrite eq 
constraint wi sec 
solve squares problem obtain closed form solution wi define sparse weight matrix wi elements 
general sparse matrix consisting neighborhood characteristic original space 
learning locally linear embedded subspace section derive closed form solution learning locally linear embedded subspace cases 
formulations subject different constraints corresponds supervised embedding unsupervised embedding respectively 
solution supervised embedding supervised embedding suppose know desired low dimensional degree project 
problem embedding calculating mapping ddimensional space dimensional subspace 
obtain closed form solution linear projection eq rearrange eq follows yi xi tr tr trace operator matrices 
subject constraint xx ni obtain eigenvalue problem lagrange optimization 
xx diagonal lagrange multiplier matrix 
columns smallest eigenvectors matrix xx discarding bottom eigenvector mean 
solution unsupervised embedding unsupervised embedding suppose know desired low dimensional degree project 
formulate embedding problem projection mapping dimensional space axis low dimensional space optimal 
geometrical sense high dimensional data linearly projected optimal line preserving underlying neighborhood structure 
define diagonal matrix 
eq substitute yi xi 
eq written yi xi xi xi xi pt pt definition weight matrix sec obtain cost function recall diagonal value matrix corresponds weights summation nearest neighbors particular data point 
nonsymmetric rows columns sparse matrix follows diagonal matrix 
matrix indicates local distribution yi large value diagonal means short distance neighbors 
solve squares problem minimize objective function subject constraint 
straightforward obtain projection vector solving eigenvalue problem obviously matrices positive semidefinite symmetric 
subject constraint andd eq rewritten similar solution 
supervised locally embedded analysis supervised learning sample pattern training set associated category label cost 
task classifier distinguish patterns various labels recognize new patterns unknown labels 
lea local method suitable supervised locally embedded analysis cases 
strict constraint imposed supervised lea nearest neighbors chosen class xi 
nc samples class follows nc nc 

linear input manifold projection 
lea holds objective functions lle maintains locally linearity embedding 
basic properties lea unified input manifold projection undefined lle nonlinear methods 
novel dimensional data point lea directly find corresponding ddimensional point subspace linear projection 
furthermore lea provides possible reverse transformation pseudoinverse operation pp py constraint pp nonsingular 

locally linear embedded discriminant analysis 
lea discriminant property supervised pattern classification unsupervised clustering 
objective function see lea finds underlying embedding holds neighborhood reconstruction characteristic 
embedded low dimensional data point located particular compact cluster containing nearest neighbors coming original space 
theoretically assume symmetric supervised learning case eq rewritten xix sw sbp sb class scatter matrix class scatter matrix respectively total mean training data 
note eq exactly generalized eigenvalue problem linear discriminant analysis seeks transformation matrix maximizing ratio scatter class scatter 
objective function lea potentially contains discriminant property 

manifold visualization clustering 
seen lea seeks optimal linear projection preserve neighborhood characteristic suitable discriminant analysis supervised learning 
hand lea visualize approximated low dimensional manifold solve unsupervised problems clustering 
lea focuses local structure display manifold rough 
order smooth approximated manifold rearrange eq inserting regularization parameter gi eigenvalue matrix 
relating lea laplacian eigenmaps 
mathematical derivation solving linear embedding related laplacian eigenmaps lpp 
differences embedding processes lie different objective functions computation weight matrix 
relate lea laplacian eigenmaps yi yi yi term eq 
similar objective function laplacian eigenmaps square weights 
note laplacian eigenmaps chooses weights heat kernel intrinsic reconstruction weights lea 
second term eq reflects covariance information neighbors represents local structure better 
generally assume symmetric case unsupervised embedding eq 
rewritten eq 
generalized eigenvalue problem lpp nearest neighbor graph subject constraint 
lea automatically learns preserves locally reconstruction weights embedding 
lpp lea need tune parameter setting weight matrix 
experimental results benchmark data sets demonstrate properties lea benchmark data sets 
rotating teapot 
rgb color images resolution taken viewing degrees rotating teapot 
frey faces 
brendan frey grayscale face images taken sequential frames video 
images resolution show variations face expression view rotation 
orl face database 
database contains subjects grayscale face images 
images resolution fig 

linear embedding subspace projection rotating teapot images 
number nearest neighbors 
original images 
manifold curve embedded data points novel projections 
regularization parameter lea 

manifold curve data points lle 
manifold curves data points lea 

show frontal slight tilt head 
images cropped resized 
yale face database :10.1.1.10.3247
database contains grayscale images individuals 
images subject resolution taken conditions center light glasses happy left light glasses normal right light sad surprised 
images cropped resized 
umist face database 
database contains grayscale face images subjects 
subject images resolution various angles left profile frontal view 
images cropped resized 
ifp internal face database 
internal face database contains grayscale images taken video sequences subjects images 
cropped image resolution large variations facial expression illumination pose occlusion 
results linear embedding space projection 
experiment project teapot rotating images data set dimensional space lea 
shows results embedding image space manifold space nearest neighbors 
select training 
testing 
figures show new data projected precisely manifold curve constructed training data 
different values regularization parameter smooth manifold curves 
results lle lea maintains main structures nonlinear manifold turning points point sequence 
fig 

feature selection dimensionality reduction 
eigenfaces vs orl face database 
pca vs lea 
dimensional projection ifp internal face subjects images 
feature selection dimensionality reduction 
experiment data set visualize data representation supervised lea compare pca 
shows results eigenfaces vs 
reduce dimensionality orl face image space represent top eigenvectors eigenfaces 
pca subspace apply supervised lea nearest neighbors select lowest eigenvectors span entire lea subspace 
contain sparse local textures quite different eigenfaces 
shows data projection results pca supervised lea 
arbitrarily choose subjects images ifp internal face database project image dimensional pca subspace lea subspace 
find lea concentrates data points class densely line suitable discriminant analysis comparing sparse global distribution pca subspace 
fig 

manifold visualization frey face images lea nearest neighbors 
dimensional linear embedding unsupervised clustering 
original images divided emotion categories happy red sad yellow blue neutral green 
dimensional linear embedding unsupervised clustering 

manifold visualization clustering 
evaluate unsupervised clustering property lea entire data set 
face images projected dimensional lea subspace nearest neighbors 
shows visualized manifold 
label original data emotional expressions happy red sad yellow blue neutral green 
obvious data belonging class certain region space 
general cluster corresponding positive emotion happy distinctly separated clusters 
cluster corresponding neutral faces located middle positive negative emotion clusters 
cluster data points corresponding head rotation distributed orderly manifold surface 
see embedding reveals cluster distribution 
smoothing manifold regularization parameter obtain results 
clusters compact distinct 
visual recognition 
interested application lea face recognition 
data sets adopted experiments 
shows cropped image examples orl yale umist ifp internal database 
choose euclidean distance nearest neighbor classifier recognition experiments 
assume gallery set experiment training set 
orl yale umist respectively randomly select images fig 

face recognition orl yale umist ifp internal databases 
cropped example images 
recognition accuracy pca lda lea 
average error rates times recognition test databases random data set partitions 
recognition rate vs dimensionality reduction databases 
error rate vs dimensionality reduction 
evaluate generalization lea face recognition ifp internal database 
subject training rest testing 
training data database respectively testing data 
test recognition accuracy pca lda lea case dimensionality reduction 
generalize performance repeat data set partition times database 
shows average lowest error rates method database 
average error rates pca lda lea orl yale umist respectively 
results show lea outperforms pca lda 
comparing lpp recognition error rate yale database lea better result data set partition 
interesting result efficient dimensionality reduction lea visual recognition 
show lea recognition rate comparing pca lea 
shows error rate vs dimensionality reduction 
best reduced dimensionality pca lda lea orl yale umist respectively 
see reduced dimensionality lea close lda lower pca 
choose ifp internal face database evaluate generalization lea visual recognition 
sure gallery set training set testing set different 
images subject randomly selected training gallery testing respectively 
subjects images data sets 
shows recognition rate pca lda lea ifp internal database 
lowest error rates reduced dimensionality see 
reduced dimensionality error rates 
results indicate lea outperforms pca lda generalized recognition problems efficient dimensionality reduction 
locally embedded analysis dimensionality reduction feature selection 
objective function lle lea solves manifold embedding linear projection 
lea method aiming seeking optimal linear projection subspace analysis 
solving linear eigenspace problem closed form lea automatically learns local neighborhood relation discovers compact subspace optimally preserves intrinsic structure manifold sense linearity 
properties lea suitable practical applications pattern representation dimensionality reduction unsupervised clustering supervised visual recognition demonstrated experiments benchmark databases 

brand charting manifold proc 
nips 

hespanha kriegman eigenfaces vs fisherfaces recognition class specific linear projection ieee trans 
pami vol 
pp 


belkin niyogi laplacian eigenmaps dimensionality reduction data representation neural computation vol 
pp 


cox cox multidimensional scaling nd ed chapman hall 

donoho grimes hessian eigenmaps locally linear embedding techniques high dimensional data proc 
national academy arts sciences vol 
pp 


duda peter david pattern classification nd ed wiley interscience 

graham characterizing virtual general purpose face recognition face recognition theory applications nato asi series computer systems sciences vol pp 


niyogi locality preserving projections proc 
nips 

yan hu niyogi zhang face recognition ieee trans 
pami vol 
pp 


martinez kak pca versus lda ieee trans 
pami vol 
pp 


min lu locality pursuit embedding pattern vol 
pp 


moghaddam pentland probabilistic visual learning object representation ieee trans 
pami vol 
pp 


roweis saul nonlinear dimensionality reduction locally linear embedding science vol 
pp 


de ridder reinders local fisher embedding ieee conf 
icpr vol 
pp 


de ridder duin locally linear embedding classification tr series 
ph pr group dept delft uni 
tech netherlands 

harter parameterisation stochastic model human face identification ieee workshop applications computer vision pp 


saul roweis think globally fit locally unsupervised learning low dimensional manifolds machine learning research vol 
pp 


turk pentland face recognition eigenfaces ieee conf 
cvpr pp 


tenenbaum de silva langford global geometric framework nonlinear dimensionality reduction science vol 
pp 


weinberger saul unsupervised learning image manifolds semidefinite programming ieee conf 
cvpr vol 
pp 


zhang wang zhao zhang reconstruction analysis multi pose face images nonlinear dimensionality reduction pattern vol 
pp 

