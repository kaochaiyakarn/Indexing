stratified gradient boosting fast training conditional random fields bernd gutmann kristian kersting department computer science universiteit leuven celestijnenlaan heverlee belgium csail massachusetts institute technology street cambridge ma usa 
boosting shown promising approach training conditional random fields crfs allows efficiently induce conjunctive relational features 
potentials represented weighted sums regression trees induced gradient tree boosting 
large scale application relational domains suffers drawbacks induced trees spoil previous number generated regression examples quite large 
propose tackle problem injecting randomness regression estimation procedure subsampling regression examples 
experiments real world data set show sampling approach comparable sophisticated boosting algorithms early iterations provides interesting alternative simpler implement 
sequential data ubiquitous interest communities 
data virtually application areas machine learning including computational biology activity recognition information extraction 
example consider task marking proteins abstracts biological publications 
appealing approach sequence labeling problems lafferty conditional random fields crfs 
undirected models encoding conditional dependency outperformed hmms rabiner language processing tasks information extraction shallow parsing 
contrast trained hmms discriminatively trained crfs designed handle non independent input features molecular weight neighboring acids amino acid 
flexibility comes expense severe training costs 
fast integrated feature induction parameter estimation techniques earlier version appeared pages extended electronic working notes th international workshop mining learning graphs august universita degli studi di firenze florence italy 
xt xt fig 

graphical representation linear chain crf 
proposed 
mccallum mallet system employs bfgs algorithm second order parameter optimization method deals parameter interactions induces features iteratively 
starting single feature conjunctions features iteratively constructed significantly increase conditional log likelihood added current model 
dietterich proposed boosting approach called competitive mallet 
follows friedman gradient tree boosting algorithm potential functions represented sums regression trees grown stage wise manner adaboost freund schapire 
regression tree viewed defining new feature combinations path tree root leaf 
features quite complex relational conjunctions shown gutmann kersting 
major drawback gradient tree boosting approach number generated regression examples large 
labels training sequences length number training examples label 
get propose sampling strategy tailored gradient tree boosting relational crfs 
precisely introduce stratified sampling approach crfs conduct experimental evaluation examine influence subsampling line search gradient method steepest ascent vs conjugated gradient predictive performance 
proceed follows 
reviewing crfs gradient tree boosting discuss stratified sampling scheme 
concluding evaluate approach real world information extraction dataset 
gradient tree boosting crfs crfs undirected graphical models encode conditional probability distributions set features 
focus linear chain crf models cf 

undirected graphical model sets random variables linear chain crfs xi ti ti yi correspond input output sequences labeling observed sequence conditional probability state sequence observed sequence defined exp yt yt yt yt yt yt potential functions normalization factor state sequences potential contributes probability 
training typically assumed potentials factorize set features fk fixed yt kgk yt yt yt kfk yt yt respectively 
model parameters set real valued weights weight feature 
estimate conditional maximum likelihood approach typically followed 
conditional likelihood training data current parameter improve parameters 
normally uses sort gradient search doing 
log yi xi gradient multiplied constant obtained doing line search gradient 
training gradient tree boosting gradient tree boosting interleaves parameter estimation feature selection 
precisely starts initial potential zero function adds iteratively corrections 
contrast standard gradient approach denotes called functional gradient ex log joint distribution unknown evaluate expectation ex ones evaluates gradient function position potential function real valued function captures degree assignment yt output variable fits transition yt algorithm inner loop gradient tree boosting generating regression examples function data initialize relational regression examples xi yi data iterate training examples xi forwardbackward xi compute forward backward probabilities ti iterate positions iterate class labels compute value gradient position class label yt yt xi exp wt xi yt yt yt yt xi wt xi update set relational regression examples return function training example fit regression tree derived examples 
precisely setting yt yt yt yt gradient see dietterich gutmann kersting details log wd yd yd yd yd wd indicator function denotes matches subsumes yd yd wd probability class labels fit class labels positions 
evaluating gradient known position training data fitting regression model relational regression tree cf 
values gets approximation expectation ex gradient 
order speed computations complete input typically window wd xd 
xd 
xd fixed window size 
conjugate direction boosting reconsider basic gradient ascent optimization approach 
problems choosing step size doing line search maximization direction spoil past 
avoid conjugate gradient boosting methods kersting gutmann compute called conjugate directions function space orthogonal spoil previous 
step size estimated directions doing line searches 
algorithm conjugated gradient tree boosting line search sampling 
function data iterate functional gradient iterate class labels sp os sp os data fm sample sp os initial conjugate direction polak re formula dm dm conjugate direction linesearch data fm dm line search dm fm fm dm model return fm return potential function precisely empirical angle training examples current gradient computed 
shown alg 
current gradient plus old weighted gradient multiplied calculated angle added current model dm dm 
angle calculated evaluating polak re formula example 
weighted gradient dm linear combination gradients 
shown dt fig 

relational regression tree 
nodes denote tests leaves predicted values 
city true true false true false false true true false algorithm stochastic gradient tree boosting 
function data iterate functional gradient data randomly sample subset size data iterate class labels data fm fm fm dm model return fm return potential function stochastic gradient tree boosting major drawback gradient tree boosting approach number generated regression examples large 
iteration regression examples generated number labels number training examples average sequence length 
obvious modification speed gradient tree boosting subset original data 
subset drawn randomly iteration ensures complete training data contributes learned model 
modification stochastic gradient tree boosting originally proposed friedman 
algorithm shows pseudocode size sample 
size data algorithm behaves standard gradient tree boosting 
stochastic gradient tree boosting kind bagging approach 
experimental results show improves runtime obvious result increase accuracy learned model 
stochastic gradient tree boosting directly fast training conditional random fields 
general technique methods tailored problem hand training conditional random fields improve performance 
kersting gutmann showed gradient tree boosting crfs induces expectation bias generated regression examples observed called positive example training data unobserved negative regression examples generated number possible labels output sequence 
higher regression tree learner spends lot time fit tree unobserved examples predictive accuracy positive examples lower 
proposed reweight examples empirical ratio positive negative examples equal 
problem remained regression tree learner generating step gradient tree boosting hat consider data 
approach positive negative examples 
regression tree learner consider examples 
propose stratified sampling 
means different sampling strategies observed regression examples grouping sampling observed regression examples training examples generated fig 

stratified sampling methodology 
negative examples 
idea reduce difference number positive negative examples 
doing increase predictive performance positive examples reduce time needed regression tree learner 
stratified sampling possible sampling approaches gradient tree boosting 
friedman suggested techniques speed learning process reducing number regression examples 
influence trimming ignores regression examples absolute value smaller 
investigate method consider sampling randomly drawn subset generated examples train regression trees 
friedman original proposal subsample uniformly training examples 
strategy suboptimal training crfs 
generated examples observed 
recall observed yt generate expected labels yt 
increasing number labels probability sampling observed regression examples decreases keeping training set fixed roughly scale 
turn influence expected examples ones observed increases gradient boosting induce meaningless models 
undesirable property uniform sampling 
overcome propose stratified sampling scheme 
stratified sampling cf 
method sampling desirable maintain certain characteristics data set subset sampled 
achieved firstly partitioning data set number mutually exclusive subsets cases strata representative aspect real world process involved 
sampling population achieved randomly sampling various subsets achieve representative proportions 
log likelihood training set log likelihood test set cg ls cg ls cg ls cg cg time sec cg ls cg ls cg ls cg cg time sec fig 

fold cross validated log likelihoods training test set regression examples positive times negative regression examples sampled half positive half negative examples sampled cg conjugated gradients ls line search case hand natural strata observed generated regression examples 
generate regression examples mark examples positive identity function see equation 
sample different strategies positive negative examples sp os 
instance take positive examples sample fraction negative examples times bigger positive examples 
denote strategy 
experiments intention examine influence sampling rate line search gradient method predictive performance 
aim integrated stratified sampling boosting relational crfs implemented yap prolog 
regression tree learner implemented hipp prolog 
ran experiments subset data set 
data set consists sentences medical abstracts word input sequence augmented word type phrase type 
task find proteins location dna 
precisely output sequence consists sequences protein location 
example training example unk rat cop located prep art nuclear unk rim 
protein location 
removed training sequences contain protein location tag 
memory limitations took resulting data set input output sequence pairs 
sampling techniques running 
compare deterministic approach 
experiments fold cross validation examples training testing trained model 
tried sampling strategies ran experiments line search conjugated gradients 
window size elements tree learner learned regression trees depth splitting node pre pruning variance smaller 
shows results plotted training time 
strategy uses positive times negative regression examples positive ones outperforms approaches early iterations 
case training examples examples positive negative 
remarkable better cg ls strategy 
examples run furthermore sophisticated training algorithm 
understand see line search slow furthermore observed data considered 
uses observed data 
furthermore methods achieve similar performances terms likelihood objective function maximizing iterations 
note stratified sampling approach achieves highest training test likelihoods sec 
coincide stopping point optimization implemented stopping rule 
order see long term behaviors boosting algorithms 
summarize larger data sets deterministic boosting technique ran memory 
stochastic versions chance keep running due reduction memory consumptions 
accuracy test set cg ls cg ls cg ls cg cg time sec fig 

fold cross validated accuracy test set 
stratified sampling speeds computations achieves higher loglikelihood estimates 
complete view performance investigated fold accuracy 
shows accuracy trained model test data 
accuracy defined number correctly predicted positions sequences number positions sequences 
see strategy performs best early iterations cg outperforms strategies 
words subsampling negative examples yields slightly lower accuracies degrade performance 
basically methods close range comparable 
supports results kersting gutmann 
furthermore see line search significantly increase accuracy 
believe line search tends overfit ran sampling strategies 
results follow general picture stratified sampling positive examples subsampling negative ones faster deterministic gradient tree boosting achieves comparable performance 
examples particular positive ones worse performance 
stochastic gradient tree boosting approach training relational crfs 
contrast existing stochastic techniques follow uniform sampling strategy sample positive observed negative generated regression examples different strategies 
experimental results shown stratified sampling scheme speeds computations improve performance 
performs better uniform sampling 
compared advanced boosting techniques conjugate direction boosting simpler implement 
view stochastic boosting stratified sampling attractive promising approach scaling crf training large data sets 
experiments larger data sets conducted confirm scale behavior 
reduction examples small data set encouraging larger data sets expect higher reduction rates 
interesting investigate stratification output class positive negative regression examples 
acknowledgments authors anonymous reviewers valuable comments luc de raedt support tor santos costa help prolog system yap 
supported research foundation flanders fwo bibliography dietterich 

training conditional random fields gradient tree boosting 
proc 
st international conf 
machine learning pp 

acm 
freund schapire 

experiments new boosting algorithm 
proceedings icml pp 

morgan kaufman 
friedman 

greedy function approximation gradient boosting machine 
annals statistics 
friedman 

stochastic gradient boosting 
computational statistics data analysis 
gutmann kersting 

conditional random fields logical sequences 
proc 
th european conference machine learning ecml pp 

berlin germany springer 
kersting gutmann 

unbiased conjugate direction boosting conditional random fields 
working notes ecml workshop mining learning graphs pp 

berlin germany 
short 
lafferty mccallum pereira 

conditional random fields probabilistic models segmenting labeling sequence data 
proc 
th int 
conf 
machine learning icml pp 

mccallum 

inducing features conditional random fields 
proc 
st conference uncertainty artificial intelligence uai pp 

edinburgh scotland 
rabiner 

tutorial hidden markov models selected applications speech recognition 
proceedings ieee pp 

craven ray 

hierarchical hidden markov models information extraction 
proceedings th international joint conference artificial intelligence ijcai pp 

