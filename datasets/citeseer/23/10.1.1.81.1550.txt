journal machine learning research submitted published large scale multiple kernel learning ren sonnenburg sonnenburg fraunhofer de fraunhofer ida berlin germany gunnar tsch gunnar tuebingen mpg de friedrich laboratory max planck society bingen germany sch fer schaefer fraunhofer de fraunhofer ida berlin germany bernhard sch lkopf bernhard tuebingen mpg de max planck institute biological cybernetics bingen germany editors kristin bennett hern ndez classical kernel learning algorithms single kernel practice desirable multiple kernels 
lanckriet 
considered conic combinations kernel matrices classification leading convex quadratically constrained quadratic program 
show rewritten semi infinite linear program efficiently solved recycling standard svm implementations 
generalize formulation method larger class problems including regression class classification 
experimental results show proposed algorithm works thousands examples hundreds kernels combined helps automatic model selection improving interpretability learning result 
second part discuss general speed mechanism svms especially sparse feature maps appear string kernels allowing train string kernel svm real world splice data set computational biology 
integrated multiple kernel learning machine learning toolbox source code publicly available athttp www fml tuebingen mpg de projects 
keywords multiple kernel learning string kernels large scale optimization support vector machines support vector regression column generation semi infinite linear programming 
kernel methods support vector machines svms proven powerful wide range different data analysis problems 
employ called kernel function xi xj intuitively computes similarity examples xi result svm ren sonnenburg gunnar tsch sch fer bernhard sch lkopf 
sonnenburg tsch sch fer sch lkopf learning weighted linear combination kernels bias sign xi xi labeled training examples yi 
developments literature svms kernel methods shown need consider multiple kernels 
provides flexibility reflects fact typical learning problems involve multiple heterogeneous data sources 
furthermore shall see leads elegant method interpret results lead deeper understanding application 
called multiple kernel learning mkl problem principle solved cross validation papers focused efficient methods multiple kernel learning chapelle bennett ong bach lanckriet bi 
problems kernel methods compared techniques resulting decision function hard interpret difficult order extract relevant knowledge problem hand 
approach problem considering convex combinations kernels xi xj kkk xi xj kernel kk uses distinct set features 
appropriately designed sub kernels kk optimized combination coefficients understand features examples importance discrimination able obtain accurate classification sparse weighting quite easily interpret resulting decision function 
important property missing current kernel algorithms 
note contrast kernel mixture framework bennett 
bi 
kernel example assigned independent weight offer easy way interpret decision function 
illustrate considered mkl formulation provides useful insights time efficient 
consider framework proposed lanckriet 
results convex optimization problem quadratically constrained quadratic program qcqp 
problem challenging standard svm qp principle solved general purpose optimization 
algorithms feasible small problems data points kernels bach 
suggested algorithm sequential minimization optimization smo platt 
kernel learning problem convex non smooth making direct application simple local descent algorithms smo infeasible 
bach 
considered smoothed version problem smo applied 
part follow different direction reformulate binary classification mkl problem lanckriet semi infinite linear program efficiently solved shelf lp solver standard svm implementation cf 
section details 
second step show easily mkl formulation algorithm generalized larger class convex loss functions cf 
section 
proposed wrapper method works kernel loss functions order obtain efficient mkl large scale mkl algorithm new loss function suffices lp solver corresponding single kernel algorithm assumed efficient 
general algorithm able solve mkl problems examples kernels reasonable time 
consider chunking algorithm considerably efficient optimizes svm multipliers kernel coefficients time 
large scale problems needs compute cache kernels separately kernel single kernel algorithm 
particularly important sample size large 
hand number kernels large amount memory available caching drastically reduced kernel caching effective anymore 
statements apply smo mkl algorithm proposed bach 

kernel caching help solve large scale mkl problems sought ways avoid kernel caching 
course possible certainly class kernels feature map explicitly computed computations implemented efficiently 
section describe string kernels frequently biological sequence analysis exhibit property 
feature space high dimensional typically sparse 
section discuss methods efficiently dealing high dimensional sparse vectors interest mkl speeding ordinary svm classifiers 
suggest modification previously proposed chunking algorithm exploits properties section 
experimental part show resulting algorithm times faster plain chunking algorithm examples large kernel caches 
able solve mkl problems examples kernels real world splice site classification problem computational biology 
conclude illustrating usefulness algorithms examples relating interpretation results automatic model selection 
provide extensive benchmark study comparing effect different improvements running time algorithms 
implemented algorithms discussed interfaces matlab octave python 
source code freely available www fml tuebingen mpg de projects 
examples generate figures implemented matlab matlab interface toolbox 
data sets www fml tuebingen mpg de projects 

general efficient multiple kernel learning algorithm section derive mkl formulation binary classification case show extended general cost functions 
subsection propose algorithms solving resulting semi infinite linear programs 
multiple kernel learning classification multiple kernel learning problem binary classification data points xi yi yi xi translated mappings dk input 
results shown 
sonnenburg tsch sch fer sch lkopf feature spaces xi xi dk denotes dimensionality th feature space 
solves optimization problem bach equivalent linear svm mkl primal classification min wk wk dk yi wk xi note problem solution written wk kw bach 
note norm constrained penalizing norm wk block separately 
idea norm constrained penalized variables tend sparse optimal solutions norm penalized variables tsch chapter 
optimization problem offers possibility find sparse solutions block level non sparse solutions blocks 
bach 
derived dual problem 
problem dk squaring constraints gamma multiplying constraints substituting leads equivalent multiple kernel learning dual mkl dual classification min iyi xi xj kk xi xj xi note quadratic constraint kernel sk 
case problem reduces original svm dual 
move term constraints 
equivalently done adding sides constraints substituting 
assume tr kk set bach 

mkl dual classification large scale mkl min iyi xi xj sk order solve may solve saddle point problem minimize sk iyi maximize 
setting derivative zero obtains constraint simplifies ksk 
minimizes objective time maximizes kernel weighting 
leads min max problem max min ksk iyi 
problem similar equation bi 
composite kernels linear combinations kernels considered 
term sk moved constraint including missing 
assume optimal solution minimal subject constraints 
finding saddle point equivalent solving semi infinite linear program semi infinite linear program max ksk yi 
bi 
argued approximation quality composite kernels inferior mixtures kernels weight assigned example kernel bennett 

reason efficient methods available solve composite kernel problem considered mixtures kernels experimental validation uniform weighting composite kernel experiment 
consider composite kernels method interpret resulting classifier looked classification accuracy 
sonnenburg tsch sch fer sch lkopf note linear program linearly constrained 
infinitely constraints satisfying iyi 
problems solution 
illustrate consider fixed minimize 
solution minimizes 
increase value long infinitely constraints violated ksk 
hand increase fixed maximizing 
discuss section solve semi infinite linear programs 
multiple kernel learning general cost functions section consider general class mkl problems arbitrary strictly convex differentiable loss function derive mkl formulation 
investigate general mkl different loss functions particular soft margin loss insensitive loss quadratic loss 
define mkl primal formulation strictly convex differentiable loss function mkl primal generic loss functions min wk wk dk xi xi yi xi wk analogy bach 
treat problem second order cone program socp leading dual see appendix derivation mkl dual generic loss functions min xi yi yi il yi denotes inverse derivative prediction 
derive formulation follow recipe section deriving lagrangian leads max min problem formulation eventually reformulated generic loss functions large scale mkl max sk yi yi ksk il yi xi assumed strictly convex differentiable unfortunately soft margin insensitive loss properties 
consider separately sequel 
soft margin loss loss order approximate soft margin loss easy verify log exp xy 
lim xy 
strictly convex differentiable 
loss assuming yi obtain cf 
appendix sk log log iyi xi terms vanish provided yi yi 
substituting iyi obtain sk iyi xi 
iyi class soft margin loss class svm soft margin sch lkopf smola similar class case leads subject 
sk xi sonnenburg tsch sch fer sch lkopf insensitive loss obtain technique epsilon insensitive loss sk xi yi yi 
easy derive dual problem loss functions quadratic loss logistic loss see appendix 
note dual differ definition sk domains algorithms solve semi infinite linear programs considered structure max ksk optimized respect 
constraints depend definition sk choice cost function 
theorem tsch 
show solution corresponding primal feasible bounded see 
duality gap ifm sk closed set 
loss functions considered condition satisfied 
propose technique called column generation solve 
basic idea compute optimal restricted subset constraints 
called restricted master problem 
second algorithm generates new unsatisfied constraint determined 
best case algorithm finds constraint maximizes constraint violation intermediate solution argmin ksk 
satisfies constraint ksk solution optimal 
constraint added set constraints iterations continue 
algorithm special case set algorithms known exchange methods 
methods known converge cf 
theorem 
convergence rates algorithm known 
sufficient obtain approximate solution define suitable convergence criterion 
note problem solved constraints satisfied 

shown solving semi infinite problems method related boosting meir tsch requires log iterations remaining constraint violation constants may depend kernels number examples tsch tsch warmuth warmuth 
small values technique produces reasonably fast approximate solutions 
large scale mkl natural choice normalized maximal constraint violation convergence criterion algorithm stops mkl mkl kk ksk mkl accuracy parameter optimal solution iteration corresponds newly maximally violating constraint iteration 
formulate algorithms alternately optimize parameters 
wrapper algorithm wrapper algorithm see algorithm divides problem inner outer subproblem 
solution obtained alternatively solving outer problem results inner problem input vice versa convergence 
outer loop constitutes restricted master problem determines optimal fixed shelf linear optimizer 
inner loop identify unsatisfied constraints fortunately turns particularly simple 
note considered cases exactly dual optimization problem single kernel case fixed 
instance binary classification soft margin loss reduces standard svm dual kernel xi xj kkk xi xj min rn jk xi xj iyi 
standard svm implementation single kernel order identify violated constraint 
exists large number efficient algorithms solve single kernel problems sorts cost functions easy way extend applicability problem multiple kernel learning 
kernels computed thefly svm single kernel cache required 
wrapper algorithm easy implement generic reasonably fast small medium size problems 
determining fixed high precision intermediate solutions far away global optimal unnecessarily costly 
room improvement motivating section 
chunking algorithm simultaneous optimization goal simultaneously optimize svm training 
usually infeasible standard optimization tools minos cplex loqo solving svm training problems data sets containing examples 
called decomposition techniques chunking joachims overcome limitation exploiting special structure svm problem 
key idea decomposition freeze small number optimization variables working set solve sequence constant size problems subproblems svm dual 
propose extension chunking algorithm optimize kernel weights example weights time 
algorithm motivated insufficiency wrapper algorithm described previous section optimal optimization optimality necessary inefficient 
sonnenburg tsch sch fer sch lkopf algorithm mkl wrapper algorithm optimizes convex combination kernels employs linear programming solver iteratively solve semi infinite linear optimization problem 
accuracy parameter mkl parameter algorithm 
sk andc determined cost function 
compute argmin st st sk st mkl break argmax sk single kernel algorithm ks kkk considerably faster newly obtained chunking iterations efficiently recompute optimal continue optimizing new kernel weighting 
intermediate recomputation recomputing involves solving linear program problem grows additional induced constraint 
iterations solving lp may infeasible 
fortunately facts making possible small number added constraints remain active may remove inactive ones prevents lp growing arbitrarily simplex lp optimizers cplex exists called hot start feature allows efficiently recompute new solution instance additional constraints added 
svm light optimizer going modify internally needs output gi jy jk xi xj training examples order select variables optimization joachims 
changes kernel weights stored gi values invalid need recomputed 
order avoid full recomputation additionally store matrix gk jy xi xj outputs kernel separately 
change gi quite efficiently recomputed gi kgk implemented final chunking algorithm mkl regression classification case display algorithm 
discussion wrapper chunking algorithm merits wrapper algorithm relies repeated efficient computation single kernel solution typically large scale algorithms exist 
chunking algorithm faster exploits intermediate needs compute cache kernels separately particularly important large scale mkl algorithm outline mkl chunking algorithm classification case extension svmlight optimizes kernel weighting simultaneously 
accuracy parameter mkl subproblem size assumed algorithm 
simplicity omit removal inactive constraints 
note iteration lp differs additional constraint 
usually exploited save computing time solving lp 
gk gi check optimality conditions optimal select suboptimal variables iq old solve svm dual respect selected variables update gk gk iq old iq xi st gk st st mkl argmax rk ksr gi gk large 
hand large amount memory available caching drastically reduced kernel caching effective anymore 
statements apply smo mkl algorithm proposed bach 

case left wrapper algorithm able exploit properties particular problem sub kernels see section 

sparse feature maps parallel computations section discuss strategies accelerate svm training 
consider case explicit mapping kernel feature space known sparse 
case show mkl training svm training general drastically faster particular large 
second part discuss simple efficient way parallelize mkl svm training 
explicit computations sparse feature maps assume sub kernels kk sonnenburg tsch sch fer sch lkopf mappings explicitly 
suppose mapped examples sparse 
start giving examples kernels discuss kernels biological sequence analysis section 
section discuss strategies efficiently storing computing high dimensional sparse vectors particular kernels 
section discuss exploit properties accelerate chunking algorithms svm light factor chunking subproblem size 
string kernels spectrum kernel spectrum kernel leslie implements gram bagof words kernel joachims originally defined text classification context biological sequence analysis 
idea count mer contiguous string length contained sequences summing product counts possible mer note exponentially gives rise kernel value formally defined follows alphabet mer number occurrences spectrum kernel defined inner product note spectrum kernels extract positional information sequence goes mer length 
suited describing content sequence suitable instance analyzing signals motifs may appear certain order specific positions 
note spectrum kernels capable dealing sequences varying length 
spectrum kernel efficiently computed ino tries leslie denotes length sequence easier way compute kernel sequences separately extract sort mers sequence done preprocessing step 
note instance dna mers length efficiently represented bit integer value 
iterates mers sequences simultaneously counts mers appear sequences sums product counts 
computational complexity kernel computation iso log 
weighted degree kernel called weighted degree wd kernel tsch sonnenburg efficiently computes similarities sequences positional information account 
main idea wd kernel count exact occurrences mers corresponding positions sequences compared 
wd kernel order compares sequences xi length summing contributions mer matches lengths weighted coefficients xi xj uk xi uk 
uk string length starting position sequence indicator function evaluates argument true 
weighting coefficients tsch sonnenburg proposed matching substrings rewarded score depending length substring 

note case longer matches contribute strongly shorter ones due fact long match implies short matches adding value 
exploiting large scale mkl note wd kernel understood spectrum kernel mers starting different positions treated independently 
consider substrings length exactly shorter matches 
feature space position dimensions additionally duplicated times leading dimensions 
computational complexity wd kernel worst dl directly seen 
efficient storage sparse weights considered string kernels correspond feature space huge 
instance case wd kernel dna sequences length corresponding feature space dimensional 
dimensions feature space different mers appear sequences 
section briefly discuss methods efficiently deal sparse vectors assume elements vector indexed index setu sequences need operations clear add lookup 
operation sets vector zero add operation increases weight dimension element amount vu vu lookup requests value vu 
operations need performed quickly possible performance operation higher importance 
explicit map dimensionality feature space small consider keeping vector memory perform direct operations elements 
read write operation iso 
approach expensive memory requirements fast best suited instance spectrum kernel dna sequences protein sequences 
sorted arrays memory efficient computationally expensive sorted arrays index value pairs vu 
assuming indexes sorted advance efficiently change look single vu corresponding employing binary search procedure log 
look indexes may sort advance simultaneously traverse arrays order determine elements appear array operations omitting sorting second array log 
method suited cases comparable size instance computations single spectrum kernel elements proposed leslie 
binary search procedure preferred 
tries way organizing non zero elements tries fredkin idea tree siblings depth leaves store single value element vu mer path leaf corresponds knowledge allows ao reformulation kernel block weights done sonnenburg 


position dependent tolerate positional shift 
reason proposed tsch 
wd kernel shifts tolerates small number shifts lies wd spectrum kernel 

precisely logd small assume anyway computational effort exactly memory access 
sonnenburg tsch sch fer sch lkopf add lookup element needs operations reach leaf tree create necessary nodes way add operation 
note worst case computational complexity operations independent number mers elements stored tree 
tries faster sorted arrays need considerably storage pointers parent siblings useful previously discussed wd kernel 
lookup substring prefixes sorted arrays amounts separate lookup operations tries prefixes known bottom tree reached 
case trie store weights internal nodes 
illustrated wd kernel 
sequences aaa aga gaa weights added trie 
displays resulting weights nodes 
speeding svm training feasible standard optimization solving large scale svm training problem decomposition techniques practice 
chunking algorithms selecting variables working set current solution solve reduced problem respect working set variables 
steps repeated optimality conditions satisfied see joachims 
selecting working set checking termination criteria iteration vector gi jy jk xi xj usually needed 
computing scratch iteration kernel computations 
avoid recomputation typically starts computes updates working set gi old old jk xi xj large scale mkl result effort decreases qn kernel computations speed kernel caching joachims 
kernel caching efficient large scale problems time spend computing kernel rows updates working set note update computing kernel rows easily parallelized cf 
section 
exploiting xi xj xi iyi xi rewrite update rule gi old old xi old xi old normal update vector working set 
kernel feature map computed explicitly sparse discussed computing update accelerated 
needs compute store xq add operations performing scalar product xi xi lookup operations 
depending kernel way sparse vectors stored section sparseness feature vectors speedup quite drastic 
instance wd kernel kernel computation ld operations length sequence 
computing times requires operations 
tries needs ql add operations nl lookup operations 
qld basic operations needed total 
large leads speedup factor note kernel caching longer required small practice resulting trie leaves needs little storage 
pseudo code svm chunking algorithm algorithm 
algorithm outline chunking algorithm exploits fast computations linear combinations kernels tries 
initialization gi loop convergence check optimality conditions optimal select working set store old solve reduced problem update clear old update gi gi xi mkl case elaborated section algorithm mkl stores vectors gk kernel order avoid full recomputation kernel weight updated 
idea algorithm store normal vectors 
instance examples fit rows gb 
caching rows insufficient instance having thousands active variables 
tries sonnenburg tsch sch fer sch lkopf old update matrix gk old ww xi gi kgk computed 
simple parallel chunking algorithm time spent evaluating training examples speedups gained parallelizing evaluation 
algorithm constructs trie possible appropriate data structures performs parallel lookup operations cpus shared memory copies data structure separate computing nodes 
implemented algorithm multiple threads shared memory gain reasonable speedups see section 
note part computations ideal distribute cpus updated depending communication costs size transfered cpu computes large chunk ik xi ik transfered master node computes illustrated algorithm 
results discussion subsections apply multiple kernel learning knowledge discovery tasks demonstrating automated model selection interpret learned model section followed benchmark comparing running times svms mkl proposed algorithmic optimizations section 
mkl knowledge discovery section discuss toy examples binary classification regression showing mkl recover information problem hand followed brief review problems mkl successfully 
classification example deal binary classification problem 
task separate concentric classes shaped outline stars 
varying distance boundary stars control separability problem 
starting non separable scenario zero distance data quickly separable distance stars increases boundary needed separation gradually tend circle 
scatter plots data sets varied separation distances displayed 
generate training test sets wide range distances radius inner star fixed outer stars radius varied 
data set contains observations positive negative moderate noise level gaussian noise zero mean standard deviation 
mkl svm trained different values large scale mkl algorithm outline parallel chunking algorithm exploits fast computations linear combinations kernels 
master node initialization gi loop convergence check optimality conditions optimal select working set store old solve reduced problem update transfer slave nodes old fetch slave nodes update gi gi hi signal convergence slave nodes slave nodes loop convergence converged fetch master node old clear old node computes xi transfer master regularization parameter set mkl value averaged test errors setups choose value led smallest error 
choice kernel width gaussian rbf denoted rbf kernel classification expected depend separation distance learning problem increased distance stars correspond larger optimal kernel width 
effect visible results mkl mkl svms rbf kernels different widths 
show obtained kernel weightings kernels test error circled line quickly drops zero problem separable 
column shows mkl svm weighting 
courses kernel weightings reflect development learning problem long problem difficult best separation obtained kernel smallest width 
low width kernel looses importance distance stars increases larger kernel widths obtain larger weight mkl 
increasing distance stars kernels greater widths 
note rbf kernel largest width appropriate chosen 
illustrates mkl recover information structure learning problem 

note aware fact test error slightly underestimated 
kernel weight sonnenburg tsch sch fer sch lkopf width width width width width separation distance class toy problem dark gray green star shape distinguished light gray red star inside dark gray star 
distance dark star shape light star increases left right 
regression applied newly derived mkl support vector regression formulation task learning sine function rbf kernels different widths 
generated data sets increasing frequency sine wave 
sample size chosen 
analogous procedure described choose value minimizing test error 
sine waves depicted frequency increases left right 
frequency computed weights kernel width shown 
see mkl sv regression switches width rbf kernel fitting regression problem best 
regression experiment combined linear function sine waves lower frequency high frequency sin ax sin bx cx 
furthermore increase frequency higher frequency sine wave varied leaving unchanged 
mkl weighting show combination different kernels 
rbf kernels different width see trained mkl svr display learned weights column 
sample size value chosen previous experiment mkl 
largest selected width models linear component rbf kernels large widths effectively linear medium width corresponds lower frequency sine 
varied frequency high frequency sine wave low high left right 
observes mkl determines appropriate combination kernels low high widths decreasing rbf kernel width increased frequency 
kernel weight large scale mkl width width width width width frequency mkl support vector regression task learning sine wave please see text details 
additionally observe mkl leads sparse solutions kernel weights depicted blue zero 
real world applications bioinformatics mkl successfully real world data sets field computational biology lanckriet sonnenburg 
shown improve classification performance task ribosomal membrane protein prediction lanckriet weighting different kernels corresponding different feature set learned 
result included random channels obtained low kernel weights 
data sets small examples kernel matrices precomputed simultaneously kept memory possible sonnenburg 
splice site recognition task worm elegans considered 
data available abundance examples larger amounts needed obtain state art results sonnenburg 
data set able solve classification mkl examples kernels examples kernels optimizations weighted degree kernel 
result able learn weighting choosing heuristic able mkl tool interpreting svm classifier sonnenburg 
tsch 

example learned weighting wd kernel degree consist weighted sum sub kernels counting matching mers 
learned 
training time mkl svr setup examples minutes kernel caches size mb 

section human splice data set containing examples train wd kernel svm classifiers examples algorithm 
rbf kernel width sonnenburg tsch sch fer sch lkopf frequency mkl support vector regression linear combination functions sin ax sin bx cx 
mkl recovers original function combination functions low high complexity 
details see text 
kernel weight kernel index length substring learned wd kernel weighting examples 
weighting displayed shows peak mers mers 
noted obtained weighting experiment partially useful interpretation 
case splice site detection mers length playing important role 
important substrings length 
believe large weights longest mers artifact comes fact combining kernels kernel weight large scale mkl quite different properties th th kernel leads combined kernel matrix diagonally dominant sequences similar sequences believe reason having large weight 
example consider weight position 
case combined kernels similar expect interpretable results 
shows exon start position relative exon start shows importance weighting position dna sequence called splice site 
mkl determine weights corresponding sub kernel uses information position discriminate splice sites non splice sites 
different peaks correspond different biologically known signals see text details 
examples training sub kernels 
importance weighting position dna sequence called acceptor splice site start exon 
mkl examples compute weights corresponding sub kernel uses information position discriminate true splice sites fake ones 
repeated experiment bootstrap runs data set 
identify interesting regions match current biological knowledge splice site recognition region nucleotides nt nt corresponds donor splice site previous exon introns elegans short nt region nt nt coincides location branch point region closest splice site greatest weight nt nt weights ag dimer zero appears splice sites region nt nt 
slightly surprising high weights region suspect model triplet frequencies 

problem partially alleviated including identity matrix convex combination 
norm soft margin svms implemented adding constant diagonal kernel cortes vapnik leads additional norm penalization 
sonnenburg tsch sch fer sch lkopf decay weights seen nt nt explained fact exons long 
furthermore sequence ends case nt decay nt edge effect longer substrings matched 
benchmarking algorithms experimental setup demonstrate effect proposed algorithmic optimizations svm training algorithm mkl formulation extension single cpus applied algorithms human splice site data set comparing original wd formulation case weighting coefficients learned multiple kernel learning 
splice data set contains true acceptor splice site sequences leading total sequences base pairs length 
generated procedure similar sonnenburg 
elegans contained examples 
note data set unbalanced examples negatively labeled 
data set benchmark experiments trained mkl svms machine learning toolbox contains modified version svm light joachims randomly sub sampled examples measured time needed svm training 
classification performance evaluation remaining examples test data set 
set degree parameter wd kernel spectrum kernel fixing svms regularization parameter 
mkl case sub kernels 
svm light subproblem size parameter convergence criterion parameter epsilon mkl convergence criterion set sv mkl respectively 
kernel cache gb kernels precomputed kernel algorithms smo extension kernel cache disabled 
measure changing quadratic subproblem size influences svm training time 
experiments performed pc powered ghz amd opteron tm processors running linux 
measured training time algorithms single quad cpu version data set sizes 
benchmarking svm obtained training times different svm algorithms displayed table 
svms trained standard svm light weighted degree kernel precomputed standard wd kernel wd precomputed standard spectrum kernel spec 
svms utilizing extension trained wd spectrum kernel 
svms trained cpus parallel version algorithm 
wd wd demonstrate effect simple parallelization strategy computation kernel rows updates working set parallelized works kernel 
training times obtained precomputing kernel matrix includes time needed precompute full kernel matrix lower examples 

splice data set downloaded fromhttp www fml tuebingen mpg de projects 

precisely ando block formulation wd kernel proposed sonnenburg 

large scale mkl note direct cause relatively large subproblem size 
picture different say data shown training time cases larger times obtained original wd kernel demonstrating effectiveness svm light kernel cache 
overhead constructing trie examples visible starting examples optimization efficient original wd kernel algorithm kernel cache hold kernel elements anymore 
appropriate lower chunking size seen table 
formulation outperforms original wd kernel factor examples 
picture similar spectrum kernel speedups factor examples reached stems fact explicit maps tries wd kernel case discussed section leading lookup cost dramatically reduced map construction time 
reason parallelization effort benefits wd kernel spectrum kernel examples parallelization cpus cpus leads speedup factor wd kernel spectrum kernel 
parallelization help kernel computation slow 
training original wd kernel sample size takes hours version requires hours cpu parallel implementation hours conjunction optimization single hour minutes needed 
training examples takes days 
note data set gb size 
classification performance table show classification performance terms classification accuracy area receiver operator characteristic roc curve metz fawcett area precision recall curve prc see davis svms human splice data set different data set sizes wd kernel 
recall definition roc prc curves sensitivity recall defined fraction correctly classified positive examples total number positive examples equals true positive rate pr fn 
analogously fraction fpr fp fp negative examples wrongly classified positive called false positive rate 
plotting fpr tpr results receiver operator characteristic curve roc metz fawcett 
plotting true positive rate positive predictive value precision fp fraction correct positive predictions positively predicted examples obtains precision recall curve prc see davis 
note unbalanced data set accuracy area roc curve meaningless measures independent class ratios 
sensible steadily increases training examples learning 
train available data obtain state art results 
varying svm light parameter discussed section algorithm algorithm computing output training examples working set speed factor size quadratic subproblems svm light 
trade choosing solving larger quadratic subproblems expensive quadratic cubic effort 
table shows dependence computing time example gain speed choosing examples 
sticking mid range idea task 

single precision byte floating point numbers caching kernel elements possible training examples 
svm training time seconds logarithmic svm training time seconds logarithmic sonnenburg tsch sch fer sch lkopf wd precompute wd cpu wd cpu wd cpu wd cpu wd cpu wd cpu number training examples logarithmic spec precompute spec orig spec cpu spec cpu spec cpu number training examples logarithmic comparison running time different svm training algorithms weighted degree kernel 
note log log plot small appearing distances large larger slope corresponds different exponent 
upper weighted degree kernel training times measured lower displays spectrum kernel training times 
large variance observed svm training time depends large extend variables selected optimization step 
example related elegans splice data set optimal large sample sizes midrange lead best large scale mkl wd wd wd spec table top speed comparison original single cpu weighted degree kernel algorithm wd svm light training compared wd wd cpus parallelized version precomputed version pre extension conjunction original wd kernel cpus 
bottom speed comparison spectrum kernel spec lin spec processors 
denotes precomputed version 
column shows sample size data set svm training columns display time measured seconds needed training phase 
performance 
observes trend larger training set sizes slightly larger subproblems sizes decrease svm training time 
classification performance percent sonnenburg tsch sch fer sch lkopf accuracy area roc area prc number training examples comparison classification performance weighted degree kernel svm classifier different training set sizes 
area receiver operator characteristic roc curve area precision recall curve prc classification accuracy displayed percent 
note unbalanced data set accuracy area roc curve meaningful area prc 
benchmarking mkl wd kernel degree consist weighted sum sub kernels counting matching 
mkl learned weighting splice site recognition task examples displayed discussed section 
focusing speed comparison show obtained training times different mkl algorithms applied learning weightings wd kernel splice site classification task 
mkl svms trained precomputed kernel matrices kernel matrices computed fly employing kernel caching mkl mkl extension parallel implementation cpus 
results displayed table 
precomputing kernel matrices beneficial applied large scale cases examples due kn memory constraints storing kernel matrices 
fly computation kernel matrices computationally extremely demanding kernel caching possible examples hours 
note wd kernel specific optimizations involved expects similar result arbitrary kernels 

algorithm 

algorithm extensions including parallelization algorithm 

kernels examples requires gb examples gb required single precision floats 

kernel cache gb 
large scale mkl accuracy table comparison classification performance weighted degree kernel svm classifier different training set sizes 
area roc curve area precision recall curve classification accuracy accuracy displayed percent 
larger values better 
optimal classifier achieve note unbalanced data set accuracy area roc curve meaningless 
comparison classification performance achieved th order markov chain examples order chosen model selection order pseudo counts tried displayed row marked 
variants outperform algorithms far speedup factor examples applicable data sets size 
note parallelization mkl examples take week compared days quad cpu cpu version 
parallel versions outperform single processor version start achieving speedup examples quickly reaching plateau speedup factor level examples approaching speedup factor examples efficiency 
note performance gain cpus relatively small solving qp constructing tree parallelized 

part proposed simple efficient algorithm solve multiple kernel learning problem large class loss functions 
proposed method able exploit existing single kernel algorithms extending applicability 
experiments illustrated mkl classification regression useful automatic model selection obtaining comprehensible information learning problem hand 
interest develop evaluate mkl algorithms unsupervised learning kernel pca sonnenburg tsch sch fer sch lkopf table influence training time varying size quadratic program svm light formulation wd kernel 
training times vary dramatically observes tendency larger sample size larger optimal 
column displays result column table 
mkl training time seconds logarithmic mkl wd precompute mkl wd cache mkl wd cpu mkl wd cpu mkl wd cpu number training examples logarithmic comparison running time different mkl algorithms weighted degree kernel 
note log log plot small appearing distances large larger slope corresponds different exponent 
class classification try different losses kernel weighting 
second part proposed performance enhancements large scale mkl practical wrapper chunking special case kernels written inner product sparse feature vectors string kernels algorithm speeds large scale mkl mkl table speed comparison determining wd kernel weight multiple kernel learning chunking algorithm mkl mkl conjunction parallelized algorithm processors 
column shows sample size data set svm training columns display time measured seconds needed training phase 
standalone svm training 
standalone svm spectrum kernel achieved speedups factor weighted degree kernel 
mkl gained speedup factor 
proposed parallel version algorithm running cpu multiprocessor system lead additional speedups factor mkl vanilla svm training 
acknowledgments authors gratefully acknowledge partial support pascal network excellence eu dfg ja mu 
guido olivier chapelle cheng soon ong qui candela sebastian mika jason weston manfred warmuth 
ller great discussions 
appendix derivation mkl dual generic loss functions start mkl primal problem equation min wk xi yi wk dk xi xi wk sonnenburg tsch sch fer sch lkopf introducing allows move wk constraints leads equivalent problem min xi yi wk dk xi wk xi wk tk equivalently transformed min xi yi tk wk dk xi wk tk xi wk tk recall second order cone dimensionality defined 
reformulate original mkl primal problem equation equivalent second order cone program norm constraint wk implicitly taken care conic primal min xi yi tk wk tk dk xi tk xi wk going derive conic dual recipe boyd vandenberghe see 
derive conic lagrangian infimum primal variables order obtain conic dual 
introduce lagrange multipliers living self dual conic lagrangian large scale mkl xi yi xi wk tk xi wk ktk 
obtain dual derivatives lagrangian primal variables vanish leads constraints wkl bl xi ul xi xi xi yi xi yi 
equation derivative loss function inverse exist required strictly convex differentiable 
plug obtained primal variables vanish 
dual function yi yi xi wk yi yi il yi xi wk il yi 
constraints remain due bias second order cone constraints xi leads max yi yi xi il yi sonnenburg tsch sch fer sch lkopf squaring constraint multiplying relabeling dropping constraint fulfilled implicitly obtain mkl dual arbitrary strictly convex loss functions 
conic dual min yi yi il yi xi adding second term objective constraint relabeling leads reformulated dual equation starting point derive formulation analogy classification case 
appendix loss functions quadratic loss quadratic loss case obtain derivative inverse derivative 
recall definition sk plugging leads logistic loss sk yi yi yi yi iyi il yi yi xi xi xi similar hinge loss derivation logistic loss log xy completeness 
ye xy ye xy xy xy inverse function log obtain sk smooth hinge loss log yi large scale mkl yi log yi xi hinge loss log xy fixed obtains derivative xy xy xy xy note fixed bounded abs abs cy sign sign cy 
inverse function derived cy define ze xy xy cy xy xy cy xy log cy xy log cy log cy log cy xi yi log ingredients follows sk sk yi log log log yi yi yi log yi log 
bach lanckriet jordan 
multiple kernel learning conic duality smo algorithm 
brodley editor international conference machine learning 
acm 
sonnenburg tsch sch fer sch lkopf bennett embrechts 
mark boosting algorithm heterogeneous kernel models 
proceedings eighth acm sigkdd international conference knowledge discovery data mining pages 
acm 
bi zhang bennett 
column generation boosting methods mixture kernels 
kim kohavi gehrke dumouchel editors proceedings tenth acm sigkdd international conference knowledge discovery data mining pages 
acm 
boyd vandenberghe 
convex optimization 
cambridge university press cambridge uk 
chapelle vapnik bousquet mukherjee 
choosing multiple parameters support vector machines 
machine learning 
cortes vapnik 
support vector networks 
machine learning 
davis 
relationship precision recall roc curves 
technical report university wisconsin madison january 
fawcett 
roc graphs notes practical considerations data mining researchers 
technical report hpl hp laboratories palo alto ca usa january 
fredkin 
trie memory 
communications acm 

adaptive scaling feature selection svms 
thrun becker obermayer editors advances neural information processing systems pages cambridge ma 
mit press 

semi infinite programming theory methods applications 
siam review 
joachims 
text categorization support vector machines learning relevant features 
dellec rouveirol editors ecml proceedings th european conference machine learning lecture notes computer science pages berlin heidelberg 
springer verlag 
joachims 
making large scale svm learning practical 
sch lkopf burges smola editors advances kernel methods support vector learning pages cambridge ma usa 
mit press 
lanckriet de bie cristianini jordan noble 
statistical framework genomic data fusion 
bioinformatics 
leslie eskin noble 
spectrum kernel string kernel svm protein classification 
altman hunter lauderdale klein editors proceedings pacific symposium biocomputing pages hawaii 
leslie eskin 
inexact matching string kernels protein classification 
kernel methods computational biology mit press series computational molecular biology pages 
mit press 
large scale mkl meir tsch 
boosting leveraging 
mendelson smola editors proc 
machine learning summer school canberra lncs pages 
springer 
metz 
basic principles roc analysis 
seminars nuclear medicine viii october 
ong smola williamson 

thrun becker obermayer editors advances neural information processing systems volume pages cambridge ma 
mit press 
platt 
fast training support vector machines sequential minimal optimization 
sch lkopf burges smola editors advances kernel methods support vector learning pages cambridge ma usa 
mit press 
tsch 
robust boosting convex optimization 
phd thesis university potsdam potsdam germany 
tsch sonnenburg 
accurate splice site prediction elegans pages 
mit press series computational molecular biology 
mit press 
tsch warmuth 
efficient margin maximization boosting 
journal machine learning research 
tsch demiriz bennett 
sparse regression ensembles infinite finite hypothesis spaces 
machine learning 
special issue new methods model selection model combination 
neurocolt technical report nc tr 
tsch sonnenburg sch lkopf 
recognition alternatively spliced exons elegans 
bioinformatics 
sch lkopf smola 
learning kernels 
mit press cambridge ma 
sonnenburg tsch sch fer 
learning interpretable svms biological sequence classification 
miyano mesirov kasif pevzner waterman editors research computational molecular biology th annual international conference recomb volume pages 
springer verlag berlin heidelberg 
sonnenburg tsch sch lkopf 
large scale genomic sequence svm classifiers 
raedt wrobel editors icml proceedings nd international conference machine learning pages new york ny usa 
acm press 
warmuth liao tsch 
totally corrective boosting algorithms maximize margin 
icml proceedings nd international conference machine learning 
acm press 

