learning dissimilarity functions wang cis edu cn cheng yang cis edu cn feng cis edu cn state key laboratory machine perception school electronics engineering computer science university beijing china study problem learning classification task dissimilarity function objects accessible 
data represented feature vectors terms pairwise dissimilarities 
investigate sufficient conditions dissimilarity functions allow building accurate classifiers 
results advantages apply unbounded dissimilarities invariant order preserving transformations 
theory immediately suggests learning paradigm construct ensemble decision stumps depends pair examples find convex combination achieve large margin 
develop practical algorithm called dissimilarity boosting learning dissimilarity functions theoretical guidance 
experimental results demonstrate compares favorably existing approaches variety databases different conditions 

classification problems objects represented feature vectors euclidean space 
euclidean feature space provides analytical tools classification representations 
representation requires selection features usually difficult domain dependent 
example area fingerprint analysis took scientists years discover useful features fingerprint recognition 
clear today kind features discrimination ability human face recognition existing feature extraction appearing proceedings th international conference machine learning corvallis 
copyright author owner 
algorithms reliable accurate zhao 
alternative way describe patterns dissimilarity functions 
applications image retrieval representation advantage convenient define dissimilarity measure set meaningful features jacobs jain zongker 
addition dissimilarity functions defined structured objects 
procedure provides bridge classical structural approach pattern classification graepel goldfarb 
simplest method classify objects dissimilarity representations nearest neighbor nn rule 
despite theoretical result linking asymptotic error rate bayes optimal risk nn suffers number drawbacks high computational complexity sensitive choice distance measure noisy data breiman 
due fact nn relies entirely local topology 
dissimilarity measure perfect noisy examples performance nn degrades significantly 
contrast nn authors proposed global classification algorithms 
underlying idea global decision rules sensitive choice distance measure noise 
type methods embeds data possibly pseudo euclidean space applies traditional euclidean classification algorithms modifications adapted pseudo euclidean necessary graepel type methods explicitly constructs feature representations objects dis similarities set prototypes runs standard linear separator algorithms svm new space duin algorithms demonstrate superior performance nn number datasets 
blum developed theory learning similarity functions 
defined tion means pairwise function similarity function give sufficient conditions normalized similarity function allow learn 
theory immediately suggests algorithms second type described provides theoretical explanation empirical performances 
study theoretical practical issues learning dissimilarity functions 
extend blum theory normalized similarity functions unbounded dissimilarity functions 
give sufficient conditions dissimilarity functions allow learn 
advantage result notions dissimilarity functions invariant order preserving transformations 
interestingly theory suggests learning paradigm different aforementioned algorithms construct ensemble decision stumps special forms find convex combination achieve large margin 
develop practical algorithms theoretical guidance 
particular boosting adopted due ability obtaining large margin distribution 
organized follows describe theory section 
section practical algorithm called proposed learning dissimilarity functions consequence theory 
provide experimental evidence benefits algorithm section conclude section 
theory 
notations dissimilarity mean nonnegative parameter function instance space 
axioms metric reflectivity symmetry triangle inequality necessary dissimilarity function 
labeled examples represented examples drawn randomly independently conditionally independently underlying distribution problem clear context 
denotes indicator function sgn 
worth point focus dissimilarity functions extension similarity functions trivial 
denote similarity function 
just replacing bys definitions theorems obtains theory similarity functions 
theory unified framework learning similarity dissimilarity functions 
learning dissimilarity functions 
sufficient conditions learning dissimilarity functions propose section number sufficient conditions dissimilarity function useful learning main results established definition theorem 
give notion dissimilarity function quite intuitive 
definition expresses examples close random examples class opposite class dissimilarity function 
definition dissimilarity function said strongly learning problem probability mass examples satisfy 
probability random examples 
advantage definition strongly goodness dissimilarity function invariant transformations 
formally proposition suppose dissimilarity functions strongly 
arbitrary iff strongly 
par ticular strictly increasing function strongly proposition immediate definition 
notion strongly dissimilarity function suggests simple learning algorithm draw number pairs examples different labels voting class test example close 
summarized theorem 
theorem strongly dissimilarity function probability choice ln pairs examples labels classifier error rate sgn sgn proof proof uses technique blum 
letm set examples satisfying 
note sgn probability random examples inequality equivalent sgn 
chernoff bound implies sgn learning dissimilarity functions 
inequality holds take expectation results expected error markov inequality obtain probability error rate set larger arbitrary 
setting adding probability examples completes proof 
definition suggested algorithm natural notion strongly goodness restrictive 
similar argument blum show examples strong learnable satisfy definition 
imposing appropriate weighting functions instances inequality valid respective new distribution allows learn way described 
definition core 
definition merely assume existence weighting functions necessarily known priori 
clear assumption sufficient learn accurate classifier 
captures broad class dissimilarity functions 
definition denote conditional pdf learning problem 
dissimilarity function said learning problem 
exist conditional pdf 
probability mass examples satisfy 
probability respect dx dx easily seen goodness invariant order preserving transformations dissimilarity functions 
theorem says goodness guarantees existence low error large margin classifier convex combination number base classifiers 
theorem dissimilarity function probability choice ln pairs examples labels exists convex combination classifier base classifiers hi ihi hi sgn error rate combined classifier margin proof denote 
set examples satisfying 
fixed dx dx wy sgn dx dx wy sgn equivalent wy sgn 
note inequality hoeffding inequality implies sgn sgn obtain sgn take expectation markov inequality proof theorem complete proof 
theorem suggests dissimilarity function order learn needs draw number pairs examples build ensemble decision stumps independent set examples train large margin convex combination base classifiers 
note boosting especially suitable computing large margin composite classifier schapire singer 
section adopt boosting develop practical algorithms learning dissimilarities 

discussion sufficient conditions require definitions dissimilarity functions examples probability mass close random example class example opposite class 
natural weaker notion definition dissimilarity function said pseudo learning problem 
probability taken random examples 
expect pseudo goodness imply learnable possibly weak sense 
show majority voting scheme previous theorems general guarantee learnability weakest sense 
result means definitions dissimilarity section weakened 
simplicity assume proposition class probability equal 
proposition exists learning problem pseudo dissimilarity function problem high probability close choice pairs examples labels error rate voting classifier higher sgn proof sketch fixed example learning dissimilarity functions law large numbers gives sgn converges probability sgn note sgn 
shown sufficiently large probability close error occurs yf denote 
easy construct distribution inequalities hold simultaneously 
inequality equivalent meaning dissimilarity function pseudo problem 
second inequality implies error rate voting classifier larger 

algorithm section propose practical algorithm learning dissimilarity functions previous theoretical guidance 
algorithm essentially dissimilarity boosting referred 
main theoretical result theorem suggests randomly draw large number pairs examples construct base classifiers form hi boosting adopted train independent set examples convex combination hi composite classifier ihi large margin 
practice limited examples data effectively 
algorithm data training set consider pairs examples different labels candidates build stump base classifiers 
practical learning dissimilarity functions table 
description datasets uci repository 
data set classes examples data set classes examples balance letter breast liver cleveland monk diabetes monk echo monk german satimage hayes vehicle hepatitis vowel image ionosphere wine iris issue base classifier weak strengthen discrimination ability 
modifications described detail follows 
consider base classifier 
decision stump partitions instance space subspaces defined instance space euclidean space decision surface decision stump hyperplane orthogonal line joining bisects line 
suggest base classifier decision stump partitions threshold introduced free parameter increase discrimination ability base learner 
euclidean space decision surface decision stump hyperplane orthogonal line passing biased straightforward generalization 
important issue design algorithm train decision stump boosting framework 
traditional feature representation boosting decision stump weak learners extensively studied 
stump trained exhaustively searching candidates selecting minimum loss respect distribution current round algorithm called freund schapire 
words attributes possible thresholds considered 
method dissimilarity setting modifications 
implementation needs time number training examples 
intractable small size tasks 
reduce computational cost maintain information possible suggest strategy 
strategy round adaboost randomly select pairs training examples different labels 
selection example current distribution 
examples harder classify higher probability selected 
increases algorithm concentrates boundary data difficult recognize 
base classifier round obtained selecting best decision stumps determined pairs training examples 
search consumes time 
consider problem storage cost 
easy see storage mainly determined number distinct examples selected training algorithm 
note computational time recognizing new example depends number distinct examples need compute dissimilarities new instance examples 
boosting runs large number rounds number may large 
control suggest alternative strategy training base classifier 
strategy ii construct prototype set initially empty 
round boosting count number elements predefined number train base classifier strategy put prototypes selected base learner round contained elements generate pairs training examples different labels example chosen randomly current weights 
base classifier obtained choosing best decision stump determined pairs examples 
learning dissimilarity functions table 
comparison performances algorithms uci datasets 
data set nn balance breast cleveland diabetes echo german hayes hepatitis image ionosphere iris letter liver monk monk monk satimage vehicle vowel wine average wins strategy algorithm selects prototypes 
implementation base classifiers constructed combined real version adaboost algorithm schapire singer decision stumps output unbounded real values 
theory binary classification algorithm applies multiclass problems simply multiclass boosting algorithm adaboost mh booster 

experiments section evaluate algorithm compare existing approaches learning dissimilarity functions 
experiments strategy train base classifiers 
space provide detailed data strategy ii usually results minor deficit accuracy storage recognition time saved 
number pairs examples selected round set 
approaches compare nearest neighbor nn rule method duin linear svm algorithm blum 
give brief description follows 
duin method represents object vector dissimilarities set prototypes pm viewed mapping instance dimensional space 
pm regularized linear quadratic normal density classifier ripley employed classify data 
blum algorithm suggested theory 
mapping described normalized similarity functions linear svm adopted find large margin separator 
experiment normalized similarity function obtained transforming dissimilarity function exp 
libsvm chang lin svm implementation 
parameters number prototypes selected tuned training set 
set experiments conducted benchmark datasets uci repository 
dataset cross validation fashion 
datasets described table 
datasets objects represented feature vectors feed algorithms euclidean distances data 
learning dissimilarity functions 
comparison algorithms usps dataset dissimilarity functions 

comparison algorithms qualitative dissimilarity functions usps database 
goal experiment compare algorithms variety domains 
results shown table 
outperforms approaches terms average error rate databases number datasets algorithm achieves best performance 
experiment focus image classification 
mentioned earlier convenient directly define dissimilarities images construct meaningful features 
dissimilarity measures images proposed literature 
adopt experiment measures euclidean distance fuzzy image metric li tangent distance simard 
aim experiment compare algorithms different dissimilarities 
perform experiment usps database consists images handwritten digits 
dataset partitioned fixed training set testing set consisting examples respectively 
results depicted fig 
best performance euclidean distance fuzzy image metric 
tangent distance 
comparison algorithms usps database noise 
incorporates strong domain specific knowledge believed perfect distance task nn outperforms algorithms 
consider situation dissimilarity far accurate 
instance people subjective evaluation similarity images qualitative discrete values 
example similar average dissimilar possible values level qualitative dissimilarity 
compare performance algorithm qualitative measures 
conduct experiments euclidean distances quantized levels respectively 
results usps datasets shown fig 
outperforms algorithms consistently qualitative levels 
observe insensitive choice dissimilarity measures 
level measures information local topology lost low error rate 
final experiment evaluate performance algorithm noisy data 
add usps images gaussian white noise different energies feed algorithms euclidean distances 
results depicted fig 
relatively robust noise 

discussion contribution sufficient conditions dissimilarity functions allow learn 
definition dissimilarities applies unbounded functions invariant order preserving transformations 
theory immediately suggests simple algorithm boosting construct combine large number base classifiers depends pair examples 
develop practical algorithm generalizing base learner suggesting strategies training base classifiers computation tractable 
approach compares favorably existing algorithms variety databases 
stumps base classifiers algorithm decision trees splitting node 
incidentally splitting approach decision trees previously suggested hinton revow expected decision tree base classifiers bring additional benefits 
acknowledgments supported cb program new century excellent university 
authors xu useful comments 
blum 

theory learning similarity functions 
international conference machine learning 
blum vempala 

kernels margins low dimensional mappings 
international workshop algorithmic learning theory 
blum vempala 

kernels features kernels margins low dimensional mappings 
machine learning 
breiman friedman olshen stone 

classification regression trees 
wadsworth 
chang lin 

library support vector machines 
available www csie ntu edu tw cjlin libsvm 
freund schapire 

experiments learning dissimilarity functions new boosting algorithm 
international conference machine learning 
goldfarb 

new approach pattern recognition 
rosenfeld ed progress pattern recognition 
graepel herbrich obermayer 

classification pairwise proximity data 
advances neural information processing systems 
hinton revow 

pairs datapoints define splits decision trees 
advances neural information processing systems 
jacobs weinshall gdalyahu 

classification nonmetric distances image retrieval class representation 
ieee transactions pattern analysis machine intelligence 
jain zongker 

representation recognition handwritten digits deformable templates 
ieee transactions pattern analysis machine intelligence 
li chen chi 

fuzzy image metric application fractal coding 
ieee transactions image processing 
maio jain prabhakar 

handbook fingerprint recognition 
new york springer 
duin 

dissimilarity representations allow building classifiers 
pattern recognition letters 
duin 

dissimilarity representation pattern recognition 
world scientific 
duin 

generalized kernel approach dissimilarity classification 
journal machine learning research 
ripley 

pattern recognition neural networks 
cambridge cambridge university press 
schapire singer 

improved boosting algorithms confidence rated predictions 
machine learning 
simard cun denker 

efficient pattern recognition new transformation distance 
advances neural information processing systems 
zhao chellappa phillips rosenfeld 

face recognition literature survey 
acm computing surveys 
