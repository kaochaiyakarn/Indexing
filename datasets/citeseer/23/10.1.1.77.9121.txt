journal machine learning research submitted revised published framework learning predictive structures multiple tasks unlabeled data rie ando rie ibm com ibm watson research center yorktown heights ny tong zhang yahoo com yahoo research new york ny editor peter bartlett important issues machine learning improve performance supervised learning algorithm including unlabeled data 
methods labeled unlabeled data generally referred semi supervised learning 
number methods proposed current stage don complete understanding effectiveness 
investigates closely related problem leads novel approach semi supervised learning 
specifically consider learning predictive structures hypothesis spaces kind classifiers predictive power multiple learning tasks 
general framework structural learning problem formulated analyzed theoretically relate learning unlabeled data 
framework algorithms structural learning proposed computational issues investigated 
experiments demonstrate effectiveness proposed algorithms semi supervised learning setting 

machine learning applications find large amount unlabeled data difficulty labeled data costly obtain 
natural question unlabeled data build accurate classifier amount labeled data 
problem referred semi supervised learning 
general semi supervised learning algorithms labeled unlabeled data train classifier 
number methods proposed effectiveness clear 
example vapnik introduced notion transductive inference vapnik may regarded approach semi supervised learning 
success reported see joachims criticism pointing method may behave circumstances zhang oles 
popular semi supervised learning method training blum mitchell related bootstrap method nlp applications yarowsky em nigam 
basic idea label part unlabeled data high precision classifier put automatically rie ando tong zhang 
ando zhang labeled data training data 
pointed pierce cardie method may degrade classification performance assumptions method satisfied noise introduced labels non perfect classification 
phenomenon observed experiments reported section 
approach semi supervised learning different philosophy 
basic idea define functional structures unlabeled data 
bootstrap labels label noise potentially corrupt learning procedure 
example approach unlabeled data create data manifold graph structure proper smooth function classes defined szummer jaakkola zhou zhu 
smooth functions characterize underlying classifier able improve classification performance 
worth pointing smooth function classes graph structures necessarily predictive power 
general approach underlying principle directly learn underlying smooth function class classifiers 
learning procedure takes advantage unlabeled data obtain semi supervised learning method specifically aimed finding structures predictive power 
motivates general framework going develop 
want learn underlying predictive functional structures smooth function classes characterize predictors 
call problem structural learning 
key idea learn structures considering multiple prediction problems simultaneously 
intuitive level observe multiple predictors different problems sample underlying predictor space analyzed find common structures shared predictors 
important predictive structures predictor space discovered information improve individual prediction problem 
main focus formalize intuitive idea analyze properties structural learning rigorously 
idea benefit considering multiple problems appeared statistical literature 
particular bayesian hierarchical modeling motivated principle 
framework developed frequentist setting relevant statistical studies shrinkage methods multiple output linear models see section hastie 
particular algorithm proposed section form similar shrinkage method proposed breiman friedman 
framework specific algorithm section general earlier statistical studies 
machine learning literature related referred multi task learning example see baxter ben david caruana pontil micchelli 
shall call procedure structural learning accurate description method semi supervised learning setting 
transfer predictive structure learned multiple tasks unlabeled data target supervised problem 
literature idea referred inductive transfer 
success approach depends learned structure helpful target supervised problem 
learning predictive structures follows motivated semi supervised learning general structural learning multi task learning problem considered independent interest 
semi supervised learning shall show multiple prediction problems needed structural learning generated unlabeled data 
basic framework applied applications multiple prediction problems necessarily derived unlabeled data earlier statistical machine learning studies 
part focuses development general structural learning paradigm algorithm 
main implication reliably learn underlying structure shared multiple prediction problems 
second part shall demonstrate apply idea learning structure semi supervised learning demonstrate effectiveness proposed method context 
short version mainly reporting empirical results appeared acl ando zhang 
version includes complete derivation proposed algorithm theoretical analysis additional experimental results 
section formally introduce structural learning problem framework standard machine learning 
propose specific algorithm finds common low dimensional feature space shared multi problems 
algorithm studied section theoretical analysis appendix section shows apply structural learning context semi supervised learning 
basic idea unlabeled data generate auxiliary prediction problems useful discovering important predictive structures 
structures estimated algorithm developed section 
give intuitive justifications structure shared artificially created auxiliary problems helpful supervised problem 
experiments provided section illustrate effectiveness algorithm proposed section semi supervised tasks 
section presents high level summary main ideas developed 

structural learning problem section introduces problem learning predictive functional structures 
related ideas explored earlier statistical machine learning studies completeness shall include self contained description 
framework considered basis algorithm section 
supervised learning standard formulation supervised learning seek predictor maps input vector corresponding output usually selects predictor set functions finite set training examples xi yi independently generated unknown probability distribution set called hypothesis space consists functions predict output input datum goal find predictor error respect small possible 
assume quality ando zhang predictor measured expected loss respect ex 
set training data frequently method finding predictor minimize empirical error training data called empirical risk minimization erm arg min xi yi 
known fixed sample size smaller hypothesis space easier learn best predictor error caused learning best predictor finite sample called estimation error 
smaller hypothesis space accurate best predictor 
error caused restricted referred approximation error 
supervised learning needs select size balance trade approximation error estimation error 
typically done model selection learn set predictors set candidate hypothesis spaces pick best choice validation set 
learning hypothesis spaces practice hypothesis space small approximation error small estimation error 
problem choosing hypothesis space central performance learning algorithm requires specific domain knowledge assumptions world 
assume set candidate hypothesis spaces 
observes single prediction problem underlying domain standard approach hypothesis space selection model selection cross validation 
observes multiple prediction problems underlying domain possible better estimate underlying hypothesis space considering problems simultaneously 
describe simple model structural learning foundation 
similar point view baxter 
consider learning problems indexed samples indexed independently drawn distribution 
problem assume set candidate hypothesis spaces indexed common structural parameter shared problems 
th problem interested finding predictor minimizes expected loss 
notational simplicity assume problems loss function requirement essential analysis 
fixed structural parameter predictor problem estimated empirical risk minimization erm hypothesis space arg min 
purpose structural learning find optimal structural parameter expected risks predictors respect corresponding distribution averaged minimized 
learning predictive structures cross validation structural parameter selection immediately notice stable estimate optimal obtained considering multiple learning tasks 
particular problem validation set structural learning total number validation data 
effectively data purpose selecting optimal shared hypothesis space structure 
implies sample sizes small individual problems long large able find optimal accurately 
pac style analysis provided appendix state similar result cross validation 
general expect hypothesis space determines functional structure learned predictor 
parameter continuous parameter encodes assumption predictor 
large parameter space explore possible functional structures 
argument rigorous results appendix implies possible discover optimal shared structure number problems large 
structures input space purpose section provide intuitive discussion principle exist functional structures hypothesis spaces shared multiple tasks 
conceptually may consider simple case different problems share exactly underlying hypothesis space 
arbitrary input space known structure argue possible learn predictor looks multiple prediction problems 
key reason practice predictors equally equally observed 
real world applications usually observes smooth predictors smoothness respect certain intrinsic underlying distance input space 
general points close intrinsic distance values predictor produces points similar 
particular completely random predictors bad predictors rarely observed practical applications 
machine learning smoothness condition enforced hypothesis space select 
example kernel methods constrain smoothness function certain reproducing kernel hilbert space rkhs norm 
functions rkhs closeness points certain metric implies closeness predictive values 
may consider complicated smoothness conditions explore observed data manifold graph semi supervised learning methods mentioned 
smoothness condition useful correlates predictive ability 
general distance measure induces hypothesis space enforces smoothness respect underlying distance 
reality clear best distance measure underlying space 
example natural language processing space consists discrete points words appropriate distance easily defined 
continuous vector valued input points difficult justify euclidean distance better 
ando zhang distance function selected clear define appropriate smoothness conditions respect distance 
observe multiple tasks important common structures discovered simply analyzing multiple predictors learned data 
tasks similar actual learning task interested benefit significantly discovered structures 
tasks directly related discovered structures useful 
general predictors tend share similar smoothness conditions respect certain distance intrinsic underlying input space 
example illustrate main argument graphically consider discrete input space points 
assume obtain estimates functions different prediction problems plot obtained function values input points 
example notice function values points similar function values points similar 
observing estimated functions may conclude intrinsic distance metric points close points close 
function smooth respect intrinsic distance 
come back argument section text data concrete example discuss semi supervised learning section 
illustration discovering functional structure multiple prediction tasks learning predictive structures form structural learning may pose structural learning slightly form useful don empirical risk minimization learner 
assume problem learning algorithm takes set training samples structural parameter produce predictor 
note algorithm estimates predictor hypothesis space empirical risk minimization 
assume procedure estimates performance learned predictor possibly additional information example validation set 
structural learning find regularized estimator arg min regularization parameter encodes belief value preferred 
number problems behaves sample size standard learning 
fundamental estimation method structural learning 
obtain estimate structural parameter learning algorithm obtain predictor 
example assume estimate accuracy validation set may simply weighting parameters 
possible estimate accuracy learned predictor training set standard learning theory empirical risk minimization 
approach employed section leads practical algorithms formulated optimization problems 

algorithms section develop specific learning algorithm standard machine learning framework 
basis learner joint empirical risk minimization analyzed appendix consider linear prediction models shown effective practical applications 
methods include state art machine learning algorithms kernel machines boosting 
joint empirical risk minimization framework outlined section interested finding hypothesis space estimator form 
pointed section conceptually achieved validation set 
approach lead quite difficult computational procedure optimize empirical risk training data possible value choose optimal validation set 
complicated structures continuous parameter model consider section approach feasible 
natural method perform joint optimization training set respect predictors structural parameter 
ando zhang consider model equation pose joint optimization problem problems shared structural parameter arg min 
shared structural parameter depends problems reliably estimated joint minimization 
completeness include theoretical analysis appendix structural learning linear predictors order derive practical algorithm shall consider specific joint model solved numerically 
specifically employ linear prediction models multiple tasks assume underlying structure shared low dimensional subspace 
necessarily general model leads simple intuitive computational procedure 
shall see quite effective semi supervised learning problems interested 
input space linear predictor necessarily linear original space regarded linear functional high dimensional feature space assume known feature map linear predictor determined weight vector 
order apply structural learning framework consider parameterized family feature maps 
setting goal structural learning may regarded learning feature map 
specific formulation consider assume feature map contains components component known high dimensional feature map component parameterized low dimensional feature map 
linear predictor form weight vectors specific prediction problem common structure parameter shared problems 
order simplify numerical computation consider simple linear form feature map dimensional matrix known dimensional vector function 
write linear predictor 
hypothesis space appropriate regularization conditions analyzed appendix theorem 
point key idea formulation discover shared low dimensional predictive structure parameterized 
applying regularized empirical risk obtain formulation arg min learning predictive structures appropriate regularization condition weight vector appropriate regularization condition structural parameter 
formulation weight problem equally dividing number instances problem dominate 
may choose weighting schemes 
note regularized erm method form 
main difference replaced hard constrained regularization picking predictors hypothesis space computationally convenient version penalized regularization 
appropriately defined lagrangian multipliers formulations equivalent 
consider kernel learning assume feature map belongs reproducing kernel hilbert space equation kernelized 
ways 
possibility parameter simply replace vector parameter dual parameters linear score wt 
alternating structure optimization 
simplicity consider kernel methods possible solve general purpose optimization methods 
section show exploring special structure formulation develop interesting conceptually appealing computational procedure 
general pick formulation convex fixed 
joint optimization non convex 
typically find local minimum respect 
usually doesn lead serious problems local optimal structural parameter solution globally optimal 
algorithm propose section uses svd dimension reduction 
conceptual level possible local optimality major issue simply svd procedure finding globally optimal low dimensional structure 
fixed computation problem decoupled various optimization algorithms applied purpose 
specific choice algorithms important purpose 
experiments convenience simplicity employ stochastic gradient descent sgd widely neural networks literature 
argued simple method large scale convex learning formulations zhang 
consider special case simple iterative svd solution 
rp square regularization weight vectors 
arg min ih constants 
note formulation regularization condition absorbed orthonormal constraint ih need explicitly included 
ando zhang order solve optimization problem may introduce auxiliary variable problem 
may eliminate obtain arg min ih optimal solution 
order solve alternating optimization procedure fix optimize respect fix optimize respect 
iterate convergence 
may propose alternating optimization procedures 
example step may fix optimize respect 
alternating optimization procedure outlined convex choice step convex optimization problem 
established methods solving mentioned earlier sgd simplicity 
shall focus second step crucial derivation method 
easy see optimization fixed equivalent problem arg min ih 
simple linear algebra know fixed min optimal value achieved 
eliminating equality rewrite arg max ih matrix arg max tr uut ih tr trace matrix known solution problem svd singular value decomposition dv svd assume diagonal elements arranged decreasing order rows rows left singular vectors corresponding largest singular values 
summarize derivation algorithm described solves alternating optimization 
input training data learning predictive structures parameters output dimensional matrix initialize arbitrary iterate fixed approximately solve arg minw endfor wt vt compute svd mum diagonals descending order dv rows rows converge svd alternating structure optimization algorithm note objective value decreases iteration procedure produces parameters converging objective values 
general parameters converge local optimal solution 
reality usually sufficient parameter iteration procedure 
performance model sensitive small perturbation structural parameter 
main dimensional reduction effect captured svd iteration 
important point svd alternating structure optimization svd aso procedure fundamentally different usual principal component analysis pca regarded dimension reduction data space dimension reduction performed svd aso algorithm predictor classifier space data space 
possible observe multiple predictors multiple learning tasks 
regard observed predictors sample points predictor distribution predictor space corrupted estimation error noise algorithm interpreted finding principal components predictors 
consequently method directly looks low dimensional structures highest predictive power 
contrast principal components input data data space necessarily predictive power 
extension basic svd aso algorithm may extend svd aso procedure various ways 
example belongs infinite dimensional hilbert space svd replaced corresponding kernel principal component analysis 
generalization outside scope analysis 
ando zhang experiments extension features components grouped different types svd dimension reduction computed separately group 
important applications features homogeneous 
know features similar type reasonable perform localized dimension reduction similar features 
formulate idea divide input features groups rewrite input data point feature type specifies group feature 
rpt rp pt 
associate group structural parameter ht pt projection operator feature type ht dimensional space 
equation replaced structural learning method arg min ht similarly introduce auxiliary variables perform alternating optimization 
resulting algorithm essentially svd aso method svd dimension reduction step performed separately feature group extensions basic algorithm useful certain applications 
example may choose regularize components correspond non negative part may regularize negative part corresponding components 
reason doing positive weights linear classifier usually directly related target concept negative components yield specific information 
resulting method easily formulated solved variant basic svd aso algorithm 
effect svd computation positive components 

semi supervised learning ready illustrate apply structural learning paradigm developed earlier semi supervised learning setting 
basic idea create auxiliary problems unlabeled data employ structural learning reveal predictive structures intrinsic underlying input space learning unlabeled data structural learning systematically create multiple prediction problems unlabeled data 
call created prediction problems auxiliary problems call original supervised prediction problem interested target problem 
method consists steps learning predictive structures 
learn structural parameter performing joint empirical risk minimization auxiliary problems originally unlabeled data automatically labeled auxiliary class labels 

learn predictor target problem empirical risk minimization originally labeled data computed step 
particular bi linear formulation section fix optimize respect target problem 
step seeks hypothesis space learning predictive functional structure shared auxiliary predictors 
auxiliary problems degree related target task obtained hypothesis space improves average performance auxiliary predictors help target problem 
relevancy auxiliary problems plays important role method 
return issue section 
alternative step procedure perform joint empirical risk minimization target problem labeled data auxiliary problems unlabeled data 
intended applications number labeled data available target problem usually small 
inclusion target predictor joint erm significant impact 
auxiliary problem creation approach semi supervised learning requires auxiliary problems characteristics automatic labeling need automatically generate various labeled data auxiliary problems unlabeled data 
relevancy auxiliary problems related target problem share certain predictive structure degree 
consider strategies automatic generation auxiliary problems completely unsupervised fashion partially supervised fashion 
example auxiliary problems introduced section experiments described section 
briefly discussed relationship pca svd aso section 
mentioned framework semi supervised learning standard pca applied unlabeled data roughly regarded result generating auxiliary problems unlabeled data points th problem positive example th data point 
general strategies suggest flexible effective 
clarity introduce mini target tasks running examples 
text genre categorization consider task assigning categories science sports economy text documents 
problem suppose frequencies content words features 
ando zhang word tagging consider task assigning part speech tags noun verb words english sentences 
instance word test test procedure assigned tag noun test assigned tag verb 
problem suppose strings current surrounding words features 
unsupervised strategy predicting observable sub structures strategy regard observable substructures input data auxiliary class labels try predict labels parts input data 
instance word tagging task mentioned word position create auxiliary problems regarding current word auxiliary labels want predict surrounding words 
create binary classification problem possible word value obtain auxiliary problems idea 
generally feature representation input data may mask features unobserved learn classifiers predict masked features functional mapping masked features bi grams left current words features masked 
actual implementation just replace masked feature values zero effect 
automatic labeling requirement satisfied auxiliary labels observable 
see technique may naturally meet relevancy requirement note feature components predict certain masked feature correlated masked feature correlated 
technique helps identify correlated features predictive power 
optimal performance clear choose mask predict features correlation target classes auxiliary labels 
creation auxiliary problems strategy easy hard designing features usual supervised learning tasks 
educated guess task specific knowledge 
wrong guess result adding irrelevant features originating irrelevant components hurt erm learners severely 
hand potential gains right guess significant 
note abundance unlabeled data wider range choices standard feature engineering supervised setting 
example high order features suffer data sparseness problem supervised setting may auxiliary problems due vast amount unlabeled data provide reliable statistics 
low dimensional predictive structure discovered high order features supervised task causing data sparseness problem 
rare features properly combined projection matrix combined low dimension directions appear frequently correlated class label 
example provided section demonstrates point 
examples illustrate auxiliary problems potentially useful example mini tasks 
ex 
predict frequent words text genre categorization 
intuitive content words occur frequently document indicators genre document 
split content words sets removing learning predictive structures appropriate words 
auxiliary task define follows 
document predict word occurs frequently words set 
learner uses words prediction 
task breaks binary prediction problems content word 
example stadium scientist stock baseball basketball physics market treat members unobserved learn predict word stadium occurs frequently document scientist stock observing occurrences baseball basketball physics market 
similarly second problem predict scientist frequent baseball stock 
essentially auxiliary problem learn textual context implies word stadium occurs frequently 
assuming stadium strong indicator sports problem indirectly helps learn correlation members target class sports unlabeled data 
ex 
predict word strings word tagging 
discussed example auxiliary task word tagging predict word string current position observing corresponding words left right 
idea obtain binary prediction problems set possible word strings 
example predict word left observing words current right positions 
underlying assumption word strings current left positions strong correlations target problem word noun verb 
partially supervised strategy predicting behavior target classifier second strategy motivated training 
distinct feature maps train classifier target task feature map labeled data 
auxiliary tasks predict behavior classifier predicted labels assigned confidence values forth feature map 
note training classifier means creating auxiliary problems meet relevancy requirement bootstrap labels 
semi supervised learning procedure strategy summarized follows 

train classifier labeled data target task feature map 
generate labeled data auxiliary problems applying unlabeled data 

learn structural parameter performing joint erm auxiliary problems feature map 
may consider variations idea predicting content word appears certain threshold top frequent list ando zhang 
train final classifier labeled data computed appropriate feature map 
ex 
predict prediction classifier 
simplest auxiliary task created strategy prediction class labels proposed classifier 
target task way classification binary classification problems obtained manner 
example suppose train classifier half content words text genre categorization task 
auxiliary problems predict classifier propose sports category half content words 
ex 
predict top choices classifier 
example predict combinations classes assigns highest confidence values 
problem fine grained distinctions related intrinsic sub classes target classes learned 
way classification problem 
binary prediction problems created 
instance predict assigns highest confidence values sports economy order 
ex 
predict range confidence values produced classifier 
example predict proposed labels conjunction range confidence values 
instance predict propose sports confidence greater 
discussions introduced strategies creating auxiliary problems section 
unsupervised partially supervised 
unsupervised strategy try predict sub structures input parts input 
idea related discussion section argued structures smoothness conditions intrinsic input space 
structures discovered auxiliary problems 
text data words linguistic usages similar meanings 
smoothness condition related fact interesting predictors text data associated underlying semantic meanings take similar values linguistic usage substituted closely related 
smoothness structure discovered structural learning specifically method proposed section 
case space smooth predictors corresponds predictive low dimensional directions may discover svd aso algorithm 
example computed section supports argument 
easy see reasoning specific text 
idea applied data images 
briefly explain underlying intuition un supervised auxiliary problems create helpful supervised task leave development rigorous general theory investigation 
suppose split features parts predict 
suppose features correlated class labels necessarily correlated 
auxiliary prediction problems related target task reveal useful structures 
appropriate conditions features learning predictive structures similar predictive performance tend map similar low dimensional vectors 
effect empirically observed section 
shall simple concrete example illustrate main idea 
assume words divided disjoint sets tj binary label 
assume simplicity document contains words tj tj 
assume class label independent uniformly distributed ty uniformly distributed 
predicting squares obtain identical weight components words due exchangeability words 
dimension reduction rows non zero singular values 
feature reduces words single vector words single vector gives helpful grouping effect words similar predictive performance grouped 
clear example gain predictive ability unsupervised structure discovery 
original word spaces may extremely large means able learn small number training examples word occur 
grouping words obtain feature completely correlated class label due data generation process 
grouping effect originally hard problem easier learn 
example extended general theory shall leave exploration 
consequence example observable practice demonstrated section 
discussion implies possible find useful predictive structures intentionally create problems mimic target problem clear auxiliary problems closely related target problem beneficial 
motivation propose partially supervised strategy creating auxiliary problems 
idea possible create relevant auxiliary problems closely related supervised problem knowing effectiveness individual features 
practical applications observe desirable create auxiliary problems possible long reason believe relevancy target task 
risk relatively minor potential gain structure large 
auxiliary problems introduced experiments section merely possible examples 
advantage approach semisupervised learning may design wide variety auxiliary problems learning various aspects target problem unlabeled data 
structural learning provides theoretical foundation general framework carrying possible new ideas 

experiments study performance structural learning semi supervised method text categorization natural language tagging chunking image classification tasks 
experimental results show able improve state art supervised learning methods problems relatively large number labeled data labeled data named entity recognition 
implementation ando zhang experiment semi supervised learning procedure 
required auxiliary label generation train classifiers ti labeled data appropriate feature maps 
auxiliary problems assign auxiliary labels unlabeled data 

compute structure matrix performing svd aso procedure section auxiliary problems data generated 
extended version take advantage natural feature splits iterate 

fix obtain final classifier optimizing respect labeled data settings including baseline methods loss function modification huber robust loss regression max py py py square regularization 
known modified huber loss works classification advantages may select loss functions svm logistic regression 
specific choice important purpose 
training algorithm stochastic gradient descent sgd zhang 
fix ht dimension settings specified 
text categorization experiments report text categorization performance newsgroup corpus reuters rcv corpus known new reuters 
feature representation feature representation uses word frequencies removing function words common stopwords normalizes feature vectors unit vectors 
auxiliary problems text categorization experiment types auxiliary problems freq predicts frequent word observing half words section 
ex 
top predicts combinations top choices classifier trained labeled data section 
ex 
multi multi category target task predicts top choices classifier trained labeled data regarding multi category auxiliary labels 
learning predictive structures number set average number categories instance obtained labeled data 
feature splits randomly generated 
data newsgroup corpus newsgroup corpus standard data sets text categorization consists documents newsgroups documents group 
task classify documents newsgroups ranging variety topics computer hardware baseball bikes middle east issues 
pre processing removed header lines subjects newsgroup names senders forth documents 
held documents test set arbitrarily split rest corpus training set documents unlabeled data set documents 
reuters rcv corpus new reuters reuters rcv corpus randomly generate disjoint sets labeled unlabeled test examples 
reuters rcv corpus differs newsgroup corpus ways 
number categories times larger newsgroup corpus categories organized level hierarchies document may assigned multiple categories categories document average 
reuters rcv corpus preserves natural distribution categories newsgroup corpus completely uniform distribution generated intentionally choosing number documents newsgroup 
evaluation metric measure performance final classifier test sets singly labeled tasks choose category produces highest confidence value inner product report classification accuracy 
multiply labeled tasks choose categories produce positive confidence values report micro averaged measure 
text categorization performance results newsgroup results shows accuracy results newsgroup data comparison supervised setting baseline 
show averaged results runs labeled examples randomly drawn training set 
vertical bars standard deviations 
symbol semi stands semi supervised followed types auxiliary problems 
semi supervised methods obtain significant performance improvements supervised method settings 
reuters rcv results shows micro averaged measure reuters rcv data comparison supervised baseline 
performance trend similar newsgroup experiments 
significant performance improvements supervised method obtained settings 
auxiliary problems unsupervised vs partially supervised results observe relatively small number labeled data accuracy text categorization newsgroup labeled examples supervised semi freq semi top semi top semi freq top ando zhang measure text categorization new reuters labeled examples supervised semi freq semi multi semi multi freq text categorization performance results 
average runs 
vertical bars standard deviations 
newsgroup reuters rcv 
freq uses auxiliary problems created unsupervised manner outperforms topk multi partially supervised 
top multi relatively large number labeled data 
freq learns unlabeled data unsupervised fashion effectiveness insensitive number labeled data 
contrast top multi take advantage information labeled data reasonable amount 
best performance achieved types auxiliary problems 
performance comparison methods mentioned idea partially supervised auxiliary problems motivated training 
test training comparison 
training implementation implementation follows original blum mitchell feature splits auxiliary problems 
initial classifiers trained labeled instances drawn training sets 
maintain pool unlabeled instances randomly choosing instances unlabeled set 
classifiers propose labels unlabeled instances pool 
classifier choose instances high confidence preserving class distribution observed initial labeled data 
done choosing class label probabilities distribution highest confident instance class label 
chosen instances added pool labeled data automatically proposed labels 
process repeats unlabeled instances exhausted 
comparison training shows best possible performance cotraining optimally stopped training procedure averaged runs labeled examples newsgroup reuters rcv data 
method accuracy learning predictive structures newsgroup comparison training labeled examples supervised semi freq top training best measure new reuters comparison training labeled examples supervised semi multi freq training best comparison training averaged performance runs standard deviations 
accuracy newsgroup labeled examples training iterations training semi top freq accuracy newsgroup labeled examples training iterations training semi top freq measure new reuters labeled examples training iterations training semi multi freq measure new reuters labeled examples training iterations training semi multi freq training performance typical runs versus number iterations 
ando zhang labeled bn best aso semi examples manifold comparison similar settings bn belkin niyogi newsgroup 
outperforms best training performance settings 
plot training performance versus training iterations typical runs 
shown results outperform bn belkin niyogi semi supervised learning method 
consistent nigam em results 
didn report exact numbers approximately read results graph include 
performance em usually similar training sel training frequently nlp 
quite successful newsgroup data shall see training self training perform difficult tasks 
performance dependency recall experiments fix number rows constant ht described section 
recall text categorization derived auxiliary problems th feature map 
section study performance dependency dimensionality ht 
interested range ht roughly 
plots performance newsgroup reuters rcv corpora relation ht 
results show method insensitive change dimension ht relatively large range 
practice significant advantage dimension reduction approaches typically sensitive choice dimensions bootstrapping approaches sensitive parameter settings 
interpretation order gain insights information obtained unlabeled data show significant entries matrix entries absolute values largest columns corresponding features largest positive negative entries rows 
table show entries chosen manner rows corresponding significant singular values 
computed freq unsupervised auxiliary problems newsgroup unlabeled data 
accuracy newsgroup labeled examples dimension semi freq training best supervised accuracy learning predictive structures newsgroup labeled examples dimension semi freq training best supervised measure new reuters labeled examples dimension semi freq training best supervised measure new reuters labeled examples dimension semi freq training best supervised performance dependency ht rank particular runs 
second row appears capture distinctions computers religion 
third row distinguishes sports middle east issues 
positive entries fifth row appear motor vehicles negative entries printers 
topics relevant themes newsgroups 
row features pc ibm boards god christian bible exist doctrine nature rutgers edu team detroit series leafs play cup played penguins israel peace land civilians files jpeg pov utility ms windows icon agents attorney oil bikes front brake rear transmission owner driving dogs highway printer hp ink appreciate bj printing gcc named entity chunking experiments report named entity chunking performance conll shared task corpora english german 
choose task original intention shared task test effectiveness semi supervised learning methods label bootstrap training large number unlabeled data available 
turned top performing systems unlabeled data 
possible reason may number labeled data relatively large 
show contrast structural learning semi supervised learning possible obtain results better top systems unlabeled data additional resource 
particular gazetteer information systems 
conll corpora annotated types named entities persons organizations locations miscellaneous names world cup 
commonly done en 
uia ac conll ner 
ando zhang code chunk information word tags cast chunking problem word tagging perform viterbi style decoding 
official training development test splits provided shared task organizers 
unlabeled data sets consist words english words german respectively 
chosen sources reuters eci multilingual text corpus training development test sets disjoint 
feature representation feature representation slight modification simpler configuration reported zhang johnson uses token strings parts speech character types characters tokens token window current position token strings syntactic chunk window labels tokens left current position bi grams current token label left labels assigned previous occurrences current word 
features easily obtained deep linguistic processing 
auxiliary problems named entity chunking types auxiliary problems combinations word prediction predicts word current left right position features derived tokens 
top predicts top choices classifier 
split features left context vs right context vs 
rest ex section 
svd applied feature types separately 
word prediction auxiliary problems consider instances current words nouns adjectives named entities consist types 
leave auxiliary problems type largest numbers positive examples 
ensure auxiliary predictors adequately learned unlabeled data 
performance results conll english german corpora figures show english measure results small word labeled data entire training set words 
german results entire training set shown 
precision recall results settings 
note facilitate comparisons supervised baselines gazetteers name lexicons 
kinds information sources labeled data unlabeled data 
confirm performance improvements gained unlabeled data significant semi supervised settings gains small english labeled data larger english labeled data improvements german data 
note word prediction unsupervised auxiliary problems particularly effective number labeled examples relatively small training data differ measure conll english ne labeled examples dev set test set supervised semi word semi top semi word top self training best learning predictive structures measure conll english ne labeled examples dev set test set supervised semi word semi top semi word top self training best measure conll german ne labeled examples dev set test set supervised semi word semi top semi word top self training best named entity chunking measure performance 
gazetteer 
self training performance best parameter settings including number iterations shown 
conll english corpus labeled examples 
conll english corpus entire labeled examples 
conll german corpus entire labeled examples 
measure iterations semi word top tr tr self training self training named entity chunking performance typical runs versus number iterations 
tested german development set 
legend stand current words left context right context respectively 
significantly test data 
english test set known similar training set development set apparently time periods articles drawn 
best performance achieved combining aux ando zhang problems 
performance trend line text categorization experiments 
comparison self training comparison test training exploring parameter settings pool size increment size commonly feature splits current left context vs current right context current vs context 
single view bootstrapping called self training 
addition test basic self training replaces multiple classifiers training procedure single classifier employs features 
self training performance shown figures best possible performance parameter settings including number iterations 
self training best improve recall degrades precision 
consequently measure improvements relatively low demonstrates easy benefit unlabeled data task 
shown self training may degrade performance severely iteration optimally stopped 
performance degradation caused contamination automatically assigned labels observed previous training studies nlp tasks pierce cardie 
comparison previous best results english test set system measure additional resources semi word top unlabeled data florian gazetteers word labeled data cn ng gazetteers elaborated features klein rule post processing german test set systems measure additional resources semi word top unlabeled data gazetteers rule post processing zj zhang johnson gazetteers comparison previous best results conll shared task compare performance results previous top systems conll shared task participants 
english german data able achieve performance better top participants elaborated features 
note previous best english results achieved help knowledge intensive resources gazetteers provided organizer plus additional gazetteers large number names cn large amount words labeled data annotated finer grained named entities rule post processing 
recall study semi supervised learning motivated potential unavailability learning predictive structures labor intensive resources 
feel results obtained unlabeled data additional resource encouraging 
part speech tagging report part speech pos tagging results brown corpus 
corpus annotated parts speech standard corpora pos tagging research 
arbitrarily split corpus labeled set words unlabeled set words test set words 
auxiliary problems feature representation named entity chunking experiments part speech syntactic chunk information 
convention error rate measure performance 
seen error reductions achieved learning unlabeled data 
supervised semi left curr semi top semi left curr top part speech tagging error rates 
numbers parentheses error reduction ratio respect supervised baseline 
hand written digit image classification experiment uses mnist data downloaded yann lecun com exdb mnist 
consists training set examples test set examples grayscale hand written digits 
task classify image data digits 
feature representation composed location sensitive bags pixel blocks similar bag word model text categorization 
consists normalized counts pixel blocks various shapes regions top left top right bottom right 
normalization done scaling vector shape region unit vector 
pixel blocks black white patterns pixels shape squares rectangles crossing lines top left bottom right top right bottom left dotted lines horizontal vertical 
features trained entire training set examples error rate supervised setting 
matches surpasses state art algorithms data reported mnist data website additional image processing transformation distortion 
auxiliary problems partially supervised 
feature splits halving image features derived top left top right regions vs bottom right top left bottom left vs top right bottom right top left vs top right bottom left 
run labeled examples randomly chosen training set remaining training set unlabeled data 
aso semi consistently produced ando zhang significant performance improvements supervised baseline 
outperforms manifold semi supervised learning method bn belkin niyogi number labeled data 
method bn performs small labeled data 
disadvantage method requires dimension reduction sensitive number reduced dimensions 
example labeled data achieved error rate dimensions error rate dimensions error rate dimensions 
labeled supervised aso semi bn best nd best error rates average runs standard deviation 
mnist hand written digit image classification results test set 
bn results belkin niyogi unlabeled portion training set 
newsgroup labeled examples supervised semi freq semi top semi top semi top freq reuters rcv corpus labeled examples supervised semi freq semi multi semi multi freq text categorization 
average runs 
run labeled examples randomly drawn training set 
accuracy newsgroup corpus measure micro averaged reuters rcv corpus 
numbers parentheses performance improvements obtained unlabeled data 
best performance column italicized 

contrast training feature splits degraded performance 
learning predictive structures data set newsgroup reuters rcv labeled examples training highest training lowest training text categorization performance 
highest lowest performance iterations averaged runs 
accuracy newsgroup micro averaged measure reuters rcv shown 
numbers parentheses improvements supervised settings 
english labeled examples development set test set prec 
recall prec 
recall supervised self best semi word semi top semi word top english labeled examples development set test set prec 
recall prec 
recall supervised self best semi word semi top semi word top german labeled examples development set test set prec 
recall prec 
recall supervised self best semi word semi top semi word top named entity chunking results conll corpus 
gazetteer 
training self training baseline best performance parameter settings including number iterations shown 
english labeled examples 
english entire labeled examples 
german entire labeled examples 
best performance column italicized 

discussions ando zhang presents general framework learning predictive functional structures multiple tasks 
idea concept multiple problems share common predictive structure structure reliably estimated considering problems 
process learning shared functional structure referred structural learning 
learning theory framework structural learning discover common structure hypothesis spaces shared problems 
main theoretical justification approach shared structural parameter reliably estimated large 
optimally estimated structural parameter better generalization performance averaged problems achieved 
showed framework structural learning applied semisupervised learning 
achieved creating auxiliary problems unlabeled data reveal important underlying predictive structures data 
examples auxiliary problems provided experimental results demonstrated discovered structures useful 
rigorously speaking theory developed appendix directly apply semi supervised learning 
theory performance measured averaged generalization ability multiple prediction problems 
setting semi supervised learning interested performance original supervised task auxiliary problems 
semisupervised learning relevant consequence analysis shared structure stably estimated multiple tasks 
usefulness shared structure different issue directly answered appendix intuitive justification auxiliary problems created section complete theory requires investigation 
summary approach semi supervised learning bet existence shared predictive structure useful supervised problem auxiliary problems 
method proposed section robust discovered structure help supervised problem potential negative effect non predictive features 
typical discriminative learning methods appropriate regularization small number bad features minor impact performance 
features discovered auxiliary problems useful performance improvement significant 
method derived section intuitive interpretation discovering low dimensional predictive structures classifier space 
model predictive dimensions correspond principal components multiple classifiers 
algorithm joint empirical risk minimization method strong foundation learning theory see appendix principle consider general approach mining structures classifier space 
general principle design structural learning algorithms necessarily joint empirical risk minimization method proposed 
fact general principle may call structural mining heart analysis 
shall conclude comparing underlying concepts structural mining datamining 
table predictor regarded real valued function defined data space 
final row points may consider data point learning predictive structures predictor predictor space associating predictor functional value 
sense data mining viewed special structural mining 
data mining structural mining space interest data space predictor space instances data points predictors multiple tasks uncertainty measurement error estimation error goal find patterns data find structures predictors predictive power acknowledgments duality data point predictor points predictor space data mining versus structural mining authors trevor hastie robert tibshirani helpful discussions pointing related statistical studies 
part supported arda program sw 
appendix analysis structural learning include theoretical analysis joint empirical risk minimization method structure learning 
main purpose demonstrate joint minimization shared structure reliably estimated 
consider idealized case performance interests averaged loss tasks 
particular interested behavior large 
bound shows joint empirical risk minimization method possible estimate shared hypothesis space reliably 
note practice interested performance particular task averaged performance multiple tasks 
non idealized scenario directly covered analysis 
particular semi supervised learning setting additional theoretical analysis needed show structure shared artificially created tasks improve performance supervised task see section 
analysis relevant implies shared structure reliably estimated joint empirical risk minimization method employ 
providing general analysis tightest possible generalization bounds adopt relatively simple approach 
purpose illustrate main benefit structural learning ability obtain accurate estimate best hypothesis class number problems large 
analysis closely related baxter see ben david 
different related technical approach different covering number definition 
modifications necessary results directly applicable specific method proposed section 
ando zhang clarity analysis simplify arg min consider case nm hm 
simplification critical allows consider behavior fixed simplicity covering number approach analysis 
treatment similar case standard empirical risk minimization 
need introduce definitions order state main theorem 
definition consider set distance function covering number denoted minimal number balls radius needed cover definition xn yn set points 
define distance functions xi yi xi yi class functions 
empirical covering number covering number respect distance 
uniform covering number sup supremum samples size definition define distance hypothesis spaces sup inf define covering number 
sup 
theorem gives sided uniform convergence result joint erm method 
theorem set points problem independently drawn distribution 
assume bounded lipschitz function 
independent constants learning predictive structures universal constant probability inf true empirical risks problem ln ln sup ln ln 
ln nm shall delay proof appendix discuss implications theorem 
summary result justifies joint erm method minimizes empirical risk right hand side theorem 
theorem implies method implicitly minimizes upper bound true risk averaged problems left hand side leads theoretical guarantee performance method 
statistical complexity joint erm method depends joint entropy ln components term sup ln learning complexity associated individual estimation problems fixed 
second term ln complexity estimating best structural parameter 
important consequence analysis complexity structural space measured discounted entropy ln approaches zero 
implies able find near optimal measured generalization bound shared structural parameter large 
theorem analyze method propose section bi linear structural model form ih pre defined vector functions feature maps vector dimensional vector orthonormal dimensional matrix 
ih denote dimensional identity matrix 
model matrix shared different prediction problems structural parameter 
fix hypothesis space parameterized weight vectors high dimensional vector regularized low dimensional vector dimensionality 
idea model find common low dimensional predictive structure shared problems parameterized projection matrix 
discover structure need ando zhang small regularize vector leads improved generalization performance 
words trade dimensionality common low dimensional predictive structure regularization size optimal trade shared structural parameter reliably estimated structural learning formulation large 
intuitive argument rigorously justified theorem appropriate covering number estimates 
specifically shown universal constants sup ln hln ln hp ln shall include detailed proof estimates essential purpose outline basic ideas derivation 
term follows simple estimate rademacher complexity sub function class corresponding straight forward application see ledoux talagrand chapter 
ln terms obtained explicit discretization corresponding finite dimensional parameter spaces dimensional sub function class corresponding fixed variable direct discretization hp dimensional variable 
simply note bounded set dimensional parameter space covered grid points width greater direction 
covering number estimates complexity term theorem ln hln hp ln third term complexity estimating structural parameter vanishes 
second terms characterize trade regularization size parameter dimensionality parameter 
estimated model approximates underlying true predictor better fixed regularization size fixed complexity term ln theorem implies better generalization behavior 
proof theorem training data define vector function class fs notation fm 
similarly define introduce lemmas 
lemma bounds fs 
ln ln fs sup ln ln 
learning predictive structures proof inequality direct consequence lipschitz condition theorem 
shall prove second inequality 
consider cover metric 
simplicity denote cover 
find cover 
fs find fs 
approximate 
follows 

means fs cover form 
size cover 
lemma sup fs probability es ln mn proof create new dataset changing th datum th problem keep data points identical 
easy verify sup sup mn mn lemma direct consequence mcdiarmid concentration inequality mcdiarmid 
ready prove main theorem 
consider sequence binary random independent probability 
empirical sample variables rademacher complexity fl sup fs mn il known exists universal constant variant corollary van der wellner inf mn ln fl nm applying lemma obtain inf ando zhang ln inf ln standard symmetrization argument example see lemma van der wellner es es ln inf theorem direct consequence lemma 
ando zhang 
high performance semi supervised learning method text chunking 
acl 
baxter 
model inductive bias learning 
journal artificial intelligent research pages 
belkin niyogi 
semi supervised learning riemannian manifolds 
machine learning special issue clustering 
ben david 
exploiting task relatedness multiple task learning 
colt 
blum mitchell 
combining labeled unlabeled data training 
proceedings eleventh annual conference computational learning theory pages 
breiman friedman 
predicting multivariate responses multiple linear regression 
roy 
statist 
soc 

discussion 
caruana 
multi task learning 
machine learning pages 
ng 
named entity recognition maximum entropy approach 
proceedings conll pages 
pontil 
regularized multi task learning 
proc 
conf 
knowledge discovery data mining 
florian jing zhang 
named entity classifier combination 
proceedings conll pages 
hastie tibshirani friedman 
elements statistical learning 
springer 
joachims 
transductive inference text classification support vector machines 
proceedings sixteenth international conference machine learning pages 
learning predictive structures klein nguyen manning 
named entity recognition character level models 
proceedings conll pages 
ledoux talagrand 
probability banach spaces 
springer verlag berlin 
isbn 
processes 
mcdiarmid 
method bounded differences 
surveys combinatorics pages 
cambridge university press 
micchelli 
kernels multi task learning 
nips 
appear 
nigam mccallum thrun mitchell 
text classification labeled unlabeled documents em 
machine learning special issue information retrieval 
pierce cardie 
limitations training natural language learning large datasets 
proceedings conference empirical methods natural language processing emnlp 
szummer jaakkola 
partially labeled classification markov random walks 
nips 
van der wellner 
weak convergence empirical processes 
springer series statistics 
springer verlag new york 
isbn 
vapnik 
statistical learning theory 
john wiley sons new york 
yarowsky 
unsupervised word sense disambiguation rivaling supervised methods 
proceedings acl pages 
zhang 
solving large scale linear prediction problems stochastic gradient descent algorithms 
icml pages 
zhang johnson 
robust risk minimization named entity recognition system 
proceedings conll pages 
zhang oles 
probability analysis value unlabeled data classification problems 
icml pages 
zhou bousquet lal weston 
learning local global consistency 
nips pages 
zhu ghahramani lafferty 
semi supervised learning gaussian fields harmonic functions 
icml 

