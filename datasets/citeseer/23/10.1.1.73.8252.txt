ieee journal selected areas communications vol 
jan iterative decoding compound codes propagation graphical models frank kschischang member ieee brendan frey unified graphical model framework ing compound codes deriving iterative decoding algorithms 
reviewing variety graphical models markov random fields bayesian networks derive general distributed marginalization algorithm functions described factor graphs 
algorithm pearl belief propagation algorithm easily derived special case 
point developed iterative ing algorithms various codes including turbo decoding convolutional codes may viewed probability tion graphical model code 
focus bayesian network descriptions codes give natural input state output channel descrip tion code channel indicate iterative decoders developed parallel serially concatenated coding systems low density parity check codes 
compound codes codes composed collection constituent codes de coded tractably 
describe various graphical models describe wide variety codes derive variety iterative decoding algorithms codes 
prominent turbo codes introduced berrou glavieux constituent convolutional parallel concatenation interleaver 
probably fair say near capacity error rate perfor mance turbo codes sparked current interest iterative decoding techniques evidenced 
examples compound codes include classical serially concatenated codes see gallager low density parity check codes various product codes 
observed iterative decoding algorithms de veloped compound codes instances probability propagation algorithms operate graphical code 
algorithms developed past decade expert systems literature notably pearl lauritzen spiegelhalter 
see textbook treatments probability belief propagation algo rithms extensive treatment graphical models 
connect pearl belief propagation coding mackay neal showed gallager year old algorithm decoding low check codes essentially instance pearl algorithm 
extensive simulation results mackay neal gallager codes perform nearly turbo codes manuscript received sept revised july kschischang department electrical computer engineering university toronto toronto ontario canada 
department electrical computer engineering university toronto 
beckman institute advanced science university illinois urbana champaign north mathews avenue urbana il publisher item identifier xxx 
indicating probably closer capacity years ago appreciated interim 
mackay mceliece cheng issue inde observed turbo decoding instance belief propagation 
provide description pearl algo rithm explicit connection basic turbo decoding algorithm described independently developments expert systems literature wiberg loeliger otter doctoral dissertation attention graphical models codes 
show type model called tanner graph introduced tanner describe generalization gallager codes pro vides natural setting describe study iterative soft decision decoding techniques code trellis appropriate model describe study conventional maximum likelihood soft decision decoding algorithm 
forney gives nice description history various way algorithms coding theory 
seek unify develop ing graphical model framework describe broad class compound codes derive corresponding iter ative decoding algorithms 
section ii review relate various graphical models markov random fields tan ner graphs bayesian networks 
graphs support basic probability propagation algorithm section iii general setting factor graph section iv specific case bayesian network graphical code model probability propagation compute conditional probability observed channel output 
richly connected graphs containing cycles exact probability computationally infeasible case iterative decoding yield excellent results 
basic iterative de coding algorithm proceeds cycles graph guarantee computed conditional prob abilities close correct values converge 
excellent performance turbo gallager codes testimony efficacy iterative decoding procedures section describe bayesian network models variety compound codes describe probability tion decode codes 
straightforward exercise develop bayesian network models multilevel codes coset codes channels general usual memoryless channels possibilities application iterative decoding techniques described date 
ieee journal selected areas communications vol 
jan ii 
graphical code models section graph models describe conditional dependence structure codes channels 
set fv vn ran dom variables joint probability distribution vn graphical model attempts capture conditional dependency structure inherent distribution essentially ex pressing distribution factors product local functions conditional probabilities involving various graphical models useful describing structure key probability propagation iterative decoding 
markov random fields markov random field see graphical modelbased undirected graph random variable element neighbors set vertices con nected single edge graph markov ran dom field mrf distribution vn satisfies markov property words mrf variable non neighboring variables graph values immediate neighbors 
mrfs developed statis tics variety applications see joint probability mass density function vertices mrf expressed terms gibbs poten tial function defined maximal cliques clique gis collection vertices pairwise neighbors clique maximal properly contained clique 
corresponding clique set max cliques collection vertices vq denote sv sample space random variable nonnegative potential function clique function qv vq sv joint density function fv vn vn gamma fv gamma normalizing constant assuming zero 
possible define mrf terms potential functions defined cliques maximal cliques potential function defined non maximal clique absorbed potential func tion defined maximal clique containing structure potential functions straightforward exercise see show resulting prob ability distribution satisfies local markov property 
strictly positive mrf expressed terms proof result ch 
straightforward 
lauritzen pp 
example due moussouris non strictly positive mrf satisfying local markov property express joint distribution product potentials 
illustrate mrfs describe codes con sider mrf binary variables shown fig 

maximal cliques dashed loop 
joint probability distribution writ product gibbs potential functions defined variable subsets indicated cliques 
mrf canbe describe hamming code setting whichis equivalent neglecting letting po parity indicator functions 
delta arguments form configuration parity oth 
mrf places uniform probability distribution configurations satisfy parity cliques zero probability configurations satisfying relations 
potential functions chosen example define code clear potential functions determine code satisfying set local check conditions 
inparticular set variables fv vn bea collection subsets corresponding element eof local check condition enforces structure restricting values variables 
example check condition enforce parity example 
defining indicator func tion local check condition assumes value unity valid configurations zero invalid configurations defining graph element forms clique mrf description assigns uniform probability distribution valid configurations obtained provided atleast valid configuration exists 
shall see tanner graph way represent local check structure 
fig 

graphical models hamming code markov maximal clique indicated tanner graph bayesian network 
tanner graphs tanner graphs introduced construction long error correcting codes terms shorter codes 
treatment follows slightly different presentation wiberg 
tanner graph bipartite graph representation similar described 
graph types vertices corresponding variables andthe checks respectively edges connecting vertices type 
example tanner graph hamming code described shown fig 

check vertex set check vertices shown asa filled circle 
case check vertex ensures set neighbors satisfies parity valid configuration 
kschischang frey iterative decoding probability propagation see check vertices play precisely role ina tanner graph maximal cliques mrf 
general check vertex neighbors non negative real valued potential function fv assigns positive potential valid configurations arguments 
write probability distribution vn gamma fv gamma normalizing constant 
course 
mrf converted tanner graph check vertex maximal clique edges connecting check vertex variable clique 
assigned check vertex assigned clique tanner graph converted mrf eliminating check vertices forming cliques variables nally connected check vertex 
potential associated clique assigned vertex 
possible new cliques may formed process associated check vertex ofthe tanner graph 
unit potential assigned induced cliques 
different tanner graphs may map mrf tanner graphs may specific dependencies mrfs 
example graph fig 
addi tional check vertex connected mrf fig 

bayesian networks introduce bayesian networks mrfs graphs directed acyclic graphs 
directed acyclic graph graph cycles directions followed may cycles edge directions ignored 
mrf random vari able associated graph vertex 
directed graph parents direct ancestors vertex set vertices directed edges bayesian network joint probability written vn ny vi vi 
vi parents take vij 
vi distribution described bayesian network chain rule probability vn jv jv theta delta delta delta theta vn jv vn gamma follows pick ordering variables variable variables precede 
trivial network capture useful tic structure factor vn jv vn gamma contains variables really just complicated asthe full joint distribution 
bayesian network hamming code described shown fig 

joint distribution obtained parent child relationships jv theta jv jv factors express prior probabilities factors capture parity checks jv tanner graph extension mrf converted bayesian network simply directing edges vertices 
binary indicator random variable check site qi qi random variables set qi satisfy corresponding vertex tanner graph 
potential advantage bayesian networks di edges arrows model causality explicitly 
inspecting arrows models easy variables directly influence 
possible simulate network draw configuration consistent distribution specified network 
simply draws configuration variables having consistent prior distribution affecting variables 
configuration drawn parents variable configuration drawn consis tent conditional probability 
example 
simply pick values parent vertices determine remainder code word 
explicit representation causality useful modeling physical effects channel noise intersymbol interference noted simulating bayesian network hard problem variables required take specific value child variables clamped 
example drawing variables consistent observed output channel essentially hard harder decoding 
similarly tanner graph converted bayesian network manner described may difficult draw variables indicator variables nonempty set parents required take 
coding relationships information symbols encoder state variables trans codeword symbols received signals com pletely define encoding decoding problem code 
loss generality relationships ex pressed probabilistically depicted pictorially graphical models 
bayesian network channel coding gen eral shown fig 

inspection network joint distribution xju yjx usually uniform distribution xju deterministic probability mass placed ieee journal selected areas communications vol 
jan encoder channel fig 

general bayesian network channel coding 
single outcome 
channel likelihood yjx noise inter symbol interference introduced channel 
fig 
shows bayesian network systematic con code memoryless channel 
systematic codeword symbols xk simply copies infor mation symbols uk 
codeword symbols puts encoder xk depends uk state sk 
parents received signals find yjx qk yk jxk yk jxk expresses ab memory channel 
fig 
shows cycle free network code obtained grouping state variables 
eliminates undirected cycles expense increasing complexity net variables 
uuu fig 

bayesian network systematic convolutional code channel 
cycle free connected network code channel 
examples bayesian networks codes dis cussed section section describe basic distributed marginalization algorithm form iterative decoding 
iii 
framework distributed marginalization section develop basic probability propagation algorithm compute marginal probabilities graphical models observations 
common graphical models described previous section describe global joint probability distri bution product local functions 
computation conditional probability amounts essentially marginal ization global function 
structure local functions may possible greatly simplify com putation show 
derivation similar lines carried aji mceliece develop algorithm information distribution graph 
notation introducing notation 
finite dex set ig collection finite symbol alphabets indexed configuration space defined cartesian product symbol alphabets qk ak elements called configurations ae jj denote projection coor indexed jj qk ak empty empty 
configuration denote xjj image pro 
denote complement relative notation equate pair xjj re ordering coordinates may necessary equality strictly hold function set configurations global function 
initially assume codomain set real numbers allow commutative semiring 
useful introduce families global func tions indexed set finite dimensional real valued parameters fixed instance distributed marginal ization 
case write value configuration introducing parameters take account influence continuous valued variables channel outputs 
notational omit explicit dependence set ae define marginal function zj jj respect zj xjj jjc xjj words value marginal function point xjj obtained summing global func tion configurations agree xjj coordi indexed variable indexed said zj note 
constant summing configurations variables zi chosen symbol global function 
sum states par function statistical physics see 
function joint probability mass function random variables indexed za joint probability mass function random variables indexed 

re introducing parameter suppose function conditional joint proba bility mass function collection random variables observation continuous valued random vector thenthe marginal functions represent conditional probability mass functions 
example condi tional probability mass function xi observed valueof formulations useful decoding prob lems continuous valued output noise channel observed jij number arguments small modified notation marginal functions 
replace argument xi sign indicate thatthe corresponding variable summed marginalized 
jij zf kschischang frey iterative decoding probability propagation zf 
useful marginalize variables variables constant example case computing conditional probability mass function variables observed 
key operation computation marginal function computation probability marginalization shall focus attention developing efficient algorithms operation 
local functions factor graphs key efficient marginalization take structure global function possesses 
suppose separable written product ofa number local functions function variables contained subset precisely nonempty subsets suppose ny functions jaj called local functions example suppose random markov chain order specific observation 
example random variables rep resent state sequence convolutional code successive time intervals represent corresponding chan nel output 
conditional joint probability mass function written jy jy jx jx translating notation section observing thata conditional probability mass function xi jxi essen tially function variables constant write occasion consider products local functions example product isa function variables denote apply sign notation local functions products useful display particular factorization global function means bipartite graph graph called graph 
suppose factors 
factor graph bipartite graph vertex set edges vertex vertex aj aj ffi aj words vertex factor graph corresponds variable local function 
variable local function argument example fig 
shows factor markov chain 
note factor graph essentially generalization tanner graph lo cal checks involving incident variables replaced local functions involving incident variables straightforward exercise convert various graphical models described section ii factor graph repre sentation 
markov random field expresses gibbs yx yx fig 

factor graphs markov chain loopy example second higher power graphs omitting self loops 
potential function yields factor graph local maximal clique local function vertex factor 
tanner graph directly yields factor associating check vertex binary indicator function indicates local check condition satisfied generally factor associated local function vertex mrf case 
converted factor graph introducing local function vertex factor variable vertex foreach variable 
clearly local function vertex associated vi connected edge variable vertices inthe set vi 
bayesian network factor graph vertices 
marginalization message passing aim derive graph algorithm marginal functions func tions called local functions assume ac cess functions local specific vertex graph 
knowledge local functions functions derived propagated non local vertices message passing edges graph 
messages descriptions possibly marginalized local function products 
illustrate mean motivate situation graph message passing paradigm precise consider specific defined consider computation jy 
xx identified various factors need obtain 
primary observation computed knowing just 
factor computed analyzing computation remaining marginal func ieee journal selected areas communications vol 
jan tions manner find examining see marginal functions recursively chain local function products view messages passed vertices propagation tree shown fig 

comparing observe information passed vertex associated precisely needed compute xi choose vertex fusion site variable xi 
fig 

computation marginal functions message passing 
descriptions possibly marginalized local function products passed graph edges indicated arrows 
arrows selves part graph 
consider general case 
graph describing way global function factors product local functions 
second higher power graph defined having set edges connecting vertices path length vertices self loops ignored 
bipartite disconnected components having variables having vertices associated functions 
example fig 
shows second higher power graphs associated factor graphs shown 
observe variables joined arguments local function arguments particular local function form clique 
functions factor graph correspond gibbs potential functions corresponding markov random field words mrf recovered component second higher power graph omitting self loops cor responding factor graph 
consider call propagation graph cor responding assume consists single con nected component marginalization carried independently various components 
vertices cor respond local functions 
vertices joined edge argument corresponding local functions collapse multiple edges vertices single edge 
description general message passing algorithm simplified imagining vertex active processing element capable receiving transmitting messages marginalized local function products incident vertex capable performing computations involving messages local function vertex 
describe general distributed marginalization algo rithm operates tree spanning propagation graph refer spanning tree propagation tree 

specify spanning tree 
identify fusion vertex marginal function tobe computed 
note general quite different graphical model mrf tanner graph bayesian network de rived 
general may difficult problem choose opti mally minimize computational complexity 
assume chosen arbitrary manner 
say variable xi involved xi argument corresponding local function edge say variable xi part path joins vertex xi involved fusion vertex xi 
essence xi carried edge subtree spans xi vertices xi involved 
side subtree marginal knowledge xi needed xi marginalized 
propagation tree assignment fusion vertices easy variables carried edge 
forexample edge trees shown figs 
labeled indices variables carried edge 
size messages sent edge greatly number variables carried edge number possible values vari able assume 
simplistic measure complexity associated edge thickness defined number carried edge 
useful measure product sizes symbol alphabets corre sponding variables size minimal description corresponding local function product 
may find propagation tree assignment fusion vertices maximum thickness minimized hard problem general 
propagation tree maximum thickness minimized fusion vertex avariable xi vertex subtree spans ver xi involved problem find tree 
illustrate possible achieve unit max imum thickness consider global function 
factor graph sec ond higher power graph shown fig 

symmetry essentially propagation tree shown fig 

numbering vertices left right choose vertex xi 
observe carried ofthe edges propagation tree need tobe carried edge indicated fig 

thickness edge assignment fusion reduce number 
kschischang frey iterative decoding probability propagation fig 

example marginalization message passing 
sets variables carried edge shown 
marginalization algorithms fundamental idea marginalization algorithms isto compute product local functions messages marginalizing unnecessary variables 
describe general message passing algorithm compute marginal functions 
version way schedule messages passed direction edge propagation tree 
reason way schedule best suited serial implementation 
way propagation tree 
allow vertex way states inbound outbound 
initially vertices placed inbound state 
inbound state 
vertex inbound state wait messages arrived edges call edge prime edge 
compute product incoming messages local function marginalize variables carried prime edge send result prime edge 
toggle outbound state 
outbound state 
vertex outbound state wait message arrives prime edge 
non prime edge compute product incoming local function message variables carried pass result vertex fusion vertex desired marginal func tion computed product incoming messages local function marginalizing 
observe single edge incident leaf vertex leaf vertices required wait inbound state 
message passing initiated leaf vertices 
mes sages propagate leaves interior graph back leaves 
algorithm terminates leaf vertices received message 
edge convey exactly inbound message exactly outbound message total number messages transmitted twice number propagation tree edges gamma 
second version algorithm flooding schedule nodes necessarily wait passing messages messages may pass edge propagation tree 
flooding schedule better suited parallel implementation number messages usually passed 
flooding schedule 
initialization collection vertices carry flooding procedure described step arriving fictitious edge 

flooding message received edge eat vertex define message current message designate incoming edges outgoing outgoing edge compute product current messages current message marginalize variables carried passing result 
leaf vertex simply absorb message received 

marginalization algorithm terminates ver messages pass point marginalization fusion vertex way 
marginalization take place time previous step yield current estimate function 
basic principle flooding schedule message vertex triggers vertex send messages edges 
leaf vertex edge received message absorbed 
propagation tree cycles eventually messages absorbed vertices 
clear way schedule versions algorithm difference order messages propagated 
devise hybrid schedules 
message arrives edge vertex message effectively creates pend ing messages edges incident vertex 
pending messages need sent immediately messages may change reflect content messages arriving vertex 
general wide variety schedules possible way flooding schedules represent extremes 
simulations schedule turbo decoder observed compared standard message passing schedule magnitude messages passed messages passed concurrently orders time steps required achieve decoding performance note regardless message passing schedule message passing tree guaranteed converge state inwhich messages pending 
point marginalization fusion vertex carried exactly different message passing schedule 
generalization commutative semirings assumption global func tion real valued 
properties essentially commutativity multiplication distribution multiplication addi tion 
distributed marginalization algorithm described commutative semiring see sec 
example replacing real valued product operation summation summation opera tion max operator yield generalization algorithm equivalent pearl belief revision algo ieee journal selected areas communications vol 
jan rithm sec 
min sum algorithm described 
conjecture distributed algorithms data networks routing network ogy determination distributed bellman ford algorithm see ch 
instances algorithm appropriately defined semiring 
iv 
probability propagation bayesian networks specialize general message passing algorithm de scribed previous section special case bayesian networks 
special case bayesian neglecting direction edges cycles tree 
cycle free case bayesian network random variables xl 
definition bayesian network mass function random variables written xl xi 
words probability mass function written product local functions representing conditional function variable parents 
subsection assume forms tree undirected cycles 
case easy see corresponding factor graph form tree propagation graph case phic undirected version vertex corre sponding variable xi associated local function xi 
choose vertex fusion site xi example fig 
shows simple cycle free bayesian network corresponding factor graph propagation shown fig 
respectively 
simplicity refer vertex propagation xi translates vertex xi 
xj child parent xi bayesian network refer xj child parent xi propagation tree edges ofthe propagation tree undirected 
xx xp xx xp xx xp fig 

translating cycle free bayesian network factor graph anda propagation tree 
observe variable xi propagation network vertex xi children vertex propagation network xi carried connecting vertex xi children xi carried 
edge con precisely parent child precisely variable carried edge parent variable 
words cycle free bayesian network yields tion tree unit thickness 
observation key pearl belief propagation algorithm determine messages propagated 
pearl denote child parent messages messages parent child messages ss messages xi xj xi respectively messages transmit ted vertex xi parent xj child xk 
note thata message function parent variable 
consider vertex xi suppose children xi set parents xi xi indexed bythe set written explicitly fj message sent xi child xk xi xk xi xj delta delta delta yj xj xi message sent xi parent xj xi xj xi yk xk xi theta xj delta delta delta omit xj xj xi conditional probability mass function xi set received denote observations xk xi theta xj delta delta delta yj xj xi note xi xk xi expressions may glance cated really simple application general propagation rule states outgoing message sent edge computed product incoming messages edge local function variables tobe carried marginalized 
bayesian networks com mon coding applications propagation updates quite simple applications bayesian networks useful include variables continuous valued model forexample channel outputs 
strictly speaking variables enter framework de scribed parameter allow networks continuous valued variables provided variables observed correspond ing vertices children 
restricted child problems describing continuous conditional density function needed ss message sent avoided diagrams bayesian networks continuous valued observed vertices shown filled circles complexity probability propagation bayesian network way schedule depends manner kschischang frey iterative decoding probability propagation messages represented 
assuming mes sage vector values size size parent sample space computational complexity follows 
denote set children vertex andlet jd denote size set 
recall parents denoted 
jv denote number table corresponding local function andlet jvj denote size sample space message sent parent requires compu tation product incoming messages re quires order jvj delta jd operations ja delta jv operations sum jv products ja factors marginalize product ofthe incoming ss messages 
ss message sent child computed approximately jd delta 
total number operations approxi mately jvj delta jd ja jv usually domi nated second term 
conclude total number operations performed way sched ule bayesian network vertex set scales pv ja jv 
probability propagation bayesian networks cycles tactic deriving propagation tree structure ofthe bayesian network apply bayesian networks undirected cycles clearly bayesian form tree 
subsection show small example exact probability propagation possible expense greater unit thickness propagation tree 
may acceptable tradeoff simple net works may acceptable complicated networks 
xp xp xp xp xp fig 

bayesian network cycles factor graph prop graph corresponding propagation trees maximum thickness 
examples shown 
consider bayesian network shown fig 

pos sible propagation trees bayesian network shown figs 
thickness 
network possible achieve propagation tree unit thickness 
compound codes section defined compound code described interaction constituent codes tractably decodable 
graphically constituent code represented cycle free constituent bayesian network 
constituent networks share vari ables taken total bayesian network cycle free 
general exist unique tion compound code constituent codes compound code usually designed known 
bayesian networks known codes bayesian network systematic rate code shown fig 
cycle free constituent networks 
compound code consists chain type networks connected different ordering information symbol vertices 
upper chain di rectly uses information sequence lower chain uses sequence obtained applying interleaver 
systematic codeword component included constituent code 
wiberg probably describe turbo codes type graphical model 
fig 

bayesian networks compound codes systematic rate turbo code rate serially concatenated convolutional code low density parity check code product code 
serially concatenated convolutional compound code benedetto 
system essentially forney concatenated codes con inner outer codes 
bayesian network non systematic rate compound code sort fig 
cycle free constituent networks 
notice output outer convolutional code includ ing systematic part input inner convolutional code interleaver 
output code transmitted channel 
fig 
shows example gallager low density code 
code consists parity check restrictions subsets codeword symbols denoted 
information blocks codewords directly specified 
variable row variables codeword sym bol row binary indicator takes nonzero value configurations parents parity 
outputs indicated row codeword symbols 
mackay neal describe gallager bayesian networks 
fig 
shows example simple product code inwhich single parity bit checks parity row col ieee journal selected areas communications vol 
jan umn information bit array enclosed dotted box channel output symbols shown 
iterative decoding probability propagation constituent bayesian network compound cycle free probabilities random variable observed random variables efficiently computed constituent network 
compound network cycles probability propagation algorithm computing probabilities 
see general idea iterative decoding probability propagation algorithm constituent network ignoring cycles account 
graphical framework unifies iterative decoding algorithms 
turbo decoding algorithm separable map filter algorithm new iterative decoding algorithm decoding serially concatenated convolutional codes pointed mackay neal gallager algorithm decoding low density codes form probability propagation compound code networks shown fig 
decoding procedure essentially consists applying probability propagation ignoring graph cycles procedure broken series processing cycles 
cycle probabilities propagated particular constituent network producing current estimates distributions information symbols state variables symbols observed channel output 
cycle uses probability estimates produced previ ous cycle processing constituent network 
usually constituent codes processed order codes called iteration 
iteration essentially consists propagating probabilities network cycle free stopping vertex processed 
fact compound code network propagation procedure self terminates 
usually cyclic procedure allowed iterate ter criterion satisfied 
information symbols detected usually fashion maximum posteriori map symbol probability decoding rule 
turbo decoding probability propagation turbo codes example shown explicitly issue prob ability propagation algorithm applied turbo codes standard turbo decoding algorithm 
turbo decoding gorithm uses forward backward algorithm approximation process constituent trellis 
algo rithm uses extrinsic information produced previous step processing trellis 
infor mation passed trellis information symbols 
probability propagation terminology extrinsic information set parent child probability messages passed information symbols network response child parent messages received network fig 
shows message passing dynamics simplified turbo code bayesian network 
channel output 

probability propagation bayesian network turbo code 
polygon encloses steps single iteration 
black dots arrows represent pending messages 
served shown filled discs messages propagate tothe state vertices constituent networks creating pending messages incident edges indicated black dots 
messages codeword symbol likelihoods determined channel model channel output 
network processed time manner forward backward algorithm 
information probability messages constituent network just processed 
case single code information sym connected single edge propagation terminates 
case set messages constituent network messages extrinsic information 
messages pending procedure repeats basic iteration shown pictures outlined dashed polygon 
vi 
attempted unify various de veloped themes iterative decoding 
reviewed graphical codes models including markov random fields bayesian networks encode local probabilistic structure codes channels developed distributed marginalization algorithm general setting factor graph 
function written product local potential functions marginalization carried message propagation tree derived second higher power factor graph 
thickness edge equal number variables carried edge perform exact marginalization 
cycle free network propagation tree achieves maximum edge thickness unity 
key pearl belief probability propagation algorithm computes posteriori distribution exactly free bayesian network 
compound codes bayesian networks free 
networks broken tractable kschischang frey iterative decoding probability propagation subnetworks describing constituent code propagation applied 
iterating constituent decoders result excellent decoding practice demonstrated berrou 
shown proposed iterative decoders message passing graphical code model 
general straightforward exercise develop models coding schemes multilevel codes coset codes channels general usual memoryless channels 
believe possibilities application iterative decoding techniques described literature date 
acknowledgments grateful useful comments earlier version reviewers forney jr 
loeliger 
concept factor graphs generalization tanner graphs devised group isit ulm included authors 
loeliger mackay wiberg tanner 
grateful discussions topic aji forney jr otter mceliece 
berrou glavieux near shannon limit error correcting coding decoding turbo codes proc 
ieee int 
conf 
commun 
icc geneva switzerland pp 
forney jr concatenated codes 
cambridge ma mit press 
benedetto serial concatenation block codes electronics letters vol 
pp 

benedetto iterative decoding serially codes electronics letters vol 
pp 

gallager low density parity check codes 
cambridge ma press 
young hagenauer separable map fil ters decoding product concatenated codes proceedings ieee international conference communications pp 

hagenauer offer iterative decoding binary convolutional codes ieee transactions information theory vol 
pp 
frey kschischang probability propagation iterative decoding proc 
th annual allerton conf 
communication con trol computing allerton house monticello illinois pp 
october frey bayesian networks pattern classification data compression channel coding 
toronto canada department electrical engineering university toronto 
doctoral dissertation available www cs utoronto ca frey pearl fusion propagation structuring belief networks artificial intelligence vol 
pp 
lauritzen spiegelhalter local computations probabilities graphical structures application expert systems royal statistical society series vol 
pp 

pearl probabilistic reasoning intelligent systems networks sible inference 
san francisco ca morgan kaufmann 
revised second printing neapolitan probabilistic reasoning expert systems theory algorithms 
toronto john wiley sons jensen bayesian networks 
new york springer verlag lauritzen graphical models 
oxford university press 
mackay neal codes sparse ma cryptography coding 
th ima conference boyd ed lecture notes computer science pp 
berlin ger springer 
mackay neal near shannon limit performance parity check codes electronics letters vol 
pp 

reprinted vol 
march pp 
mackay error correcting codes sparse matrices 
submitted ieee transactions information theory mceliece mackay 
cheng turbo decoding instance pearl belief propagation algorithm 
appear ieee selected areas commun jan 
wiberg 
loeliger otter codes iterative general graphs european trans 
vol 
pp 
sep oct wiberg codes decoding general graphs 
phd thesis link oping university sweden tanner recursive approach low complexity codes ieee trans 
inform 
theory vol 
pp 
sept forney jr old new communications cryptography sides tapestry blahut costello jr maurer eds pp 
kluwer academic publishers forney jr forward backward algorithm proc 
th annual allerton conf 
communication control computing ton house monticello illinois pp 
october 
kindermann snell markov random fields applica tions 
providence rhode island american mathematical society 
preston gibbs states countable sets 
cambridge 
spatial point processes markov int 
stat 
rev vol 
pp 

hinton sejnowski learning relearning parallel distributed processing explorations microstructure cognition rumelhart mcclelland eds vol 
pp 
cambridge ma mit press 
aji mceliece general algorithm distributing formation graph proc 
ieee int 
symp 
inform 
theory ulm germany july poor dynamic programming models commutativity conditions siam control optimization vol 
pp 
july 
mceliece trellis linear block codes ieee trans actions information theory vol 
pp 
july 
schr statistical thermodynamics 
cambridge university press 
frey kschischang concurrent turbo decoding proc 
ieee int 
symp 
inform 
theory ulm germany bertsekas gallager data networks 
englewood cliffs new jersey prentice hall nd ed baum petrie statistical inference probabilistic functions finite state markov chains annals mathematical statistics vol 
pp 

bahl cocke jelinek raviv optimal decoding minimizing symbol error rate ieee transactions information theory vol 
pp 

frank kschischang associate professor inthe department electrical computer engineering university toronto 
academic year visiting scientist massachusetts institute technology 
received sc 
honors electrical engineering university british columbia sc 
ph electrical engineering university toronto respectively dr kschischang research interests focussed area coding techniques primarily soft decision decoding algorithms trellis structure codes iterative decoders 
teaches graduate courses information theory coding currently ieee transactions information theory associate editor coding theory 
brendan frey received ph electrical engineering university toronto member neural net works research group nserc science engineering scholar 
currently fel low beckman institute advanced science technology university illinois urbana champaign 
researches interesting approximate solutions hard problems digital communi cation machine learning 
particular interested applying graphical models asso ciated algorithms areas 
