hybrid discriminative generative approach modeling human activities jonathan lester choudhury kern gaetano borriello blake department electrical engineering university washington seattle wa usa intel research seattle seattle wa usa department computer science darmstadt university technology darmstadt germany department computer science university washington seattle wa usa accurate recognition tracking human activities important goal ubiquitous computing 
advances development multi modal wearable sensors enable gather rich datasets human activities 
problem automatically identifying useful features modeling activities remains largely unsolved 
hybrid approach recognizing activities combines boosting discriminatively select useful features learn ensemble static classifiers recognize different activities hidden markov models hmms capture temporal regularities smoothness activities 
tested activity recognition system hours wearable sensor data collected volunteers natural unconstrained environments 
models succeeded identifying small set maximally informative features able identify different human activities accuracy 
task modeling human activities body worn sensors received increasing attention years especially ubiquitous computing ubicomp field bao intille lukowicz patterson 
originally research activity recognition done vision sensors gavrila pentland increasingly dominated various types wearable sensors accelerometers audio 
fertile application domain activity recognition health care arena especially elder care support long term health monitoring assisting cognitive disorders 
addition activity recognition important component modeling higher level human behavior tracking routines rituals social interactions 
majority research wearable devices concentrated multiple sensors single modality typically accelerometers locations body bao intille kern 
placement sensors multiple pre defined locations quite obtrusive limitations approach 
ultimate goal embed devices clothing technology far commercially available widely accepted 
result single sensing device integrated existing mobile platforms cell phone appealing users garner greater user acceptance 
bao intille shown appropriate sensor subset locations effect recognition scores significantly compared system sensors single sensor reduced average accuracy 
hypothesis incorporating multiple sensor modalities offset information lost single sensing device 
furthermore multiple modalities better suited record rich perceptual cues environment cues single modality fails capture 
multiple modalities shown promise earlier activity recognition experiments lukowicz 
capture diverse cues movement sound light ongoing activities built small sensing unit sq 

includes different sensors accelerometer audio ir visible light highfrequency light barometric pressure humidity temperature compass 
sensors collected large annotated dataset various human activities volunteers period weeks 
compute different features sensor modalities attempt capture various attributes raw signal 
activity recognition choice sensors features derived driven human intuition easily available performance practicality 
right features crucial recognition 
working developing framework allows systematically identify modalities features useful machine recognition discrimination natural human activities 
want models accurately recognize track variety activities system lightweight run devices cell phones people carry 
minimizing computation cost recognition system important goal 
main approaches classification machine learning generative techniques model underlying distributions classes ii discriminative techniques focus learning class boundaries rubinstein hastie 
approaches extensively vision wearable sensing communities recognizing various human behavior activities 
hybrid approach combines techniques 
modified version adaboost proposed viola jones automatically select best features learn ensemble discriminative static classifiers activities wish recognize 
second classification margins static classifiers compute posterior probabilities inputs hmm models 
discriminative classifiers tuned different activities distinguishable hmm layer top static classification stage ensures temporal smoothness allows continuously track activities 
rest organized follows section provides overview activity recognition system 
section presents feature selection discriminative classifier training method 
section describes results classifiers combined hmms 
section describes experimental results performance system section discusses possible directions activity recognition system overview problem address systematic identification modalities features suited accurate recognition natural human activities 
second problem tackle features effectively develop models accurately recognize track various activities 
give brief overview different components activity recognition system 
sensing feature extraction shoulder mounted multi sensor board collect approximately samples data second 
reduce dimensionality bring details data compute total features include linear mel scale fft frequency coefficients cepstral coefficients spectral entropy band pass filter coefficients integrals mean variances 
combine features various sensors produce dimensional feature vector hz 
sensors different sampling rates multiple instances feature second window operate different portions data 
furthermore calculating features integral features incorporate longer time window varies seconds long minute 
restrict time windows data past system functions latency 
feature selection discriminative activity models earlier shown discriminative methods outperform generative models classification tasks ng jordan 
additionally techniques bagging boosting combine set weak classifiers improve accuracy fitting training data schapire 
viola jones shown boosting method combining classifiers method selecting discriminative features 
proposed approach select fraction total features train simple ensemble classifiers recognize broad set activities 
capturing temporal regularities activities people perform certain natural regularities temporal smoothness people abruptly switch back forth walking driving car history help predicting 
sequence posterior probabilities computed instantaneous classifiers train hidden markov models hmms significantly improve performance smoothness recognition system 
incorporating static classification results overcome weakness hmms effective classifiers jaakkola haussler 
selecting right features rich set sensor data features classifiers best select right features enable classifiers discriminate classes remove features useful confuse classifiers 
possible hand pick optimal features certain activities viable solution number activities large sensor signals intuitive 
scenarios automatic techniques finding right set features increasingly important 
practical system minimal number features simplest possible models needed high accuracy 
feature selection activity classification boosted decision stumps assume people engage different type activities 
set activities assume set training data activities 
sample training set consists feature vector extracted sensors 
activity flow diagram describing classification system 
sensor board records sequence raw sensor recordings compute feature vector 
pick top features class feature vector supply inputs ensemble decision stumps classifier 
decision stumps classifier outputs margin time 
sequence margins converted probabilities fitting sigmoid 
sequence probabilities supplied hmm classifiers outputs likelihood 
class highest likelihood classified class 
interested finding ranking feature set usefulness recognizing activity want find cut point ranked feature set adding features significantly improve accuracy classifier error error 
reason estimating reduce computational complexity classifiers extracting useful features 
final goal classifiers run devices users carry wear computational costs classifiers critical 
activity iteratively train ensemble weak binary classifiers obtain ranking features variation adaboost algorithm proposed viola jones 
weak classifiers constrained feature iteration boosting select feature associated weak learner minimizes training error weighted data 
error re weight data iteration compute weight 
process ranking features useful feature discriminating activities set weak classifiers weights classifiers final output weighted combination weak classifiers estimating error function number features find data point prediction sign classifier uses top features fraction total number features available tried different weak classifiers system discriminative decision stump ii generative na bayes model conditional probability distributions modeled histograms bins dimension weak classifier 
experiments decision stump consistently outperformed na bayes classifier results decision stump classifier sections 
decision stump finds optimal threshold feature fm minimizes weighted error 
boosted static classifiers classification margin data point reflect confidence prediction schapire 
margin example weighted fraction weak classifiers votes assigned correct class 
mh sign constructing classifiers output posterior probability useful especially want combine results multiple classifiers 
method computing posterior probability directly fit sigmoid function output ensemble classifier platt 
case posterior probabilities derived follows constant static classifier predicts label data point independently 
time independence assumption clearly invalid prediction previous data points help current classification 
temporal model uses confidence predictions classifiers cs raw features greater impact performance 
ability recognize activities continuous time chunks allow learn people transition activities allow learn people behavior activity patterns 
section describe combine confidence values static classifiers build time series models activities 
incorporating prediction history hidden markov models hmms successfully modeling different types time series data speech recognition gesture testing error rates class function number features selected 
features selected testing errors classes leveled 
data graphed averaged smaller feature selection runs 
tracking hmms capture temporal dynamics directly raw features selected previous section trained hmms posterior probabilities static classifiers 
advantage posterior probabilities take advantage results discriminatively trained classifier reduce complexity hmms 
earlier jaakkola haussler shown benefits combining generative models discriminative classifiers deriving kernel functions probability models 
clarkson pentland oliver output hmm model input hmm 
yin output static classifiers directly hmms speech reading application compute margin class posterior probability classifiers effective raw outputs platt 
activity new hmm model learned sequence examples individual instances 
construct new input feature space posterior class probabilities set observations activity learn parameters hmm standard expectation maximization em method 
testing continuous sequence compute likelihood value time sliding window duration final segmentation classification hmm highest likelihood value max 
alternatively trained various states single hmm recognize different activity classes learn transition characteristic activities 
choose activities primitive single transition statistic meaningful 
believe output hmms train single dynamic model complex behavior transition statistics informative 
experiments accelerometer ambient light ir vis audio barometric pressure digital compass hi freq vis light ir light relative humidity temp 
temp 
relative humidity visible light table percentage features top originated different sensors averaged activity classes 
validate approach recorded hours data consisting large number activities sitting walking jogging riding bike driving car wearable multi sensor board 
dataset collected multiple days volunteers researchers various indoor outdoor locations 
recordings done long stretches hour average duration activities ranged seconds entering building hours driving car 
volunteers asked go performing series activities naturally specific constraints order example go building walk inside 
capture day day variations activities collected multiple instances various activities course weeks 
average hour data activity instances activity 
feature selection feature selection stage selected total data available class training 
training examples derived ranking features activity individually boosted decision stump procedure described section 
shows testing error function number features classification 
results see classification error tapers features classes 
pick features top performance improves slightly testing error improving features 
practical advantage features selection significantly reduce computational burden resource constrained devices drastically affecting output static decision stumps classifiers hz hmm classifiers trained output probabilities static classifiers continuous minute segment data 
results overlaid top ground truth obtained annotating video recorded worn volunteers 
video determining ground truth additional sensor input 
performance 
performing boosting reweighting data selecting discriminatory features successively error perform better non boosted re weighting approach selecting best features 
accuracy true positive true negative total examples boosted features selection average higher non boosted method 
table lists contribution different sensors final classifier 
majority top features came accelerometer audio barometric pressure sensors 
barometric pressure data useful distinguishing activities involved floor transitions walking stairs elevator sensor sensitive pick pressure differences single floor 
static classification results top features tested performance ensemble classifier different weak classifiers decision stump discriminative ii na bayes generative 
total duration test dataset half hours 
decision stumps outperformed na bayes classifiers large percentage 
table shows precision true positive true positive false positive recall true positive true positive false negative numbers activities dataset ensemble decision stumps 
table lists average precision recall numbers na bayes decision stump classifiers 
continuous classification results decision stumps results table quite illustrates classification errors encountered continuous trace 
majority trace tends correctly classified decision stumps scattered misclassifications 
addition hmm layer top static classifier helps smooth classification errors shown line 
parameters hmms trained example scenes average minutes scenes class 
hmm hidden states gaussian observation probabilities 
classification performed second sliding window second overlap 
table shows sliding window performance results hmm posterior probabilities inputs tested concatenated test scenes 
accuracy case 
interesting note points hmm ground truth differ appear somewhat natural realistic example classifying region ground truth walking sitting standing 
fact hmm output reveals deficiencies ground truth 
example segments ground truth marked walking fact standing determined post analysis video experimenters correctly recognized standing hmm 
compare performance standard hmm approach trained new set hmms top raw features inputs output static classifiers 
performance hmms precision recall labeled activities labeled activities sitting standing walking jogging walking classified activity decision stumps walking riding driving stairs bicycle car stairs riding riding elevator elevator sitting standing walking jogging walking stairs walking stairs riding bicycle driving car riding elevator riding elevator sitting standing walking jogging walking stairs walking stairs riding bicycle driving car riding elevator riding elevator table precision recall numbers decision stumps classifier 
randomly selected set data set aside test results 
accuracy 
precision recall sitting standing walking jogging walking classified activity hmm walking riding driving stairs bicycle car stairs riding riding elevator elevator sitting standing walking jogging walking stairs walking stairs riding bicycle driving car riding elevator riding elevator labeled activities labeled activities sitting standing walking jogging walking stairs walking stairs riding bicycle driving car riding elevator riding elevator table precision recall numbers hmm classifier posterior probabilities inputs 
accuracy 
na decision hmm hmm bayes stumps decision stumps raw features precision recall table precision recall numbers various generative discriminative classifiers evaluating system 
significantly worse worse static classifier demonstrating importance discriminative classifiers distinguishing activities see table comparison precision recall numbers various classifiers experiments 
recognize modeling complex activities may require generative model personal behavior 
believe discriminative classifiers map sensor data primitive activity classes reduce large amount sensor noise allow learn complex behaviors effectively 
problem recognizing human activities sensor data presents diverse statistical challenges different classes actions need actively distinguished model needs incorporate fact people actions extended time 
study approached problems combining discriminative power ensemble decision stumps generative temporal powers hmms 
results shown combination discriminative generative classifiers effective classifiers 
hmm top discriminative classifier perform better discriminative classifier produces smooth accurate outputs shows 
feature selection plays important role system improving performance classifier creating practical system 
selection best features top classes leaves features original 
reduces number features necessary significant computational saving 
savings improved optimizing calculation features take advantage new subset example efficient techniques obtain fft coefficients required sub bands 
addition subset reduced allowing feature selection process determine optimal stopping point 
lays directions automatic recognition human actions pursued 
multi modal wearable sensors provide cheap lightweight unobtrusive means obtaining richly detailed data unconstrained environments long periods time 
second richness sensor readings mass data collected demand suitable preprocessing data reduction techniques applied 
selecting informative features computational costs cut greater savings probably attainable 
third multi faceted nature human activities presents opportunities multiple machine learning approaches combined complementary strengths different approaches instance boosted decision stumps hmms meeting different aspects computational challenge 
initial studies yielded high recognition rates suggesting fruitful approach 
focus incorporating building techniques recognize complex behavior patterns cooking cleaning development ideas hope lead activity recognition systems move research lab real world offering applications areas diverse smart rooms ethnography health care young aging population 
acknowledgments authors dieter fox helpful comments discussions 
undergraduate students collected hours data experiments 
bao intille bao intille 
activity recognition user annotated acceleration data 
proc 
pervasive vienna austria 
clarkson pentland clarkson pentland 
unsupervised clustering ambulatory audio video 
proceedings international conference acoustics speech signal processing phoenix az 
gavrila gavrila 
visual analysis human movement survey 
computer vision image understanding 

algorithm evaluation finite trigonometric series 
american mathematical monthly 
jaakkola haussler jaakkola haussler 
exploiting generative models discriminative classifiers 
advances neural information processing systems 
kern kern schiele multi sensor activity context detection wearable computing 
proc 
lncs eindhoven netherlands 
lukowicz paul lukowicz junker distributed multi sensor system context aware wearables 
proceedings th international conference ubiquitous computing 
teborg sweden springer verlag 
lukowicz paul lukowicz jamie ward recognizing workshop activity body worn microphones accelerometers 
pervasive computing proceedings nd international conference 
ng jordan ng jordan 
discriminative vs generative classifiers comparison logistic regression naive bayes 
advances neural information processing systems 
oliver oliver horvitz layered representations human activity recognition 
fourth ieee int 
conf 
multimodal interfaces 
patterson patterson liao inferring highlevel behavior 
proceedings fifth annual ubiquitous computing seattle washington usa 
pentland pentland 
smart rooms 
scientific american 
platt platt 
probabilities sv machines 
advances large margin classifiers 
smola bartlett scholkopf schuurmans mit press 
rubinstein hastie rubinstein hastie 
discriminative vs informative learning 
proceedings knowledge discovery data mining 
schapire schapire 
brief boosting 
sixteenth international joint conference artificial intelligence 
schapire robert schapire yoav freund boosting margin new explanation effectiveness voting methods 
viola jones paul viola michael jones 
rapid object detection boosted cascade simple features 
computer vision pattern recognition 
yin yin essa asymmetrically boosted hmm speech reading 
ieee computer society conference computer vision pattern recognition cvpr pp ii washington dc usa 
