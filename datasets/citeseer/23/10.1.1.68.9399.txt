web reduce data sparseness pattern information extraction sebastian philipp cimiano institute aifb university karlsruhe germany cimiano aifb uni karlsruhe de 
textual patterns effectively extract information large text collections 
rely heavily textual redundancy sense facts mentioned similar manner order generalized textual pattern 
data sparseness problem trying extract information hardly redundant sources corporate intranets encyclopedic works scientific databases 
results applying weakly supervised pattern induction algorithm wikipedia extract instances arbitrary relations 
particular apply different configurations basic algorithm pattern induction different datasets 
show lack redundancy leads need large amount training data integrating web extraction process leads significant reduction required training data maintaining accuracy wikipedia 
particular show web similar effects produced increasing number seeds leads better results 
approach allows combine advantages sources high reliability closed corpus high redundancy web 
techniques automatic information extraction text play crucial role scenarios manually scanning texts certain information unfeasible costly 
nowadays information extraction example applied biochemical texts discover unknown interactions proteins compare texts available corporate intranets purpose knowledge management compare 
state art systems textual patterns extract relevant information 
textual patterns essence regular expressions defined different levels linguistic analysis 
approach rely simple regular expressions defined string tokens 
extraction systems easily adaptable domain scenario considerable research devoted automatic induction patterns compare :10.1.1.143.8051
due fact patterns typically induced specific corpus approach course affected problem data sparseness problem data learn relevant patterns 
computational linguistics community shown web copyright springer proceedings th european conference principles practice knowledge discovery databases pkdd cases effectively overcome data sparseness problems compare 
explore web effectively help overcome data sparseness supplementary data source information extraction limited corpora 
particular build weakly supervised pattern learning approach patterns derived basis seed examples 
bootstrapping approach induces patterns matches corpus extract new tuples alternates process iterations 
approach investigated applied web see local corpus :10.1.1.101.3197
combine advantages sources high reliability closed corpus high redundancy web 
idea follows seed examples 
aris rance specific relation locatedin extracted appearing local corpus consult web patterns examples appear 
newly derived patterns essence generalization plain string occurrences tuples matched web order extract new examples taken iteration seeds 
search patterns increased set examples coming web corpus effectively leading patterns 
experiment different variations basic pattern induction algorithm different relation datasets 
experiments show hand lack redundancy definitely compensated increasing number seeds provided system 
hand usage web yields better results rely provision training data system form seeds 
wikipedia local corpus access web google api 
section motivate need approach overcome data sparseness quantitatively qualitatively giving examples 
section bootstrapping approach pattern induction alternates usage local corpus web implementation system 
section presents experiments results 
concluding discuss related section 
motivation specialized text corpora intranets collections scientific papers non redundant design 
constitute valuable source information extraction typically reliable focussed general web cf 
analysis structure content corporate intranets 
experiments wikipedia non redundant highly reliable somewhat specialized text collection limited size freely accessible entire community 
observation wikipedia hardly redundant 
computed number page occurrences instances test relations google result count estimates searches individual relation instances limited wikipedia site 
result relation instances en wikipedia org occur times median 
doing counts entire web hardly instance occurs times median lies 
effect increases considering page occurrence suffice relation instance extracted 
patterns match limited context 
case match tokens link relating document title 
reduces number times candidate relation instance occurs corpus dramatically average derived counting number times top relation instances relation occur index 
goal assess effectively web extraction serve background knowledge extraction smaller corpus 
web extract additional information lack redundancy small corpus 
particular information web goes result set verified small corpus benefits smaller corpus higher quality domain specificity availability background knowledge lost 
follows describe approach detail 
approach system uses generic pattern learning algorithm typically applied web 
works analogously approaches mentioned implementing similar bootstrapping procedure 
system previously described detail 
algorithm starts set initial tuples relation question called seeds loops procedure starts acquiring occurrences tuples currently patterns learned abstracting text occurrences tuples 
new patterns evaluated filtered matched 
matches new tuples extracted evaluated filtered 
process stopped termination condition done fulfilled typically fixed number iterations set 
learning inductive nature abstracting individual positive examples bottom manner 
learning essentially takes place generate test manner 
describes modification algorithm 
basically consists subsequent application loop body web wiki 
web matching wiki matching contribute evolving set tuples maintain separate pattern pools 
separation done allow different types pattern representation different corpora 
important novelty checking tuple derived web wiki 
ensures knowledge wikipedia goes set results 
extraction procedure able benefit higher quality terms precision wiki corpus assumed 
extraction web number seeds start algorithm iterations occurrences seed tuples searched web 
example tuple web wiki pattern induction done web match tuples learn patterns evaluate web patterns web pattern filter condition web match patterns extract tuples wiki evaluate web tuples tuple filter condition wiki match tuples learn patterns evaluate wiki patterns wiki pattern filter condition wiki match patterns extract tuples evaluate wiki tuples tuple filter condition fig 

combined web wiki pattern induction algorithm starting initial patterns tuples maintaining pattern pools stockholm sweden locatedin relation query sent google web search api stockholm sweden instance locatedin relation fixed number results retrieved maximum instances 
occurrences serve input pattern learning arguments tokens apart 
experiments chose 
learn patterns generates versions patterns 
take generate test approach learning 
learn patterns produces large amount patterns combining merging sets occurrences keeping common tokens replacing tokens patterns differ wildcards 
generalization effectively calculating general generalization lgg patterns typically done bottom ilp approaches compare 
avoid general patterns minimum number non wildcard tokens enforced 
avoid specific patterns required merged occurrences reflect different tuples 
evaluate web patterns assigns confidence score pattern 
confidence score derived number different tuples pattern derived merging 
measure performs better strategies shown 
evaluation followed filtering applying web pattern filter condition ensures top patterns kept 
note patterns kept iterations old patterns compete newly derived ones iteration 
evaluate web patterns matches filtered pattern set web retrieving results pattern 
pattern flights arg arg airport locatedin relation translated google query follows flights airport subsequent selective matching step enforces case punctuation ignored google 
occurrences stored extract tuples extracts relevant relation instances identifying occurs positions arg arg 
experiments chose 
mentioned wiki check ensures web extractions corresponding link title pair wikipedia eliminated 
way high quality content wikipedia filter web results instances kept principle extracted wikipedia 
web results increase yield extraction process 
parameters employed determined extensive initial tests 
extraction wikipedia section presents approach pattern matching relation extraction wikipedia 
describe pattern structure index creation going detail individual steps algorithm 
pattern matching wikipedia encyclopedic nature corpus limiting focussing pairs hyperlinks document titles 
common assumption investigating semantics documents wikipedia key information entity described page lies set links page particular salient semantic relation 
consider patterns consisting document title hyperlink context 
context tokens link taken account assume context indicative nature semantic relation expressed entity described article linked hyperlink 
addition flag set indicate second argument relation occurs title 
token required equal particular string hold wildcard character 
experiments chose 
allow efficient matching patterns tuples created index hyperlinks wikipedia 
created database table row title link pair featuring column link title context token position 
link created wiki syntax version document texts database dump december th 
table records 
omitted entries links lying templates maintain generality templates special syntactic feature wikipedia may transfer similar corpora 
tokenization done white space 
hyperlinks considered token 
punctuation characters common sequences punctuation characters html markup sequences considered separate tokens separated white space 
html comments templates omitted 
tuple matching pattern learning tuples wiki match tuples sends queries index 
possibility map argument title link 
web case maximum limit matches hardly enforced virtually tuple mentioned times link title pair 
learn patterns method applied web setting 
web setting evaluate wiki patterns takes account number distinct tuples participated creation pattern 
wiki pattern filter condition retains top patterns matching 
pattern matching tuple generation wiki match patterns retrieves index random sequence matches pattern selecting entries non wildcard context tokens patterns correct positions 
extract tuples generates tuple instance distinct title link pair occurring selected index entries 
evaluate wiki tuples tuple filter condition currently enabled maximize yield wiki 
termination condition done currently implemented terminate processing iterations 
summary extraction web wiki index follow basic procedure 
parameters adapted different levels redundancy text collections 
addition pattern structure patterns chosen different allow link title matches wiki window occurrences web 
wiki check ensures web facilitates extraction provide knowledge wiki 
evaluation goal study show information extraction web improve extraction results smaller corpus extraction precise specialized corpus benefit noisy redundant source 
running system configurations employing web extraction additional baseline condition 
assumption web extraction lack redundancy particularly important bootstrapping process compare different configurations behave provided smaller bigger amounts seed examples 
datasets selection seed instances automatic evaluation results data sets consisting extensions relations created titles music albums artists generated wikipedia category albums artist 
persons year birth generated wikipedia category births year 
countries official currency daml export cia world fact book manual modifications done reflect euro official currency european countries 
names companies country generated wikipedia category companies country 
locatedin names cities state federal states located generated wikipedia category cities countries 
note considerable number cities contained data set state federal state 
vehicle product names brand names makers generated wikipedia category vehicles brand 
soccer players national teams playing 
important note wikipedia collections compiled manually authors assigned documents respective categories checked community members 
datasets regarded high quality 
due vast coverage wikipedia extensions relations assumed relatively complete 
described datasets obtained wikipedia automatically resolving category membership help tool daniel 
applied iteratively obtain members sub categories 
data sets chosen differ various dimensions notably size 
dataset example relatively small constitutes relation clear boundaries 
relations reflected fully data sets 
small samples size datasets taken input seeds 
exceptions took number links wikipedia articles mentioned www daml org factbook data set courtesy consortium see www 
project de 
meta org wiki user cities took average living costs indicator ensure athens greece ranked higher athens new york 
population skewed sample asian cities prominently mentioned english wikipedia 
albums required titles characters length discourage titles heart friends tuple indicator significance corpus selected top samples respect harmonic mean counts 
initial tests showed prominent instances seeds strongly increases system performance random seeds 
expected real scenarios prominent seeds available best known users 
experimental conditions assess added value web extraction compare configurations algorithm 
dual exactly formalized condition iterates bootstrapping performing web wiki extraction iteration 
web processing runs lines executed iteration 
seed set augmented set learned relation instances 
processing left wikipedia extraction 
wiki baseline condition extraction done wikipedia 
line omitted entirely 
simplified respect 
initial tests revealed performing wiki filter iteration strict bootstrapping 
decided apply filter third iteration considerable number correct instances filtered applying filter 
consequently results iteration comparability reasons 
performed extraction configurations iterations varying size seed set 
presenting prominent relation instances seed sets test different configurations affect system ability bootstrap extraction process 
evaluation measures experiments rely widely recision measures evaluate system output 
measures compute ratio correctly instances tuples extracted precision tuples recall 
fixed number iterations experiments poses fixed limit number possible extractions notion assuming maximally extracted number tuples configuration iteration relation 
yr yield number extractions correct incorrect iteration relation method pr precision respectively formalize relative recall rrr yr pr maxi yr filter applied tuples lead presence non wiki patterns final results 
non wiki patterns help bootstrapping eliminated 
fig 

precision relative recall measure results derived different configurations seed set sizes 
grayed columns indicative due low recall results largely consist seed set 
mark indicate performance statistically significantly worse runs 
measure precisely measure combination precision recall harmonic mean 
results presents results extraction runs different configurations starting seed sets different sizes 
figures show precision relative recall fmeasure iterations extraction algorithm 
scores averaged performance relations testbed 
precision web supported configurations ranges depending configuration 
grayed precision bars wiki conditions seeds output contains largely seed tuples seeds seeds accounts precision score 
observe purely wiki extraction performs bad seeds far optimal seeds 
sided pairwise student test indicates fact wiki strategy performs significantly worse web configurations seed set size seed set size 
clearly corroborates claim integration web improves results respect wiki strategy seeds 
shows number correctly extracted tuples averaged test relations iterations 
seeds provided training 
wiki configuration square markers system able quickly derive large number instances shows slow increase knowledge iteration 
fig 

correct yield counts iterations 
triangles mark web results diamonds dual squares wiki 
strong lines indicate results seeds 
results seeds higher lower 
configurations show stronger incline iterations 
confirms expected assumption low number results extracting solely wiki due early convergence process 
interesting observe web condition slightly outperforms dual condition 
allows assume major benefit integrating web process lies initial extension seed set 
investigation observation require iterations modifications configuration 
conclude setting web background knowledge allows produce recall hardly redundant corpora maintaining precision level 
larger seed set compensate lack redundancy 
related iterative induction textual patterns method widely large scale information extraction 
sergey brin pioneered web search indices purpose 
successful systems include knowitall extended automatic learning patterns espresso 
espresso tested typical taxonomic part relations political succession chemical reaction relations 
precision ranges relations 
setup uses algorithms similar described knowitall able reach limited task named entity classification 
apart pattern approaches variety supervised semi supervised classification algorithms applied relation extraction 
methods include kernel methods graph labeling techniques 
advantage methods abstraction partial matches inherent features learning algorithm 
addition kernels allow incorporating complex structures parse trees reflected text patterns 
classifiers require testing possible relation instances text patterns extraction significantly speeded search indices 
classification requires linear time processing corpus search patterns lead faster extraction 
study wikipedia corpus 
simulate intranet scenario shares wikipedia properties spam prone redundant world wide web 
wikipedia currently widely corpus information extraction text 
example study focus high precision ontology learning population methods specifically tailored wikipedia 
wikipedia category system exploited assuming typical composition categories allow deduce semantic relations category membership 
information extraction wikipedia text done hyperlinks indicators relations just study 
opposed relies wordnet hand crafted formal taxonomy limited relations sources exist 
precision achieved comparable results relative relations tests performed 
results indicate web information extraction help improving extraction results task hand requires extraction closed non redundant corpus 
particular showed extraction seed examples incorporating web background knowledge better results achieved seeds solely wikipedia 
potential approach lies fact additional information require formalization wordnet limited particular domain 
studies improve results including additional techniques part speech tagging named entity tagging omitted maintain generality study 
addition title link pairs considered indicators relatedness considered increase coverage 
see applications derived results domains science particular fields research focussed relations protein interaction large non redundant text collections available 
authors egon technical assistance wikipedia clone 
funded media project www org sponsored european commission part information society technologies ist program ec number ist fp 
google giving enhanced access api 

agichtein gravano 
snowball extracting relations large plain text collections 
proceedings fifth acm conference digital libraries dl pages 

cimiano 
harvesting relations web impact filtering functions 
proceedings nd international conference association advancement artificial intelligence aaai 
appear 

brin 
extracting patterns relations world wide web 
proceedings webdb workshop th international conference extending database technology edbt 

chen ji tan niu 
relation extraction label propagation semisupervised learning 
proceedings st international conference computational linguistics coling th annual meeting association computational linguistics acl pages 

ciravegna 
adaptive information extraction text rule induction generalisation 
proceedings international joint conference artificial intelligence ijcai pages 

sorensen 
dependency tree kernels relation extraction 
proceedings nd meeting association computational linguistics acl pages 

downey etzioni soderland weld 
learning text patterns web information extraction assessment 
proceedings aaai workshop adaptive text extraction mining 

fagin kumar mccurley novak sivakumar tomlin williamson 
searching workplace web 
proceedings th international conference world wide web www pages 
acm press 

grefenstette editors 
special issue web corpus volume journal computational linguistics 
mit press 

muggleton feng 
efficient induction logic programs 
proceedings st conference algorithmic learning theory pages 

pantel 
espresso leveraging generic patterns automatically harvesting semantic relations 
proceedings st international conference computational linguistics coling th annual meeting association computational linguistics acl pages 

ruiz alfonseca castells 
automatic extraction semantic relationships wordnet means pattern learning wikipedia 
natural language processing information systems 
springer berlin heidelberg may 

jensen rojas 
extraction regulatory gene expression networks pubmed 
proceedings annual meeting association computational linguistics acl pages 

soderland 
learning information extraction rules semi structured free text 
machine learning 

weikum 
core semantic knowledge 
proceedings th international conference world wide web www pages 
acm press 

cimiano handschuh vargas vera motta ciravegna 
semantic annotation knowledge management requirements survey state art 
journal web semantics science services agents world wide web 

kr tzsch haller studer 
semantic wikipedia 
proceedings th international conference world wide web www pages 

aone 
kernel methods relation extraction 
journal machine learning research 
