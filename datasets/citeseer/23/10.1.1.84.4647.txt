inverse multi objective robust evolutionary design lim yew soon ong jin bernhard sendhoff bu sung lee school computer engineering nanyang technological university nanyang avenue singapore ntu edu sg honda research institute europe gmbh carl germany jin bernhard sendhoff honda ri de 
inverse multi objective robust evolutionary imore design methodology handles presence uncertainty making assumptions uncertainty structure 
model clustering uncertain events families nested sets multi level optimization search 
reduce high computational costs proposed methodology proposed schemes adapting step size estimating uncertainty trimming number calls objective function nested search 
offline online adaptation strategies considered conjunction imore design algorithm 
design experiments doe approaches reduce number objective function calls online adaptive imore algorithm 
empirical studies conducted series test functions having diverse complexities show proposed algorithms converge pareto set design solutions non dominated nominal robustness performances efficiently 
evolutionary algorithms eas modern stochastic optimization technique emerged prominent contender global optimization complex engineering design 
popularity lies ease implementation ability arrive close global optimum design 
studies application eas complex engineering design focused locating global optimal design deterministic computational models 
real world design problems uncertainties practically impossible avoid 
case solution sensitive small variations design variables operating conditions may desirable design 
optimization uncertainty considerations produce designs labeled optimal perform differently put practice 
various classifications uncertainty design optimization suggested years 
types uncertainty described 
noise fitness function uncertainty design environmental parameters approximation errors fitness function time varying fitness function 
similar categorization 
classify uncertainty epistemic 
uncertainty refers naturally irreducible variability quantities inherently variable time space 
contrast epistemic uncertainty caused incomplete knowledge designs optimized reducible knowledge acquired 
uncertainty defined gap known unknown facts 
follow categorization uncertainty 
particular focus uncertainty design environmental parameters 
date approaches exist coping uncertainty complex engineering design optimization 
include time experiments taguchi orthogonal arrays bounds fuzzy probabilistic methods 
detail analysis deterministic optimization framework dealing uncertainty linear programming general convex programming 
context stochastic optimization especially evolutionary algorithms number prominent new studies handling presence uncertainty engineering designs emerged 
genetic algorithm robust searching scheme ga rs introduced 
probabilistic noise vector added genotype fitness evaluations 
study evolutionary strategy es isotropic normal mutations noisy phenotype scheme subsequently reported 
considers trade robustness nominal performance potential solution multi objective ea approach described combined max min baldwinian trust region optimization strategy conservative robust design 
reduce high computational costs robust evolutionary design computationally cheap local surrogate models introduced estimating expected fitness variance potential solutions place exact fitness functions 
success robust evolutionary design shown series realistic mechanical aerodynamic problems including aerodynamic airfoil lightweight space structures multilayer optical coating design 
existing schemes proposed prior knowledge structure uncertainty instance distribution properties assumed available 
quality solution attainable assumptions structure uncertainty exactly reflect actual uncertainty 
evolutionary design optimization handles presence uncertainty view desired robust performance call inverse multi objective basic strategies memetic algorithms lamarckian learning forces genotype reflect result improvement local search placing locally improved individual back population compete reproductive opportunities baldwinian learning alters fitness individuals improved genotype encoded back population 
robust evolutionary design imore short 
contrast conventional robust optimization proposed approach avoids making assumptions uncertainty structure formulating optimization process lead erroneous designs catastrophic consequences 
drawback imore methodology massive computational effort nested evolutionary searches involved defined step size maximum fitness function calls 
improve efficiency proposed methodology offline online strategies adapting step size minimizing calls fitness function design experiments doe sampling methods 
rest organized follows 
section provide overview robust evolutionary design proposed imore methodology 
section introduces adaptation strategies improving computational efficiency imore 
illustrate efficacy adaptive imore section provides empirical study series test functions diverse complexities 
enhancements speed adaptive imore algorithm doe methods section 
section concludes 
robust evolutionary optimization presence uncertainty section brief overview robust evolutionary design presence uncertainties 
particular consider general bound constrained nonlinear programming problem form maximize subject xl scalar valued objective function vector design vari ables xu vectors lower upper bounds design variables 
focus eas robust design optimization presence uncertainty arises design parameters noise vector design parameters perturbed function value design vector ii operating environmental conditions nominal vector environmental parameters random vector model variability operating conditions 
forms uncertainties may treated equivalently 
core mechanism evolutionary techniques handling uncertainty relied probability theory assuming prior knowledge structure uncertainty 
example uncertainties assumed gaussian normal cauchy uniform distribution 
gaussian noise zero mean variance considered virtue central limit theorem effective fitness defined probability distribution practice approxi mated monte carlo simulation mcs assuming samples noise term follows locate robust design solution presence uncertainty design vector may consider ga rs proposed outlined 
central limit theorem random samples distribution mean variance approach gaussian normal distribution sample size increases 

genetic algorithm robust solution searching scheme ga rs consider dimensional function depicted 
ea maximization problem generate population designs termination condition satisfied individual population perturb individual arrive ij 


evaluate xij xij determine effective fitness xi individual ij apply mutation crossover create new population perform selection individuals 
ea 







equation defines multimodal function nominal global maximum located sharp peak local optima located robust optimal solution ga rs converges dependent perturbation assumed assumptions distribution 
instance figures illustrate resultant effective fitness landscapes denoted circles dimensional function defined equation assuming uniform distribution respectively 
note denotes range bound 
configured robust global maximum observed located 
hand set global robust maximum approaches nominal fitness function see 
note represents nominal global optimum maximum 
note represents robust global optimum maximum 
range uncertainty range uncertainty 
effective fitness function defined eq 
assuming uniform distribution inverse multi objective robust evolutionary imore design optimization section inverse multi objective robust evolutionary imore design optimization strategy locating solutions non dominated nominal performance robustness presence uncertainties 
contrast existing robust ea schemes assumption uncertainty involved little knowledge structure uncertainty involved available priori realistic problems 
focusing making probably mathematical model uncertainty imore focus design may deteriorate presence uncertainties 
account necessity deal trade robustness nominal fitness imore consider bound constrained multi objective optimization problem form maximize objective nominal fitness objective robustness dt subject xl basic steps proposed imore algorithm outlined 
step maximum degradation tolerable final design dt step size conduct nested searches initialized 
population designs generated randomly doe methods latin hypercube sampling minimum discrepancy sequences 
individual population evaluated obtain nominal fitness 
subsequently individual undergoes sequence nested searches establish uncertainty maximum variations design parameters maximum performance degradation tolerable spirit info gap theory 
particular solve sequence constrained optimization subproblems chromosome form maximize subject xl appropriate bounds design parameters updated iteration defined step size optimization sub problem th iteration optimal solution th sub problem sought 
objective sub problem search find worst possible performance degradation solving bound constrained maximization problem 
iteration search bounds design parame ters updated step size xl xu conducting sequence nested searches family ascending nested bounds parameterized uncertainty vector arrive monotonically increas ing function performance degradation versus uncertainty illustrated opt opt opt denotes optimum worst case design vector th iteration opt opt corresponding maximum performance degradation obtained addition opt obtained associated search iteration stored create database uncertainties corresponding performance degradations 
example consider design point set fig opt opt ure labeled points correspond respectively 
individual iterative searches terminate optimal solution th sub problem exceeds maximum degradation defined opt imore consider maximizing problem initialization phase initialize maximum degradation tolerable final design dt initialize step size nested search generate population design vectors search phase termination condition satisfied individual population objective nominal fitness objective robustness dt max repeat maximize xi xi subject xl xl xu obtain opt opt store opt associate opt dt estimate maximum uncertainty linear interpolation max opt different apply standard moea operators create new population imore 
imore design optimization algorithm 
iterative search maximum uncertainty design may max handle maximum performance degradation tolerable interpolated database previous uncertainties corresponding maximum formance degradations illustrated represents point maximum performance degradation reached corresponding maximum uncertainty design max handle 
imore search proceeds multi objective evolutionary operators create new population stops termination condition met 
nominal fitness uncertainty max input design variable performance degradation 
steps imore xi 
illustrate results obtained imore algorithm may useful robust design consider multimodal test function onedimensional michalewicz function defined ix sin sin dt 
pareto front corresponding offspring function eq 

function contains mixture flat robust region having moderate nominal fitness noisy peaks nominal fitness depicted 
solutions pareto front represent diverse set designs having non dominated nominal performances robustness presence uncertainties 
explain results cluster solutions pareto front separate groups 
group consists solutions excellent nominal fitness expense poor robustness 
hand group solutions gives balance nominal fitness robustness solution members group poor nominal fitness excellent robustness measure 
case real world engineering problem availability set non dominated solution provide wide range option selecting design vectors requirements robustness nominal fitness 
adaptive inverse multi objective robust evolutionary design optimization section study computational complexity proposed imore methodology subsequently introducing possible strategies achieve better efficiency minimum impact performance algorithm 
computational complexity imore algorithm described section maximum number imore generations number individuals average number nested search iterations required individual reach average number function evaluations incurred nested max search 
computational costs locate pareto optimal solutions intractable objective function computationally expensive 
approx individual max represents approximated robustness fitness exact imore max accurate robustness fitness obtained exhaustive search average approximated robustness aar average exact robustness aer ea population measured approx max aar max aer population size xu xl upper lower bounds search space respectively 
standard evolutionary parameters imore additional control parameter inversely proportional illustrate effect varying imore algorithm searching solutions michalewicz dimensional function 
aar aer population differing step size imore tabulated table 
results worth noting average error robustness accuracy typical population aar aer aer varies greatly observed average error estimating robustness increases incurs lower computational cost due smaller sense larger step size generally gives rise greater interpolation errors 
accuracy lead imore search convergence false pareto optimal solutions 
hand fine step size provides lower average error expense higher computational costs 
number iterations inversely proportional step size intuitive way exact reduce search time evolutionary optimization algorithm achieve appropriate balance imore search 
table 
average approximated robustness aar average exact robustness aer imore population different step size applied test function eq 

step size average number iterations average approximated robustness aar average exact robustness aer aar aer aer subsections introduce offline online adaptive imore robust design presence uncertainty 
offline adaptive imore design optimization sub section offline adaptive imore optimization algorithm robust design presence uncertainty 
basic steps proposed adaptive algorithm outlined 
offline adaptive imore consider maximizing problem initialization phase initialize maximum degradation tolerable final design dt 
initialize levels step sizes 
initialize step size update interval maximum number imore search generations termination 
generate population design vectors 
search phase termination condition satisfied non duplicated individual population current generation objective nominal fitness xi objective robustness max obtained procedure described 
apply standard moea operators create new population offline adaptive imore 
offline adaptive imore design optimization algorithm 
initialization phase offline adaptive imore search levels stepsizes defined high robustness accuracy may exceedingly crucial exploration stage imore search consider finer step sizes increasing search generations 
indicates number function calls proportional reduced initial stage search 
particular adjust step size decrements generations defined number imore search generations 
individual search phase series nested searches conducted solving sequence bound constrained optimization sub problems described eq 

appropriate bounds nested search defined current generation counter 
offline adaptive imore search operates exactly imore search described stops termination criterion reached 
online adaptive imore design optimization alternative achieve suitable balance imore search online adaptation strategy 
contrast offline adaptation strategy fixed step sizes various phases imore search advance online adaptation strategy decides values online feedback accuracy approximated robustness fitness search 
detailed procedure online adaptive imore described 
online adaptive imore algorithm studied consider straightforward toggling different step sizes particularly fine coarse stepsizes denoted respectively 
start update interval initialized 
imore search begins coarse stepsize subsequently error estimating robustness fitness assessed generations 
error th individual denoted determined max max max max max robustness fitness measurements obtained respectively 
large robustness fitness error individuals fine step size generations gain better accuracy 
hand individuals population coarse step size adopted generations accuracy robustness fitness considered adequate 
empirically max determine population size cut values represent majority population classify error low respectively 
online adaptive imore consider maximizing problem initialization phase initialize maximum degradation tolerable final design dt 
initialize fine coarse step size set generate population design vectors 
search phase termination condition satisfied set non duplicated individual population generation objective nominal fitness xi objective robustness scribed 
mod 
online adaptive imore design optimization algorithm 
empirical studies obtained procedure de max obtain max max respectively 
max max obtain max mod population having population having left unchanged online adaptive imore facilitate detailed study imore algorithms number test functions created expansion terms gaussian basis functions follows ij xd design vector dimension function number basis functions standard deviation magnitude th basis function ij centroid th dimension th basis func tion 
parameters constructed test functions listed table 
table 
parameters construct test functions gaussian basis function eq 
test function centroid standard deviation magnitude dimensionality numerical studies employ bit binary coded non dominated sorting genetic algorithm nsga ii 
population size maximum search generation allowed configured 
uniform crossover mutation applied probabilities respectively 
offline adaptive imore configured grades step sizes having 
parameters online adaptive imore configured follows 
iteration nested search set maximum computational budget fitness function calls low high dimensional problems respectively 
pareto front convergence metric pc reported measuring ability adaptive imore algorithms converging true optimum pareto front 
known metrics evaluate convergence set non dominated solutions 
determine pc target pareto front 
solution pareto front shortest euclidean distance di calculated equation 
min number objectives upper lower bounds th objective respectively 
pareto front convergence metric pc average design solutions final pareto front eq 

pc note smaller pc indicates greater accuracy convergence true target pareto front 
target pareto front obtained imore 
comparative study imore adaptive imore subsection provide empirical study imore adaptive imore algorithms abovementioned test functions 
typical pareto fronts imore depicted figures respectively 
plotted figures final pareto fronts offline online adaptive imore 
observed offline online adaptive imore algorithms converge approximately pareto front obtained imore 
worth noting results obtained imore indication possible false 
imore offline adaptive imore 
pareto fronts test function th generation imore offline online adaptive imore offline adaptive imore imore online adaptive imore imore online adaptive imore 
pareto fronts test function th generation imore offline online adaptive imore offline adaptive imore 
pareto fronts test function th generation imore offline online adaptive imore imore offline adaptive imore imore online adaptive imore online adaptive imore 
pareto fronts test function th generation imore offline online adaptive presents average normalized computational costs different imore algorithms independent runs 
note computational cost incurred offline online adaptive imore algorithms significantly reduced comparison imore fixed 
described earlier section computational cost imore algorithm 
offline online adaptive imore hand incur computational cost average number nested searches required 
sense adaptive imore algorithms significantly faster original non adaptive counterpart 
small value pc indicates offline online adaptive imore provide convergence true optimal pareto front 
normalized computational cost test function imore imore offline adaptive imore online adaptive imore 
normalized computational cost incurred functions imore offline online adaptive pareto convergence metric pc test function imore offline adaptive imore online adaptive imore 
pareto front convergence metric functions imore offline online adaptive effect update interval setting online adaptive imore consider effect step size update interval computational efficiency online adaptive imore 
normalized computational cost respect cost incurred imore pareto front convergence metric step size update intervals figures respectively 
average results independent runs 
observed settings update interval considered study lead great savings computational cost imore 
hand final pareto front online adaptive imore converges highly sensitive configurations update interval may observed pareto front convergence metric shown increase indicates high dissimilarity final true pareto front increasing order avoid convergence false pareto front maintain utility online adaptive imore appropriate values chosen 
appears give appropriate balance computational cost convergence accuracy online adaptive imore search 
outcome sense easily explained 
extreme online adaptive imore computational cost equivalent imore due overheads determine robustness accuracies search generation 
conversely total computational budget fixed generations lower adaptation frequency may achieved large values instance search initial search generations upper bound adaptation frequency leading high possibility converging false optimal pareto front 
may observed convergence accuracies deteriorate increases 
consequently providing adaptation frequency upper bound serves appropriate configuration providing balance computational cost accurate convergence online adaptive imore 
normalized computational cost step size update interval 
normalized computational cost incurred functions different generation intervals online adaptive imore pareto front convergence metric pc step size update interval 
pareto front convergence metric functions different generation intervals online adaptive imore enhancing computational efficiency online adaptive imore design experiments doe sampling techniques efforts enhance computational efficiency online adaptive imore study possibility replacing series nested optimization sub problems computationally intensive known sampling methods 
particular consider design experiments doe sampling approaches including random sampling rs stratified sampling ss latin hypercube sampling lhs generate sampled design points ap proximation worst case performance design iterations please refer eq 

empirical results obtained complex test functions include 
generate possible savings computational cost approximation doe approaches online adaptive imore required number calls objective function required doe approaches nested optimization sub problems respectively 
experimental study configured lower dimensional problems higher dimensional problem types problems 
note guarantees computational cost reduction approximately doe approaches imore 
configurations online adaptive imore kept section 
consequently aim determine incorporated approximation doe samplings lead convergence true pareto front 
normalized computational cost respect cost incurred imore pareto front convergence metric various imore algorithms averaged independent runs figures respectively 
online adaptive imore doe samplings labeled oas rs imore oas ss imore oas lhs imore 
normalized computational cost test function imore online adaptive imore oas rs imore oas ss imore oas lhs imore 
normalized computational cost incurred functions imore online adaptive imore oas imore algorithms pareto front convergence metric pc test function imore online adaptive imore oas rs imore oas ss imore oas lhs imore 
pareto front convergence metric functions online adaptive oas imore algorithms oas imore algorithms lead significant savings computational costs imore online adaptive imore expected arrive approximately increase search efficiency compared online adaptive imore 
clear trade convergence accuracies reduction computational cost approximation oas imore algorithms 
oas imore algorithms remain converge true pareto front accurately 
indicated pareto front convergence accuracies pc observed maintain clearly competitive online adaptive imore 
addition oas imore algorithms rs results poorer convergence accuracies compared ss lhs approximations searching functions considered 
due poor coverage search space random sampling 
larger convergence inaccuracies pareto front observed 
worth noting effect curse dimensionality implies sample size may require increase exponentially order provide coverage nested search space search dimension grows 
cope issue possible solution adapt sample size oas imore search 
figures average performances oas lhs imore search increasing sample size 
sample size increment gradually increasing search generations formulated follows mmax min min max mi sample size th generation gmax maximum total search generations termination mmax minimum maximum sample sizes 
setting mmax gmax experiments listed table 
table 
min max max test function 
test function min max max results compared online adaptive oas lhs imore reported previously 
adapting sample size reduction computational cost 
importantly trade significant cost saving convergence accuracy minimum shown higher dimensional problem 
normalized computational cost test function imore online adaptive imore oas lhs imore oas lhs imore adaptive 
normalized computational cost incurred functions online adaptive oas lhs fixed sample size oas lhs adaptive sample size imore algorithms 
pareto front convergence metric pc test function imore online adaptive imore oas lhs imore oas lhs imore adaptive 
pareto convergence metric functions online adaptive oas lhs fixed sample size oas lhs adaptive sample size imore algorithms inverse multi objective robust evolutionary imore algorithm design optimization presence uncertainty 
prior information desired robustness final design algorithm shown capable converging set solutions gives nominal performances exhibiting maximum robustness 
importantly solutions discovered requirement assumptions structure uncertainties involved 
realized major drawback imore massive computational cost incur 
adaptation strategies introduced imore algorithm reduce massive computational efforts incurred nested design searches 
particular consider adapting step size determining search bound sub problem optimization trimming number objective function calls doe sampling methods 
empirical results diverse test functions show proposed adaptive imore algorithms provide convergence true pareto fronts functions considered 
computational costs incurred adaptive imore algorithms significantly reduced 
adaptive imore typically requires enormous number function evaluations locate near pareto optimal solutions imore computationally prohibitive class problems computationally expensive objective functions 
desirable retain appeal inverse multiobjective robust evolutionary algorithms handle computationally expensive design problems produce high quality designs limited computational budgets 
intriguing consider meta modeling strategies imore design methodology solving problems computationally expensive objective functions 
funded honda research institute germany 
authors rner honda research institute europe parallel distributed computing centre nanyang technological university support 
goldberg genetic algorithms search optimization machine learning addison wesley 
jin evolutionary optimization uncertain environment survey ieee transactions evolutionary computation vol 
pp 
june 
ong nair lum max min surrogate assisted evolutionary algorithm robust aerodynamic design ieee transactions evolutionary computation press expected august 
estimation total uncertainty modeling simulation sandia report sand 
mathematical representation uncertainty aiaa proceedings non deterministic approaches forum reston va 
ben haim information gap decision theory academic press california 
ben haim uncertainty probability information gaps reliability engineering system safety pp 

ben haim robust reliability mechanical sciences springer verlag berlin 
solving problems optimization uncertainty statistical decision problems aiaa 
ben tal nemirovski robust convex optimization math 
oper 
res 
ghosh genetic algorithms robust solution searching scheme ieee transaction evolutionary computation vol 
pp 

arnold beyer local performance es noisy environment ieee trans 
evolutionary computation vol 
pp 
jin sendhoff trade performance robustness evolutionary multiobjective approach proceedings second international conference evolutionary multi criteria optimization 
lncs springer pp 
jin efficient search robust solutions means evolutionary algorithm fitness approximation ieee transactions evolutionary computation press expected august 
ong lim zhu wong classification adaptive memetic algorithms comparative study ieee transactions systems man cybernetics part vol 
february 
ong keane meta lamarckian learning memetic algorithm ieee transactions evolutionary computation vol 
pp 
april 
zhou ong nair keane lum combining global local surrogate models accelerate evolutionary optimization ieee transactions systems man cybernetics part press 
lewis aerodynamic shape optimization dimensional uncertain operating conditions hampton virginia icase nasa langley research centre 
anthony keane robust optimal design lightweight space structure genetic algorithm aiaa journal pp 

hammel back robust design multilayer optical means evolutionary algorithms ieee trans 
evolutionary computation vol 
pp 
srinivas deb multi objective optimization non dominated sorting genetic algorithms evolutionary computation vol 
pp 
deb jain running performance metrics evolutionary multi objective optimization proceeding fourth asia pacific conference simulated evolution learning pp 
performance metrics multi objective optimization evolutionary algorithms proceedings conference applied industrial mathematics romania 
fang ma centered discrepancy random sampling latin hypercube design construction uniform designs mathematics computation vol 
pp 

simpson lin wei sampling strategies computer experiments design analysis international journal reliability applications 
tang orthogonal array latin hypercubes journal american statistical association vol 
pp 

ong nair keane evolutionary optimization computationally expensive problems surrogate modeling aiaa journal vol 
pp 
jin comprehensive survey fitness approximation evolutionary computation soft computing journal vol 
pp 
