improving performance openmp smp clusters overlapped page migrations woo yang kee ha electrical engineering computer science seoul national university seoul korea sha iris ac kr peace ac kr research parade computer science engineering university california san diego usa ucsd edu 
costly page migration major obstacle integrating openmp page software distributed shared memory realize easy programming paradigm smp clusters 
reduce impact page migration overhead execution time application previous researches mainly focused reducing number page migrations hiding page migration overhead overlapping computation communication 
propose collective prefetch technique overlaps page migrations prior approach effectively applied 
experiments communication intensive application show technique reduces page migration overhead significantly execution time reduced 
openmp specification proposed standard shared memory programming model variety efforts adopt openmp easy programming paradigm parallel processing platforms range small chip level systems grids 
commodity shelf symmetric multi processors smps high speed network devices widely deployed smp clusters attractive platform high performance computing 
accordingly studies applying openmp programming model cluster systems 
intuitive approach realizing openmp smp clusters software distributed shared memory system transparently provides single shared address space distributed memory 
specifically utilize page systems keep memory consistency user level page fault signal handler 
execution time application page system decomposed computation time page migration overhead synchronization overhead signal handler overhead 
define page migration procedure sends page request home node receives page reply request define page migration overhead net time complete page migrations 
known systems suffer poor performance due high synchronization overhead excessive accesses remote pages 
common challenge prior studies openmp smp clusters confronted overcome intrinsic performance bottlenecks conventional systems 
specific efforts reduce page migration overhead synchronization overhead 
specifically solution techniques categorized implementing synchronization directives efficiently reducing shared address space lessening number page migrations reducing page migration delay fast communication hw page update protocol hiding page migration overhead overlapping computation time page migration overhead 
techniques complementary consequently applied independently improve performance 
assume number page migrations minimized techniques 
lessen page migration overhead number page migrations previous studies mainly focused hiding page migration overhead overlapping computation time page migration overhead 
effective computation time large hide page migration overhead 
specifically proposes collective prefetch technique lessen page migration overhead 
technique analyzes page access patterns prefetch remote pages overlapping page migrations 
note technique different prior techniques computation overlap communication 
technique applied target system experiments communication intensive application show page migration overhead reduced execution time reduced 
organized follows 
section motivation research communication intensive application 
explain proposed technique section 
detail implementation experimental results openmp programming environment section 
draw idea research direction 
motivational example parade target system 
parade openmp parallel programming environment consists openmp translator thread safe home lazy release consistency protocol migratory home multiple writer protocol mpi library mpi pro 
executing program hybrid model software dsm mpi parade shows better performance pure environment omni 
motivational application ft contains computational kernel fast fourier transform fft spectral method 
table shows number page migrations execution time breakdown ft parade nodes 
isolate page migration overhead clearly computation thread node avoid overlapping computation communication 
page migration overhead total execution time larger computation time 
consequence overlapping computation time page migration overhead little effect application 
possible improvement computation time 
note poor performance due inefficiency parade implementation 
ran ft program omni execution time longer parade 
table 
number page migrations execution time breakdown ft class parade nodes synchro handler compu migration execution page migrations time sec node times page systems parade uses segmentation fault signal handler page migrations 
demonstrates memory consistency mechanism page 
unprivileged access computation thread node page invokes segmentation fault signal handler handler fetches valid page negotiating owner node page 
problem handler blocked long time waiting reply 
application experience long memory access latency runtime 
node node page server computation page server computation thread thread page thread thread request active sigsegv signal idle handler page reply kb time fig 

page migration nodes parade 
time runs page commonly approaches overcome long latency problem pipelining prefetching 
pipelining prefetching proposed different contexts web hpf openmp 
techniques enable computation communication overlapped 
contrast overlap page migrations 
technique reduce latency page migration reduce page migration overhead 
furthermore effective computation workload small 
overlapping page migrations section proposed technique inspector executor model 
technique consists steps generating inspector executor code translating openmp code 
modification openmp source code openmp translator automatically generates inspector executor code source code 
inspector checks shared memory collects page migrations 
executor overlaps page migrations executes actual parallel loop 
openmp translator replaces original parallel loop code step 
openmp translator translates code second step 
inspector executor code openmp translator pre processing step generate inspector executor code openmp translates parallel loop 
example shows simple openmp code segment shared 
pragma omp parallel default shared fig 

target parallel loop original openmp code openmp translator analyzes parallel loop generates code openmp translation 
shows inspector code parallel loop 
inspector collects page migration requests executor overlaps page migrations 
codes parallel loop structure original loop just identify pages accessed executing entire code block 
simple macro check shared address inform openmp runtime system pages need fetched remotely 
inspector collects information pages migrated real page migrations occur 
pragma omp parallel default shared check shared address check shared address fig 

inspector code generated openmp translator shows executor code parallel loop 
executor code consists prefetch function call actual parallel loop 
prefetch function initiates page migrations proposed page migration techniques discussed 
prefetch pragma omp parallel default shared multiple prefetch technique fig 

executor code generated openmp translator shows simplified original page migration scenario nodes 
master thread creates computation thread asks page requests 
computation thread sequentially performs computation page migrations joins master thread parallel region ends 
node node node node master compu page master compu prefetch page thread tation server thread thread page create request thread tation threads server thread page thread request create join page reply kb join create join active idle page reply kb time original page migration multiple prefetch technique fig 

original page migration multiple prefetch technique proposed technique reduces page migration overhead overlapping page migrations 
intuitive approach invoke multiple prefetch threads concurrently 
refer approach multiple prefetch technique 
master thread creates prefetch threads prefetch function creating computation thread 
shows prefetch threads overlap page migrations home node 
prefetch thread completes page migrations prefetch thread send page request home node prefetch threads waiting page replies home node 
prefetch threads concurrently send page requests multiple nodes 
ideally prefetch threads may overlap page migrations number prefetch threads true reality 
prefetch threads join master thread page migrations 
master thread creates computation thread computation page migrations joins master thread 
drawback basic approach difficult decide proper number prefetch threads best performance 
prefetch threads cause large context switching overhead small number threads fully utilize overlapping effect 
waiting time grows longer multiple page requests arrive node simultaneously 
prefetch threads compete page server thread node lengthen delay page server thread 
collective prefetch technique propose collective prefetch technique overcome problems multiple prefetch technique 
technique send list page request home node sending page requests 
master thread creates collective prefetch thread sends page request list home node receives pages home node continuously shown 
page server thread creates collective page server thread 
page requests initially arrive home node collective page server thread send page replies continuously idle time page replies 
reduces waiting time multiple page migrations waiting time page migration 
thread joins parent thread page migrations completed 
master thread creates computation thread computation page migrations joins master thread 
node node master thread computation thread create join create join experiments collective collective page prefetch page server server thread page thread thread request list create page reply kb active idle fig 

collective prefetch technique join time experiment environment experiment platform linux cluster consisting nodes interconnected gigabit ethernet switch 
node dual ghz intel pentium xeon processors gb memory 
red hat linux smp kernel parade red hat linux smp kernel omni score package installs red hat omni 
evaluated technique ft kernel npb suite 
ported original fortran program version parade 
gcc compiler optimization level 
results implemented proposed technique parade analyzed execution time ft table shows page migration overhead ft parade computation thread node varying number prefetch threads multiple prefetch technique 
performance original parade proposed technique baseline performance 
table 
page migration overhead ft parade computation thread seconds nodes original multiple prefetch prefetch threads collective prefetch table shows collective prefetch technique reduces page migration overhead consistently number nodes increases 
number prefetch threads increases multiple prefetch technique reduces page migration overhead 
multiple prefetch reduce page migration overhead additionally number prefetch threads 
multiple prefetch shows performance degradation nodes 
main cause page server thread competes multiple prefetch threads delay responses 
multiple prefetch technique reduces page migration overhead nodes multiple prefetch technique increases page migration overhead nodes 
note collective prefetch technique outperforms multiple prefetch technique 
collective prefetch technique reduces execution time nodes 
shows execution time breakdown ft parade nodes 
original parade collective prefetch represented org col 
inspector indicates overhead inspector check page requests list page request technique 
overhead cases 
synchronization overhead computation time handler overhead constant time change page migration overhead determines change total execution time 
fig 

execution time breakdown ft parade computation thread seconds table shows page migration overhead ft parade computation threads node 
performance original parade computation threads baseline performance 
computation thread computation page migration somewhat overlapped 
collective prefetch reduces page migration overhead consistently number nodes increases 
multiple prefetch shows characteristics number computation threads 
difference multiple prefetch shows performance degradation nodes earlier case computation thread 
technique reduces page migration overhead nodes multiple prefetch technique increases page migration overhead nodes 
table 
page migration overhead ft parade computation threads seconds nodes original multiple prefetch prefetch threads collective prefetch number computation threads varies expect page migration overhead constant regardless number computation threads number page migrations mainly dependent number nodes ft page migration overheads table table meet expectation 
original parade shows reduced page migration overhead computation threads hide page migrations 
evaluate effect overlapping computation page migration increased number computation threads smp node shown table 
execution time decreases computation threads created execution time starts increasing computation threads 
addition ft shows nondeterministic behavior computation threads 
ft takes long execution time seconds 
non deterministic behavior believed due fact computation threads affect thread scheduling page server thread interrupted 
table 
execution time ft parade varying computation threads seconds nodes computation threads collective prefetch reduces execution time 
shows execution time breakdown ft parade nodes 
change page migration overhead determines change execution time 
difference execution times computation time reduced computation threads 
fig 

execution time breakdown ft parade computation thread seconds summary collective prefetch reduces page migration time ft seconds seconds consistently number nodes increases 
percentage notation collective prefetch technique reduces page migration overhead reduces execution time ft 
related experiments invoke multiple computation threads overlap computation page migration 
approaches overlap overlapping computation page migration overhead 
seung jai dynamic scheduling balance execution time nodes considering page migration overhead 
computation performed node experiencing page migration 
costa technique similar technique 
analyzes access patterns prefetches necessary pages parallel loop soon current parallel loop ends 
words learns sequence loops registers memory regions accessed parallel loop 
thread page access list sends pages specified list finishes loop 
overlaps page migration overhead execution serial section parallel loop 
serial section large page migration overhead effectively hidden 
focused overlapping computation page migrations 
consider effect overlapping page migrations experiments shows overlapping page migrations improve performance application 
page list parallel loop 
page prediction difficult parallel loop accesses shared array parameter function 
example ft argument function changes parallel loop accesses different memory region 
case approach compute correct page list newly page list time 
mowry proposed technique system prefetch requests invoked computation 
requested page arrived earlier computation thread needs experiences page migration overhead 
summary prior works overlap computation page migration effective sufficient computation workload hide overhead 
long latency page migrations major performance bottleneck openmp page clusters 
shows proposed collective prefetch technique reduce page migration overhead effectively computation hide page migration overhead previous works 
experiments communication intensive application show technique reduces page migration overhead significantly execution time reduced 
technique implemented manually 
currently working automating translation openmp translator 
research issue develop hybrid technique proposed prior techniques considering computation communication ratio application 
acknowledgments supported national research laboratory program brain korea project leading project funded korean mic 
ict seoul national university provides research facilities study 

openmp api version www openmp org 
liu practical openmp compiler system chips 
lecture notes computer science vol 

springer verlag berlin heidelberg new york 
hu lu cox zwaenepoel openmp networks smps 
parallel distributed computing vol 



sato harada ishikawa openmp compiler software distributed shared memory system 
proc 

kee kim ha parade openmp compiler software distributed shared memory systems 
proc 
ieee acm supercomputing 
min eigenmann optimizing openmp programs software distributed shared memory systems 
int 
parallel programming 
vol 

issue 

tao karl implementing openmp execution environment clusters 
proc 

chun performance analysis improvement openmp software distributed shared memory systems 
proc 

ishikawa openmp software distributed shared memory 
proc 

costa cortes running openmp applications efficiently shared 
proc 
ipdps 
li hudak memory coherence shared virtual memory systems 
acm transactions computer systems vol 



harris rob van der alex woo maurice nas parallel benchmarks 
technical report nas 
kee kim ha memory management multi threaded software dsm systems 
parallel computing vol 

jan 

padmanabhan mogul predictive prefetching improve world wide web latency 
sigcomm computer communication review 
muller compiler generated vector prefetching architectures distributed memory 
high performance computing science engineering 
krause eds springer verlag 
mowry chan lo comparative evaluation latency tolerance techniques software distributed shared memory 
proc 
hpca 
koelbel mehrotra compiling global name space parallel loops distributed execution 
ieee transaction parallel distributed systems vol 


oct 

pc cluster consortium score cluster system 
www org 
