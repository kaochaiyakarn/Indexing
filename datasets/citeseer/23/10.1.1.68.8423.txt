speaker localisation audio visual synchrony empirical study iyengar neti ibm tj watson research center po box yorktown heights ny 
usa 

reviews definitions audio visual synchrony examines empirical behaviour test sets times larger authors 
results give new insights practical utility existing synchrony definitions justify application audio visual synchrony techniques problem active speaker localisation broadcast video 
performance evaluated test set twelve clips alternating speakers multiple speaker corpus 
accuracy obtained task identifying active member speaker pair different points time comparable performance purely video image schemes 
accuracy obtained challenging task locating point pixel square centered active speaker mouth prior face detection performance upper bound perfect face detection available 
result significantly better purely video image schemes 
papers discuss idea audio visual synchrony considers strength relationship audio signals video image sequences 
example soundtrack containing image sequence showing person beating drum strongly related strongly synchronous 
similarly faces saying words heard speech soundtrack audio video signals synchronous 
conversely soundtrack unrelated images screen audio video signals synchronous 
fig 

keyframe trec video track corpus definitions audio visual synchrony possible 
proposed far fall extremes 
extreme generic definitions synchrony weak models assign scores audio video signals displaying consistency regard type audio video signal consideration 
examples include defines synchrony gaussian mutual information audio energy pixel intensities 
extreme definitions synchrony specific particular types signal existing type considers evaluating consistency facial movements speech 
examples include suggests speech signal synthesise talking head differences synthesised actual faces compared 
measures proposed fall extremes 
suggestions include uses canonical correlation analysis set training data find linear projection audio video face data single axis maximises linear correlation projected variables synchrony new data evaluated calculating linear correlation new audio video space 
uses pre trained time delay neural network classifier predict audio video features particular frame correspond person talking 
practice appropriate definition audio visual synchrony may vary intended application 
research currently focussing synchrony measures useful deciding facial movements related speech signals measure evaluating consistency face speech relationships potential applications 
important part research consider audio visual synchrony measures scale larger challenging test sets authors 
addition evaluating measures relatively large artificial test sets considers synchrony measures speaker localisation 
broadcast video 
robust solution problem benefit multimedia retrieval applications ways firstly robust method speaker localisation broadcast video allow wider application ibm audio visual speech recognition systems 
robust audio speech recognition noisy environments 
improve transcription accuracy traditionally challenging non studio environments improving speech indexing performance video segments 
secondly systems automatic semantic concept annotation video 
exploit speaker localisation information distinguishing monologues dialogues 
illustrate consider absence synchrony implies shot high synchrony localised right face implies monologue alternating pattern high synchrony implies dialogue 
similar ideas trec video track automatic monologue annotator 
structured follows 
section reviews selected measures audio visual synchrony investigates behaviour artificial test sets 
motivated findings section describes synchrony approach active speaker localisation broadcast video presents empirical results corpus 
ends 
preliminary studies section reviews generic specific speech face consistency measures defined discusses empirical behaviour artificial test sets 
potential applications outside digital library arena include face voice biometrics systems marking speaker level metadata speaker turns video systems assessing quality systems animating lip movements cartoon characters 
definitions audio visual synchrony assume test video clip speech soundtrack moving face video 
rn feature vector describing acoustic signal time 
vector mel frequency cepstral coefficients 
vt rm feature vector describing image time 
vector discrete cosine transform coefficients face region frame st 
vt represent joint sequence audio video feature vectors 
goal define measure synchrony sequences 
vt 
vt derived test clip 
generic measures 
ignore information vt correspond audio containing speech images containing face 
consider vector independent sample joint distribution explicitly modelling temporal dependence individual sequences 
suggested measure audio visual synchrony mutual information random variables forms unknown practice assumptions 
earlier investigated simple assumptions allowing straightforward evaluation mutual information discrete distributions continuous multivariate gaussian distributions 
note assumption discrete distributions requires preparation phase constructs codebooks quantize vt vt prior discrete distribution estimation test time constrast assumption multivariate gaussian distributions allows parameter estimation test time prior preparation 
term implementations discrete mutual information discrete mi gaussian mutual information gaussian mi 
face speech specific measures 
require vt correspond speech audio images containing faces 
assume know word sequence spoken audio define likelihood st measure synchrony 
practice may unknown audio speech recognition gives reasonable approximation 
similarly form unknown practice implementation uses hidden markov models hmms trained joint sequences audio visual data hmms audio visual speech recognition 

term implementation audio visual likelihood av ll 
empirical behaviour audio visual synchrony section discusses empirical findings definitions audio visual synchrony discussed 
experiments artificial test sets constructed ibm audio visual database comprising full face frontal video audio multiple speakers reading prompts large vocabulary continuous speech fashion 
experiments assume prior existence face speech detection focus success different measures assessing synchrony facial movements speech 
audio features extracted follows 
mel frequency cepstral coefficients mfccs extracted audio signal hz rate 
features mutual information see measure dependence random variables phrased differently amount information random variable tells 
illustrate discrete case discrete random variables joint distribution marginal distributions 
log kullback leibler distance 
relative entropy joint product distributions 
directly discrete gaussian mi schemes presentation audio visual hmms consecutive mfcc vectors concatenated projected lower dimensional space linear discriminant analysis lda rotated maximum likelihood linear transform mllt give dimensional vector 
video features vt extracted follows 
operating hz sampling rate normalised mouth region interest roi extracted frame compressed discrete cosine transform transform dct 
highest energy dct coefficients retained linearly interpolated give hz frame rate matching audio processing 
features directly discrete gaussian mi schemes presentation audio visual hmms consecutive frames concatenated lda mllt transforms applied give dimensional vector 
experiment choose synchronised speaker small set 
previous artificial test set intended simulate video conferencing panel discussion scenarios want identify talking face shot 
total true speech face combinations seconds length extracted database true cases examples formed pairing true audio seconds video randomly chosen speaker 
table briefly reviews results show gaussian mi significantly outperforms definitions synchrony task reasons discussed 
conclude face speech detection available gaussian mi solve practical problems video analysis 
firstly solves speaker localisation problem arises identified faces screen need identify talking face 
secondly gives information classifying video shots known contain talking face monologues dialogues pattern activity faces 
number discrete mi gaussian mi av ll table 
synchronised speaker detection accuracy experiment classifying speakers synchronous non synchronous 
second experiment examined gaussian mi successful synchrony measure experiment useful absolute classification tasks distinguishing monologues 
artificial test set second examples constructed examples contain synchronised faces speech 
despite earlier success measure correctly ranking synchronised speakers absolute classification performance poor 
result explained shows distribution synchrony scores synchronous non synchronous test clips 
classes highly overlapping measure decision boundary defined 
conclude gaussian mi adequate video annotation applications distinguishing shots monologue shots 
fig 

class histograms gaussian mi scores synchrony speaker localisation section studies gaussian mi applying speaker localisation real test set 
corpus experiments corpus speaker independent multiple speaker corpus connected continuous digit strings designed support research audio visual speech recognition adverse conditions 
clips groups partition form validation set required second twelve clips form held test set 
clip involves speakers arranged clips speakers turns reading digit strings speakers simultaneously reading different digit sequences 
defer challenge determining people speaking consider parts clip single person speaks time 
properties making distinctly non trivial localisation example left speaker parts text uttered right speaker 
addition single speaker portion clip average seconds long 
dataset larger varied authors largest comparable test set aware comprises second utterances weather taipei different speakers 
fig 

example person clip pixel wise gaussian mutual information experiments nature dataset perfect apriori speech detection 
constrast artificial dataset experiments earlier assume priori face detection 
tests hypothesis face detection may essential prerequisite successful speaker localisation 
comparative results case perfect priori face detection see performance improves 
basic approach calculate gaussian mi individual pixels audio 
specifically replace vt earlier discussion value related pixel frame time assume gaussian forms generates pixel value 
obtain mutual information value pixel 
full set pixel mutual information values estimated test clip thought mutual information image shows examples lighter pixels indicate higher mutual information values 
localise speaker searching mutual information image compact regions having high mutual information audio pixel wise gaussian mi implementation similar differs choice described 
fig 

mutual information images pixel intensities pixel intensity changes preliminary experiments showed results improve choosing cepstral coefficient vector audio energy 
results improve choosing related grey scale pixel intensity changes grey scale pixel intensity illustrated 
image mutual information image calculated grey scale pixel intensity 
high mutual information occurs heads isolated speaker mouth trend seen mutual information images pixel intensity 
suggests problem reduced defining related pixel intensity changes experimental results find computation full mutual information image dimensionality matching original image essential practice naive subsampling original image factors prior forming mutual information images significantly degrade results tasks discussed section 
confirm finding 
specifically defines pixel intensity change value ic defined ic original image pixel value frame time refer image fixed time pixel values correspond ic values intensity change image ic shows mutual information image analogous calculated ic mutual information highest speaker mouth 
detecting active speaker experiment considers pixel wise gaussian mi detect active speaker 
left right second intervals test clips 
chance performance 
solve problem window seconds length mutual information image calculated estimate active speaker obtained considering total mutual information left image relative total right half 
higher values assumed indicate active speaker 
window shifted second procedure repeated 
preliminary experiments investigated effects different window lengths window shifts performance significant variation noted 
estimates scored second intervals clip total test points 
table shows results cases pixel intensity pixel intensity change 
performance schemes significantly chance higher active speaker detection case unexpected discussion previous subsection experiments remainder pixel intensity changes 
analysis shows third errors pixel intensity change case occur close speaker turn points 
left speaker stops speaking right speaker starts 
surprising estimates points pixel wise mutual information estimated window spanning data active left speaker active right speaker 
possible solution detect speaker turn points audio technique 
adjust estimates regions 
baselines compare simple video techniques estimate active speaker time intensity change image ic scheme intensity change image sums simply compare total pixel intensity changes left right halves image ic gives performance 
second scheme intensity change image projection peak sum intensity changes column ic column maximum sum identify active speaker gives performance 
conclude simple task determining active speaker video projection peak technique adequate little benefit computationally expensive noncausal pixel wise gaussian mi 
assumptions experiment including known number speakers known regions screen lack background motion clear video performance maintained conditions hold informal experiments clip pixel intensity intensity change active speaker detection clip active speaker mouth localisation table 
results accuracy data sets indicate pixel wise gaussian mi maintains performance situations 
challenging tasks active speaker mouth localisation section pixel wise gaussian mi gives clear benefit 
results priori face detection 
repeat pixel wise gaussian mi intensity change image sums experiments time assuming perfect head detection 
time techniques compare average total mutual information intensity change head regions 
results change technique 
conclude priori face detection useful essential speaker localisation data high background motion 
detecting active speaker mouth second set experiments investigated challenging task locate mouth region active speaker test clips 
computing mutual information images test point locate active speaker mouth mutual information image searching region highest concentration mutual information values fraction maximum mutual information value full mutual information image parameters tuned validation set 
shows images demo illustrating results held data image top half shows original video bottom mutual information image wide narrow red rectangle placed true speaker small white square indicates best region algorithm 
hope optimal previously validation set correspond region size speaker mouth 
quantify results estimate mouth region centre defined correct falls pixel square centred true mouth centre 
illustrates correct regions white squares upper images 
estimates scored second intervals clip total test points 
table shows results 
smoothing mouth estimates frames experiments test points separated time obvious direction 
fig 

speaker localisation examples successful unsuccessful baselines compare video techniques estimate active speaker mouth time intensity change image ic scheme high intensity change region locate active speaker mouth time searching intensity change image ic region highest concentration intensity change values fraction maximum gives performance 
second scheme intensity change image projection peaks sum intensity changes row column ic row column maximum sums locate mouth gives performance 
task conclude pixel wise gaussian mi performs significantly better 
result plausible projection scheme information available high intensity change region scheme high intensity change region scheme information available pixel wise gaussian mi uses longer temporal window incorporates audio information 
results priori face detection 
repeat pixel wise gaussian mi high intensity change region experiments assuming existence perfect head detection 
constrain techniques search head region 
high intensity change region improves gain pixel wise gaussian mi improves gain 
conclude best speaker localisation performance priori face speech detection essential 
interesting pixel wise gaussian mi gives reasonable speaker localisation performance data background motion 
fig 

speaker localisation correctness regions empirical study audio visual synchrony measures application speaker localisation video 
artificial dataset experiments support 
firstly gaussian mi outperforms synchrony measures proposed potentially solves specific practical problems video analysis identifying active speaker set faces screen distinguishing monologues dialogues known occurs shot 
secondly gaussian mi suitable making absolute decisions degree synchrony distinguishing monologues 
experiments support 
pixel wise gaussian mi gives performance close video techniques task active speaker localisation informal experiments suggest scales better tasks 
active speaker mouth localisation additional visual context audio information available pixel wise gaussian mi leads performance significantly better video techniques 
follow directions identify new definitions synchrony suitable distinguishing monologues dialogues develop methods robustness background video motion consider efficiency issues similar approach implemented mpeg compressed domain 
potamianos miroslav novak technical assistance anonymous reviewers comments 

adams iyengar 
lin naphade neti smith 
semantic indexing multimedia content visual audio text cues 
eurasip journal applied signal processing 

butz 

feature space mutual information speech video sequences 
proc 
icme lausanne switzerland 

chen gopalakrishnan 
speaker environment channel change detection clustering bayesian information criterion 
proc 
darpa broadcast news transcription understanding workshop va usa 

connell haas neti potamianos 
real time prototype small vocabulary audio visual asr 
icme submitted 

cover thomas 
elements information theory 
wiley interscience 

cutler davis 
look talking speaker detection video audio correlation 
proc 
icme ny usa 

fisher iii darrell 
informative subspaces audiovisual processing high level function low level fusion 
proc 
icassp 

gopinath 
maximum likelihood modeling gaussian distributions classification 
proc 
icassp volume pages wa usa 

hershey movellan 
audio visual synchrony locate sounds 
proc 
nips 

iyengar neti 
audio visual synchrony detection monologues video archives 
proc 
icassp hong kong 

iyengar neti 
assessing face speech consistency monologue detection video 
proc 
acm multimedia juan les pins france 

patterson 
moving talker speaker independent feature study baseline results multimodal speech corpus 
eurasip journal applied signal processing 

potamianos luettin neti 
hierarchical discriminant features audio visual lvcsr 
proc 
icassp pages 

slaney 
linear operator measuring synchronization video facial images audio tracks 
proc 
nips 
