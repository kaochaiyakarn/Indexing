visual categorization bags keypoints gabriella csurka christopher dance fan dric bray xerox research centre europe de france xrce xerox com 
novel method generic visual categorization problem identifying object content natural images generalizing variations inherent object class 
bag keypoints method vector quantization affine invariant descriptors image patches 
propose compare alternative implementations different classifiers na bayes svm 
main advantages method simple computationally efficient intrinsically invariant 
results simultaneously classifying semantic visual categories 
results clearly demonstrate method robust background clutter produces categorization accuracy exploiting geometric information 

proliferation digital imaging sensors mobile phones consumer level cameras producing growing number large digital image collections 
manage collections useful access high level information objects contained image 
appropriate categorization image contents may efficiently search recommend react reason new image instances 
confronted problem generic visual categorization 
identify processes sufficiently generic cope object types simultaneously readily extended new object types 
time processes handle variations view imaging lighting occlusion typical real world intra class variations typical semantic classes everyday objects 
task dependent evolving nature visual categories motivates machine learning approach 
presents bag keypoints approach visual categorization 
bag keypoints corresponds histogram number occurrences particular image patterns image 
main advantages method simplicity computational efficiency invariance affine transformations occlusion lighting intra class variations 
important understand distinction visual categorization related problems recognition concerns identification particular object instances 
instance recognition distinguish images structurally distinct cups categorization place class 
content image retrieval refers process retrieving images basis low level image features query image manually constructed description low level features 
descriptions frequently little relation semantic content image 
detection refers deciding member visual category image 
previous detection centered machine learning approaches detecting faces cars pedestrians possible perform generic categorization applying detector class interest image approach inefficient large number classes :10.1.1.9.6021
contrast technique proposed existing detection techniques require precise manual alignment training images segregation images different views necessary approach 
bag keypoints approach motivated analogy learning methods bag words representation text categorization 
idea adapting text categorization approaches visual categorization new 
zhu investigated vector quantization small square image windows called 
showed features produced results color texture approaches combined analogues known vector histogram gram models text retrieval 
contrast approach possess invariance properties 
idea clustering invariant descriptors image patches previously applied problem texture classification 
clearly problem texture classification different generic categorization 
natural approaches differ 
method uses clustering obtain quite high dimensional feature vectors classifier texture recognizers clustering obtain relatively low dimensional histograms evaluate similarity histograms previously seen probability densities 
filter responses clustered recognition done closest model measured test 
lazebnik cluster affine invariant interest points image individually summarize distribution descriptors form signature composed representative cluster members weights proportional cluster sizes 
signatures different images compared earth mover distance 
fergus proposed visual categorization method invariant descriptors image patches 
method exploits probabilistic model combines likelihoods appearance relative scale position model statistics patch detector 
elegant approach number limitations 
firstly method efficient models restricted image patches training images contain patches days cpu time required learn categories 
secondly views objects training segregated instance cars rear cars side 
unsurprising explicit model relative positions 
section explain categorization algorithms choice components 
section results applying algorithms dataset fergus challenging class dataset 
demonstrate approach robust presence background clutter produces state art recognition performance 

method main steps method detection description image patches assigning patch descriptors set predetermined clusters vocabulary vector quantization algorithm constructing bag keypoints counts number patches assigned cluster applying multi class classifier treating bag keypoints feature vector determine category categories assign image 
ideally steps designed maximize classification accuracy minimizing computational effort 
descriptors extracted step invariant variations irrelevant categorization task image transformations lighting variations occlusions rich carry information discriminative category level 
vocabulary second step large distinguish relevant changes image parts large distinguish irrelevant variations noise 
refer quantized feature vectors cluster centres keypoints analogy keywords text categorization 
case words necessarily repeatable meaning eyes car wheels obvious best choice vocabulary 
goal vocabulary allows categorization performance training dataset 
steps involved training system allow consideration multiple possible vocabularies detection description image patches set labeled training images constructing set vocabularies set cluster centres respect descriptors vector quantized 
extracting bags keypoints vocabularies training multi class classifiers bags keypoints feature vectors selecting vocabulary classifier giving best classification accuracy 
discuss choices step detail 
feature extraction computer vision local descriptors features computed limited spatial support proved adapted matching recognition tasks robust partial visibility clutter 
tasks require descriptors repeatable 
mean repeatable sense transformation instances object corresponding points detected ideally identical descriptor values obtained 
motivated development scale affine invariant point detectors descriptors resistant geometric illumination variations 
shown affine transformation images scale invariant point detector sufficient stability point location 
preferred harris affine detector described 
reader aware benefits choice clear cut firstly real world objects dimensional structures variations captured affine transformations secondly attempts increase invariance feature typically result loss discriminative information 
harris affine points detected iterative process 
firstly positions scales interest points determined local maxima position harris function local extrema scale laplacian operator 
elliptical affine neighborhood determined 
size selected scale shape eigenvalues image second moment matrix 
selection position scale elliptical neighborhood estimation iterated point kept process converges fixed number iterations 
affine region mapped circular region normalizing affine transformations 
scale invariant feature transform sift descriptors computed region 
sift descriptors multi image representations image neighborhood 
gaussian derivatives computed orientation planes grid spatial locations giving dimension vector 
fig 
shows example maps gradient magnitude corresponding orientations 
fig 

left right harris affine region normalized region maps gradient magnitude constituting sift descriptor 
prefer sift descriptors alternatives steered gaussian derivatives differential invariants local jet reasons 
simple linear gaussian derivatives 
expect stable typical image perturbations noise higher gaussian derivatives differential invariants 

simple euclidean metric feature space justified 
case differential invariants obtained combination components local jet mahalanobis distance appropriate 
instance expect second derivative feature higher variance derivative 
selecting appropriate mahalanobis distance priori challenging 
appropriate covariance matrix sift descriptors entire dataset predominantly influenced inter class variations precisely variations keypoints wish ignore 
measuring mahalanobis distance probably requiring manual specification multiple homologous matching points different images objects category seriously working objective producing simple automated categorization system 

far components feature vectors 
richer potentially discriminative representation 
mikolajczyk compared descriptors matching sift descriptors perform best 
visual vocabulary construction method vocabulary way constructing feature vector classification relates new descriptors query images descriptors previously seen training 
extreme approach compare query descriptor training descriptors impractical huge number training descriptors involved data set 
extreme try identify small number large clusters discriminating class instance operates parts category 
practice find best tradeoffs accuracy computational efficiency obtained intermediate sizes clustering 
clustering vector quantization algorithms iterative partitioning hierarchical techniques 
square error partitioning algorithms attempt obtain partition minimizes cluster scatter maximizes cluster scatter 
hierarchical techniques organize data nested sequence groups displayed form dendrogram tree 
need heuristics form clusters frequently square error partitioning techniques pattern recognition 
chose simplest square error partitioning method means 
algorithm proceeds iterated assignments points closest cluster centers recomputation cluster centers 
difficulties means algorithm converges local optima squared distortion determine parameter exist methods allowing automatically estimating number clusters 
example pelleg cluster splitting splitting decision done computing bayesian information criterion 
case really know density compactness clusters 
interested correct clustering sense feature distributions accurate categorization 
run means times different number desired representative vectors different sets initial cluster centers 
select final clustering giving lowest empirical risk categorization 
categorization descriptors assigned clusters form feature vectors reduce problem generic visual categorization multi class supervised learning classes defined visual categories 
categorizer performs separate steps order predict classes unlabeled images training testing 
training labeled data sent classifier adapt statistical decision procedure distinguishing categories 
available classifiers compared na bayes classifier simplicity speed support vector machine known produce state art results highdimensional problems 
categorization na bayes na bayes simple classifier text categorization 
viewed maximum posteriori probability classifier generative model document category selected class prior probabilities word document chosen independently multinomial distribution words specific class 
independence na assumption accuracy na bayes classification typically high 
considering visual categorization assume set labeled images ii vocabulary vi representative keypoints cluster centers 
descriptor extracted image labeled keypoint lies closest feature space 
count number times keypoint occurs image categorize new image apply bayes rule take largest posteriori score prediction evident formula na bayes requires estimates probabilities keypoint category cj 
order avoid probabilities zero estimates computed laplace smoothing categorization svm svm classifier finds hyperplane separates class data maximal margin 
margin defined distance closest training point separating hyperplane 
observations corresponding labels takes values finds classification function sign 
represents parameters hyperplane 
data sets linearly separable 
svm takes approaches problem 
firstly introduces error weighting constant penalizes misclassification samples proportion distance classification boundary 
secondly mapping original data space feature space 
second feature space may high infinite dimension 
advantages svm formulated entirely terms scalar products second feature space introducing kernel kernel penalty problem dependent need determined user 
kernel formulation decision function expressed sign 
training features data space label parameters typically zero equivalently sum taken select feature vectors known support vectors 
shown support vectors feature vectors lying nearest separating hyperplane 
case input features binned histograms formed number occurrences keypoint vocabulary image ii order apply svm multi class problems take approach 
class problem train svm distinguishes images category images categories equal query image assign class largest svm output 
experiments results experiments 
explore impact number clusters classifier accuracy evaluate performance na bayes classifier 
explore performance svm problem 
experiments conducted house class dataset 
experiment describe results class dataset employed 
house database contains images classes faces buildings trees cars phones bikes books 
fig 
shows examples dataset 
fig 

example images house database challenging dataset large number classes contains images highly variable poses significant amounts hope dataset publicly available soon 
contains faces buildings trees cars phones bikes books 
background clutter presence objects multiple classes large proportion image area occupied target category 
images resolutions acquired diverse set cameras 
images color luminance component method 
gathered xrce graz university excepting faces downloaded web 
performance measures evaluate multi class classifiers 
confusion matrix ij set test images category ik category obtained highest classifier output image error rate nc cj mean ranks 
mean position correct labels labels output multi class classifier sorted classifier score 
jj performance metric evaluated fold cross validation 
na bayes results fig 
error rates na bayes function number clusters point fig 
best random trials means 
standard error maximum range 
error rate improves slightly move 
assert trade accuracy speed experiment maximum repeated multiple times different random clusterings standard deviations result different classes range 
takes minute get predicted labels database naive bayes classifier ghz processor 
fig 

lowest error rate percentage different choices table shows performance function category obtained table 
confusion matrix mean rank best vocabulary 
true classes faces buildings trees cars phones bikes books faces buildings trees cars phones bikes books mean ranks clustering process risk bias images different categories contain different numbers interest points 
random samples training data clustering step sample containing interest points randomly chosen class interest points extracted training images 
fig 
illustrates example clusters obtained selected best vocabulary 
fig 

left patches detected image 
right patches selected clusters occurring image yellow magenta ellipses 
algorithm handles easily multiple objects images see fig 
occlusion partial view orientation fig 

fig 

images correctly classified containing multiple objects category 
fig 

profile face partial view car roof house correctly classified face cars building 
fig 
show examples images classified detected interest points background 
fig 

images background clutter higher percentage interest points object 
fig 
shows images database objects categories image ranked categories 
images considered multiple labels labeled main object image 
phones books cars bikes buildings cars buildings cars faces fig 

images multiple objects ranked labels 
fig 
shows false alarm label true label rank 
face book trees face phones cars fig 

examples incorrectly ranked images 
correct label rank shown 
svm results results applying svm table 
table 
confusion matrix mean rank svm linear kernel 
true classes faces buildings trees cars phones bikes books faces buildings trees cars phones bikes books mean ranks expected svm outperformed na bayes reducing error rate 
obtained better mean ranks case cars 
error rate faces comparable state art approaches surprising method exploits virtually geometric information 
direct comparison possible method cope background images classes 
low error rate comes price increased confusion categories faces larger number faces training set 
training svm best vocabulary na bayes 
compared linear quadratic cubic svm linear method gave best performance case cars quadratic svm gave better results 
parameter determined svm values typically gave best results 
results database tested approach freely available database object classes set background images faces images airplanes side images cars rear images cars side images side images 
results table shows images database easier classify database 
results directly compared results stated 
correct classification rate result classifying images class background images class problem different case conduct full multi class comparison ranks hope publish direct comparison soon 
diagonal elements table suggest approach give results theirs class downloaded www robots ox ac uk data cars side comes rcs cs uiuc edu index research html 
classification generally class classification robust multi class 
confusion matrices table directly comparable results suggest high confusion rate multi class case 
table 
confusion matrix mean rank svm linear kernel 
true classes faces airplanes cars cars frontal side rear side side faces frontal airplanes side cars rear cars side side mean ranks simple novel approach generic visual categorization feature vectors constructed clustered descriptors image patches 
approach evaluated category database demonstrating method robust background clutter produces categorization accuracy exploiting geometric information 
results svm clearly superior obtained simple na bayes classifier 
best knowledge largest number visual categories subjected simultaneous experiment 
remains 
extend visual categories discriminative power appearance unordered image patches suffice need extend categorizer incorporate geometric information 
need extend method cases object interest occupies small fraction field view investigate promising alternatives step basic method point detection clustering classification 
acknowledgments supported european project ist lava learning adaptable visual assistants www org 
grateful permission acquire images shops inria multi scale affine interest point detector tu graz bikes image database 
osuna freund girosi :10.1.1.9.6021
training support vector machines application face detection cvpr computer vision pattern recognition 
papageorgiou poggio 
trainable pedestrian detection system ieee conference intelligent vehicles 
schneiderman kanade statistical method object detection applied faces cars cvpr 
viola jones rapid object detection boosted cascade simple features cvpr li zhu zhang blake zhang shum statistical learning multi view face detection eccv european conference computer vision 
ronfard schmid triggs learning parse pictures people eccv 
joachims 
text categorization support vector machines learning relevant features ecml 
tong koller 
support vector machine active learning applications text classification 
icml 
lodhi shawe taylor watkins text classification string kernels 
nips advances neural information processing systems vol 
cristianini shawe taylor lodhi latent semantic kernels journal intelligent information systems 
zhu rao zhang theory image retrieval acm transactions information systems 
th 
leung malik representing recognizing visual appearance materials dimensional textons iccv 
varma zisserman classifying materials images cluster cluster eccv 
lazebnik schmid ponce sparse texture representation affine invariant neighborhoods cvpr 
rubner tomasi texture image retrieval segmentation iccv 
fergus perona zisserman object class recognition unsupervised scale invariant learning cvpr 
scale space theory computer vision kluwer academic publishers 
lowe object recognition local scale invariant features iccv international conference computer vision 
matas kittler 
object recognition invariant pixel set signature bmvc british machine vision conference 
schaffalitzky zisserman 
viewpoint invariant texture matching wide baseline stereo iccv 
mikolajczyk schmid 
affine invariant interest point detector eccv 
mikolajczyk schmid performance evaluation local descriptors cvpr 
duda hart stork pattern classification john wiley sons 
pelleg moore 
means extending means efficient estimation number clusters international conference machine learning 
vapnik 
statistical learning theory 
wiley lewis na bayes independence assumption information retrieval ecml 
domingos pazzani optimality simple bayesian classifier loss machine learning 
