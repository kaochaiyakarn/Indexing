machine learning kluwer academic publishers boston 
manufactured netherlands 
self improving reactive agents reinforcement learning planning teaching long ji lin cs cmu edu school computer science carnegie mellon university pittsburgh pennsylvania 
date reinforcement learning studied solving simple learning tasks 
reinforcement learning methods studied far typically converge slowly 
purpose fold investigate utility reinforcement learning solving complicated learning tasks previously studied investigate methods speed reinforcement learning 
compares reinforcement learning frameworks adaptive heuristic critic ahc learning due sutton learning due watkins extensions basic methods speeding learning 
extensions experience replay learning action models planning teaching 
frameworks investigated connectionism approach generalization 
evaluate performance different frame works dynamic environment testbed 
moderately complex istic 
describes frameworks algorithms detail presents empirical evaluation frameworks 
keywords 
reinforcement learning planning teaching connectionist networks 
reinforcement learning interesting learning technique 
requires scalar rein signal performance feedback environment 
reinforcement learning involves difficult subproblems 
known temporal credit assign ment problem sutton 
suppose learning agent performs sequence actions obtains certain outcomes 
assign credit blame individual situation situation action pair adjust decision making improve performance 
second subproblem generalization problem known structural credit assignment problem 
problem space large explore com pletely learning agent ability guess new situations experi ence similar situations 
course learning subproblems solved 
popular best understood approach temporal credit assignment problem td methods sutton 
td methods solid math foundation closely related dynamic programming barto 
forms td reinforcement learning proposed adaptive heuristic critic ahc learning architecture sutton barto learning watkins 
methods applied solve simple learning problems sutton anderson watkins kaelbling sutton converge slowly 
approaches generalization problem studied reinforcement learning 
anderson lin successfully combined td 
lin methods connectionist error backpropagation algorithm rumelhart 
watkins cmac algorithm grefenstette 
genetic algorithm method mahadevan connell statistical clustering method chapman kaelbling method similar decision tree algorithms moore method basically look tables uses variable state resolution 
goal study connectionist reinforcement learning solving non trivial learning problems study methods speeding reinforcement learning 
study took connectionist approach algorithms results appear relevant generalization methods 
studied frameworks connectionist reinforcement learning connection ist ahc learning extensions 
number studies anderson chap man kaelbling mahadevan connell ahc learning converge slowly 
speedup extensions basic methods investigated experience replay learning action models planning teaching 
different frameworks evaluated nondeterministic dynamic environment testbed 
environment consists kinds objects agent stationary food obstacles moving enemies 
task agent survive environment means trivial knowledge poor agent agent process large number input signals actions choose multiple goals 
rest organized follows 
section presents learning frameworks algorithms 
section describes dynamic environment survival task 
sec tions implementation performance learning agents 
section assesses different learning frameworks comparing agents performance 
sec tion discusses limitations 
section concludes summarizing lessons learned study 

reinforcement learning frameworks reinforcement learning paradigm learning agent continually receives sensory inputs environment selects performs actions affect environment action receives environment scalar signal called reinforcement 
rein positive reward negative punishment 
objective learning construct optimal action selection policy simply policy maximizes agent performance 
natural measure performance discounted cumulative rein short utility barto rt discounted cumulative reinforcement starting time reinforcement received transition time discount factor adjusts importance long term consequences actions 
reinforcement learning planning teaching section describes frameworks connectionist reinforcement learning 
summarized follows ahcon connectionist ahc learning ahcon ahcon plus experience replay ahcon ahcon plus action models ahcon ahcon plus experience replay plus teaching qcon connectionist learning qcon con plus experience replay qcon qcon plus action models qcon qcon plus experience replay plus teaching basic idea frameworks learn evaluation function see eval util predict discounted cumulative reinforcement received 
evaluation function represented connectionist networks learned combination temporal difference td methods sutton error tion algorithm rumelhart 
essence td methods compute error called td error temporally successive predictions backpropagation algorithm minimizes error modifying weights networks 
starting discuss learning frameworks terms defined eval expected discounted cumulative reinforcement received starting world state simply utility state util expected discounted cumulative reinforcement received execution action response world state simply utility state action pair 
deterministic world util equal immediate reinforcement plus utility state discounted util eval equation generalized nondeterministic world consideration probabilities multiple outcomes watkins 
note meaningful estimate functions eval util relative policy different policies result different reinforcements received 
explicitly mentioned functions assumed relative current policy 

ahc learning framework ahcon framework ahcon connectionist implementation adaptive heuristic critic learning architecture barto 
consists components evalua tion network policy network stochastic action selector 
essence framework decomposes reinforcement learning subtasks 
subtask construct 
lin agent utility selector merits lt np world state rei action effectors 
framework ahcon 
bold lines indicate vector signals thinner lines indicate scalar signal 
td methods evaluation network accurately models function eval 
policy network takes world state inputs assigns action value called action merit indicating relative merit performing action response world state 
second subtask adjust policy network assign higher merits actions result higher utilities measured evaluation network 
ideally outputs policy network approach extremes example best action 
policy network state agent policy choose action highest merit 
order learn optimal policy effectively agent take actions appear suboptimal relative merits actions assessed 
stochastic action selector favors actions having high merits purpose 
course learning evaluation policy networks adjusted tally 
summarizes learning algorithm simplest td method td 
td methods learn evaluation function step write recursive definition desired function 
definition utility state immediate payoff plus utility state discounted 
desired function satisfy equation eval eval learning equation may hold true 
difference sides equation called td error reduced adjusting weights evaluation net backpropagation algorithm step 
policy network adjusted step td error 
idea follows meaning action better previously ex pected policy modified increase merit action 
hand policy modified decrease merit 
policy network updated respect actions single experience know merits actions 
reinforcement learning planning teaching 
ac current state eval 

perform action new state reinforcement 
eval 
adjust network backpropagating td error input 
adjust policy network backpropagating error input 
go 

algorithm framework ahcon 
learning stochastic action selector select function algorithm chooses actions randomly probability distribution determined action merits 
probability choosing action ai computed follows prob ai emi merit action ai temperature randomness action selection 

learning framework qcon framework qcon implementation learning 
qcon learns network called utility network accurately models func tion util 
utility network state agent policy choose agent stochastic action selector utilities utility network world state sensors action effectors 
framework qcon 
bold lines indicate vector signals thinner lines indicate scalar signal 

lin 
current state action ui util 
elect 
perform action new state reinforcement 
max util actions 
adjust utility network backpropagating error au ui input aui 
go 

algorithm framework qcon 
action util maximal 
utility network eval uation function policy 
learning algorithm depicted briefly described 
generally speaking utility action response state equal immediate payoff plus best utility obtained state discounted desired util function satisfy equation util max util lk actions learning difference sides equation minimized backpropagation algorithm step 
note network modified respect actions single experience know utilities actions 
utility network described multiple outputs action 
replaced multiple networks action single output 
implementation desirable single utility network modified respect action matter desired network modified respect actions result shared hidden units actions 
similar argument apply policy network framework ahcon 
study multiple networks single outputs 
stochastic action selector needed explore consequences different actions 
action selector framework ahcon employed purpose 

experience replay frameworks ahcon qcon reinforcement learning trial error method may result undesirable dam ages learning agent hostile environment 
faster agent learns damage agent suffer 
generally speaking temporal difference learning slow process temporal credit assignment especially credits propagated long action sequence 
rest section describes techniques speed credit propagation process shorten trial error process 
reinforcement learning planning teaching basic ahc learning algorithms described inefficient obtained trial error utilized adjust networks thrown away 
wasteful experiences may rare involving damages costly obtain 
experiences reused effective way 
experience quadruple meaning execution action state results new state reinforcement lesson temporal sequence experiences starting initial state final state goal may may achieved 
straightforward way reusing experiences call experience replay 
experience replay learning agent simply remembers past experiences repeat presents experiences learning algorithm agent experienced experienced 
result doing process credit blame propagation sped networks usually converge quickly 
important note condition experience replay useful laws govern environment learning agent change time change rapidly simply laws changed past experi ences may irrelevant harmful 
experience replay effective propagating credit blame sequence experiences replayed temporally backward order 
effective td methods recency factor greater zero 
td error adjusting network weights determined discrepancy multiple consecutive predictions 
lin presents detailed algorithm study backward replay nonzero mobile robot domain 
results encouraging 
backward replay simplicity reason 
algorithm ahcon simply ahcon plus repeated presentation past experiences algorithm experiences involving non policy actions current policy 
presenting experience algorithm mean bind variables algorithm experi ence 
reason exception ahcon estimates eval relative current policy sampling current policy executing policy actions 
replaying past experiences equivalent sampling policies past disturb sampling current policy past policies different current 
consider agent current policy chooses action state agent changes choose bad action state utility state eval drop dramatically 
similarly utility underestimated bad action replayed times 
experience replay useful agent replay experiences involving actions follow current policy 
just ahcon different reason qcon replay actions follow current policy 
consider previous example 
agent changes choosing action choosing bad action state values util util change 
matter fact util function implemented look tables watkins shown learning guaranteed learn optimal policy long state action pairs tried infinite number times weak conditions 
words harmful tabular version learning 
lin replay bad actions 
hold true connectionist learning backpropagation algorithm modifies utility network respect input state affects network respect possible input states 
utility network trained bad experiences times consecutively net come underestimate real utilities state action pairs 
short ahcon qcon replay policy actions 
agent may stochastic policy suggested previously actions possibility chosen policy 
difficult say action current policy action 
action considered non policy action prob ability see equation chosen current policy lower pt 
ahcon pt give roughly best performance learning task described 
qcon pl tried give similar performance 
pt gave bad performance 
words ahcon quite sensitive replaying non policy actions qcon sensitive 
difference sensitivity explained fact replaying non policy actions case look tables bad ahc learning fine learning 
frameworks ahcon qcon efficient terms time memory space experiences need stored replayed 
matter fact unnecessary harmful replay experience times possible networks trained specific experience usually harms generalization 
consider agent living nondeterministic world 
agent state performs action time receive great penalty time receive penalty 
agent just experienced situation penalty trained experience times agent come believe harmless perform certainly wrong 
section describes partial solution prevent training 

action models frameworks ahcon qcon way reusing past experiences investigated dyna architecture sutton experiences build action model planning learning 
action model function state action state immediate reinforcement 
stochastic worlds action may multiple outcomes 
accurate action model may need model probability distribu tion outcomes issue address 
action model intended mimic behaviors environment 
ac action model available agent experience consequences ac tions participating real world 
result agent learn faster projecting faster acting importantly fewer mistakes committed real world 
frameworks ahcon qcon discussed idea 
reinforcement learning planning teaching 
framework ahcon framework ahcon similar framework ahcon differs ahcon learns action model uses sutton called relaxation planning sutton 
relaxation planning closely related dynamic programming incremental planning process consists series shallow usually step look ahead searches ultimately produces results conventional deep search 
sutton dyna architecture learning algorithm similar ahcon applied real world situations faced agent hypothetical situations randomly generated 
case state obtained executing action case state obtained applying action model 
approach inefficiencies 
hypothetical situations randomly generated agent may spend effort planning hypothetical situations happen real world 
second clear approach decides relaxation planning longer necessary doing 
relaxation planning algorithm see proposed addresses cies projecting actions states visited states chosen random 
actions state examined time relative merits actions directly effectively assessed kind policy iteration howard ahcon dyna architecture 
compared dyna architecture disadvantage algorithm number hypothetical experiences generated limited number states visited 
deterministic world utility state equal immediate payoff obtained executing best action plus discounted utility new state eval max eval la actions note function correct action model available utility state accurately estimated looking ahead step 
learning evaluation network adjusted minimize difference sides equation step 
policy network updated manner compute average utility state assumption current policy stochastic action selector step 
prob probability distribution function action selection equation 
policy network modified increase decrease merits actions average step 
algorithm framework similar ahcon relaxation planning may take place step step case agent reactive case agent may better action choices benefit directly step look ahead 
efficient relaxation planning performed selectively steps 
idea situations policy decisive best action relaxation planning needed 
policy sure best action relaxation planning performed 
way learning actions equally relaxation planning performed frequently 
learning proceeds relaxation planning performed 
study promising actions probability chosen greater 
lin 
current state eval 
select promising actions 
action go 
simulate action predicted new state reinforcement eval 
ea max max 
adjust evaluation network backpropagating error max input 
adjust policy network backpropagating error ea input 
exit 

relaxation planning algorithm ahcon 
prob probability function stochastic action selection 
framework qcon framework qcon framework qcon plus action models 
algorithm similar 
main difference step step agent perform relaxation planning looking step ahead see 
reason harmful qcon replay non policy actions section experiencing bad actions model harmful 
relaxation planning performed selectively 
situations best action obvious look ahead needed 
promising actions actions tried model 
qcon similar sutton dyna architecture currently visited state start hypothetical experiences 
study promising actions probability chosen greater 

current state action ui util 
select promising actions 
action go 
fora simulate action predicted new state reinforcement 
maz actions 
adjust utility network backpropagating error au ua input 
exit 

relaxation planning algorithm qcon 
reinforcement learning planning teaching 
teaching frameworks ahcon qcon reinforcement learning trial error process 
success learning relies agent luck achieving goal chance place 
probability achieving goal chance arbitrarily small time needed learn arbitrarily long whitehead 
learning barrier prevent agents shortening learning time dramatically 
way probably best way overcome barrier learn expertise directly external experts 
teaching plays critical role human learning 
teaching shorten learning time turn intractable learning tasks tractable 
learning viewed search problem mitchell teaching sense viewed external guidance search 
teaching useful reasons teaching direct learner explore promising part search space contains goal states 
important search space large thorough search infeasible 
second teaching help learner avoid stuck local maxima 
real example section 
learning experts efficient way learn experts may available may available part time 
having overcome formidable learning barrier help experts learners able rest 
shown teaching easily gracefully integrated ahc style reinforcement learning 
frameworks discussed able learn teachers reinforcement learning 
frameworks ahcon qcon ahcon qcon plus teaching exactly experience replay algorithms ahcon qcon respectively 
teaching conducted manner teacher shows learning agent instance target task achieved initial state 
sequence shown actions state transitions received reinforcements recorded taught lesson 
taught lessons collected repeatedly replayed way experienced self generated lessons replayed 
experienced lessons taught lessons replayed selectively words policy actions replayed 
taught actions known optimal replayed time 
term lesson means taught experienced lessons 
unnecessary teacher demonstrate optimal solutions order agent learn optimal policy 
fact agent learn positive negative examples 
property approach teaching different supervised learning approaches mozer pomerleau 
supervised learning paradigm learning agent tries mimic teacher building mapping situations demonstrated actions generalizing mapping 
consequent drawbacks teacher required create training instances cover situations encountered agent new situation encountered agent strategy teacher available give solu tion teacher expert agent learn optimal strategy 
third drawback neglected researchers important humans want build robots supervised learning techniques 
humans robots different sensors see different things optimal action human 
point view may optimal robots take account fact robots may able sense information humans decisions 
human teachers teach terms robots sense 
obviously frameworks ahcon qcon second draw backs 
third drawback learn rote 
determine real utilities shown actions shown action results state agent believes bad agent increments decrements likelihood performing action situation 
hand typically learners going benefit experts naive teachers 

dynamic environment date reinforcement learning studied solving simple learning tasks pole balancing route finding 
researchers instance mahadevan connell chapman kaelbling begun study complicated problems 
unclear reinforcement learning scale deal realistic problems 
goals study utility various frameworks solving nontrivial learning problems 
devised simulated dynamic environment testbed evaluating performance various frameworks 
dynamic environment cell world 
sample environment shown 
kinds objects environment agent food enemies obstacles 
perimeter world considered occupied obstacles 
bottom energy indicator 
start agent enemies placed initial positions shown ooo ooo ooo ooo ooo ooo oo 
dynamic environment involving agent enemies food obstacles 
indi cate agent energy level 
reinforcement learning planning teaching fifteen pieces food randomly placed unoccupied cells 
move agent actions choose walk adjacent cells 
agent attempts walk obstacles remain position 
agent moves enemies allowed stay move adjacent cell occupied obstacles 
allow agent escape chasing enemies enemy speed limited full speed agent 
enemies move ran tend move agent tendency stronger agent gets closer 
appendix gives algorithm choosing enemy actions 
agent goals get food possible avoid caught enemies 
note goals conflict situations agent learn arbitrate 
play ends agent gets food dies 
agent dies collides enemy runs energy 
start new play agent units energy 
piece food provides agent units additional energy move costs agent unit 
parameter values empirically chosen survival environment easy difficult 
learning task interesting realistic agent allowed see local area surrounding 
agent point view world nondeterministic enemies behave randomly world partially observable seen move completely predictable 
human player avoid enemies get food time survival environment trivial 
survive agent learn approach food escape enemies avoid obstacles identify stay away certain dangerous places corridors easily seek food food sight 

learning agents section presents implementation ahc agents agents ahcon ahcon ahcon ahcon agents agents qcon qcon qcon qcon learn survive environment 
agents named learning frameworks 
order compare perfor mance agents exactly reinforcement signals sensory inputs de scribed 
connectionist networks trained symmetrical version error backpropagation algorithm squashing function variation sigmoid function 
note agents die get re try learned networks preserved 

reinforcement signals move learning agent receives environment reinforcement signals 
lin agent dies agent gets food 
negative reinforcement considered bad positive 
food reward smaller penalty dead important stay alive 
food reward chosen empirically tried different values agent ahcon gave roughly best performance 
theoretically speaking agent learn get food food reward agent required get food stay alive 
sooner action decisions rewarded sooner agent learn 

input representation described ahc agents evaluation policy networks agents utility networks 
subsection describes input representation networks subsection describes output representation 
evaluation policy utility networks structurally similar layer feed forward network consisting input units output unit 
number hidden units parameter tuned performance 
networks fully connected connections input output units 
input units networks divided groups enemy map food map obstacle map energy level history information 
maps shows cer tain kind object local region surrounding agent thought obtained array sensors fixed agent 
sensor array moves agent moves 
local action representation see section array rotates agent rotates 
shows configuration food enemy obstacle sen sor arrays 
food sensor may activated nearby food objects food object may activate nearby food sensors 
food sensor array composed different ooo ooooo xxx xxx xxx xxx ooooo ooo 
food enemy obstacle sensor arrays 
reinforcement learning planning teaching types sensors types 
different food sensor types different resolution different receptive fields sensors activated food thirteen nearby cells respectively 
technique encoding spatial positions objects known coarse coding hinton 
multiple resolution coarse coding techniques food positions effectively coded loss critical information 
enemy sensor array similar layout food sensor array con sists types sensors 
obstacle sensors organized differently type obstacle sensor obstacle sensor activated obstacle corresponding cell 
coarse coding technique encode obstacle positions works effectively features encoded sparse hinton case obstacles 
agent energy level coarse coded sixteen input units 
sixteen units represents specific energy level activated agent energy level close specific level 
units encode agent previous action choice unit indicate previous action resulted sion obstacle 
units convey kind history information allows agent learn heuristics moving back previous positions generally bad interesting objects food keep moving direction see interesting 
action representation mentioned agent actions 
experimented differ ent representations agent actions global representation actions move north south west east regardless agent current orientation environment 
local representation actions move front back right left relative agent current orientation 
local representation agent begins random orientation new play orientation changed move direction step 
instance agent takes consecutive right moves collision occurs cell orientation 
output representation single output evaluation network represents estimated utility input state 
corresponding action representations different output rep policy utility networks 

lin global representation action representation agent uses policy network single output represents merit moving north 
merits moving directions computed network rotating state inputs including food map enemy map obstacle map bits encoding agent previous action degrees 
similarly agent uses utility network single output represents utility moving north re sponse input state 
utilities moving directions computed rotating state inputs appropriately 
advantage action symmetry learning task simplified learned situation automatically situations symmetric 
local representation representation take advantage action symmetry 
ahc agent uses policy networks agent utility networks 
network single output 
outputs networks 
definition eval util functions may greater agent close pieces food 
rare cases truncated compute td errors 
output units evaluation utility networks mainly linear part sigmoid function 
alternative truly linear activation function output units 

action model agents ahcon qcon learn action model 
action model intended model input output behavior dynamic environment 
specifically world state action performed model predict reinforcement signal received food enemies obstacles appear agent energy level 
nondeterministic environment action possi ble outcomes 
alternatives modeling environment action model generate list outcomes associated probabilities happening outcome 
second alternative adopted simplicity 
food obstacles move positions quite easy predict connectionist networks 
shorten simulation time significantly simplifying model learning task agents ahcon qcon required learn reinforcement networks predicting immediate reinforcement signal enemy networks predicting enemy positions 
reinforcement enemy networks layer networks hidden layers 
reinforcement networks inputs input bits mentioned enemy networks enemy obstacle maps 
single output enemy network trained predict particular enemy sensor turned agent moves 
reinforcement enemy networks learned line just networks 
learning networks kind supervised learning encountered experiences saved queue limited length repeated presentation networks 
enemies nondeterministic agent know exact enemy positions due coarse coding prediction enemy positions incorrect networks trained 
interesting reinforcement learning planning teaching see performance agent perfect model learned quickly ahcon qcon provided perfect model just environment simulator 
section presents performance comparison perfect model learned potentially incorrect 

active exploration important problem faced autonomous learning agents tradeoff acting gain information acting gain rewards 
kaelbling proposed inter val estimate method control tradeoff 
unclear method apply connectionist reinforcement learning 
thrun er pro posed agent learn competence map estimates errors agent learned world 
map display small errors explored world states large errors rarely explored states 
active exploration achieved driving agent regions large errors 
mentioned uses stochastic action selector crude strategy allow active exploration 
stochastic selector give compromise gaining information gaining rewards 
complementary strategy agent dead trajectory increases temperature finds stuck small area getting food 
similar strategy robot learning tasks lin 
domains strategy quite effective improving learning speed quality 
prevention training mentioned section undesired training may occur experiences replayed times matter experiences come teacher trial error 
reduce possibility training save computa tion heuristic strategies employed complete play agent replays lessons chosen randomly experienced lessons lessons exponentially chosen 
decreasing number 
appendix gives algorithm choosing experiences stochastically 
complete play agent ahcon qcon stochastically chooses taught lessons replay 
taught lessons chosen decreasing probability 
recall policy actions replayed 
policy actions probability chosen greater ahcon type agents pl type agents 
taught lessons gave agents nearly optimal fact set replaying taught lessons 

lin 
experimental results section presents performance various learning agents 
study learning agents global action representation took advantage action symmetry 
second study agents local action representation exploit action symmetry 

experimental design study consisted experiments 
experiment took sparc station days complete 
experiment training environments test environments randomly generated 
agents allowed learn playing training environments 
time agent played training environments tested test environments learning turned temperature set zero 
average number food pieces obtained agent test environments plotted versus number training environments agent played far 
learning curves show mean performance experiments 
instructive mention difference objective learning algorithms set agents 
maximize discounted cumulative rein food possible 
achieving ob necessarily achieving 
reinforcement function defined section objectives quite different 
studies lessons generated agents ahcon qcon 
generate lessons agent trying survive manually chosen environment involved instructive situations 
got food lessons 
learning agent parameters tuned performance discount factor fixed temperature stochastic action selector hp hu number hidden units evaluation policy utility networks te learning rate backpropagation algorithm evaluation policy utility networks momentum factor backpropagation algorithm fixed networks range random initial weights networks fixed 

study global action representation study agents global action representation exploited action sym 
table shows parameter settings generate mean learning curves shown 
parameter values empirically chosen give roughly best formance agents ahcon qcon 
little search done agents 
ahc agents temperature lower agents action reinforcement learning planning teaching table 
parameter values study 
agent ahcon hp ahc agents hp qcon lit agents food ii food perfect model 
ahcon learned model food 



qcon ahcon 
ahcon plays bo play 
go play 
learning curves agents global action representation 

lin merits supposed approach best action utilities state action pairs usually small numbers 
cooling temperatures fixed low temperatures 
learning agents learning speedup techniques smaller learning rates basic learning agents trained networks 

study local action representation study agents local action representation exploit action sym 
agents parameter settings study learn ing rates doubled 
shows learning curves agents 
food food ii 




plays bo bo oo food plays 
qcon learned del plays 
learning curves agents local action representation 
reinforcement learning planning teaching 
observation learning agents qcon roughly best 
see absolute performance best agent trials qcon tested randomly gen erated environments 
table shows agent got food got killed ran energy 
got food got killed ran energy average agent got pieces food play 
table shows agent got food piece food pieces pieces 
mentioned previously learning curves figures show mean perfor mance experiments 
standard deviations curves point point roughly food pieces learning food piece asymptotic performance reached 
generally speaking agents better performance smaller performance deviations experiments 
agent performance difference experiments imply repeatability experimental results experiment different set training test environments 
really matters relative performance agents consistent time entire experiment 

discussion section compares learning agents analyzing experimental results section 

ahc agents vs agents study asymptotic performance agents significantly better ahc agents 
number previous studies lin sutton superiority learning ahc learning 
study confirm superior ity fact types learning similarly effective terms asymptotic performance learning speeds 
open question learning works better ahc learning general 
note different action representations significant differ ence agents 
ahc agents local action representation apparently worked better global action representation far asymptotic performance concerned 
result reveals different action representations different input represen tations may big difference performance learning system 


effects experience replay compared ahcon qcon learning speed ahcon qcon improved 
difference asymptotic performance significantly different trials study get significant study learning continued 
concludes experience replay effective way speed credit assignment process 
experience replay easy implement 
cost mainly extra memory needed storing experiences 
prevent training efficient learning agent needs strategy determining experiences remember replay 

effects action models advantage action models relaxation planning inconclusive 
hand compared ahcon qcon learning speed ahcon qcon significantly improved agent need learn action model perfect model provided 
hand agent needed learn model model going help depended relative difficulty learning model learning survival task directly 
second study learning model faster learning task 
ciently model learned agents ahcon qcon began benefit 
hand study ahcon qcon learn task quite rapidly 
potentially incorrect model turned misleading 
matter fact qcon learned model performed worse qcon study explained fact agent able learn perfect model nondeterministic world hundreds trials 
speculated section relaxation planning needed learn ing proceeds 
speculation confirmed studies 
shows average number hypothetical actions taken step 
curves show mean hypothetical actions ahcon ect 
od 
ect model 
qc od 
pla 
average number hypothetical actions taken step study 
reinforcement learning planning teaching results experiments study 
maximum actions choose step 
see amount planning dropped quickly learning approached asymptote 
drop close zero task situations actions fact equally tried simulation 
tasks small portion actions relevant time saving significant 

effects teaching recall ahcon qcon ahcon qcon plus replaying taught lessons 
study performance differences ahcon ahcon qcon qcon negligible entire experiment 
task study difficult experience replay done job improvement expect teaching 
second study noticeable differences learning speeds ahcon ahcon qcon qcon 
differences asymptotic performance insignificant 
result reveals advantage teaching significant learning task gets difficult 
lin reported similar results learn ing robot 
robot tasks dock battery charger sonar sensors detecting obstacles light intensity sensors locating charger light top 
dock robot position front charger small errors drive collide charger order get tight connection 
teacher robot failed learn task early stage learning robot quickly learned avoid obstacles turn prevented colliding charger learning stuck local maximum 
shown lesson dock sample position robot able learn task 
shown lessons robot learned task rapidly 

experience replay vs action models comparing learning curves agents ahcon ahcon qcon qcon clear consistent superiority experience replay action models agents learn model 
clear consis tent superiority agents provided perfect action model 
result simply explained fact took time agents ahcon qcon learn sufficiently model agents start advantage 
model learned model misleading impede learning 
matter fact ahcon qcon perform relaxation planning trials avoiding completely model 
couldn relaxation planning perfect action model outperform experience replay 
relaxation planning algorithms may effective 
lin way action models 
example relaxation planning algorithms performed just step look ahead 
number hypothetical experiences ahcon qcon generate limited number states visited 
hand agents allowed generate hypothetical experiences randomly chosen states may spend time planning hypothetical situations happen 
secondly experience replay kind relaxation planning 
experience replay uses action model need learn action model simply obtained sampling past experiences 
essence collection past experiences model 
represents explicitly environment input output patterns implicitly probability distributions multiple outcomes actions 
experience replay effective easy implement learn action model 
answer unclear 
model learned merely doing relax ation planning worthwhile learn model experience replay thing relaxation planning 
may argue model generalizes provide learning algorithms induced interpolated experiences seen extra experiences result better learning evaluation functions 
evaluation functions generalize case non toy tasks unclear extra experiences extra 
hand having action model useful 
investigated action models learning evaluation functions 
ways model improve performance study 
example evaluation function action model look ahead planning technique help find best actions whitehead ballard thrun 
complex domain optimal policy may hardly obtainable looking ahead steps computer chess non optimality policy compensated accurate action model available 
action models effectively interesting issue needs study 

perfect performance 
agents fail reach perfect performance get food 
reasons local information agents may insufficient determine optimal actions perfect policy may complex rep resented connectionist networks 
matter fact played simulator 
allowed see objects local region learning agents got food pieces time 
em techniques play successfully look ahead planning crude remembering food positions come back get food lost sight food chasing enemies 
learning agents techniques 
reinforcement learning planning teaching 
limitations learning frameworks promising com mon limitations 
representation dependent 
learning system input representation critical successful learning 
coarse coding concentric multi ple resolution maps number input units needed task dramatically reduced 
techniques agents successful 
choosing action representation crucial 
example local action representation gave ahc agents better performance global action representation 
discrete time discrete actions 
far td methods exclusively domains discrete time actions 
possible extend idea temporal difference handle continuous time actions clear easily extension done 
note frameworks continuous states connectionist networks take real binary numbers inputs 
unwise sensing 
learning agent required sense entire environment step determine world state 
real world things look 
may irrelevant 
practice agent afford sense time 
important issue decide sensors effi ciently knowing current state whitehead ballard tan 
history insensitive 
humans usually decisions information currently sensed information past sensed learning agent reactive perceived current moment insensitive perceived past 
possibility history sensitive time delay networks lang recurrent networks williams zipser 
perceptual aliasing 
perceptual aliasing problem occurs agent internal state representation insufficient discriminate different external world states 
con sequence perceptual aliasing learning agent learn correct evaluation func tion act optimally 
consider packing task involves steps open box put object box close box 
agent driven current visual percepts accomplish task facing closed box agent know object box 
ways resolve problem 
solution actively choose perceptual actions measuring weight box resolve ambiguity 
kind solutions investigated whitehead ballard block stacking problem tan route finding problem 
second solution history information box opened help determine current state world 
solution general powerful 
deal complex real world problems kinds solutions needed 
hierarchical control 
theory td methods able accurately propagate credit long sequence actions 
true evaluation function modeled arbitrary accuracy example look tables 
practice compact function approximators allow generalization connec networks decision trees 
approximators td methods hardly propagate credit accurately action sequence longer 
lin say 
furthermore longer sequence time needed credit propa gation 
potential solution problem hierarchical control 
hierarchical control top level task decomposed subtasks subtask learned separately reinforcement learning top level policy learned control tion subtasks lin mahadevan connell 

studied connectionist ahc learning moderately complex task survival dynamic environment 
results encouraging suggest reinforce ment learning promising approach building autonomous learning systems 
previous studies lin sutton superiority learning ahc learning 
confirmed finding case comparable performance methods 
studies required possible late conditions ahc learning effective 
reinforcement learning algorithms converge slowly 
investigated extensions speedup experience replay learning action models relaxation planning teaching 
relaxation planning kind incremental dynamic programming watkins caches results repeated shallow searches evaluation function action model looking ahead 
learning action model planning known idea 
model going speed learning depends relative difficulty learning model learning solve task directly 
study learn ing model nondeterministic dynamic world turned difficult limited utility model cases 
sampling past experiences experience replay performs process relaxation planning need learn action model 
key point replay policy actions 
study experience replay quite effective speeding credit assignment process 
matter fact experience replay better relaxation planning learned model learning model takes time 
worthwhile learn model model learned merely doing relaxation planning 
action model thing model evaluation function utilized instance perform conventional look ahead search optimal actions optimal policy available 
described approach integrating teaching reinforcement learn ing 
learn solve problem reinforcement learning learning agent achieve goal trial error 
teaching effective technique shorten trial error process simply providing success examples learner 
teach ing help learner avoid stuck local maxima instance section 
teaching critical learning survival task expect teaching important tasks get complicated rewards get obtain luck lin 
reinforcement learning planning teaching acknowledgments express gratitude tom mitchell rich sutton fruitful discussions 
jeffrey schlimmer nils nilsson reviewers sebastian thrun valuable comments 
supported partly nasa contract partly fujitsu laboratories notes 
normally agent internal description world obtained vector sensory readings may may pre processed 
assumed agent input adequately represents external world sufficient determine optimal actions 
reinforcement learning works assumption relaxed 

see lin watkins dayan td 
simplicity assume world deterministic discussion learning algorithms straightforward extend discussion handle nondeterministic worlds barto watkins 
reinforcement learning algorithms deterministic nondeterministic worlds 

way reduce negative effects ahcon replaying non policy actions treat action policy action matter probability 
words computed td error adjust networks weighted probability choosing replayed action 
method effectively thresholding method pl 

term world model sutton decided term action model refer model predicting effects actions 
term world model mean agent internal representation world 
example knowledge state function referred action model cognitive map familiar environment referred world model 

ahcon problem planning step promising action determine amount change eval 

may appear learning task simplified making obstacles symmetric 
true objects world enemies food pieces positioned randomly symmetrical pattern 
number different input patterns agent come estimated greater 

previous studies lin lin reported similar results 
fact differences tween experimental designs study previous studies 
example action model different parameter values changed implementation details learning algorithms different 
see lin lin details differences 
anderson 

strategy learning multilayer connectionist representations 
proceedings fourth international workshop machine learning pp 

barto sutton watkins 

learning sequential decision making 
gabriel moore eds learning computational neuroscience 
mit press 
barto bradtke singh 

real time learning control asynchronous dynamic programming 
technical report 
university massachusetts computer science department 
chapman kaelbling 

input generalization delayed reinforcement learning algorithm performance comparisons 
proceedings 
dayan 

convergence td general machine learning 
grefenstette ramsey schultz 

learning sequential decision rules simulation models competition 
machine learning 

lin hinton mcclelland rumelhart 

distributed representations 
parallel explorations microstructure cognition vol 
bradford books mit press 
howard 

dynamic programming markov processes 
wiley new york 
kaelbling 

learning embedded systems 
ph thesis department computer science stanford university 
lang 
time delay neural network architecture speech recognition 
ph thesis school computer science carnegie mellon university 
lin long ji 

self improving reactive agents case studies reinforcement learning frameworks 
pro ceedings international conference simulation adaptive behavior animals animats pp 

technical report cmu cs carnegie mellon university 
lin long ji 

self improvement reinforcement learning planning teaching 
proceedings eighth international workshop machine learning pp 

lin long ji 

programming robots reinforcement learning teaching 
proceedings aaa pp 

mahadevan connell 

scaling reinforcement learning robotics exploiting subsumption architecture 
proceedings eighth international workshop machine learning pp 

mitchell 

generalization search 
artificial intelligence 
moore 

variable resolution dynamic programming efficiently learning action maps multivariate real valued state spaces 
proceedings eighth international workshop machine learning pp 

mozer 


connectionist expert system learns example 
institute cognitive science report 
university california san diego 
pomerleau 

alvinn autonomous land vehicle neural network technical report cmu cs 
carnegie mellon university 
rumelhart hinton williams 

learning internal representations error propagation 
parallel distributed processing explorations microstructure cognition 
vol 

bradford books mit press 
sutton 

temporal credit assignment reinforcement learning 
ph thesis dept computer information science university massachusetts 
sutton 

learning predict methods temporal differences 
machine learning 
sutton 

integrated architectures learning planning reacting approximating dynamic programming 
proceedings seventh international workshop machine learning pp 

tan ming 

learning cost sensitive internal representation reinforcement learning 
proceedings eighth international workshop machine learning pp 

thrun er linden 

planning adaptive world model 
touretzky ed advances neural information processing systems morgan kaufmann 
thrun er 

active exploration dynamic environments 
appear touretzky ed advances neural information processing systems morgan kaufmann 
watkins 

learning delayed rewards 
ph thesis king college cambridge 
williams zipser 

learning algorithm continually running fully recurrent neural networks institute cognitive science report 
university california san diego 
whitehead ballard 

role anticipation reactive systems learn 
proceedings sixth international workshop machine learning pp 

whitehead ballard 

learning perceive act trial error 
machine learning 
whitehead 

complexity cooperation learning 
proceedings eighth international workshop machine learning pp 

appendix algorithm choosing enemy actions enemy dynamic environment behaves randomly 
step time enemy move time choose actions east south west north probability distribution reinforcement learning planning teaching prob ai pi po pi ai result pi obstacles exp angle dist angle angle direction action direction enemy agent dist distance enemy agent angle dist dist dist dist dist appendix algorithm choosing lessons agents replay experiences keep lessons memory 
lessons randomly chosen replay lessons exponentially chosen 
algorithm choosing lesson memory shown 
input sequence lessons 
ln ln latest 
output integer algorithm 
min 
random number 
logo tm 
