building text classifiers positive unlabeled examples bing liu department computer science university illinois chicago cs uic edu yang dai department university illinois chicago uic edu li wee sun lee school computing national university singapore singapore mit alliance comp nus edu sg philip yu ibm watson research center yorktown heights ny usa studies problem building text classifiers positive unlabeled examples 
key feature problem negative example learning 
techniques solving problem proposed literature 
techniques idea builds classifier steps 
existing technique uses different method step 
introduce new methods steps perform comprehensive evaluation possible combinations methods steps 
propose principled approach solving problem biased formulation svm show experimentally accurate existing techniques 

text classification process assigning predefined category labels new documents classifier learnt training examples 
traditional classification training examples labeled set pre defined category class labels labeling done manually 
text classification techniques proposed researchers far rocchio algorithm naive bayesian method nb support vector machines svm see 
main problem classic approach large number labeled training examples class needed accurate learning 
labeling typically done manually labor intensive time consuming 
years researchers investigated idea small labeled set class large unlabeled set help learning :10.1.1.1.5684
reduces manual labeling effort 
studies problem building class bing liu partially supported national science foundation nsf iis 
classifiers positive unlabeled examples negative examples 
algorithms proposed solve problem 
class algorithms step strategy 
algorithms include em pebl roc svm 
step identifying set reliable negative documents unlabeled set 
step em uses spy technique pebl uses technique called dnf roc svm uses rocchio algorithm 
step building set classifiers iteratively applying classification algorithm selecting classifier set 
step em uses expectation maximization em algorithm nb classifier pebl roc svm svm 
em roc svm methods selecting final classifier 
pebl simply uses classifier convergence poor 
steps seen iterative method increasing number unlabeled examples classified negative maintaining positive examples correctly classified 
shown theoretically sample size large maximizing number unlabeled examples classified negative constraining positive examples correctly classified give classifier 
introduce method step nb method method step svm perform evaluation possible combinations methods step step 
results benchmark system called learning positive unlabeled data available author web page 
propose principled approach solving problem biased formulation svm 
experimental results show new method superior existing step techniques 

related traditional text classification techniques require labeled training examples classes build classifier 
suitable building classifiers positive unlabeled examples 
theoretical study probably approximately correct pac learning positive unlabeled data conducted 
presents theoretical study bayesian framework 
sample complexity results learning maximizing number unlabeled examples labeled negative constraining classifier label positive examples correctly 
em technique reported 
pebl technique reported 
roc svm technique reported 
discuss detail 
techniques step strategy reports logistic regression technique solve problem :10.1.1.11.258
maximizing number unlabeled examples labeled negative methods learning positive unlabeled examples possible 
nb method called pnb tries statistically remove effect positive data unlabeled set proposed 
main shortcoming method requires user give positive class probability hard user provide practice 
possible discard unlabeled data learn positive data 
done class svm tries learn support positive distribution 
results show performance poorer learning methods take advantage unlabeled data 
related learning small labeled set large unlabeled set :10.1.1.1.5684
works small set labeled examples class large unlabeled set classifier building 
shown unlabeled data helps learning 
works different negative example 

techniques step section introduce na bayesian technique nb new method step identify set rn reliable negative documents unlabeled set denote positive example set 
describe rocchio technique roc svm spy technique em dnf technique pebl facilitate evaluation 
na bayesian classifier na bayesian technique popular method classification 
set training documents document considered ordered list words 
xd denote word xt position document di xt word vocabulary 
vocabulary set words considered classification 
set predefined classes consider classes 
perform classification compute posterior probability pr cj di cj class di document 
bayesian probability multinomial model pr pr additive smoothing pr di di pr xt pr smoothing factor xt di number times word xt occurs document di pr cj di depending class document :10.1.1.1.5684
experimental results show performs text data 
commonly laplacian smoothing 
significantly inferior 
experiments 
assuming probabilities words independent class obtain nb classifier di xd cr classifying document di class highest pr cj di assigned class document 
identifying set rn reliable negative documents unlabeled set done follows 
assign document class label 
assign document class label 
build nb classifier 
classifier classify documents classified negative form reliable negative set rn 
nb method step rocchio technique rocchio early text classification method 
method document represented vector feature value vector computed classic tf idf scheme 
set training documents cj set training documents class cj 
building rocchio classifier achieved constructing prototype vector class cj 
parameters adjust relative impact relevant irrelevant training examples 
recommends 
classification test document td uses cosine similarity measure compute similarity td prototype vector 
class prototype vector similar td assigned td 
algorithm uses rocchio identify set rn reliable negative documents replace nb rocchio 
spy technique em spy technique em 
randomly selects set positive documents put lines 
default value em 
documents act spy documents positive set unlabeled set spies behave similarly unknown positive documents allow algorithm infer behavior unknown positive documents runs em algorithm set positive set negative lines 
em basically runs nb twice see em algorithm 
em completes resulting classifier uses probabilities assigned documents decide probability threshold th identify possible negative documents produce set rn 
see details 
dnf technique pebl dnf method builds positive feature set pf contains words occur positive set frequently unlabeled set lines 
lines tries filter possible positive documents document 
rn null 
sample 

ps 
assign document ps class label 
assign document class label 
em ps produces nb classifier 

classify document nb classifier 
determine probability threshold th 
document 
probability pr th 
rn rn spy technique em 
positive feature pf regarded strong negative document 

assume word feature set xn xi 
positive feature set pf null 

freq xi freq xi 
pf pf xi 
rn 
document 
xj freq xj xj pf 
rn rn dnf technique pebl 

techniques step techniques second step 
run svm sets rn step 
method 

run em 
method em 

run svm iteratively 
method pebl 

run svm iteratively select final classifier 
method roc svm 
discuss methods turn 
support vector machines svm support vector machines svm linear functions form inner product weight vector input vector svm classifier setting class 
main idea svm select hyperplane separates positive negative examples maximizing smallest margin 
set training examples xn yn xi input vector yi class label yi 
problem finding hyperplane stated optimization problem minimize subject yi deal cases may separating hyperplane due noisy labels positive negative training examples soft margin svm proposed formulated minimize subject yi parameter controls amount training errors allowed 
em algorithm em expectation maximization em algorithm popular iterative algorithm maximum likelihood estimation problems missing data 
em algorithm consists steps expectation step maximization step 
expectation step basically fills missing data 
case produces revises probabilistic labels documents rn see 
parameters estimated maximization step missing data filled 
leads iteration algorithm 
em converges parameters stabilize 
nb iteration em employs equations building nb classifier equations expectation step equation maximization step :10.1.1.1.5684
class probability document takes value 
algorithm 
document assigned class label 
document rn assigned class label 
document rn assigned label initially 
iteration em assigned probabilistic label pr 
subsequent iterations set participate em newly assigned probabilistic classes 

run em algorithm document sets rn converges 
em algorithm nb classifier 
basically em iteratively runs nb revise probabilistic label document set rn 
iteration em produces nb classifier em mechanism select classifier 
iterative svm pebl pebl uses rn rn run svm iteratively 
basic idea iteration svm extract possible negative data rn put 
document assigned class label 
document rn assigned class label 

loop 
rn train svm classifier si 
classify si 
set documents classified negative 
exit loop 

rn rn 
running svm iteratively 
rn pebl step able identify small set negative documents 
set remaining unlabeled documents rn 
iteration converges document classified negative 
final classifier result 
iterative svm classifier selection roc svm method similar method section decides classifier algorithm converges svm iteration builds different svm classifier classifier may classifier 
iterative svm converges add lines 
svm classifier classify 
positive classified negative 
final classifier 
final classifier classifier selection 
reason selecting classifier danger running svm repetitively 
svm sensitive noise iteration svm extracts positive documents put rn svm classifier poor 
problem pebl 
algorithm decide svm classifier 
basically svm classifier convergence called line classify positive documents classified negative indicates svm gone wrong 
classifier 
final classifier 
threshold want conservative select weak svm classifier convergence 
method svm classifiers poor 
case pebl step extracts negative documents results weak classifier 
pebl classifier may weak iterative svms may go wrong 
spy rocchio step svm quite strong may best 
note svm may best classifier 
cases svm classifier middle best 
hard catch best classifier 

proposed biased svm proposed biased svm formulation problem 
set training examples xn yn xi input vector yi class label yi 
assume examples positive examples labeled rest unlabeled examples label negative 
shown sample size large minimizing number unlabeled examples classified positive constraining positive examples correctly classified give classifier 
noiseless case results svm formulation error positive examples unlabeled examples 
minimize subject xi distinguish formulation classic svm call biased svm 
allow noise error positive examples soft margin version biased svm formulation uses parameters weight positive errors negative errors differently 
minimize subject yi vary achieve objective 
intuitively give big value small value unlabeled set assumed negative contains positive data 
note asymmetric cost formulation solve data problem class data small class large 
formulation different motivation 
choose common practice separate validation set verify performance resulting classifier selected values 
need learn positive unlabeled examples arises retrieval situations employ commonly score performance measure pr precision recall 
unfortunately clear estimate score negative examples 
performance criteria comparing different classifiers proposed estimated directly validation set need negative examples :10.1.1.11.258
random variable representing input vector actual label 
criteria pr pr pr probability actual positive documents 
shown pr pr pr pr probability document classified positive :10.1.1.11.258
estimated positive examples validation set pr estimated validation set 
criteria works behaves similarly score sense large large small small 

empirical evaluation evaluate techniques step approach new biased svm problem formulation 
experimental setup datasets popular text collections experiments 
reuters documents collected reuters newswire 
categories populous 
category employed positive class rest negative class 
gives datasets 
second collection usenet articles collected lang different newsgroups 
group approximately articles 
newsgroup positive set rest groups negative set creates datasets 
data pre processing applied stopword removal feature selection stemming done 
rocchio svm tf idf values feature vectors 
dataset documents randomly selected test documents 
rest create training sets follows percent documents positive class selected positive set rest positive documents negative documents unlabeled set range create wide range scenarios 
experimental systems experiments em roc svm conducted systems 
pebl publicly available implemented 
svm svmlight system linear kernel 
methods implemented 
evaluation measure experiments popular score positive class evaluation measure 
score takes account recall precision pr 
accuracy results 
due space limitations list 
accuracies behave similarly scores 
results step strategy summarize methods studied step approach 
step 
spy method em 

dnf method pebl 

rocchio method roc svm 
www research att com lewis reuters html www cs cmu edu afs cs project theo www naivebayes newsgroups tar gz 
nb method proposed 
step 
em method em 
svm method proposed 
runs svm step 
svm method pebl 
runs svm iteratively 
classifier convergence final classifier evaluated test data 

svm method roc svm 
runs svm iteratively classifier selection iterative svm converges selects classifier final classifier 
clearly technique step combined technique step 
empirically evaluate possible combinations table shows macro averaged score reuters datasets setting 
due space limitations unable list detailed results 
scores obtained unseen test sets 
columns show scores combinations methods steps 
pebl dnf combined svm em spy combined em roc svm svm column gives results nb setting 
case nb simply treats documents unlabeled set negative examples 
results allow see sophisticated techniques 
table shows macro averaged scores newsgroup datasets setting 
results tables draw 

em performance stable wide range conditions 
comparable positive set small 
positive set large worse combinations 
observations true columns 
reasons em uses nb weaker classifier svm data sets suitable nb em 
see detailed discussion point 

pebl performance poor number positive examples small 
reason pebl second step go wrong large number positive documents 
positive set large stable 
results comparable reuters datasets 
newsgroup datasets worse spy svm roc svm nb svm 
pnb system compared requires user input positive class probability hard user supply practice 
furthermore see nb techniques pnb nb inferior svm techniques 
plan compare logistic regression approach near :10.1.1.11.258

spy svm combinations positive set small 
reason positive set small number spies put unlabeled set small resulting rn set reliable 
best method long positive set small 
nb svm gives results cases slightly inferior spy svm 
believe practice positive sets reasonably large user aware sufficiently large representative positive set obtain result 
methods efficient run svm 

roc svm rocchio svm techniques large positive sets 

dnf spy rocchio nb step dnf weaker step 
reuters data large positive sets 
spy rocchio robust 
nb slightly weaker 

svm vs em nb step clear methods step svm svm svm significantly outperform em methods positive set reasonably large 
known svm stronger classifier nb em uses nb 

nb interesting observe single nb column tables slightly outperforms em column rocchio em column nb em column 
known nb able tolerate noise 
running em results worse 
reason em weakness due assumptions nb em runs nb multiple times 
devising nb classifier assumptions words independent class text documents generated mixture model mapping mixture components classes means class contains documents topic category :10.1.1.1.5684
true situations negative class contains documents diverse topic categories 
run nb worse results get 
phenomenon mentioned :10.1.1.1.5684
shown em outperforms nb datasets contain documents topics 
assumption satisfied 
believe practice negative class typically contains documents topics 
noticed experiments results worse worse iteration em 
em em classifier selection mechanism able select classifier time 
means em simply nb step 
note methods just nb rocchio svm step dnf dnf dnf spy spy spy rocchio rocchio rocchio nb nb nb nb step em svm pebl svm em svm svm svm em svm svm roc svm em svm svm svm nb step dnf dnf dnf spy spy spy rocchio rocchio rocchio nb nb nb nb step em svm pebl svm em svm svm svm em svm svm roc svm em svm svm svm nb class svm 
fact experimented 
results poor listed 
example score class svm datasets 

pure nb pure svm obtain scores see table pure case original training data positive examples added negative set form unlabeled set 
reuters newsgroup nb svm comparing results results tables observe positive set large scores svm methods close pure case better pure case 
number positive documents large room improvements 
results biased svm set experiments svmlight package allows control parameters 
experiments varied 
training documents validation set experiment 
classifier selection criterion pr pr select best parameters validation set 
final test results obtained original training set parameter table average scores reuters collection table average scores newsgroup collection selected 
note need run svm large number times 
heuristics designed reduce number runs described full version 
performed experiments 
averaged results table show biased svm performs better previous best methods tables 
observe positive set small improvement significant 
especially true newsgroup collection harder collection 
table average scores collections average score biased svm previous best score reuters newsgroup 
discussed step strategy learning classifier positive unlabeled data 
new methods added existing techniques 
comprehensive evaluation combinations methods conducted compare performances enables draw important 
proposed principled approach solving problem biased formulation svm 
results show general biased svm outperforms existing step techniques 

agrawal bayardo jr srikant 
athena mining interactive management text databases edbt 

basu banerjee mooney 
semi supervised clustering seeding icml 

bennett demiriz 
semisupervised support vector machines advances neural information processing systems 

blum mitchell 
combining labeled unlabeled data training colt 

craven 
exploiting relations concepts acquire weakly labeled training data icml 

buckley salton allan 
effect adding relevance information relevance feedback environment sigir 

dempster laird rubin 

maximum likelihood incomplete data em algorithm journal royal statistical society 

denis pac learning positive statistical queries alt 

denis gilleron 

text classification positive unlabeled examples 

ghani 
combining labeled unlabeled data multiclass text categorization icml 

goldman zhou 
enhancing supervised learning unlabeled data icml 

guyon boser vapnik 

automatic capacity tuning large classifiers advances neural information processing systems vol 


joachims 
text categorization support vector machines learning relevant features ecml 

joachims 

making large scale svm learning practical advances kernel methods support vector learning sch lkopf burges smola ed 

lang 

newsweeder learning filter netnews icml 
:10.1.1.11.258
lee liu 
learning positive unlabeled examples weighted logistic regression icml 

lewis gale 

sequential algorithm training text classifiers sigir 

li liu 

learning classify text positive unlabeled data 
ijcai 



note general case bayes laplace formula inductive posteriori probabilities transactions faculty 

liu lee yu li 

partially supervised classification text documents icml 



class svms document classification machine learning research 

mccallum nigam 
comparison event models na bayes text classification aaai workshop learning text categorization 


morik joachims 
combining statistical learning knowledge approach case study intensive care monitoring icml 

muggleton 

learning positive data machine learning accepted 

muslea minton knoblock 

active semi supervised learning robust multiview learning icml 
:10.1.1.1.5684
nigam mccallum thrun mitchell 

text classification labeled unlabeled documents em machine learning 

osuna freund girosi 
support vector machines training applications 
ai memo massachusetts institute technology 

kowalczyk 

unlabeled data text classification addition cluster parameters icml 

rocchio 

relevant feedback information retrieval salton ed 
smart retrieval system experiments automatic document processing englewood cliffs nj 

salton mcgill 

modern information retrieval 
mcgraw hill 

scholkopf platt shawe smola williamson 

estimating support high dimensional distribution technical report msr tr microsoft research 

vapnik 

nature statistical learning theory springer verlag ny usa 
yang liu 

re examination text categorization methods sigir 

yu han chang 

pebl positive example learning web page classification svm kdd 

zhang 

value unlabeled data classification problems icml 
