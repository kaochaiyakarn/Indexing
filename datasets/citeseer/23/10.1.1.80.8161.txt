performance counter architecture computing accurate cpi components james smith ghent university belgium ece university wisconsin madison email jes ece wisc edu cycles instruction cpi stacks break processor execution time baseline cpi plus number event cpi components 
cpi breakdowns helpful gaining insight behavior application microprocessor consequently widely software application developers computer architects 
computing cpi stacks superscalar order processors challenging various overlaps execution events cache misses tlb misses branch mispredictions 
shows meaningful accurate cpi stacks computed superscalar order processors 
interval analysis novel method analyzing processor performance gain understanding performance impact various events 
understanding propose novel way architecting hardware performance counters building accurate cpi stacks 
additional hardware implementing counters limited comparable existing hardware performance counter architectures significantly accurate previous approaches 

key role user visible hardware performance counters provide clear accurate performance information software developer 
information provides guidance regarding kinds software changes necessary improved performance 
intuitively appealing way representing major performance components terms contributions average cycles instruction cpi 
order superscalar processors conventional performance counters provide type information accurate cpi components determined 
reason performance counters historically constructed bottom fashion focusing events affect performance example various cache rates regard performance counts combined form picture cpi 
contrast viewing performance top manner accurate cpi measures goal set performance counters defined provide basic data accurate picture cpi built 
taken top approach interval analysis superscalar processor performance model developed guide 
performance model gives indepth understanding relationships events related performance penalties 
insights performance model design novel hardware performance counter architecture computing accurate cpi components percent components computed detailed simulations 
significantly accurate previously proposed cpi breakdown approaches errors higher 
hardware complexity counter architecture limited comparable existing hardware performance counter architectures 
revisit cpi stack existing approaches measuring cpi stacks section 
performance model interval analysis determine penalties various events section 
insights subsequently propose hardware performance counter mechanism building accurate cpi stacks section 
subsequently proposed mechanism evaluated section related discussed section 
conclude section 
constructing cpi stacks average cpi computer program executing microprocessor divided base cpi plus number cpi components reflect lost cycle opportunities events branch mispredictions cache tlb misses 
breakdown cpi components referred cpi stack cpi data typically displayed stacked histogram bars cpi components stack top base cpi shown bottom histogram bar 
cpi stack reveals valuable information application behaves microprocessor gives insight application behavior raw rates 
cpi twolf branch prediction tlb cache cache tlb cache cache base example cpi stack twolf benchmark 
shows example cpi stack twolf benchmark wide superscalar order processor detailed section 
cpi stack reveals base cpi absence events equals substantial cpi components cache cache branch predictor 
cpi equals shown top cpi stack 
application developers cpi stack optimizing software 
example cache cpi relatively high improving instruction locality key reducing cpi increasing performance 
cache cpi component high case twolf example changing data layout adding software prefetching instructions viable optimizations 
note cpi stack shows maximum performance improvement optimization 
example twolf improving cache behavior improve performance cache cpi component divided cpi 
basic idea cpi stack simple computing accurate cpi stacks superscalar order processors challenging parallel processing independent operations events 
widely naive approach computing various components cpi stack multiply number events type average penalty event 
example data cache cpi component computed multiplying number misses average memory access latency branch misprediction cpi contributor computed multiplying number branch mispredictions average branch misprediction penalty 
refer approach naive approach 
number pitfalls naive approach 
average penalty event may vary programs addition number penalty cycles may obvious 
example previous shown branch misprediction penalty varies widely benchmarks substantially larger frontend pipeline length frontend pipeline length estimate branch misprediction penalty leads significant underestimation real branch misprediction penalty 
second naive approach take consideration event penalties hidden overlapped order processing independent instructions events 
example data cache misses hidden completely balanced order superscalar processor 
example data cache misses may overlap 
overlapping events account give highly skewed estimates cpi components 
naive approach distinction events mispredicted control flow paths events correct control flow paths 
naive method may count events paths leading inaccuracy 
overcome problem processors intel pentium feature mechanism obtaining event counts 
achieved implementing tagging mechanism tags instructions flow pipeline event counters get updated case instruction reaches completion 
case instruction completed instruction path event counter get updated 
refer approach naive non spec approach approach differs naive approach count events mispredicted paths 
response shortcomings designers ibm power microprocessor implemented dedicated hardware performance counter mechanism goal computing cpi stack 
best knowledge ibm power processor implementing dedicated hardware performance counter architecture measuring cpi components similar approach fact approaches proposed discuss section 
ibm power hardware performance counters programmed count particular completion stall conditions cache branch misprediction cache cache general philosophy ibm power cpi mechanism inspect completion stage pipeline 
instructions completed cycle appropriate completion stall counter incremented 
completion stall counters count number stall cycles stall condition 
primary conditions completion stall 
reorder buffer rob empty 
possible causes 
cache tlb occurred pipeline stops feeding new instructions rob 
causes rob drain eventually rob may empty 
rob empty power mechanism starts counting lost cycles cache completion stall counter instructions start entering rob 
second branch mispredicted 
mispredicted branch gets resolved pipeline needs flushed new instructions need fetched correct control flow path 
point time rob empty newly fetched instructions traversed frontend pipeline reach rob 
power mechanism counts number cycles empty rob branch misprediction stall counter 
ipc interval branch cache long cache interval interval interval time basic idea interval analysis performance analyzed dividing time intervals events 
second reason completion stall instruction head rob completed reason 
zero completion cycle attributed 
instruction head rob stalled suffered cache tlb causes cache tlb completion stall counter incremented cycle memory operation resolved 
instruction head rob instruction latency greater cycle multiply divide long latency floatingpoint operation instruction completed 
long latency completion stall counter incremented cycle completion progress 
cpi stack building approaches naive approaches complex ibm power approach built bottom fashion 
bottom approaches inadequate computing true performance penalties due events shown detail 
result components resulting cpi stack inaccurate 

underlying performance model model superscalar performance evaluation call interval analysis top approach designing hardware performance counter architecture 
interval analysis provides insight performance superscalar processor requiring detailed tracking individual instructions 
interval analysis execution time partitioned discrete intervals disruptive events cache misses tlb misses branch mispredictions 
statistical processor program behavior allows superscalar behavior performance determined interval type 
aggregating individual interval performance performance estimated 
basis model superscalar processor designed stream instructions various pipelines functional units optimal conditions design sustains level performance equal pipeline dispatch issue width 
balanced processor design large rob related structures processor width achieved issue rate closely approximates maximum processor rate 
true processor widths practical interest say way way 
instructions enter window issue ramps steady state cache occurs steady state pipeline length instructions buffered decode pipe delay window drains cache interval 
re fill pipeline pipeline length observe 
early studies riseman foster showed squared relationship instruction window size ipc observation see example 
exceptions long dependence chains due loop carried dependences loop relatively instructions 
situations uncommon practical issue widths 
counter architecture handles cases resource stalls dcache misses described section 
smooth flow instructions disrupted events 
event occurs issuing useful instructions eventually stops period useful instructions issued event resolved instructions flowing 
emphasize useful instructions mispredicted branch path considered useful 
interval behavior illustrated 
number instructions issued cycle ipc shown vertical axis time clock cycles horizontal axis 
illustrated effects events divide execution time intervals 
define intervals points instructions just issuing recovery preceding event 
interval includes time period instructions issued particular event 
dividing execution time intervals analyze performance behavior intervals individually 
particular type interval event terminates describe evaluate key performance characteristics 
underlying interval behavior different frontend backend events discuss separately 
having discussed isolated events discuss interactions events 
frontend misses frontend misses divided cache tlb misses branch mispredictions 
instruction cache tlb misses interval execution curve cache shown cache tlb misses exhibit similar behavior difference amount delay analyze collectively cache misses 
graph plots number instructions issued vertical axis versus time horizontal axis cache penalty bzip interval analysis naive approach ibm power crafty eon gap gcc gzip penalty due instruction cache access latency cache cycles 
typical behavior plot smoothed clarity 
interval instructions fill window sustained maximum dispatch width instruction issue commit ramp window fills issue commit rates increase maximum value 
point instruction cache occurs 
instructions pipeline frontend dispatched window window starts drain 
takes amount time equal number frontend pipeline stages number clock cycles equal frontend pipeline length 
offsetting effect time required re fill frontend pipeline missed line accessed cache main memory 
pipeline length delays exactly offset penalty instruction cache equals delay 
simulation data verifies case cache see obtained similar results cache tlb 
cache penalty fairly constant benchmarks 
experiments assume access latency cycles 
slight fluctuation cache penalty cycles due presence fetch buffer instruction delivery subsystem 
proposed hardware performance counter mechanism detailed effectively counts delay counts number cycles time instruction cache occurs time newly fetched instructions start filling frontend pipeline 
counts ascribed cache tlb misses 
naive approach computes cache tlb penalty accurate way multiplying number misses delay 
ibm power mechanism contrast counts number zero completion cycles due empty rob corresponds zero region rob drained 
means ibm power mechanism take account time drain rob 
leads substantial underestimation real instruction cache tlb penalty shown 
note cases ibm power mechanism may ascribe cycles instruction cache case window drain time takes longer delay happen case largely filled rob needs drained low ilp significant fraction long latency instructions 
branch mispredictions mcf parser perlbmk twolf vortex vpr issue ramps instructions enter window mispredicted branch enters window misprediction detected flush pipeline instructions branch misprediction penalty re fill pipeline pipeline length interval behavior branch misprediction 
branch misprediction penalty bzip interval analysis naive approach ibm power crafty eon gap gcc gzip average penalty mispredicted branch 
shows timing branch misprediction interval 
interval instructions fill window instruction issue ramps 
point mispredicted branch enters window 
point window begins drained useful instructions eventually commit 
instructions mispredicted branch continue filling window contribute issuing instructions 
generally speaking inhibit issuing instructions assumed oldest ready instructions allowed issue 
eventually mispredicted branch resolved pipeline flushed re filled instructions correct path 
re fill time zero issue region instructions issue complete observation zero region approximately equal time takes re fill frontend pipeline 
interval analysis follows performance penalty due branch misprediction equals difference time mispredicted branch enters window time correct path instruction enters window discovery misprediction 
words performance penalty equals branch resolution time time mispredicted branch entering window branch resolved plus frontend pipeline length 
shown branch resolution time subject interval length amount ilp pro mcf parser perlbmk twolf vortex vpr instructions enter window issue ramps steady state steady state load enters window load issues rob fill time rob fills issue window empty latency data returns memory long back penalty interval behavior isolated long data cache gram longer interval lower ilp longer branch resolution time takes 
benchmarks branch resolution time main contributor branch misprediction penalty 
interval analysis follows order accurately compute branch misprediction penalty hardware performance counter mechanism requires knowledge mispredicted branch entered rob 
reflected hardware performance counter architecture case proposed architecture 
existing approaches employ architecture consequently approaches unable compute true branch misprediction penalty see 
naive approach typically ascribes frontend pipeline length branch misprediction penalty significant underestimation branch misprediction penalty 
ibm power mechanism counts number zero completion cycles empty rob result branch misprediction 
worse naive approach number zero completion cycles smaller frontend pipeline length 
backend misses backend events distinction events short long duration 
short backend misses data cache misses long backend misses data cache misses tlb misses 
short misses short data cache misses general lead period zero instructions issued 
provided processor design reasonably balanced sufficiently large rob related structures latency short data cache misses hidden overlapped order execution independent instructions 
consider loads data cache similar manner way consider instructions issued long latency functional units see section 
long misses long data cache occurs main memory memory delay typically quite large order cycles 
similar behavior st load enters window st load issues nd load issues rob fills issue window empty latency latency load data returns memory overlapping long back penalty interval timing overlapping long cache misses 
cache penalty bzip interval analysis naive approach ibm power crafty eon gap gcc gzip penalty long data cache observed tlb misses 
handled manner 
isolated long data cache rob eventually fills load blocks rob head dispatch stops eventually issue commit cease 
shows performance interval contains long data cache rob fills missing load instruction blocks head rob 
data returns memory instruction issuing resumes 
total long data cache penalty equals time rob fill time data returns memory 
consider influence long cache closely follows long cache assume data cache misses independent load feed second load 
closely mean window size rob size instructions immediately follow long cache instructions rob blocks 
additional long cache misses occur instructions immediately long cache additional performance penalty latencies essentially overlapped 
illustrated 
assumed second follows instructions 
load data returns memory load commits longer blocks head rob 
new instructions allowed enter rob 
take approximately cycles dispatch width just time overlap remaining mcf parser perlbmk twolf vortex vpr latency second note overlap holds regardless requirement equal rob size similar argument number long cache misses occur instructions long cache analysis conclude penalty isolated overlapping long data cache misses equals time rob filling data returning main memory 
exactly proposed hardware performance counter mechanism counts 
contrast naive approach ascribes total latency long backend misses naive approach take account overlapping long backend misses 
lead severe real penalties see 
ibm power mechanism hand better approximation real penalty starts counting long data cache penalty soon data cache reaches head rob 
doing ibm power ascribes single penalty overlapping backend misses 
subtle difference mechanism ibm power approach starts counting soon data cache reaches head rob method propose waits rob effectively filled 
method count amount done overlap long cache filling rob 
small difference practice see 
interactions events far considered various event types isolation 
practice events occur isolation interact events 
accurately dealing interactions crucial building meaningful cpi stacks want double count event penalties 
treat interactions frontend events 
discuss interactions frontend backend events 
interactions frontend events degree interaction frontend pipeline events branch mispredictions cache misses tlb misses limited penalties overlap 
frontend pipeline events serially disrupt flow instructions negative effects overlap 
thing needs considered building accurate cpi stacks penalties due frontend pipeline events mispredicted control flow paths counted 
example penalty due icache mispredicted path counted 
naive approach count cache tlb misses including misses mispredicted paths lead inaccurate picture real penalties 
naive non spec method ibm power mechanism method count cache tlb misses mispredicted paths 
interactions frontend long backend events interactions frontend pipeline events long backend events complex frontend pipeline events overlapped long backend benchmark input overlap bzip program crafty ref eon rushmeier gap ref gcc gzip graphic mcf ref parser ref perlbmk twolf ref vortex ref vpr route table percentage cycles frontend penalties overlapped long backend penalties 
events 
question account event penalties 
example case branch misprediction overlaps long cache account branch misprediction penalty ignore branch misprediction penalty saying completely hidden long cache order answer questions measured fraction total cycle count overlaps observed frontend penalties cache tlb branch mispredictions long backend penalties 
fraction overlapped cycles generally small shown table benchmarks couple benchmarks 
fraction overlapped cycles limited mechanism dealing result relatively accurate meaningful cpi stacks 
consequently opt hardware performance counter implementation assigns overlap frontend long backend penalties frontend cpi component rob full triggers counting long backend penalty 
implementation results simple hardware design 

counter architecture proposed hardware performance counter architecture assume total cycle counter global cpi component cycle counters measuring lost cycles due cache misses cache misses tlb misses cache misses cache misses tlb misses branch mispredictions long latency functional unit stalls 
idea assign cycle global cpi component cycle counters possible steady state baseline cycle count equals total cycle count minus total sum individual global cpi component cycle counters 
describe global cpi component cycle counters computed hardware 
distinction frontend misses backend misses long latency functional unit stalls 
frontend misses initial design fmt measure lost cycles due frontend events propose hardware table called frontend event table fmt implemented shown 
fmt circular buffer rows processor supports outstanding branches 
fmt dispatch head dispatch tail fetch rob id mispredict bit branch penalty fmt local cache local cache local tlb number outstanding branches dispatch head dispatch tail fetch rob id mispredict bit number outstanding branches branch penalty sfmt local cache local cache shows fmt shows sfmt computing frontend penalties 
pointers fetch pointer dispatch head pointer dispatch tail pointer 
new branch instruction fetched decoded fmt entry allocated advancing fetch pointer initializing entire row zeros 
branch dispatches dispatch tail pointer advanced point branch fmt instruction rob id inserted rob id column 
branch resolved turns misprediction instruction rob id locate corresponding fmt entry mispredict bit set 
retirement branch causes dispatch head pointer increment de allocates fmt entry 
frontend penalties calculated follows 
cycle instructions fed pipeline due cache tlb causes appropriate local counter fmt entry pointed fetch pointer incremented 
example cache causes local cache counter fmt row pointed fetch pointer incremented cycle cache resolved 
doing delay computed local counter corresponds actual cache tlb penalty interval analysis 
branches local fmt branch penalty counter keeps track number lost cycles caused presumed branch misprediction 
recall branch misprediction penalty equals number cycles mispredicted branch entering rob new instructions correct control flow path entering rob branch resolution 
unknown dispatch time branch mispredicted proposed method computes number cycles branch resides rob 
branch penalty counter incremented cycle branches residing rob branches dispatch head tail pointers fmt 
done rob full classify cycles full rob long backend misses long latency misses easiest way double count cycles overlapping events 
global cpi component cycle counters updated branch instruction completes local cache icache tlb counters added respective global cycle counters 
case branch incorrectly predicted value local branch penalty counter added local tlb global branch misprediction cycle counter 
global branch misprediction cycle counter incremented cycle new instructions enter rob 
resolution mispredicted branch places fmt dispatch tail pointer point mispredicted branch entry fmt fetch pointer point fmt entry 
improved design sfmt design fmt distinction cache tlb misses past particular branches local cache tlb counters fmt updated fmt entry pointed fetch pointer fetch pointer advanced branch fetched 
avoids counting cache tlb penalties past branch mispredictions 
price paid keeping track icache tlb penalties mispredicted paths fmt requires order bits storing information 
simplified fmt design called shared fmt sfmt shared set local cache tlb counters see 
sfmt requires cache tlb bit provided entry rob done intel pentium ibm power tracking cache misses completion stage 
branch icache tlb counters sfmt sfmt requires fraction storage bits compared fmt 
sfmt operates similar fashion fmt local cache tlb counters get updated cache tlb misses 
completion instruction cache tlb bit set adds local cache tlb counters respective global counters ii resets local cache tlb counters iii resets icache tlb bits instructions rob 
case mispredicted branch completed local branch penalty counter added global branch misprediction cycle counter entire sfmt cleared including local cache tlb counters 
clearing sfmt branch misprediction avoids counting cache tlb penalties mispredicted paths 
cache tlb followed mispredicted branch turn followed icache tlb sfmt incurs inaccuracy counts cache tlb penalties mispredicted control flow paths 
fact cache misses branch mispredictions typically oc rob entries lsq entries processor width decode dispatch commit wide fetch issue wide latencies load mul div cache kb direct mapped cache kb way set assoc cycles cache unified mb way set assoc cycles main memory cycle access time branch predictor hybrid bimodal gshare predictor frontend pipeline stages table processor model assumed experimental setup 
cur bursts number cases scenario occurs limited 
additional error observe sfmt compared fmt small shown evaluation section 
long backend misses hardware performance counters computing lost cycles due long backend misses long cache misses tlb misses fairly easy implement 
counters start counting rob full instruction blocking rob cache tlb respectively 
cycle conditions hold true respective cycle counters incremented 
note doing account long backend penalty explained interval analysis 
long latency unit stalls hardware performance counter mechanism allows computing resource stalls steady state behavior 
recall steady state behavior balanced processor design implies performance roughly equivalent maximum processor width achieved absence events rob needs filled achieve steady state behavior 
observation compute resource stalls due long latency functional unit instructions including short data cache misses 
rob filled instruction blocking head rob cache count cycle cache cycle instruction blocking head full rob long latency instruction count cycle resource stall 

evaluation experimental setup simplescalar alpha validation experiments 
benchmarks inputs taken spec cpu benchmark suite see table 
binaries benchmarks taken simplescalar website 
show results cpu integer benchmarks 
collected results floating point benchmarks cpi stacks floating point benchmarks interesting cpi stacks integer benchmarks 
nearly floating point benchmarks show large cache cpi components benchmarks exhibit significant cache cpi components benchmarks show substantial branch misprediction cpi components 
baseline processor model table 
results section evaluates proposed hardware performance counter mechanism 
compare hardware implementations fmt sfmt ibm power mechanism naive naive non spec approaches simulation cpi stacks 
simulation cpi stacks serve comparison 
simulation stacks difficulty defining standard correct cpi stack look 
particular cycles reasonably ascribed event 
simulate cpi values specific order may get different numbers simulated different order 
account effect simulation cpi stacks generated follows 
run simulation assuming perfect branch prediction perfect caches branches correctly predicted cache accesses cache hits 
yields number cycles base cpi 
subsequently run simulation real data cache 
additional cycles run assumes perfect data cache gives cpi component due data cache misses 
simulation run assumes real data cache real branch predictor computes branch misprediction cpi component 
computing remaining cpi components consider orders 
order cache icache tlb cache tlb second order called inverse order computes cache tlb components computes cache cache tlb cpi components 
simulation results show order cpi components computed small effect results 
follows small percentages cycles process overlapping frontend backend event penalties previously shown table 
shows normalized cpi stacks specint benchmarks simulation approach naive naive non spec approach ibm power approach proposed fmt sfmt approaches 
summarizes cpi stacks showing maximum cpi component errors various cpi stack building methods 
shows naive approach results cpi stacks highly inaccurate meaningful benchmarks 
sum event counts times penalties larger total cycle count causes base cpi total cycle count minus event cycle count negative 
case number benchmarks gap gcc mcf twolf vpr gcc notable example 
reason naive approach fails building accurate cpi stacks naive approach adequately deal overlapped long backend misses accurately compute branch misprediction penalty addition counts cache tlb misses mispredicted paths 
benchmarks overlapped backend misses cache misses mispredicted paths naive approach fairly accurate see example eon perlbmk 
naive non spec approach count events bzip sim sim inv naive naive power fmt non spec sfmt gap sim sim inv naive naive power fmt non spec sfmt mcf sim sim inv naive naive power fmt non spec sfmt twolf sim sim inv naive naive power fmt non spec sfmt tlb tlb base tlb tlb base tlb tlb base tlb tlb base crafty sim sim inv naive naive power fmt non spec sfmt gcc sim sim inv naive naive power fmt non spec sfmt parser sim sim inv naive naive power fmt non spec sfmt vortex sim sim inv naive naive power fmt non spec sfmt tlb tlb base tlb tlb base tlb tlb base tlb tlb base eon sim sim inv naive naive power fmt non spec sfmt gzip sim sim inv naive naive power fmt non spec sfmt perlbmk sim sim inv naive naive power fmt non spec sfmt vpr sim sim inv naive naive power fmt non spec sfmt normalized cpi breakdowns specint benchmarks simulation approach inverse order simulation approach naive approach naive non spec approach ibm power approach fmt sfmt approaches 
mispredicted paths accurate naive approach cpi stacks accurate compared simulation cpi stacks 
ibm power approach clearly improvement compared naive approaches 
benchmarks naive approaches failed ibm power mechanism succeeds producing meaningful cpi stacks 
compared simulation cpi stacks ibm power cpi stacks inaccurate see example crafty eon gap gzip perlbmk vortex 
reason ibm power approach falls short ibm power mechanism underestimates cache penalty branch misprediction penalty 
fmt sfmt cpi stacks track simulation cpi stacks closely 
naive ibm power mechanisms show high errors bench max cpi component error bzip crafty eon gap gcc gzip naive naive non spec ibm power fmt sfmt mcf parser perlbmk twolf vortex vpr tlb tlb base tlb tlb base tlb tlb base tlb tlb base maximum cpi component error naive approaches ibm power approach fmt sfmt compared simulation cpi stacks 
avg marks fmt sfmt architectures show significantly lower errors benchmarks 
maximum cpi component errors see 
average error fmt sfmt respectively 

related intel itanium processor family provides rich set hardware performance counters computing cpi stacks 
hardware performance monitors effectively compute number lost cycles various stall conditions branch mispredictions cache misses digital continuous profiling infrastructure dcpi example hardware performance monitoring tool order architecture 
computing cpi stacks order architectures relatively easy compared computing cpi stacks order architectures 
ibm power mechanism hardware profiling mechanisms proposed past order architectures 
goal methods quite different 
goal build simple easy understand cpi stacks goal approaches detailed instruction profiling 
example framework randomly samples individual instructions collects cycle level information instruction basis 
collecting aggregate cpi stacks done framework profiling randomly sampled instructions aggregating individual latency information 
inherent limitation approach instruction profiling allow modeling overlap effects 
framework partially addresses issue profiling potentially concurrent instructions 
shotgun profiling tries model overlap effects multiple instructions collecting event information hot spots specialized hardware performance counters 
analysis determines simple processor model amount overlaps interactions instructions hot spots 
instruction profiling inherent limitation relying sampling may introduce inaccuracy ii instruction information computing overlap effects iii interrupts communicating event information hardware software may lead overhead perturbation issues 
number researchers looked superscalar processor models primary efforts led interval model 
focused performance aspects instruction delivery modeled instruction window issue mechanisms 
second smith extended type analysis types events built complete performance model included sustained steady state performance rate punctuated gaps occurred due events 
independently taha wilson broke instruction processing intervals call macro blocks 
behavior macro blocks analytically modeled simulation 
interval analysis combines taha wilson approach smith approach event modeling 
interval model represents advance smith gap model handles short inter val behavior straightforward way 
mechanistic interval model similar empirical model empirical model basis understanding mechanisms contribute cpi components 

computing cpi stacks order processors challenging various overlap effects instructions events 
existing approaches fail computing accurate cpi stacks main reason fact approaches build cpi stacks bottom fashion counting events regard events affect performance 
top approach hand starts performance model interval analysis gives insight performance impacts events 
insights reveal hardware performance counter architecture look building accurate cpi stacks 
proposed hardware performance counter architecture comparable existing hardware performance counter mechanisms terms complexity achieves greater accuracy 
research postdoctoral fellows respectively fund scientific research flanders belgium fwo 
research supported ghent university iwt network excellence 

ailamaki dewitt hill wood 
dbmss modern processor time go 
proceedings th large database conference 
anderson dean ghemawat henzinger leung sites vandevoorde waldspurger weihl 
continuous profiling cycles gone 
acm transactions computer systems nov 
dean hicks waldspurger weihl 
hardware support instruction level profiling order processors 
micro dec 
smith 
characterizing branch misprediction penalty 
mar 
fields bodik hill 
interaction cost shotgun profiling 
acm sept 

optimal pipeline depth microprocessor 
isca pages may 
intel 
intel itanium processor manual software development optimization may 

smith 
day life data cache held conjunction isca may 
smith 
order superscalar processor model 
isca pages june 
keeton patterson raphael baker 
performance characterization quad pentium pro smp oltp workloads 
isca june 
luo rubio john seshadri 
benchmarking internet servers superscalar machines 
ieee computer feb 

power performance measurement characterization 
tutorial ieee international symposium workload characterization oct 

performance monitoring power microprocessor 
john editors performance evaluation benchmarking pages 
crc press 
jourdan 
exploring instruction fetch bandwidth requirement wide issue superscalar processors 
pact pages oct 
shen 
theoretical modeling superscalar processor performance 
micro pages nov 
shen 
framework statistical modeling superscalar processor performance 
hpca pages feb 
ranganathan gharachorloo adve 
performance database workloads shared memory systems order processors 
asplos viii oct 

pentium performance monitoring features 
ieee micro july 
taha wills 
instruction throughput model superscalar processors 
proceedings th ieee international workshop rapid system prototyping rsp june 
zagha larson turner 
performance analysis mips performance counters 
proceedings acm ieee conference supercomputing jan 
