hierarchical probabilistic neural network language model frederic morin dept iro universit de montr box succ 
centre ville montreal qc canada iro umontreal ca years variants neural network architecture statistical language modeling proposed successfully applied language modeling component speech recognizers 
main advantage architectures learn embedding words symbols continuous space helps smooth language model provide generalization number training examples insufficient 
models extremely slow comparison commonly gram models training recognition 
alternative importance sampling method proposed speed training introduce hierarchical decomposition conditional probabilities yields speed training recognition 
hierarchical decomposition binary hierarchical clustering constrained prior knowledge extracted wordnet semantic hierarchy 
curse dimensionality hits hard statistical language models number possible combinations words words immensely larger text potentially available 
problem comes transfering probability mass tiny fraction observed cases combinations 
point view machine learning interesting consider different principles obtaining generalization 
fundamental principle explicitly non parametric models similarity objects similar similar probability 
unfortunately knowledge free notion similarity bengio dept iro universit de montr box succ 
centre ville montreal qc canada bengio umontreal ca high dimensional spaces sequences words 
case statistical language models successful generalization principle corresponding notion similarity simple interpolated back gram models jelinek mercer katz sequences share shorter subsequences similar share probability mass methods exact matching subsequences obvious word sequences may match close semantically 
account principle shown successful combination notion similarity individual words word sequences said similar corresponding words similar 
similarity words usually defined word classes brown goodman 
word classes correspond partition set words way words class share statistical properties context partition obtained various clustering algorithms 
discrete notion similarity 
way define similarity words assigning word continuous valued set features comparing words feature vector 
idea exploited information retrieval schutze deerwester singular value decomposition matrix occurrences indexed words dimension documents 
idea exploited bengio vincent bengio neural network architecture defined layer maps word symbols continuous representation feature vectors rest neural network conventional construct conditional probabilities word previous ones 
model described detail section 
idea exploit smoothness neural network sure sequences words similar learned metric assigned similar probability 
note feature vec tors part model computes probabilities estimated jointly regularized maximum likelihood 
type model related popular maximum entropy models berger della pietra della pietra correspond neural network hidden units unnormalized log probabilities linear functions input indicators presence words 
neural network approach shown generalize comparison interpolated gram models class grams brown pereira tishby lee ney kneser niesler whittaker woodland baker mccallum terms perplexity terms classification error speech recognition system schwenk gauvain schwenk xu emami jelinek 
schwenk gauvain schwenk shown model directly improve speech recognition performance 
xu emami jelinek approach generalized form various conditional probability functions required stochastic parsing model called structured language model experiments show speech recognition performance improved state art alternatives 
major weakness approach long training time large amount computations required compute probabilities time doing speech recognition application model 
note models applications statistical language modeling automatic translation information retrieval improving speed important applications possible 
objective propose faster variant neural probabilistic language model 
idea principle deliver close exponential speed respect number words vocabulary 
computations required training probability prediction small constant plus factor linearly proportional number words vocabulary approach proposed yield speed order log second term 
follows proposal goodman rewrite probability function partition set words 
basic idea form hierarchical description word sequence log decisions learn take probabilistic decisions directly predicting word probability 
important idea reuse model parameters decisions large number models required model fit computer memory special symbolic input characterizes nodes tree hierarchical decomposition 
prior knowledge word net lexical system help define hierarchy word classes 
probabilistic neural language model objective estimate joint probability sequences words estimation conditional probability word target word previous words context 
wl wt wt 
wt wt word position text wt vocabulary 
conditional probability estimated normalized function wt wt 
wt wt 
wt 
bengio vincent bengio conditional probability function represented neural network particular structure 
important characteristic input function word symbol embedded euclidean space learning associate real valued feature vector word 
set feature vectors words part set parameters model estimated maximizing empirical log likelihood minus weight decay regularization 
idea associating symbol distributed continuous representation new advocated early days neural networks hinton elman 
idea neural networks language modeling new similar proposals character text compression neural networks predict probability character schmidhuber 
variants model bengio vincent bengio outputs softmax normalization target word wt mapped feature vector context words single output represents unnormalized probability wt context words 
variants gave similar performance experiments reported bengio vincent bengio 
start second variant formalized follows boltzmann distribution form hinton wt wt 
wt wt wt wt wt wt wt 
wt learned function interpreted energy low tuple wt 
wt plausible 
embedding matrix parameter row fi embedding feature vector word energy function represented transformation input label feature vectors fi followed ordinary feedforward neural network single output bias dependent wt 
wt tanh uf bv denotes transpose tanh applied element element parameters vectors weight matrices parameters denotes concatenation input feature vectors context words 

number hidden units number rows dimension embedding number columns 
computing wt wt 
wt done steps compute requires hd multiply add operations second compute uf hd multiply add operations value 
multiply add operations 
total computation time computing order hd 
experiments reported bengio 
gives operations part independent operations second part linear 
goal drastically reduce second part ideally replacing computations log computations 
hierarchical decomposition provide exponential speed goodman shown speed maximum entropy class statistical language model idea 
computing directly involves normalization values take defines clustering partition word classes deterministic function 
mapping write 
true function 
value compatible value value 

yield correct probabilities generalization better choices word classes sense easier learn 
take values classes words class doing normalization choices need normalizations choices 
computation conditional probabilities proportional number choices reduce computation factor 
approximately gained measurements reported goodman 
suggests introduce levels decomposition push idea limit 
level decomposition provide speed order hierarchical de composition represented balanced binary tree provide exponential speed order log part computation linear number choices 
word represented bit vector 
bm depends 
achieved building binary hierarchical clustering words method doing section 
example indicates belongs top level group indicates belongs sub group top level group 
word conditional probability represented computed follows wt 
wt bj 
bj wt 
wt interpreted series binary stochastic decisions associated nodes binary tree 
node indexed bit vector corresponding path root node append left right branch decision node followed 
leaf corresponds word 
tree balanced maximum length bit vector log 
note reduce computation looking encoding takes frequency words account reduce average bit length unconditional entropy words 
example corpus experiments log unigram entropy possible additional speed word frequencies account better balance binary tree 
gain greater larger vocabularies significant improvement major obtained simple balanced hierarchy 
target class node obtained directly target word context bit encoding word 
note target gradient propagation nodes path root leaf associated target word 
major source savings training 
recognition testing main cases consider needs probability word observed word needs probabilities words 
case occurs testing corpus obtain exponential speed 
second case back computations constant factor overhead 
purpose estimating generalization performance log likelihood probability observed word needed 
practical applications speech recognition interested discriminating alternatives consistent acoustics represented treillis possible word sequences 
speed contrasted provided importance sampling method proposed bengio sen cal 
method observation log likelihood gradient average model distribution context energy gradient associated possible idea approximate average biased asymptotically unbiased importance sampling scheme 
approach lead significant speed training architecture unchanged probability computation recognition test requires computations prediction 
architecture proposed gives significant speed training test recognition 
sharing parameters hierarchy separate predictor nodes hierarchy predictors needed 
represents huge capacity predictor maps context words single probability 
create problems terms computer memory models fit time memory overfitting 
chosen build model parameters shared hierarchy 
clearly ways achieve sharing alternatives architecture motivate study 
discussion sense force word embedding shared nodes 
important matrix word features largest component parameter set 
node hierarchy presumably semantic meaning associated group hopefully similar meaning words sense associate node feature vector 
loss generality consider model predict node wt 
wt node corresponds sequence bits specifying node hierarchy bit corresponding children node 
represented model similar described section bengio vincent bengio kinds symbols input context words current node 
allow embedding parameters word cluster nodes different words 
architecture difference choices predict choices 
precisely specific predictor experiments node wt 
wt sigmoid node tanh concatenation context word features eq 
sigmoid exp bias parameter playing role bv eq 
weight vector playing role eq 
play role eq 
gives feature vector embeddings nodes way similar gave feature vector embeddings words eq 

wordnet build hierarchical decomposition important component model choice words binary encoding hierarchical word clustering 
combine empirical statistics prior knowledge wordnet resource fellbaum 
option purely data driven hierarchical clustering words ways wordnet resource influence resulting clustering 
taxonomy wordnet organizes semantic concepts associated senses graph tree 
purposes need tree manually selected parent nodes parent 
leaves wordnet taxonomy senses word associated sense 
words sharing sense considered synonymous uses 
purpose choose senses word hierarchy words selected frequent sense 
straightforward extension proposed model keep semantic ambiguity word word associated leaves senses wordnet hierarchy 
require summing leaves corresponding paths root computing word probabilities 
note wordnet tree binary node may children particularly problem verbs adjectives word net shallow incomplete 
transform hierarchy binary tree perform data driven binary hierarchical clustering children associated node illustrated 
means algorithm step split cluster 
compare nodes associate node subset words covers 
word associated tf idf salton buckley vector document word occurrence counts document paragraph training corpus 
node associated median tf idf scores 
tf idf score occurrence frequency word document times logarithm ratio total number documents number documents containing word 
comparative results experiments performed evaluate speed change generalization error 
experiments compared alternative speed technique bengio sen cal importance sampling provides speed training 
experiments performed brown corpus reduced vocabulary size words frequent ones 
corpus occurrences words split sets training validation model selection testing 
validation set select small number choices size embeddings number hidden units 
results terms raw computations time process example training test shown respectively tables 
computations performed athlon processors ghz clock 
speed training factor greater test factor close 
impressive log expected overhead constant term computational cost 
important verify learning works wordnet hierarchy binary tree nodes children 
binary hierarchical clustering children performed 
model generalizes 
usual statistical language modeling measured model perplexity test data exponential average negative log data set 
training performed epochs validation set perplexity early stopping 
table shows comparative generalization performance different architectures interpolated trigram class gram procedures bengio follow respectively jelinek mercer brown ney kneser niesler whittaker woodland 
validation set choose order gram number word classes class models 
implementation algorithms sri language modeling toolkit described stolcke www speech sri com projects srilm 
note better performance obtainable tricks goodman 
combining neural network trigram decrease time time speed architecture epoch ex 
ms original neural net importance sampling hierarchical model table training time epoch going training examples example 
original neural net described sec 

importance sampling algorithm bengio sen cal trains model faster 
hierarchical model proposed yields speed training probability predictions see table 
time speed architecture example ms original neural net importance sampling hierarchical model table test time example different algorithms 
see table 
test time hierarchical model advantage clear comparison importance sampling technique brings speed training 
plexity shown bengio 
shown table hierarchical model generalize original neural network difference large represents improvement benchmark gram models 
large speed certainly worth investigating variations hierarchical model proposed particular define hierarchy generalization better 
note speed greater larger vocabularies uncommon speech recognition systems 
proposes novel architecture speeding neural networks huge number output classes shows usefulness context statistical language modeling component speech recognition automatic translation systems 
pushes limit suggestion goodman introduces idea sharing model nodes decomposition practical number nodes large tens thousands 
implementation experiments show significant speed fold achieved little degradation generalization performance 
validation test perplexity perplexity trigram class original neural net importance sampling hierarchical model table test perplexity different architectures interpolated trigram 
hierarchical model performed bit worse original neural network better baseline interpolated trigram class model 
linguistic point view weaknesses model considers word clusters deterministic functions word uses nodes wordnet taxonomy help define clusters 
wordnet provides word sense ambiguity information linguistically accurate modeling 
hierarchy sense hierarchy word word associated number senses allowed word word net 
computing probabilities involve summing paths root corresponding different possible senses word 
side effect provide word sense disambiguation model trained sense tagged supervised data unlabeled ordinary text 
average number senses word small handful loss speed correspondingly small 
acknowledgments authors funding organizations support nserc iris canada research chairs 
baker mccallum 

distributional clustering words text classification 
sigir 
bengio vincent 

neural probabilistic language model 
leen dietterich tresp editors advances neural information processing systems pages 
mit press 
bengio vincent 

neural probabilistic language model 
journal machine learning research 
bengio sen cal 

quick training probabilistic neural nets importance sampling 
proceedings 
berger della pietra della pietra 

maximum entropy approach natural language processing 
computational linguistics 
brown pietra desouza lai mercer 

class gram models natural language 
computational linguistics 
deerwester dumais furnas landauer harshman 

indexing latent semantic analysis 
journal american society information science 
elman 

finding structure time 
cognitive science 
fellbaum 

wordnet electronic lexical database 
mit press 
goodman 

bit progress language modeling 
technical report msr tr microsoft research 
goodman 

classes fast maximum entropy training 
international conference acoustics speech signal processing utah 
hinton 

learning distributed representations concepts 
proceedings eighth annual conference cognitive science society pages amherst 
lawrence erlbaum hillsdale 
hinton 

training products experts minimizing contrastive divergence 
technical report tr gatsby unit university college london 
jelinek mercer 

interpolated estimation markov source parameters sparse data 
gelsema kanal editors pattern recognition practice 
north holland amsterdam 
katz 

estimation probabilities sparse data language model component speech recognizer 
ieee transactions acoustics speech signal processing assp 
miikkulainen dyer 

natural language processing modular neural networks distributed lexicon 
cognitive science 
ney kneser 

improved clustering techniques class statistical language modelling 
european conference speech communication technology eurospeech pages berlin 
niesler whittaker woodland 

comparison part speech automatically derived category language models speech recognition 
international conference acoustics speech signal processing pages 
pereira tishby lee 

distributional clustering english words 
th annual meeting association computational linguistics pages columbus ohio 
salton buckley 

term weighting approaches automatic text retrieval 
information processing management 
schmidhuber 

sequential neural text compression 
ieee transactions neural networks 
schutze 

word space 
giles hanson cowan editors advances neural information processing systems pages pp 
san mateo ca 
morgan kaufmann 
schwenk 

efficient training large neural networks language modeling 
ieee joint conference neural networks 
schwenk gauvain 

connectionist language modeling large vocabulary continuous speech recognition 
international conference acoustics speech signal processing pages orlando florida 
stolcke 

srilm extensible language modeling toolkit 
proceedings international conference statistical language processing denver colorado 
xu emami jelinek 

training connectionist models structured language model 
empirical methods natural language processing emnlp 
xu rudnicky 

artificial neural network learn language models 
international conference statistical language processing pages beijing china 
