gaussian process dynamical models jack wang david fleet aaron hertzmann department computer science university toronto toronto dgp toronto edu fleet cs toronto edu introduces gaussian process dynamical models gpdm nonlinear time series analysis 
gpdm comprises low dimensional latent space associated dynamics map latent space observation space 
marginalize model parameters closed form gaussian process gp priors dynamics observation mappings 
results nonparametric model dynamical systems accounts uncertainty model 
demonstrate approach human motion capture data pose dimensional 
despite small data sets gpdm learns effective representation nonlinear dynamics spaces 
webpage www dgp toronto edu gpdm central difficulty modeling time series data determining model capture nonlinearities data overfitting 
linear autoregressive models require relatively parameters allow closed form analysis model limited range systems 
contrast existing nonlinear models model complex dynamics may require large training sets learn accurate map models 
investigate learning nonlinear dynamical models high dimensional datasets 
take bayesian approach modeling dynamics averaging dynamics parameters estimating 
inspired fact averaging nonlinear regression models leads gaussian process gp model show integrating parameters nonlinear dynamical systems performed closed form 
resulting gaussian process dynamical model gpdm fully defined set lowdimensional representations training data dynamics observation mappings learned gp regression 
natural consequence gp regression gpdm removes need select parameters associated function approximators retaining expressiveness nonlinear dynamics observation 
motivated modeling human motion video people tracking data driven animation 
bayesian people tracking requires dynamical models form transition densities order specify prediction distributions new poses time instant similarly data driven computer animation requires prior distributions poses motion 
individual human pose typically parameterized parameters 
despite large state space space activity specific human poses motions smaller intrinsic dimensionality experiments walking golf swings dimensions suffice 
builds extensive literature nonlinear time series analysis time series graphical models 
nonlinear latent variable model time series 
hyperparameters shown 
gpdm model 
mapping parameters marginalized latent coordinates xn jointly correlated poses yn mention examples 
main themes switching linear models nonlinear transition functions represented radial basis functions 
approaches require sufficient amounts training data learn parameters switching basis functions 
determining appropriate number basis functions difficult 
kernel dynamical modeling linear dynamics kernelized model nonlinear systems density function data produced 
supervised learning gp regression model dynamics variety applications 
methods model dynamics directly observation space impractical high dimensionality motion capture data 
approach directly inspired unsupervised gaussian process latent variable model models joint distribution observed data corresponding representation low dimensional latent space 
distribution prior inference new measurements 
dynamical model assumes data generated independently 
accordingly respect temporal continuity data model dynamics latent space 
augment latent dynamical model 
result bayesian generalization subspace dynamical models nonlinear latent mappings dynamics 
gaussian process dynamics gaussian process dynamical model gpdm comprises mapping latent space data space dynamical model latent space 
mappings typically nonlinear 
gpdm obtained marginalizing parameters mappings optimizing latent coordinates training data 
precisely goal model probability density sequence vector valued states yt yn discrete time index yt rd basic model consider latent variable mapping order markov dynamics xt xt nx yt xt ny xt rd denotes dimensional latent coordinates time nx ny zero mean white gaussian noise processes nonlinear mappings parameterized respectively 
depicts graphical model 
linear mappings extensively auto regressive models consider nonlinear case linear combinations basis functions ai bj weights 
basis functions order fit parameters model training data select appropriate number basis functions ensure data constrain shape basis function 
ensuring conditions difficult practice 
bayesian perspective specific forms including numbers basis functions incidental marginalized 
isotropic gaussian prior columns marginalizing done closed form yield exp nd ky tr yw yn ky kernel matrix comprises kernel hyperparameters 
elements kernel matrix defined kernel function ky ky xi xj 
latent mapping currently rbf kernel ky exp 
scaling matrix diag wd account different variances different data dimensions 
equivalent gp kernel function dimension hyperparameter represents scale output function corresponds inverse width rbfs 
variance noise term ny dynamic mapping latent coordinates conceptually similar subtle 
form joint probability density latent coordinates dynamics weights 
marginalize weights da da 
incorporating markov property eqn 
gives xt xt da vector kernel hyperparameters 
assuming isotropic gaussian prior columns shown expression simplifies exp kx tr xn kx kernel matrix constructed xn assumed isotropic gaussian prior 
model dynamics rbf kernel form eqn 
linear rbf kernel kx exp 
kernel corresponds representing sum linear term rbf terms 
inclusion linear term motivated fact linear dynamical models conceptually model pair xt xt training pair regression simply substitute directly gp model eqn 
leads nonsensical expression xn xn 
second order autoregressive models useful systems 
hyperparameters represent output scale inverse width rbf terms represents output scale linear term 
control relative weighting terms represents variance noise term nx noted due nonlinear dynamical mapping joint distribution latent coordinates gaussian 
density initial state may gaussian remain gaussian propagated dynamics 
see xt variables occur inside kernel matrix outside 
log likelihood quadratic xt 
place priors hyperparameters discourage overfitting 
priors latent mapping dynamics define generative model time series observations 
multiple sequences 
model extends naturally multiple sequences ym sequence associated latent coordinates xm shared latent space 
latent mapping conceptually concatenate sequences gp likelihood eqn 

similar concatenation applies dynamics omitting frame sequence omitting final frame sequence kernel matrix kx 
structure applies learning multiple sequences learning sequence inferring 
learn sequence infer latent coordinates new sequence joint likelihood entails full kernel matrices kx ky formed sequences 
higher order features 
gpdm extended model higher order markov chains model velocity acceleration inputs outputs 
example second order dynamical model xt xt xt nx may explicitly model dependence prediction past frames velocity 
gpdm framework equivalent model entails defining kernel function function current previous time step kx xt xt exp xt similarly dynamics formulated predict velocity xt vt xt nx velocity prediction may appropriate modeling smoothly motion trajectories 
euler integration time step xt vt dynamics likelihood written redefining xn xn eqn 

fixed time step 
analogous xt mean function higher order features fused position information reduce gaussian process prediction variance 
properties gpdm algorithms learning gpdm measurements entails minimizing negative log posterior ln ln kx tr ln ln ln ky tr yw ln additive constant 
minimize respect numerically 
shows gpdm latent space learned human motion capture data comprising walk cycles 
pose defined euler angles joints global torso pose angles global torso translational velocities 
learning data mean subtracted latent coordinates initialized pca 
gpdm learned minimizing 
latent spaces experiments shown 
latent spaces leads intersecting latent trajectories 
causes large jumps appear model leading unreliable dynamics 
comparison fig 
shows learned walking data 
note latent trajectories smooth numerous cases consecutive poses walking sequence relatively far apart latent space 
contrast fig 
shows gpdm produces smoother configuration latent positions 
gpdm arranges latent positions roughly shape saddle 
shows volume visualization inverse reconstruction variance ln shows confidence model reconstructs pose latent positions effect gpdm models high probability tube data 
illustrate dynamical process fig 
shows fair samples latent dynamics gpdm 
samples conditioned initial state length time steps 
noted marginalize weights dynamic mapping distribution pose sequence factored sequence low order markov transitions fig 

draw fair samples hybrid monte carlo 
resulting trajectories fig 
smooth similar training motions 
mean prediction sequences people tracking computer animation desirable generate new motions efficiently 
consider simple online method generating new motion called mean prediction avoids relatively expensive monte carlo sampling 
mean prediction consider timestep xt conditioned xt gaussian prediction xt xt xt kx kx kx kx kx vector containing kx xi th entry xi ith training vector 
particular set latent position time step mean point previous step xt xt 
way ignore process noise normally add 
find mean prediction generates motions fair samples shown fig 
random process noise added time step 
similarly new poses yt xt 
depending dataset choice kernels long sequences generated sampling mean prediction diverge data 
data sets mean prediction trajectories gpdm rbf linear rbf kernel dynamics usually produce sequences roughly follow training data see red curves 
usually means producing closed limit cycles walking data 
motions close mean obtained hmc sampler models learned walking sequence gait cycles 
latent positions learned gpdm shown blue 
vectors depict temporal sequence 
log variance reconstruction shows regions latent space reconstructed high confidence 
random trajectories drawn model hmc green mean red 
gpdm walk data learned rbf linear kernel dynamics 
simulation red started far training data optimized green 
poses reconstructed points optimized trajectory 
mean predictions 
previous 
second learned linear kernel 
gpdm model learned swings golf club nd order rbf kernel dynamics 
plots show orthogonal projections latent space 
initializing hmc mean prediction find sampler reaches equilibrium small number 
compared rbf kernels mean prediction motions generated linear kernel deviate original data see lead smoothed animation 
shows gpdm learned swings golf club 
learning aligns sequences nicely accounts variations speed club trajectory 
optimization mean prediction efficient algorithm prevents trajectories drifting away training data 
desirable optimize particular motion gpdm reduces drift mean prediction mo gpdm walk sequence missing data learned rbf linear kernel dynamics linear kernel dynamics 
blue curves depict original data 
green curves reconstructed missing data 
tions 
optimize new sequence select starting point number time steps 
likelihood new sequence optimized directly holding latent positions previously learned latent positions hyperparameters fixed 
see optimization generates motion close data note variance pose xt determined xt lower xt nearer training data 
consequently likelihood xt increased moving xt closer training data 
generalizes preference poses similar examples natural consequence bayesian approach 
example fig 
shows optimized walk sequence initialized mean prediction 
forecasting performed simple experiment compare predictive power gpdm linear dynamical system implemented gpdm linear kernel latent space rbf latent mapping 
trained model frames hz walking sequence corresponding cycles tested remaining frames 
test frame mean prediction predict pose frames ahead rms pose error computed ground truth 
test repeated mean prediction optimization kernels order predictions linear rbf linear rbf mean prediction optimization due nonlinear nature walking dynamics latent space rbf linear rbf kernels outperform linear kernel 
optimization initialized mean prediction improves result cases reasons explained 
missing data gpdm model handle incomplete data common problem human motion capture sequences 
gpdm learned minimizing eqn 
terms corresponding missing poses yt removed 
latent coordinates missing data initialized cubic spline interpolation pca initialization observations 
produces results short missing segments frames frame walk sequence fig 
fails long missing segments 
problem lies difficulty initializing missing latent positions sufficiently close training data 
solve problem learn model subsampled data sequence 
reducing sampling density effectively increases uncertainty reconstruction process probability density latent space falls smoothly data 
restart learning entire data set kernel hyperparameters fixed 
doing dynamics terms objective function exert influence latent coordinates training data smooth model learned 
missing frames frame walk sequence optimization produces mod els fig 
smoother fig 

linear kernel able pull latent coordinates cylinder fig 
provides accurate dynamical model 
models shown fig 
produce estimates missing poses visually indistinguishable ground truth 
discussion extensions main strengths gpdm model ability generalize small datasets 
conversely performance major issue applying gp methods larger datasets 
previous approaches prune uninformative vectors training data 
straightforward learning gpdm timestep highly correlated steps 
example hold xt fixed optimization optimizer adjustment xt xt 
higher order features provides possible solution problem 
specifically consider dynamical model form vt xt vt 
adjacent time steps related velocity vt xt xt handle irregularly sampled datapoints adjusting timestep possibly different step 
number extensions gpdm model possible 
straightforward include control signal ut dynamics xt ut 
interesting explore uncertainty latent variable estimation see 
maximum likelihood latent coordinates motivated lawrence observation model uncertainty latent coordinate uncertainty interchangeable learning pca 
applications uncertainty latent coordinates may highly structured due depth ambiguities motion tracking 
neil lawrence publicly available code cmu database cs cmu edu joe conti volume visualization code mathworks com 
research supported nserc 
brand hertzmann 
style machines 
proc 
siggraph pp 
july 
ghahramani roweis 
learning nonlinear dynamical systems em algorithm 
proc 
nips pp 

girard rasmussen candela murray smith 
gaussian process priors uncertain inputs application multiple step ahead time series forecasting 
proc 
nips pp 

martin hertzmann popovi 
style inverse kinematics 
acm trans 
graphics aug 
lawrence 
gaussian process latent variable models visualisation high dimensional data 
proc 
nips 
lee chai hodgins pollard 
interactive control avatars animated human motion data 
acm trans 
graphics july 

direct identification nonlinear structure gaussian process prior models 
proc 
european control conference 
mackay 
information theory inference learning algorithms 

murray smith pearlmutter 
transformations gaussian process priors 
technical report department computer science glasgow university neal 
bayesian learning neural networks 
springer verlag 
pavlovi rehg maccormick 
learning switching linear models human motion 
proc 
nips pp 

buc 
dynamical modeling kernels nonlinear time series prediction 
proc 
nips 
rasmussen 
gaussian processes reinforcement learning 
proc 
nips 
sidenbladh black fleet 
stochastic tracking human figures motion 
proc 
eccv volume pp 

murray smith rasmussen 
derivative observations gaussian process models dynamic systems 
proc 
nips pp 

