nonnegative sparse pca ron amnon shashua describe nonnegative variant sparse pca problem 
goal create low dimensional representation collection points hand maximizes variance projected points uses parts original coordinates creating sparse representation 
distinguishes problem sparse pca formulations projection involves nonnegative weights original coordinates desired quality various fields including economics bioinformatics computer vision 
adding nonnegativity contributes sparseness enforces partitioning original coordinates new axes 
describe simple efficient iterative coordinate descent type scheme converges local optimum optimization criteria giving results large real world datasets 
nonnegative sparse decompositions data desirable domains underlying factors physical interpretation economics sparseness increases efficiency portfolio nonnegativity increases efficiency reduces risk 
biology coordinate axis may correspond specific gene sparseness necessary finding local patterns hidden data nonnegativity required due robustness biological systems observed change expression level specific gene emerges positive negative influence combination partly cancel 
computer vision coordinates may correspond pixels nonnegative sparse decomposition related extraction relevant parts images machine learning sparseness closely related feature selection improved generalization learning algorithms nonnegativity relates probability distributions 
principal component analysis pca popular wide spread method data decomposition applications science engineering 
decomposition performed pca linear combination input coordinates coefficients combination principal vectors form low dimensional subspace corresponds direction maximal variance data 
pca attractive number reasons 
maximum variance property provides way compress data minimal information loss 
fact principal vectors provide closest squares sense linear subspace data 
second representation data projected space uncorrelated useful property subsequent statistical analysis 
third pca decomposition achieved eigenvalue decomposition data covariance matrix 
particular drawbacks pca lack sparseness principal vectors data coordinates participate linear combination fact linear combination may mix positive negative weights partly cancel 
purpose incorporate nonnegativity sparseness pca maintaining maximal variance property pca 
words goal find collection sparse nonnegative principal school engineering computer science hebrew university jerusalem jerusalem israel 
vectors spanning low dimensional space preserves possible variance data 
efficient simple algorithm nonnegative sparse pca demonstrate results real world datasets 
related desire adding sparseness property pca focus attention past decade starting applied axis rotations component thresholding computational techniques norm approach elastic net regression spca dspca relaxing hard cardinality cap constraint convex approximation applies post processing renormalization steps improve approximate solution addition different algorithms search active coordinates principal component spectral bounds 
divided paradigms adding norm terms pca formulation known approximates better ii relaxing hard cardinality norm constraint principal vectors 
cases orthonormality principal vector set severely compromised abandoned left unclear degree resulting principal basis explains variance data 
methods deal nonnegativity approaches focus nonnegativity neutral variance resulting factors recover parts necessarily informative 
popular example nonnegative matrix factorization nmf sparse versions seek best reconstruction input nonnegative sparse prototypes weights 
start adding nonnegativity pca 
interesting direct byproduct nonnegativity pca coordinates split principal vectors 
principal vectors disjoint coordinate non zero vector 
view principal vectors parts 
relax disjoint property applications overlap parts desired allowing overlap principal vectors 
introduce sparseness term optimization criterion cover situations part semi part decomposition sufficient guarantee sparsity dimension input space far exceeds number principal vectors 
structure follows sections introduce formulation nonnegative sparse pca 
efficient coordinate descent algorithm finding local optimum derived section 
experiments section demonstrate effectiveness approach large real world datasets followed section 
nonnegative semi disjoint pca original pca maximizes variance add nonnegativity showing addition ensures sparseness turning principal vectors disjoint set vectors meaning coordinate non zero principal vector 
relax disjoint property excessive applications 
xn form zero mean collection data points arranged columns matrix uk desired principal vectors arranged columns matrix adding nonnegativity constraint pca gives optimization problem max ij ij square frobenius norm 
clearly combination entails disjoint meaning row contains non zero element 
having disjoint principal component may considered kind sparseness restrictive problems 
example stock may part sector genes typically involved biological processes pixel may shared image parts forth 
wish allow overlap principal vectors 
degree coordinate overlap represented orthonormality distance measure nonnegative vanishes iff orthonormal 
function typically literature cf 
pg 
measure orthonormality relaxed version eqn 
max balancing parameter reconstruction orthonormality 
see tradeoff relaxing disjoint property nonnegative pca relax maximum variance property pca constrained optimization tries preserve variance possible allows tradeoff higher variance degree coordinate overlap principal vectors 
add sparseness formulation 
nonnegative sparse pca semi disjoint principal components considered sparse number coordinates small may dense number coordinates highly exceeds number principal vectors 
case average number non zero elements principal vector high 
consider minimizing number non zero elements directly uij equals non zero zero 
adding criteria eqn 
max controls amount additional sparseness required 
norm relaxed replacing term nonnegative obtain relaxed sparseness term column vector elements equal 
relaxed problem algorithm max certain values solving problem eqn 
np hard 
example large values obtain original problem eqn 

concave quadratic programming np hard problem 
unrealistic look global solution eqn 
settle local maximum 
objective eqn 
function urs row ur column vector urs rs rs urs const const stands terms depend urs ass ri xx setting derivative respect urs zero obtain cubic equation urs rs urs evaluating eqn 
nonnegative roots eqn 
zero nonnegative global maximum urs see fig 

note urs approaches criteria goes function continues nonnegative maximum exist 
coordinate descent scheme updating entry converge local maximum rs rs rs rs th order polynomial left derivative right 
order find global nonnegative maximum function inspected nonnegative extrema derivative zero urs 
constrained objective function summarized bellow algorithm nonnegative sparse pca start initial guess iterate entries convergence set value urs global nonnegative maximizer eqn 
evaluating nonnegative roots eqn 
zero 
caching calculation results update element update done entire matrix updated 
easy see gradient convergence point alg 
orthogonal constraints eqn 
alg 
converges local maximum problem 
worthwhile compare nonnegative coordinate descent scheme nonnegative coordinate descent scheme lee seung 
update rule multiplicative holds inherent drawbacks 
turn positive values zero vise versa solution boundary drawback exist scheme 
second multiplicative nonnegativity built nonnegativity input applied problem scheme applied nmf 
words practical aspect algorithm handle general necessarily non negative input matrices zero mean covariance matrices 
experiments start demonstrating role parameters task extracting face parts 
mit cbcl face dataset aligned face images pixels dataset extensively demonstrate ability nonnegative matrix factorization nmf methods 
start extract principal vectors fig 
increase get principal vectors fig 

note increases overlap principal vectors decreases holistic nature vectors fig 
vanishes 
vectors sparser byproduct nonoverlapping nature 
fig 
shows amount overlap function showing drop overlap increases 
set back fig 
set value get factors fig 

vectors sparser increases time sparseness emerges drop informative pixels original vectors fig 
replacement holistic principal vectors ones part nature 
amount non zero elements principal vectors plotted function fig 
showing increment sparseness increases 
role demonstrated task extracting image features mit cbcl face dataset 
top row 
increase stays zero get localized parts lower amount overlap 
reset increase increase pixels explain variance dropped factors overlapping nature factors remains 
see fig 
detailed study 
show leading principal components pca factors nmf leading principal vectors allowing active pixels principal vector 
study different dimensional reduction methods aid generalization ability svm task face detection 
measure generalization ability receiver operating characteristics roc curve dimensional graph measuring classification ability algorithm dataset showing amount true positives function amount false positives 
wider area curve better generalization mit cbcl face dataset face images non face images training set rest dataset test set 
dimensional reduction performed face images training set 
run linear svm features extracted different values showing fig 
principal factors overlapping higher sparser higher roc curve higher meaning svm able generalize better 
compare roc curve produced linear svm extracted features ones produced pca nmf principal vectors displayed fig 
fig 
correspondingly 
representative sparse pca methods greedy sparse pca shows comparable better results sparse pca methods see principal vectors fig 

fig 
shows better generalization achieved extracted features reliable face detection 
limited nonnegative entries principal vectors inherently explain variance sparse pca algorithms constrained way similarly fact sparse pca algorithms explain variance pca 
limitation holds manages explain large amount variance 
demonstrate fig 
compare amount cumulative explained variance cumulative cardinality different sparse pca algorithms pit props dataset classic dataset sparse pca literature 
domains nonnegativity intrinsic problem extracted features improves generalization ability learning algorithms demonstrated face detection problem 
summary method differs substantially previous approaches sparse pca difference begins definition problem 
sparse pca methods try limit cardinality number non zero elements principal vector accept input soft limitation log log amount overlap orthogonality function higher values decrease overlap increase orthogonality amount non zero elements function higher values enforce sparseness 
true positives false positives true positives nmf pca false positives roc curve svm task face detection mit cbcl face dataset different values showing improved generalization principal vectors overlap higher sparser higher nmf pca extracted features showing better generalization 
cumulative variance pca spca dspca pcs cumulative cardinality spca dspca pcs cumulative explained variance cumulative cardinality function number principal components pit props dataset classic dataset typically evaluate sparse pca algorithms 
constrained sparse pca algorithms explain variance just sparse pca algorithms explain variance pca dataset nonnegative nature shows competitive results number principal components increases 
cardinality 
addition sparse pca methods focus task finding single principal vector 
method hand splits coordinates different principal vectors input number principal vectors parts size part 
consequence natural way algorithm search principal vectors 
sense bears resemblance nonnegative matrix factorization problem method departs significantly sense focus informative parts maximizes variance 
furthermore non negativity output rely having non negative input matrices process permitting zero mean covariance matrices fed process just done pca 

sparse factorizations gene expression guided binding data 
pacific symposium biocomputing 
alexandre laurent el ghaoui michael jordan gert lanckriet 
direct formulation sparse pca semidefinite programming 
proceedings conference neural information processing systems nips 
floudas visweswaran 
quadratic optimization 
handbook global optimization pages 
kluwer acad 
publ dordrecht 
matthias christoph schn rr 
learning non negative sparse image codes convex programming 
proc 
th ieee intl 
conf 
comp 
vision iccv 
patrik hoyer 
non negative sparse coding 
neural networks signal processing 
proceedings th ieee workshop pages 
patrik hoyer 
non negative matrix factorization sparseness constraints 
journal machine learning research 
ravi jagannathan ma 
risk reduction large portfolios imposing wrong constraints helps 
journal finance 
ian jolliffe 
rotation principal components choice normalization constraints 
journal applied statistics 
ian jolliffe 
modified principal component technique lasso 
journal computational graphical statistics september 
lee seung 
learning parts objects non negative matrix factorization 
nature october 
li hou zhang cheng 
learning spatially localized parts representation 
proceedings ieee conference computer vision pattern recognition 
moghaddam yair weiss shai avidan 
spectral bounds sparse pca exact greedy algorithms 
proceedings conference neural information processing systems nips 
parlett 
symmetric eigenvalue problem 
prentice hall upper saddle river nj usa 
zou hastie tibshirani 
sparse principal component analysis 
