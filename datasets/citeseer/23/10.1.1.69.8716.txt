journal machine learning research submitted published algorithmic implementation multiclass kernel vector machines crammer cs huji ac il yoram singer singer cs huji ac il school computer science engineering hebrew university jerusalem israel editors nello cristianini john shawe taylor bob williamson describe algorithmic implementation multiclass kernel vector machines 
starting point generalized notion margin multiclass problems 
notion cast multiclass categorization problems constrained optimization problem quadratic objective function 
previous approaches typically decompose multiclass problem multiple independent binary classification tasks notion margin yields direct method training multiclass predictors 
dual optimization problem able incorporate kernels compact set constraints decompose dual problem multiple optimization problems reduced size 
describe efficient fixed point algorithm solving reduced optimization problems prove convergence 
discuss technical details yield significant running time improvements large datasets 
describe various experiments approach comparing previously studied kernel methods 
experiments indicate multiclass problems attain state art accuracy 
keywords multiclass problems svm kernel machines 
supervised machine learning tasks boil problem assigning labels instances labels drawn finite set elements 
task referred multiclass learning 
numerous specialized algorithms devised multiclass problems building classification learning algorithms binary problems problems set possible labels size 
notable examples multiclass learning algorithms multiclass extensions decision tree learning breiman quinlan various specialized versions boosting adaboost adaboost mh freund schapire schapire singer 
dominating approach solving multiclass problems support vector machines reducing single multiclass problems multiple binary problems 
instance common method build set binary classifiers classifier distinguishes labels rest 
approach special case output codes solving multiclass problems dietterich bakiri 
multiclass learning output codes provides simple powerful framework capture crammer yoram singer 
crammer singer correlations different classes breaks multiclass problem multiple independent binary problems 
develop discuss detail direct approach learning multiclass support vector machines svm 
svms gained enormous popularity statistics learning theory engineering see instance vapnik sch lkopf cristianini shawe taylor 
exceptions support vector learning algorithms designed binary class problems 
attempts generalize svm multiclass problems weston watkins vapnik 
attempts extend binary case adding constraints class size quadratic optimization proportional number categories classification problems 
result homogeneous quadratic problem hard solve difficult store 
starting point approach simple generalization separating hyperplanes analogously generalized notion margins multiclass problems 
notion margin employed previous research allwein context svm 
definition margin multiclass problems describe section compact quadratic optimization problem 
discuss dual problem form resulting multiclass predictor 
section give decomposition dual problem multiple small optimization problems 
decomposition yields memory time efficient representation multiclass problems 
proceed describe iterative solution set reduced optimization problems 
discuss section means choosing reduced problem solve round algorithm 
discuss section efficient fixed point algorithm finding approximate solution reduced problem chosen 
analyze algorithm derive bound rate convergence optimal solution 
baseline algorithm main loop composed example selection optimization followed invocation fixedpoint algorithm example chosen 
baseline algorithm small datasets practical large ones technical improvements sought 
devote section description different technical improvements taken order approach applicable large datasets 
discuss running time accuracy results achieved experiments underscore technical improvements 
addition report section results achieved evaluation experiments comparing previous 
give section 
related naturally builds previous research advances learning support vector machines 
space clearly limited mention relevant refer reader books collections mentioned 
mentioned idea casting multiclass problems single constrained optimization quadratic objective function proposed vapnik weston watkins bredensteiner bennet 
size resulting optimization problems devised papers typically large complex 
idea breaking large constrained optimization problem small problems employs subset constraints explored context support vector machines boser 

ideas multiclass kernel vector machines developed researchers see joachims overview 
roots line research go back seminal developed yair censor colleagues see censor zenios excellent overview 
ideas distilled platt method called smo sequential minimal optimization 
smo works reduced problems derived pair examples approach employs single example reduced optimization problem 
result simple optimization problem solved analytically binary classification problems see platt leads efficient numerical algorithm guaranteed converge multiclass settings 
furthermore explored deems possible single example reduction parallel applications 
technical improvements discuss proposed previous 
particular ideas working set caching described burges platt joachims 
note part general line research multiclass learning involved 
allwein 
described analyzed general approach multiclass problems error correcting output codes dietterich bakiri 
building investigated problem designing output codes multiclass problems crammer singer 
model learning output codes differs framework studied techniques build results earlier crammer singer 
ideas build multiclass predictors online settings mistake bound model means analysis 
current research multiclass problems concentrates analogous online approaches crammer singer 

preliminaries xm ym set training examples 
assume example xi drawn domain nand label yi integer set 
multiclass classifier function maps instance element focus framework uses classifiers form arg max matrix size rth row interchangeably call value inner product rth row instance confidence similarity score class 
definition predicted label index row attaining highest similarity score setting generalization linear binary classifiers 
notation introduced linear binary classifiers predict label instance 
classifier implemented matrix size note representation efficient occupies twice memory needed 
model parsimonious maintain prototypes mk set label new input instance choosing index similar row crammer singer illustration margin bound employed optimization problem 
classifier parametrized matrix example say misclassifies example predicate holds 
empirical error multiclass problem xi yi 
goal find matrix attains small empirical error sample generalizes 
direct approaches attempt minimize empirical error computationally expensive see instance simon crammer singer 
building vapnik support vector machines vapnik describe section paradigm finding matrix replacing discrete empirical error minimization problem quadratic optimization problem 
see recasting problem minimization problem enables replace form kernel inner products form 

constructing multiclass kernel predictors construct multiclass predictors replace misclassification error example piecewise linear bound max equal 
bound zero confidence value correct label larger confidences assigned rest labels 
suffer loss linearly proportional difference confidence correct label maximum confidences labels 
graphical illustration 
circles denote different labels correct label plotted dark grey rest labels plotted light gray 
height label designates confidence 
settings plotted 
left plot corresponds case margin larger bound maxr equals zero example correctly classified 
middle shows case example correctly classified small margin suffer loss 
right plot depicts loss misclassified example 
multiclass kernel vector machines summing examples get upper bound empirical loss max xi yi xi 
say sample linearly separable multiclass machine exists matrix loss equal zero examples max xi yi xi 
matrix satisfies eq 
satisfy constraints xi yi xi 
define norm matrix norm vector represented concatenation rows mk note constraints eq 
satisfied differences xi xi arbitrarily large 
furthermore previous generalization properties large margin dags platt multiclass problems showed generalization properties depend norm see crammer singer 
seek matrix small norm satisfies eq 

sample linearly separable multiclass machine seek matrix smallest norm satisfies eq 

result optimization problem min subject xi yi xi note constraints yi automatically satisfied xi yi yi xi 
property artifact separable case 
general case sample linearly separable multiclass machine 
add slack variables modify eq 
max xi yi xi 
replace optimization problem defined eq 
primal optimization problem min subject xi yi xi regularization constant yi inequality constraints 
optimization problem soft constraints 
note crammer singer passing possible cast analogous optimization problem hard constraints vapnik 
solve optimization problem karush kuhn tucker theorem see instance vapnik cristianini shawe taylor 
add dual set variables constraint get lagrangian optimization problem xi xi yi subject seek saddle point lagrangian minimum primal variables maximum dual variables 
find minimum primal variables require 
similarly require xi xi yi xi xi results form yi xi 
eq 
implies solution optimization problem eq 
matrix rows linear combinations instances xm 
note eq 
get contribution instance xi yi say example xi support pattern row coefficient zero 
row matrix partition patterns nonzero coefficients subsets rewriting eq 
follows xi xi yi yi sum patterns belong rth class 
example xi labeled yi support pattern yi 
second sum rest patterns labels different case example xi support pattern multiclass kernel vector machines 
put way pattern xi set satisfies constraints set viewed probability distribution labels 
probabilistic interpretation example xi support pattern corresponding distribution concentrated correct label yi 
classifier constructed patterns labels uncertain rest input patterns ignored 
develop lagrangian dual variables substituting eqs 
eq 

derivation technical defer complete derivation app 
obtain objective function dual program xi xj yi yj yi vector components zero ith component equal vector components 
notation rewrite dual program vector form max subject xi xj yi yj yi easy verify concave 
set constraints convex unique maximum value 
simplify problem perform change variables 
yi difference point distribution yi concentrating correct label distribution obtained optimization problem 
eq 
describes form xi 
search value variables maximize objective function optimum value omit additive positive multiplicative constants write dual problem eq 
max xi xj yi subject yi rewrite classifier terms variable arg max arg max xi 
support vector machines cortes vapnik dual program resulting classifier depend inner products form xi 
perform inner product calculations high dimensional inner product space crammer singer replacing inner products eq 
eq 
kernel function satisfies mercer conditions vapnik 
general dual program kernel functions max xi xj yi subject yi classification rule arg max rk xi 
constructing multiclass predictor kernel inner products simple standard inner products 
note classifier eq 
contain bias parameter br augmenting terms add equality constraints dual optimization problem increasing complexity optimization problem 
inner products form equivalent bias parameters adding objective function 
note special case eq 
reduces primal program svm setting mentioned weston watkins developed multiclass version svm 
approach compared confidence correct label confidences labels slack variables primal problem 
contrast framework confidence correct label compared highest similarity score rest labels uses slack variables primal program 
describe sequel compact formalization leads memory time efficient algorithm optimization problem 

decomposing optimization problem dual quadratic program eq 
solved standard quadratic programming qp techniques 
employs mk variables converting dual program eq 
standard qp form yields representation employs matrix size mk mk leads large scale problem general 
clearly storing matrix size intractable large problems 
introduce simple memory efficient algorithm solving quadratic optimization problem eq 
decomposing small problems 
core idea algorithm separating constraints eq 
disjoint sets yi algorithm propose works rounds 
round algorithm chooses pattern improves value objective function updating variables set constraints yp 
fix example index write objective function terms variables brevity ki denote xi xj 
isolate contribution input xm ym 
initialize 
loop multiclass kernel vector machines 
choose example 
calculate constants reduced problem ap xp xp bp xi xp yp 
set solution reduced problem min ap bp subject yp output arg max rk xi skeleton algorithm learning multiclass support vector machine 
qp def ki yi kp ki ki yp yi kp yp ki ki yi 
define variables ap kp bp yp cp ki ki yi variables defined objective function qp ap bp cp crammer singer brevity omit constants affect solution 
reduced optimization problem variables constraints min ap bp subject yp 
skeleton algorithm 
algorithm initialized discuss leads simple initialization internal variables algorithm employs efficient implementation 
complete details algorithm need discuss issues 
need stopping criterion loop 
simple method run algorithm fixed number rounds 
better approach discuss sequel continue iterating long algorithm meet predefined accuracy condition 
second need scheme choosing pattern round induces reduced optimization problem eq 

commonly methods scan patterns sequentially choose pattern uniformly random 
describe scheme choosing example greedy manner 
scheme appears perform better empirically naive schemes 
address issues section 
third issue need address solve efficiently reduced problem eq 

problem constitutes core inner loop algorithm develop efficient method solving reduced quadratic optimization problem 
method efficient standard qp techniques especially suffices find approximation optimal solution 
specialized solution enables solve problems large number classes straightforward approach applicable 
method described section 
example selection optimization remind reader need solve eq 
min ki yi subject yi ki xi xj 
karush kuhn tucker theorem see cristianini shawe taylor find necessary conditions point optimum eq 

lagrangian problem ki yi ui yi subject condition ui ki yi ui vi 
vi multiclass kernel vector machines define auxiliary set variables fi ki yi 
instance xi value fi designates confidence assigning label xi 
value subtracted correct label confidence order obtain margin 
note eq 
get fp bp kp 
relation variables section discuss efficient solution quadratic problem 
derivative respect dual variables lagrangian eq 
definition fi eq 
kkt conditions get set equality constraints feasible solution quadratic optimization problem fi ui vi ui yi ui 
simplify equations 
considering cases 
case yi case eq 
holds automatically 
combining eq 
eq 
get fi vi 
second case yi order eq 
hold ui 
eq 
get fi vi replace single equality constraint inequalities fi vi fi vi 
remind reader constraints optimization problem eq 
imply yi 
constraints satisfied exist label yi get vi maxr fi note fi yi vi maxr fi fi yi unique maximum 
combine set constraints eqs 
single inequality dropping vi obtain define max fi vi min fi 
yi max fi min fi 
yi max fi min fi 
yi crammer singer maxr fi minr fi necessary sufficient condition feasible vector optimum eq 

actual numerical implementation sufficient find predefined accuracy parameter 
keep performing main loop long examples xi yi values greater 
variables serve means choosing example update 
implementation try keep memory requirements small possible manipulate single example loop 
choose example index maximal 
find vector approximate solution reduced optimization problem eq 

due change need update fi pseudo code describing process deferred section describe simple efficient algorithm finding approximate solution optimization problem eq 

lin showed scheme converge solution finite number steps 
note underlying ideas described section explored keerthi gilbert 

solving reduced optimization problem core algorithm relies efficient method solving reduced optimization eq 
equivalent problem defined eq 

section describe efficient fixed point algorithm finds approximate solution eq 

note exact solution derived 
crammer singer described closely related algorithm solving similar quadratic optimization problem context output coding 
simple modification algorithm 
algorithm needs sort values iteration slow large 
furthermore discuss section empirically quality solution quite insensitive fulfill karush kuhn tucker condition bounding find vector decreases significantly value necessarily optimal solution 
start rewriting eq 
completion quadratic form dropping pattern index perform change variables 
point omit additive constants multiplicative factor affect value optimal solution 
variable optimization problem eq 
min subject note multiclass kernel vector machines fi bi ai compute bi need store bi fi denote variables dual problem eq 

karush kuhn tucker conditions imply dr dr 
note conditions imply combining inequality constraint dr get solution satisfies min dr 
get dr eq 
holds equality solution form min dr 
get satisfies constraint min dr dr 
equation uniquely defines sum minr dr strictly monotone continuous function 
maxr dr minr dr dr minr dr 
exists unique value satisfies eq 

theorem shows optimal solution quadratic optimization problem 
theorem min dr solution minr dr dr 
point proof assume contradiction feasible point minimizes objective function 
know 
satisfy equality constraint eq 

points satisfy inequality constraint eq 
dr combining equations assumption get 
equality exists index 
denote min 
define new feasible point follows 

show norm smaller norm 
differ coordinates writing values terms get crammer singer construction get implies clearly contradiction 
characterization solution derive simple fixed point algorithm finds simple identity min dr max dr dr replace minimum function sum eq 
get dr max dr dr amounts max dr define max dr optimal value satisfies 
eq 
iterative algorithm 
algorithm starts initial value computes value eq 

continues iterating substituting new value producing series values 
algorithm halts required accuracy met successive values close 
pseudo code algorithm 
input algorithm vector initial suggestion required accuracy 
show maxr dr algorithm converge correct value theorem fixed point eq 

assume maxr dr 
number classes 
proof assume loss generality maxr dr dk dk def 
assume ds ds du du 
input 
initialize 
repeat 
multiclass kernel vector machines max dr 
assign min dr return fixed point algorithm solving reduced quadratic program 
max dr dr dr 
note maxr dr maxr dr similarly dr dr need consider cases depending relative order case case get dr second equality follows eq 

second case case get dr 
crammer singer eq 
eq 
get dr dr dr applying eq 
obtain bounded convex combination larger 
get case derived analogously second case interchanging roles proof see best convergence rate obtained large values feasible initialization minr dr case dr gives simple initialization algorithm ensures initial rate convergence fast 
ready describe complete implementation algorithm learning multiclass kernel machine 
algorithm gets required accuracy parameter value 
initialized indices value yields simple initialization variables fi iteration compute fi value example choose example index largest 
call fixed point algorithm turn finds approximate solution reduced quadratic optimization problem example indexed fixed point algorithm returns set new values triggers update fi process repeated value smaller pseudo code algorithm 
dr input xm ym 
initialize fi yi ai xi xi repeat multiclass kernel vector machines calculate max fi min fi yi set arg max set dr fp ap yp dr call 
see set update fi fi xp xi update output arg max rk xi basic algorithm learning multiclass kernel support vector machine kkt conditions example selection 

implementation details discussed far underlying principal algorithmic issues arise design multiclass kernel vector machines 
learning algorithm practical large datasets technical improvements baseline implementation 
improvements change underlying design principals lead significant improvement running time 
devote section description implementation details 
compare performance different versions section mnist ocr dataset mnist dataset contains training examples test examples underscore significant implementation improvements 
diving technical details note techniques means new prior implementation class support vector machines see instance platt joachims bengio 
implementation improvements build specific algorithmic design multiclass kernel machines 

available www research att com yann exdb mnist index html run time sec epsilon crammer singer run time left test error right function required accuracy 
starting point base line implementation algorithm described combined fixed point algorithm solving reduced quadratic optimization problem 
base line implementation simply cycle examples fixed number iterations solving reduced optimization problem ignoring kkt conditions examples 
scheme simple implement 
spends unnecessary time optimization patterns correctly classified large confidence 
scheme illustrate importance efficient example selection described previous section 
describe different steps took starting version described section 
kkt example selection algorithm described 
example label compute fi variables compute described section 
round choose example largest iterate process value smaller predefined accuracy denoted 
turns choice crucial large range values yield results 
larger sooner terminate main loop algorithm 
set large value long generalization performance effected 
show running time test error function 
results show moderate value yields generalization 
increase running time smaller values 
algorithm robust actual choice accuracy parameter long set value evidently large 
maintaining active set standard implementation described scans entire training set computes example xi set 
support patterns constitute multiclass machine vector zero vector example 
partition set examples sets 
denoted called active set composed set examples contribute solution 
second set simply complement 
course main loop search example update set example exist happen iff scan test error epsilon objective function value multiclass kernel vector machines cooling cooling iteration value objective function function number iteration fixed variable scheduling accuracy parameter 
set example 
example exists remove add call fixed point algorithm example 
procedure spends time adjusting weights examples constitute active set adds new example active set exhausted 
natural implication procedure support patterns come active set 
cooling accuracy parameter employment active set yields significant reduction running time 
scheme forces algorithm keep updating vectors long single example 
may result minuscule changes slow decrease examples updated 
plot bold line value function number iterations kept fixed 
line staircase shape 
careful examination iterations significant drop revealed iterations new examples added active set 
addition new example numerous iterations spent adjust weights accelerate process especially early iterations add new examples active set variable accuracy parameter fixed accuracy 
early iterations accuracy value set high value algorithm add new examples active set spend small time adjusting weights support patterns 
number iterations increases decrease spend time adjusting weights support patterns 
result smoother rapid decrease leads faster convergence algorithm 
refer process gradually decreasing run time crammer singer size training set comparison run time mnist dataset different versions function training set size 
version baseline implementation 
version uses kkt conditions selecting example update 
version adds usage active set cooling 
version adds caching inner products 
version uses data structures representing sparse inputs 
cooling 
tested cooling schemes exponential exp linear logarithmic log 
initial accuracy set 
cooling schemes improve rate decrease especially logarithmic scheme relatively large long period decreases moderately 
dashed line designate value function number iterations logarithmic cooling scheme 
particular setting cooling reduces number iterations running time order magnitude 
caching previous implementations algorithms support vector machines employ cache saving expensive kernel inner products see instance platt joachims bengio 
expensive steps algorithm evaluation kernel 
scheme maintaining cache follows 
small datasets store cache kernel evaluations example active set examples training set 
large problems support patterns large active set lru scheme caching strategy 
scheme cache full replace inner products example inner products new example 
lru caching svm light joachims 
multiclass kernel vector machines name 



training examples test examples classes attributes satimage shuttle mnist isolet letter vowel glass fold cval table description small databases experiments 
data structures sparse input instances platt observed components input vectors zero significant saving space time achieved data structures sparse vectors computing products non zero components kernel evaluations 
implementation uses linked list sparse vectors 
experimental evaluation performed indicate sparseness input instances achieve speedup time reduction memory non sparse implementation 
conclude section give comparison run time various technical improvements outlined 
version plot include previous improvements 
running time version includes algorithmic implementation improvements orders magnitude faster baseline implementation 
took fastest version hours minutes train multiclass kernel machine mnist dataset pentium iii computer running mhz gb physical memory 
fastest version included technical improvements discussed documented code shortly available 
comparison platt reports training time hours minutes learning single classifier discriminates digit rest 
takes hours train set classifiers constitute multiclass predictor 
platt results obtained pentium ii computer running mhz 
different factors influence running time running time results give indication power approach multiclass problems 
believe advantage direct approach multiclass problems evident problems large number classes character recognition 
note improved running time achieved expense deteriorated classification accuracy 
section describe experiments show approach comparable better standard support vector technology 

experiments section report results experiments conducted order evaluate implementation multiclass support vector machine described 
decrease error rate crammer singer satimage shuttle mnist isolet letter vowel glass comparison multiclass svm build rest approach multiclass support vector machines studied 
selected datasets uci repository mnist dataset 
datasets glass contain separate test set 
remaining dataset fold cross validation compute error rate 
description datasets table 
note mnist letter shuttle subset training set 
data set ran algorithms 
algorithm uses multiclass svm binary mode training classifier class 
classifier trained distinguish class rest classes 
classify new instance compute output binary classifiers predict label attains highest confidence value 
second algorithm compared multiclass svm described 
multiclass svm basis algorithms order common framework comparison 
algorithms gaussian kernels 
determine value cross validation training set 
fold cross validation large datasets fold cross validation small datasets 
experiments set value 
summary results depicted 
bar proportional difference test error algorithms 
positive value means algorithm proposed achieved lower error rate strawman algorithm vs rest approach 
general multiclass support vector machine achieved lower error rate vs rest method datasets large example class ratio improvement significant 

available www ics uci edu mlearn mlrepository html multiclass kernel vector machines examples images support patterns large norm mnist dataset 
crammer singer name test error mnist usps shuttle letter table summary experiments large datasets 
ran multiclass svm complete training set mnist letter shuttle usps dataset 
results summarized table 
sch lkopf 
achieved test error mnist dataset test error usps sch lkopf set binary svms training vs rest method 
dietterich achieved test error letter dataset test error shuttle dataset boosting 
note cases algorithm comparable algorithms times worse shuttle dataset 
gap explained fact instances dataset belong class phenomena easier boosting algorithms cope 
note polynomial kernel degree normalization described decoste sch lkopf achieve error rate test set mnist 
give training images taken mnist dataset high norm 
images plot attain margin value 
image give correct label parentheses probability vector sparse representation entry vector form label probability 
training error image added trailing text 
discussed previous sections vector probability vector concentrated corrected label support pattern 
images support pattern vectors designate set confusable labels labels corresponding entries non zero 
examination different images reveal confusion correlated actual digit training example confused 
instance top image left hand side mistakenly classified 
cases examples large norm correspond mislabeled examples corrupted images 
norm aid data cleaning 

summary novel approach building multiclass support vector machines 
described detail efficient learning algorithm discussed various implementation issues required making practical 
methods achieve state art results running time competitive previous methods order magnitude faster problems 
interesting questions stem research approach taken machine learning tasks kernels ranking problems regression 
leave research 

available ftp tuebingen mpg de multiclass kernel vector machines interesting direction currently pursuing analogous online algorithms multiple prototypes multiclass problems 
acknowledgments useful comments carefully reading manuscript 
chih jen lin fruitful email exchange 
appendix derivation dual optimization problem develop lagrangian dual variables 
simplicity identity kernel xi xj xi xj 
substituting eq 
eq 
obtain def def def 
substitute equation eq 
get xj yj xi xj yj xj yj yi xi xj yj yi xi xj yj yi xi xj yj yi xi xj yi yi yi yi yi yj crammer singer xi xj xi yi xj yj yi yj 
difference eqs 
get xi xj xi xj yj xi xj yi yj yi yj 
plugging values eqs 
eq 
get xi xj xi xj xi xj yi yj yi yj yi yj allwein schapire singer 
reducing multiclass binary unifying approach margin classifiers 
machine learning proceedings seventeenth international conference 
bernhard boser isabelle guyon vladimir vapnik 
training algorithm optimal margin classifiers 
proceedings fifth annual acm workshop computational learning theory pages 
bredensteiner bennet 
multicategory classification support vector machines 
computational optimizations applications 
bregman 
relaxation method finding common point convex sets application solution problems convex programming 
ussr computational mathematics mathematical physics 
leo breiman jerome friedman richard olshen charles stone 
classification regression trees 
wadsworth brooks 
burges 
tutorial support vector machines pattern recognition 
data mining knowledge discovery 
yair censor stavros zenios 
parallel optimization theory algorithms applications 
oxford university press 
multiclass kernel vector machines ronan bengio 
svmtorch support vector machines large scale regression problems 
journal machine learning research 
corinna cortes vladimir vapnik 
support vector networks 
machine learning september 
crammer yoram singer 
learnability design output codes multiclass problems 
proceedings thirteenth annual conference computational learning theory 
crammer yoram singer 
ultraconservative online algorithms multiclass problems 
proceedings fourteenth annual conference computational learning theory 
nello cristianini john shawe taylor 
support vector machines 
cambridge university press 
dennis decoste bernhard sch lkopf 
training invariant support vector machines 
machine learning 
thomas dietterich 
experimental comparison methods constructing ensembles decision trees bagging boosting randomization 
machine learning 
thomas dietterich bakiri 
solving multiclass learning problems errorcorrecting output codes 
journal artificial intelligence research january 
robert schapire 
decision theoretic generalization line learning application boosting 
journal computer system sciences august 
elisseeff 
new multi class svm uniform convergence result 
editor proceedings ijcnn 
klaus 
hans 
simon 
robust single neurons 
proceedings fifth annual acm workshop computational learning theory pages pittsburgh pennsylvania july 
thorsten joachims 
making large scale support vector machine learning practical 
sch lkopf burges smola editors advances kernel methods support vector learning 
mit press 
keerthi gilbert 
convergence generalized smo algorithm svm classifier design 
technical report cd control division dept mechanical production engineering national university singapore 

lin 
stopping criteria decomposition methods support vector machines theoretical justification 
technical report computer science information engineering national taiwan university may 
crammer singer platt 
fast training support vector machines sequential minimal optimization 
sch lkopf burges smola editors advances kernel methods support vector learning 
mit press 
platt cristianini shawe taylor 
large margin dags multiclass classification 
advances neural information processing systems pages 
mit press 
ross quinlan 
programs machine learning 
morgan kaufmann 
robert schapire yoram singer 
improved boosting algorithms confidence rated predictions 
machine learning 
sch lkopf 
support vector learning 
phd thesis gmd 
sch lkopf burges smola editors 
advances kernel methods support vector learning 
mit press 
sch lkopf sung burges girosi niyogi poggio vapnik 
comparing support vector machines gaussian kernels radial basis function classifiers 
technical report memo massachusetts institute 
vladimir vapnik 
statistical learning theory 
wiley 
weston watkins 
support vector machines multi class pattern recognition 
proceedings seventh european symposium artificial neural networks april 

