unsupervised feature subset selection madsen casper thomsen jose pe department computer science aalborg university denmark jmp cs auc dk 
studies filter hybrid filter wrapper feature subset selection unsupervised learning data clustering 
constrain search best feature subset scoring dependence feature rest features scores discriminate irrelevant features 
report experimental results artificial real data unsupervised learning naive bayes models 
filter hybrid approaches perform satisfactorily 
main problems arises great variety fields including artificial intelligence machine learning statistics called data clustering problem 
data form set instances underlying group structure data clustering may roughly defined search best description group structure true group membership instance unobserved 
groups data hand called cluster 
lack knowledge cluster membership instance data data clustering referred unsupervised learning 
different interpretations expectations term unsupervised learning gives rise concerned unsupervised learning problems basically defined assumptions database containing instances cases 
xn available 
th case represented dimensional discrete vector xl xl 
partitioned xl cl cl unobserved cluster membership xl yl 
dimensional discrete vector observations xl 
number clusters underlying denoted known 
clusters represents physical process defined unknown joint probability distribution 
case may seen sampled exactly unknown joint probability distributions 
corresponds assuming existence dimensional discrete random variable 
xn partitioned unidimensional discrete hidden random variable represents authors contributed equally 
madsen unobserved cluster membership 
yn dimensional discrete random variable represents observations 
yi called feature 
usual assume mechanism generated works stages physical processes associated clusters exist selected probability distribution instance generated joint probability distribution defines selected physical process 
parametric forms joint probability distributions govern mechanism generated known multinomial 
assumptions unsupervised learning approached model perspective description clusters accomplished probabilistic modelling mechanism generated unsupervised learning reduces learning joint probability distribution approach unsupervised learning deals 
challenges unsupervised learning involves received little attention far literature unsupervised feature subset selection 
database may happen subset original features contains information group structure underlying case features considered relevant unsupervised learning features deemed irrelevant noise 
unsupervised feature subset selection aims identifying feature subsets model describing accurately clusters obtained projection random variable unsupervised learning 
provides user better understanding relevant irrelevant features identified 
final model comprehensible involves relevant features 
remainder structured follows 
section introduces class probabilistic graphical models unsupervised learning consider naive bayes models 
section describes proposals unsupervised feature subset selection 
section evaluates proposals artificial real data 
section closes discussion 
naive bayes models unsupervised learning seen solving unsupervised learning problem model approach reduces learning joint probability distribution 
paradigms specially suited purpose bayesian networks 
random variable stated 
bayesian network bn unsupervised learning pair model structure model parameters 
model structure acyclic directed graph nodes correspond unidimensional random variables text terms node random variable interchangeably 
model parameters specify conditional probability distribution node xi parents ai xi pa 
conditional probability distributions typically multinomial 
unsupervised feature subset selection bn unsupervised learning represents joint probability distribution graphical factorization xi pa 
encodes set conditional dependencies random variables usually constrained yi child restriction imposed assumption generative mechanism underlying modelled domain works recall section 
simple competitive class bns unsupervised learning received attention literature called naive bayes nb models 
regular bns unsupervised learning peculiarity model structures enforce features conditionally independent arcs model structures due structural constraint previously stated yi child eq 
rewritten follows nb model unsupervised learning yi 
data 
xn defined unsupervised learning nb model reduces unsupervised parameter estimation fact model structure fixed 
maximum likelihood maximum posteriori estimates effectively obtained approximation techniques em algorithm 
terms introduced section nb model unsupervised learning induced represents description clusters modelling selector ii yi modelling physical processes select 
feature subset selection unsupervised learning database unusual subset original features informative clusters rest features marginally informative totally uninformative noise 
case projection random variable denoted underlying group structure difference features uninformative structure 
possible obtain nb model unsupervised learning describes clusters accurately best nb model unsupervised learning induced original database features named relevant unsupervised learning features deemed irrelevant 
feature subset selection unsupervised learning unsupervised feature subset selection ufss aims identifying feature subsets madsen nb model describing accurately clusters obtained unsupervised learning 
improves interpretability induced model relevant features involved degrading descriptive accuracy irrelevant features excluded 
additionally identification relevant irrelevant features unsupervised learning provides valuable insight nature group structure ufss reduces risk overfitting increases reliability final model 
ufss stated optimization problem terms search space search strategy objective function 
search space consists subsets original features 
exhaustive search strategy cases feature subsets exist original features 
greedy hill climbing commonly 
approach referred sequential selection forwards backwards 
sequential forward selection sfs starts feature selected iteratively rewarding feature unselected ones selected stopping criterion satisfied 
similarly sequential backward selection sbs begins set features selected iteratively rewarding feature selected ones unselected stopping criterion met 
objective function scores relevance feature subset search space alternatively relevance feature 
objective functions ufss classified filters wrappers 
filter approaches assess features feature subsets data exclusively ignoring subsequent unsupervised learning algorithm 
hand wrapper approaches evaluate feature subset performance unsupervised learning algorithm original data projected features subset 
filter unsupervised feature subset selection section describe detail proposal ufss filter approach combined sfs 
particular sfs proceeds follows 
initially feature selected 
relevant feature unselected ones significantly relevant iteratively selected 
search stops feature selected 
suggest filter relevance scores features method assessing feature significantly relevant 
filter relevance scores describe observations 
truly relevant feature dependent cluster random variable cases strongly dependent 
features pairwise dependent 
hand features independent weakly dependent rest features truly irrelevant unsupervised learning belong conjecture scoring dependence feature rest features helps discriminating irrelevant features 
reasoning assumes relevant features works similar observations 
unsupervised feature subset selection pairwise dependence score propose mutual information 
dependence score features yi yj ds yi yj computed mutual information yi yj mi yi yj ds yi yj mi yi yj yi yi yj yi yi yj entropy yi conditional entropy yi yj respectively 
second pairwise dependence score consider mutual prediction 
dependence score features yi yj ds yi yj seen mutual prediction yi yj mp yi yj ds yi yj mp yi yj yi yj yi yj yj yi yi yi yj similarly yj yj yi yi max yi yi yi yj yj yj max yi yj 
yi yi similarly yj probability predicting state yi correctly predicting probable state yi priori 
yi yj similarly yj yi expected probability predicting state yi correctly predicting probable state yi state yj known 
probability distributions involved computation dependence scores ds yi yj introduced estimated maximum likelihood criterion 
note dependence scores symmetric ds yi yj ds yj yi 
cases lower value ds yi yj weaker dependence yi yj 
suggest computing relevance score feature yi rs yi average dependence score yi rest features rs yi ds yi yj 
lower value rs yi relevant yi unsupervised learning note rs yi filter measure exclusively 
recall sfs keeps selecting features unselected ones long significantly relevant 
order decide significance relevance particular feature propose carrying distribution free hypothesis test 
yi feature considered selection main idea rs yi statistic value hypothesis test null hypothesis yi random noise irrelevant 
probability distribution statistic null hypothesis estimated empirically statistic values scored randomly chosen features known satisfy null hypothesis sufficiently large 
practice features madsen obtained randomly entries yi alternatively filling entries states yi drawn random 
regular hypothesis test empirically estimated probability distribution statistic null hypothesis set threshold rejecting null hypothesis significance level 
particular rs yi larger statistic values scored randomly created irrelevant features null hypothesis yi irrelevant rejected significance level 
see thorough extensive application distribution free hypothesis testing clustering validation 
hybrid filter wrapper unsupervised feature subset selection section second proposal ufss wrapper approach combined sbs built top filter ufss introduced section 
filter ufss described previous section behave conservatively truly irrelevant features may go undetected distribution free hypothesis testing due random effects 
preliminary experiments unsupervised learning nb models confirmed pointed simple way correct partially 
experiments showed accuracy description group structure underlying learning data decreased adding feature selected filter ufss 
words performance unsupervised learning nb models non decreasing function number features selected filter ufss 
function levelled flat features selected conservative behavior 
truly irrelevant features go undetected filter ufss spotted running wrapper ufss sbs 
paragraphs give details hybrid filter wrapper ufss 
proposal proceeds steps follows 
run filter ufss section 
second run wrapper ufss sbs considering features identified relevant filter ufss 
particular sbs starts marking features selected unselected 
proceeding reverse order features selected filter ufss relevant relevant feature iteratively unselected stopping condition met 
stopping criterion guarantees accuracy description clusters degrade dramatically 
denote features selected current iteration wrapper ufss 
stopping criterion prescribes halting wrapper ufss performance nb model obtained unsupervised learning falls performance nb model obtained user defined parameter 
intuitively represents loss performance user willing accept wrapper ufss identify new irrelevant features improve interpretability final model 
note time stopping criterion wrapper ufss evaluated nb model unsupervised learning induced 
number evaluations stopping criterion bounded number features unsupervised feature subset selection sbs performs linear search 
mentioned preliminary experiments showed performance unsupervised learning nb models non decreasing function number features selected filter ufss 
sbs easily modified perform binary search linear search 
reduces considerably number evaluations stopping criterion wrapper ufss bounded log 
note assumptions preliminary experiments harmless satisfied domains hybrid ufss perform worse filter ufss 
fact assumptions unnecessary sophisticated search strategies aimed dealing non monotonic performance scores floating search strategies 
experimental evaluation section dedicated empirical evaluation filter hybrid ufss described unsupervised learning nb models 
introduce artificial real databases evaluation 
discuss experimental setting performance assessment 
report results obtained 
databases artificial databases evaluation created sampling bns 
bns constructed adding irrelevant features respectively bn unsupervised learning reported induced part leukemia database see 
irrelevant features intend simulate noise 
added model independent rest features probability distributions randomly generated 
artificial bns consist nodes respectively having states 
bns sample cases remove cluster labels 
refer samples syn syn respectively 
third artificial database evaluation obtained processing waveform database 
database contains instances instance characterized continuous measurements 
classes 
measurements discretized bins equal width class labels removed 
refer resulting database waveform 
real database evaluation consists training data coil challenge 
database contains instances instance characterized features having states 
classes 
class labels removed evaluation 
resulting database referred coil forthcoming 
real databases evaluation obtained processing leukemia database 
database consists cases leukemia patients case characterized expression levels genes 
gene expression levels discretized states information theory madsen method 
resulting database split auxiliary databases containing data patients suffering acute leukemia containing data patients suffering acute leukemia aml 
databases transposed genes cases measurements corresponding patients features 
databases simply denoted aml respectively forthcoming discussion 
pe 
report sensible clusters aml databases 
experimental setting performance assessment experimental setting follows 
number randomly created irrelevant features random fill significance level distribution free hypothesis testing respectively 
second step hybrid ufss performs binary search sbs stopping criterion 
unsupervised learning nb models carried running em algorithm convergence criterion 
number clusters set syn syn waveform aml databases coil database 
feature subset considered filter hybrid ufss 
assess performance measuring amount information clusters carried amount information lost ignoring features implement performance score steps 
obtain nb model unsupervised learning 
second incorporate features model 
third compute report log likelihood log resulting model 
note way assessing performance allows compare feature subsets different sizes 
features added model dependent cluster random variable conditionally independent rest features probability distributions new nodes estimated re running maximization step em algorithm 
fractional completion entries expectation step em algorithm features added model 
additional evidence propagation required 
note add features model evaluation purposes exclusively 
incorporated final model real application 
results experiment aims evaluating rs yi means rank features relevance unsupervised learning nb models 
purpose run filter ufss database evaluation features selected matter significantly relevant 
report results fig 

graph vertical axis plots relevance performance feature subsets considered filter score score score syn relevance mi relevance mp performance mi performance mp features waveform relevance mi relevance mp performance mi performance mp features relevance mi relevance mp performance mi performance mp features unsupervised feature subset selection score score score syn relevance mi relevance mp performance mi performance mp features coil relevance mi relevance mp performance mi performance mp features aml relevance mi relevance mp performance mi performance mp features fig 

feature relevance rankings feature subset performance function thereof ufss horizontal axis denotes fraction original features subsets 
relevance feature subset computed summation rs yi features subset 
performance feature subset assessed indicated section 
graphs fig 
legends relevance mi relevance mp denote ds yi yj computed mi yi yj mp yi yj respectively 
likewise legends performance mi performance mp indicate performance scores correspond feature subsets examined filter ufss ds yi yj computed mi yi yj mp yi yj respectively 
sake visualization relevance performance scores scaled 
madsen table 
relevant feature subsets performance original filter ufss hybrid ufss database ds yi yj log log log syn mi yi yj mp yi yj syn mi yi yj mp yi yj waveform mi yi yj mp yi yj coil mi yi yj mp yi yj mi yi yj mp yi yj aml mi yi yj mp yi yj graphs fig 
confirm instances rs yi mi yi yj mp yi yj ds yi yj computation able induce effective decreasing relevance rankings features databases evaluation 
lower value rs yi relevant feature unsupervised learning nb models 
words lower value rs yi increase performance feature included unsupervised learning nb models 
mentioned order features selected filter ufss may vary depending mi yi yj mp yi yj ds yi yj 
instances rs yi agree features added steps ones search 
matter fact instances rank truly irrelevant features syn syn databases relevant features 
applies truly irrelevant features waveform database 
note fig 
truly irrelevant features score relevance values close 
features coil database indicating irrelevant 
hand features aml databases equally relevant 
sense patients databases suffer type acute leukemia 
second experiment evaluates filter ufss stopping criterion introduced section 
table summarizes number features declared relevant performance unsupervised learning nb models indicated section 
appreciated table filter ufss performs considerable number features deemed irrelevant syn syn waveform coil databases degrading significantly descriptive accuracy final models 
contributes improve comprehensibility final models databases 
note waveform database truly relevant features marked irrelevant unsupervised feature subset selection mi yi yj feature 
known features considerably relevant rest truly relevant features 
third experiment evaluates hybrid ufss 
clear fig 
table filter ufss performs conservatively specially aml databases 
table reports results hybrid ufss 
simplification final models obtained databases evaluation huge performance degradation kept control 
note graphs fig 
support observation section non decreasing performance unsupervised learning nb models respect fraction features selected filter ufss 
discussion feature subset selection central problem data analysis evidenced large amount literature dedicated 
vast majority research carried supervised learning paying little attention unsupervised learning 
contribute novel research feature subset selection unsupervised learning called unsupervised feature subset selection ufss 
approach unsupervised learning model perspective 
motivation performing ufss fold gain knowledge features informative relevant ones uninformative irrelevant unsupervised learning improve interpretability induced model degrading descriptive accuracy discarding irrelevant features 
propose evaluate novel ufss techniques 
proposal takes filter approach observation features independent rest features deemed irrelevant 
observation 
fact filter ufss inspired authors study filter ufss continuous data 
main difference techniques proposal insensitive number truly irrelevant features method sensitive 
matter fact useless large number irrelevant features exist 
second contribution ufss hybrid filter wrapper approach aimed alleviating conservative behavior filter ufss proposed 
evaluate filter hybrid ufss unsupervised learning naive bayes models artificial real data 
mentioned techniques readily applied unsupervised learning unrestricted bayesian networks bns tailored particular class probabilistic graphical models 
results obtained evaluation encouraging learnt models gain interpretability keeping descriptive accuracy 
unfortunately fair comparative study proposals existing ufss techniques difficult works focus prediction description coupled particular unsupervised learning algorithms data types hard fit approach unsupervised learning madsen deals 
adapting ufss techniques unsupervised learning bns may line research 
topic investigation may combining hybrid ufss floating search strategies order deal properly non monotonic performance scores 

selection informative genes gene expression diagnosis nonparametric approach 
proceedings international symposium medical data analysis 
breiman friedman olshen stone classification regression trees 
wadsworth international group 
cheeseman stutz bayesian classification autoclass theory results 
advances knowledge discovery data mining 
devaney ram efficient feature selection conceptual clustering 
proceedings fourteenth international conference machine learning 
dy brodley feature subset selection order identification unsupervised learning 
proceedings seventeenth international conference machine learning 
fisher knowledge acquisition incremental conceptual clustering 
machine learning 
golub slonim tamayo mesirov loh downing bloomfield lander molecular classification cancer class discovery class prediction gene expression monitoring 
science 
jain dubes algorithms clustering data 
prentice hall 
john kohavi pfleger irrelevant features subset selection problem 
proceedings eleventh international conference machine learning 
liu motoda feature selection knowledge discovery data mining 
kluwer academic publishers 
mitra murthy pal unsupervised feature selection feature similarity 
ieee transactions pattern analysis machine intelligence 
pe lozano larra aga dimensionality reduction unsupervised learning conditional gaussian networks 
ieee transactions pattern analysis machine intelligence 
pe lozano larra aga learning recursive bayesian multinets data clustering means constructive induction 
machine learning 
pe lozano larra aga unsupervised learning bayesian networks estimation distribution algorithms application gene expression data clustering 
submitted 
kittler floating search methods feature selection 
pattern recognition letters 
van der van someren 
eds coil challenge insurance case 
sentient machine research 
dependency feature selection clustering symbolic data 
intelligent data analysis 
