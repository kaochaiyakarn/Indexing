parallelizing boosting bagging yu skillicorn department computing information science queen university kingston canada cs ca february external technical report issn department computing information science queen university kingston ontario canada document prepared february copyright yu skillicorn bagging boosting general techniques building predictors small samples dataset 
show boosting parallelized performance results parallelized bagging boosting oc decision trees standard datasets 
main results sample sizes limit achievable accuracy regardless computational time spent parallel boosting accurate parallel bagging unexpectedly parallel boosting cheaper parallel bagging oc 
parallelizing boosting arcing yu skillicorn cs ca bagging boosting general techniques building predictors small samples dataset 
show boosting parallelized performance results parallelized bagging boosting oc decision trees standard datasets 
main results sample sizes limit achievable accuracy regardless computational time spent parallel boosting accurate parallel bagging unexpectedly parallel boosting cheaper parallel bagging oc 
common data mining scenarios construction predictors 
dataset mining typically consists large number objects attributes labelled class label indicating kind object goal build predictor new object attributes predict class label 
example credit card organization examples transactions discover fact ones fraudulent 
wishes build predictor predict new transactions ones fraudulent 
dataset known class labels called training set 
common reserve part data known class labels simulated new data correct class labels known 
possible test predictors comparing predictions known correct answers data construct predictor 
data called test set 
techniques building predictors known 
deciding properties interest accuracy usually measured error rate test set cost construction time takes build predictor training set cost deployment time takes generate predictions new data objects 
accuracy typically depends problem domain broad brush qualitative comparisons data mining techniques possible 
example decision trees prediction achieve accuracies high percents 
training set attributes producing decision tree cost construction ranges dn dn logn depending precise kind decision tree built 
cost deployment 
approach predictor generation small subsets training data build predictors subset deploy predictor combines predictions individual predictors voting regression 
advantage ensemble approaches combined prediction tends smaller variance single monolithic predictor trained dataset 
techniques small subsets shown greatest known accuracies problems 
subsets chosen independently example uniformly randomly replacement approach known bagging 
observed bagging naturally parallelized 
complex ensemble technique called variously boosting arcing uses information di cult object classify select objects succeeding samples 
intuitively idea objects hard classify samples new predictors expend ort nding tight boundaries classes 
techniques improve accuracy resulting prediction ensemble 
widely believed inherently sequential dependence selection new sample behavior previous predictors 
training datasets extremely large 
situations predictors generated updated short time scales natural consider parallelism predictor generation process 
parallel implementations bagging straightforward boosting complex sequential dependencies involved 
performance results datasets shared memory parallel computer 
results shed light appropriate tradeo parallelism sample size achieve greatest prediction accuracy 
section introduces ensemble approaches data mining predictors 
section describes sequential algorithms bagging boosting 
section describes experimental setting 
section describes parallelization strategies bagging straightforward boosting novel 
section presents experimental results interpretation 
section describes contribution 
predictors suppose training set form ji ng wherex attributes example set andy corresponding value class label example 
goal prediction compute predictor new example generates predicted value class label 
accuracy predictor test set form ji mg accuracy ig usually expressed percentage 
ensemble techniques data mining underlying prediction approach learn predictors subsets training set predictions combining predictions component predictors regression plurality voting 
select sample bag randomly replacement learn predictor sequential bagging general ensemble approach predictor computation rst select set subsets training set predictor ti learned subsets 
ensemble predictor predictor result computed argmax tk jg error predictor divided components error error bias variance rst term minimum error bayes classi er second term systematic error prediction technique third term error resulting nite sample build predictor 
bagging improves accuracy decreasing magnitude variance term boosting decreases bias variance 
sequential bagging boosting arcing bagging bagging selecting subsets randomly replacement 
example may sample generate predictor 
basic sequential algorithm 
resulting ensemble predictor plurality vote number predictors generated typically assumed input parameter 
remarks literature suggest predictors needed achieve signi cant accuracy improvements 
results suggest larger numbers predictors continue improve accuracy rate improvement slows 
boosting arcing boosting arcing tries improve quality predictors learned sample selecting examples hard classify frequency training training sett xj yj jj mg initialized select selection probabilities learn compute error js xj yji break xj yj factor factor sequential boosting set selecting points easy classify frequently 
intuition computational ort learn aspects data di cult learn 
decision examples hard incrementally ability previous predictors classify correctly 
rst predictor assumes examples equally hard 
rst predictor classify examples training set examples misclassi es may classi ed di cult 
examples disproportionately selected inclusion second sample 
selection third sample classi cation achieved rst second predictors 
process continues examples persistently misclassi ed increasingly included new samples 
variants idea described literature varying way probabilities selection varied response earlier classi cations resulting predictors outputs weighted 
predictor construction phase adaboost algorithm shown 
holds weights associated example training set modelling probability distribution sample selection global error estimate exceeds algorithm continue directly occurs distribution may reinitialized algorithm continued restarted 
variable models accuracy current predictor determines quickly correctly classi ed examples weights reduced 
combined predictor argmax log js words vote weighted accuracies individual predictors training set 
variants suggested 
arc fs removes easy examples consideration quickly adaboost setting selection probability zero soon classi ed correctly predictor 
uses weighted voting combined predictor 
arc reduces selection probabilities function involving fourth powers name number misclassi cations far predictors 
uses unweighted voting deliberate attempt show power boosting techniques comes sample selection way handle voting 
subtleties accuracy properties bagging boosting 
results large empirical study 
boosting arcing ensemble techniques implied sequential dependency selection process depends classi cation behavior 
shall show possible ectively parallelize ensemble techniques kind 
experimental setting ensemble techniques may weak learner underlying data mining technique 
experiments oblique decision tree predictor oc 
decision trees usually constrained test single attribute internal node 
oc allows linear combination attributes tested internal nodes 
geometrically decision tree partitions attribute space regions delimited axis parallel hyperplanes oc may oblique hyperplanes 
datasets actual boundaries classes modelled axis parallel hyperplanes oc expected achieve accuracy smaller decision trees 
cost improved prediction building predictors expensive ordinary decision trees 
time complexity tree construction dn logn training set 
real datasets require time shall see 
sun enterprise computer ultrasparc ii mhz processors test platform 
order reduce ects processes test programs processors 
elapsed times reported experiments averages executions code 
bulk synchronous parallel library bsplib parallel programming environment 
high performance library typically faster mpi wellsuited problems large scale structures consists alternating phases computation communication 
particular bsplib carefully optimized implement total exchange ectively extensively implementations 
datasets experiments chosen large partitioning data multiple processors allow fairly large subsets 
chosen quite di cult accuracies reported direct sequential mining left room improvement 
datasets described 
datasets available university california irvine archive kdd ics uci edu 
letters dataset unusual number classes large classi cation quite challenging 
name examples training set test set attributes classes size size letters numeric cover type numeric binary datasets parallelization strategies parallelizing bagging relatively straightforward 
training set partitioned equally available processors 
processor executes sequential algorithm local data su cient predictors constructed 
general idea partition training set randomly ensure predictors generated processor contain unnecessary bias 
cost single round parallel bagging expressed form single round cost sample oc size samples sample cost selecting sample depends size local partition size sample selected oc cost generating oc decision tree sample 
parallelization boosting complex 
partition training set available processors 
execute number rounds 
round processor builds predictor sample local data 
predictors exchanged processors processor holds copy predictors built current round 
reweighting examples preparation round done predictors processor example considered easy correctly classi ed number predictors current round 
number called threshold 
words example reweighted sequential algorithm correct prediction reinterpreted mean correct respect number predictors 
cost single round parallel boosting expressed form single round cost sample oc tpg vote pp size samples sample cost selecting sample oc cost generating oc decision tree sample tpg cost total exchange trees processors bsp network permeability architecture parameter vote cost voting local examples partition trees current round 
crucial property parallelized algorithm samples processor round information predictors previous round 
quality processor information hard versus easy examples training sett xj yj jj mg subsets tp np forall select build predictors exchange processors compute errors jdi misclassi ed threshold processors xj yj break xj yj factor factor parallelized boosting informed results learned processors predictor 
algorithm 
resulting global predictor argmax pk log parameters chosen parallel ensemble technique number processors size samples appropriate threshold boosting 
choices obvious bagging 
section report results set experiments explore tradeo parameters 
experimental results sample size limits accuracy regardless computational ort 
curves accuracy versus number iterations outer loop algorithm bagging boosting shape rising quickly attening asymptotic accuracy value 
asymptotes increase increasing sample size 
best achievable accuracy depends size sample chosen total computational ort expended 
shows bagging ect di erent sample sizes accuracy small number iterations shows ect number iterations increases 
figures show ect boosting letters covtype datasets test set 
iterations increasing sample size increases accuracy parallel bagging processors letters dataset 
test set accuracy 
iterations accuracy limited sample size regardless computation 
moderate sample sizes outperform sequential oc algorithm shown dashed line parallel bagging processors letters dataset 
test set accuracy 
iterations increasing sample size increases accuracy parallel boosting processors letters dataset 
dashed line accuracy sequential oc algorithm 
test set accuracy 
iterations increasing sample size increases accuracy parallel boosting processors covtype dataset 
dashed line accuracy sequential algorithm 
proc 
procs 
procs 
procs 
acc 
acc 
acc 
acc 
time time time time number examples seen accuracy total execution time approximately regardless computation arranged parallel bagging letters dataset sample size 
expressed product number rounds number processors 
respectively 
letters dataset boosting implementations fairly easily outperform accuracy pure sequential oc implementation 
covtype dataset di cult large sample sizes required 
important point sequential implementations longer execution times ensemble implementations letters dataset covtype dataset 
corollary dependence accuracy sample size suggest upper bound degree parallelism worth ensemble approaches 
smallest sample size achieves better accuracy sequential algorithm iss reason number processors larger value satis es fortunately nis extremely large data mining applications number processors available commonly available parallel computers relatively small 
algorithms described practical interest 
question results answer reasonable sample size allows accuracies better straightforward sequential algorithm best regarded absolute size xed fraction training dataset 
arguments suggest 
redundancy training data usually data mining applications counter intuitive doubling size training dataset require selecting bigger samples 
second clear gures relative accuracy improvement obtained larger sample quite small percentage point time examples included sample 
little gained increasing sample sizes large samples 
bagging ect parallelism 
parallelizing bagging amounts rearranging computation sequential bagging algorithm expect speedups running time bag size time required rounds increases faster linearly function sample size parallel bagging processors letters dataset 
linear number processors sample size 
potential source performance improvement fact processor selecting samples partition entire dataset fewer memory accesses select 
data shows ect negligible number examples processed total time spent remains regardless processors 
course perfect linear speedup achieved 
processors produces better accuracy slim margin 
limits scalability algorithm partition allocated processor small select samples appropriate sizes 
ect sample size 
building oc decision trees potentially requires time superlinear number examples expect small samples provide signi cant performance enhancement 
letters dataset time required build trees worse linear quadratic 
shows required execution time function sample size 
ect sample size shown 
row table gives accuracy cost computing predictor having seen number examples samples di erent sizes 
unsurprisingly comments section accuracies greater larger sample sizes 
total time required increases quite substantially increasing sample size 
time increase due increased complexity building decision trees larger samples 
words equation sample term linear andm oc term worse linear inm accounts increasing time 
parallel bagging holds surprises 
parallel algorithm simply rearrangement seq par par par par acc 
acc 
acc 
acc 
acc 
time time time time time number examples seen accuracy improves increasing sample size total execution time increases parallel bagging processors letters dataset 
sequential bagging set processors 
accuracy computational cost remains regardless rearrangement 
boosting far obvious performance expect parallel boosting 
hand considerable overheads exchanging oc trees voting round suggests speedups may poor 
hand processor acquiring information learned processors compact way led superlinear speedups data mining applications 
ect threshold 
threshold determines decision trees agree example classi ed easy 
figures show choosing threshold value best letters dataset figures show choosing threshold value best covtype dataset 
cases best choice threshold independent sample size 
cases accuracy function threshold strongly concave downwards small large values threshold perform worse moderate values 
appears best threshold constant dataset fraction number available processors unable explore detail 
ect parallelism 
implementations seen number examples clear letters dataset covtype dataset increasing parallelism results greater accuracies 
suggests sharing information gleaned processors voting improve ability algorithm learn 
clear tables improved accuracy performance cost 
total amount ofwork required increases test set accuracy 
iterations choosing threshold marginally best letters dataset parallel boosting processors sample size 
test set accuracy 
iterations choosing threshold best letters dataset parallel boosting processors sample size 
test set accuracy 
iterations choosing threshold best covtype dataset parallel boosting processors sample size 
test set accuracy 
iterations choosing threshold best covtype dataset parallel boosting processors sample size 
proc 
procs 
procs 
procs 
acc 
acc 
acc 
acc 
time time time time number examples seen accuracy increases parallelism total execution time parallel boosting letters dataset sample size 
proc 
procs 
procs 
procs 
acc 
acc 
acc 
acc 
time time time time number examples seen accuracy increases slightly parallelism total execution time parallel boosting covtype dataset sample size 
running time bag size time required rounds increases faster linearly function sample size parallel boosting processors covtype dataset 
slowly number processors increases 
words speedup sublinear 
re ects increased communication cost processors 
notice cost voting round vote term cost equation function parallelize 
practice cost tree construction dominates communication voting update distribution modest numbers processors 
example early round boosting processors letters dataset sampling took time tree generation voting updating time 
approach expected scale processors provided dataset large partitions allow reasonable samples selected 
ect sample size 
observed time required build oc tree increases faster linearly number examples letters dataset 
shows relationship quite similar covtype dataset 
expect see performance penalties larger sample sizes 
ect sample size shown figures 
row gives accuracy cost computing predictor having seen number examples samples di erent sizes 
accuracies greater larger sample sizes covtype dataset total time required increases substantially 
costs re ect cost building decision trees larger samples 
words sample cost linear arguments oc cost grows faster linearly inm communication term independent ofm tree sizes remain constant sample sizes vary vote cost constant 
parallel boosting achieve improved accuracies compared sequential boosting seq par par par par acc 
acc 
acc 
acc 
acc 
time time time time time number examples seen accuracy improves increasing sample size total execution time increases parallel boosting processors letters dataset 
seq par par par par acc 
acc 
acc 
acc 
acc 
time time time time time number examples seen accuracy improves increasing sample size 
total execution time increases parallel boosting processors covtype dataset 
accuracy zero zero zero zero round zeroing selection probabilities quickly little ect accuracy best choice threshold boosting letters dataset sample size 
cause information sharing implicit predictors round select samples subsequent round 
computational cost improved accuracy exchanging predictors adds communication cost linear inp resulting voting step parallelized recall magnitude function ofn upper limit ectiveness parallelism 
adding arcing features ect zeroing selection probabilities quickly 
arcing ensemble algorithms reduce selection probability easy examples zero rapidly comparison boosting 
figures show strategy updating selection probabilities di erence threshold selection achieved accuracy 
comparing bagging boosting accuracy boosting bagging 
shows scenario boosting achieves greater accuracy bagging 
intuition better devote resources hard examples expense easy examples 
cost bagging boosting 
surprisingly shows scenario boosting computationally cheaper bagging 
costs round compared detail clear time compute oc decision trees decreases rounds boosting decrease rounds bagging 
samples rounds boosting repetitive accuracy zero zero zero zero round zeroing selection probabilities quickly little ect accuracy best choice threshold boosting letters dataset sample size 
sample size acc 
acc 
acc 
acc 
bagging boosting bagging boosting bagging boosting bagging boosting parallel boosting accurate parallel bagging scenario 
sample size time time time time bagging boosting bagging boosting bagging boosting bagging boosting parallel boosting cheaper execute parallel bagging scenario 
earlier rounds pool examples select ectively shrinking time 
possible explanation observed results oc algorithm achieve quality tree homogeneous samples 
shows case 
solid line shows computation time oc random samples increasing size letters dataset 
dashed line shows computation time samples matching size obtained replicating random sample examples appropriate number times 
clear oc decision tree built quickly homogeneous sample 
actual samples rounds parallel boosting nearly homogeneous data gure ect smaller su ces outperform parallel bagging 
bauer kohavi postulate similar ect typical decision tree builder mc 
main contributions 
shown achievable accuracy ensemble techniques bounded choice sample size 
choosing small sample size reduces time compute predictor disproportionately underlying prediction technique complexity worse linear sample size 
shown balanced need achieve reasonable accuracy limits ne grained ensemble approaches 

wehave shown boosting usefully parallelized despite apparent sequential structure 

shown parallel boosting achieves greater accuracy parallel bagging comparable scenarios 
intuition boosting useful spend resources disproportionately hard examples dataset 
oc running time training set size time required build oc decision tree smaller multiple copies examples randomly chosen dataset size 

shown oc parallel boosting outperforms parallel bagging comparable scenarios 
result unexpected due fact oc tree construction faster homogeneous datasets 
shown parallel bagging behaves expected accuracy total execution time una ected way computing predictor arranged space time 
reason parallelizing bagging reduce elapsed time generating global predictor 
performance tradeo parallel boosting subtle 
parallelized boosting learns better predictors seeing number examples partial information learned processor ectively shared early learning process 
hand parallelized boosting require total computation learn number examples 
extra cost arises communication overheads parallel algorithm extra voting required grows fairly slowly number processors 
discovered amount agreement predictors learned di erent processors threshold required classify example easy remarkably small 
expected greatest accuracy achieved calling examples easy predictors agreed fact su ces processors agree 
indications threshold independent total number processors 
parallel boosting strategy choice parallel ensemble generation predictors 
large sample sizes produces accuracies greater corresponding sequential algorithm elapsed time computational time required 
bauer kohavi 
empirical comparison voting classi cation algorithms bagging boosting variants 
machine learning august 
bishop 
neural networks pattern recognition 
oxford university press 
breiman 
bagging predictors 
machine learning 
breiman 
bias variance arcing classi ers 
technical report department statistics university california berkeley 
breiman 
arcing classi ers 
annals statistics 
breiman 
pasting bites prediction large data sets line 
machine learning 
leo friedman olshen stone 
classi cation regression trees 
wadsworth international group 
freund schapire 
experiments new boosting algorithm 
proceedings th international conference machine learning pages 
jonathan hill bill mccoll dan stefanescu mark kevin lang satish rao torsten suel rob 
bsplib bsp programming library 
parallel computing december 
sreerama murthy simon kasif steven salzberg 
system induction oblique decision trees 
journal arti cial intelligence research 
quinlan 
programs machine learning 
morgan kaufmann 
rogers skillicorn 
bsp cost model optimize parallel neural network training 
generation computer systems 
skillicorn 
parallel predictor generation 
proceedings workshop large scale parallel kdd systems kdd number lecture notes arti cial intelligence pages 
springer verlag 
skillicorn hill mccoll 
questions answers bsp 
scienti programming 
wang skillicorn 
parallel inductive logic data mining 
workshop distributed parallel knowledge discovery kdd boston appear 
acm press 

