duplicate data elimination san file system bo hong univ california santa cruz cs ucsc edu darrell long univ california santa cruz darrell cs ucsc edu duplicate data elimination dde method identifying coalescing identical data blocks storage tank san file system 
line file systems pose unique set performance implementation challenges feature 
existing techniques improve storage network utilization satisfy constraints 
design employs combination content hashing copy write lazy updates achieve functional performance goals 
dde executes primarily background process 
design builds storage tank function ease implementation 
include analysis selected real world data sets aimed demonstrating space saving potential coalescing duplicate data 
results show dde reduce storage consumption application environments 
analysis explores additional features impact varying file block size contribution file duplication net savings 
duplicate data occupy substantial portion storage system 
duplication intentional files copied safe keeping historical records 
just duplicate data appear independent channels individuals save email attachments download files web 
intuitive addressing unrecognized redundancy result storage resources efficiently 
research goal reduce amount duplicated storage tank technology available today ibm total storage san file system 
research underlying storage tank technology may part ibm san file system product 
ibm almaden research center almaden ibm com ibm almaden research center ibm com data line file systems significantly impacting system performance 
performance requirement differentiates approach call duplicate data elimination dde backup archival storage systems :10.1.1.18.8085:10.1.1.27.2114
minimize performance impact dde executes primarily background process operates lazy best effort fashion possible 
data written file system usual time background threads find duplicates coalesce save storage 
dde transparent users 
flexible enabled disabled existing file system disrupting operation flexible parts file system select directories particular file types 
duplicate data elimination dde designed ibm storage tank heterogeneous scalable san file system 
storage tank file system clients coordinate actions meta data servers access storage devices directly involving servers data path 
dde uses key techniques address design goals content hashing copy write cow lazy update 
dde detects duplicate data logical block level comparing hashes block contents guarantees consistency block contents hashes copy write 
data coalesced changing corresponding file block allocation maps 
cow lazy update allow update file block allocation maps revoking file data locks 
techniques minimize dde performance impact 
shows example coalescing duplicate data blocks line file system 
coalescing files consume blocks 
contain common piece data blocks size 
clients unaware duplication write files 
server detects common data coalesces identical blocks 
coalescing consume blocks total sharing blocks 
blocks saved resulting storage reduction 
coalesce coalesce tank dir dir tank dir dir disk disk 
example coalescing duplicate data blocks 
background ibm storage tank multi platform scalable file system works storage area networks sans 
storage tank data stored devices directly accessed san meta data managed separately specialized storage tank meta data servers 
storage tank clients designed direct metadata operations storage tank servers direct data operations storage devices 
storage tank servers involved data path 
current version storage tank works ordinary block addressable storage devices disk drives raid systems 
basic operation unit storage tank block 
storage devices required intelligence ability read write blocks volumes 
storage tank file data managed block units 
size file block typically multiple device block size 
storage tank exposes new abstractions called file sets storage pools arenas 
addition traditional abstractions file systems files directories volumes 
file set subtree global namespace 
groups set storage tank files directories purpose load balancing management 
storage pool collection volumes 
provides logical grouping volumes allocation space file sets 
file set cross multiple storage pools 
arena provides mapping file set storage pool 
arena file set files particular storage pool 
arena abstraction strictly internal storage tank server important element duplicate data elimination 
arena storage tank track free space owned file set storage pool specifies logical physical mapping space file set volumes storage pool 
storage tank protocol provides rich locking scheme enables file sharing storage tank clients necessary allows clients exclusive ac cesses files 
storage tank server locks clients lock granularity file 
file lock modes storage tank exclusive allows single client cache data metadata read modify shared read sr allows clients cache data metadata read operations shared write sw mode clients cache data cache metadata read mode 
storage tank protocol provides copy write capability support file system snapshots 
server mark blocks read enforce copy write 
storage tank technology available today ibm total storage san file system 
research underlying storage tank technology may part ibm san file system product 
related data duplication ubiquitous 
different techniques proposed identify commonality data exploit knowledge reducing storage network resource consumption due storing transferring duplicate data 
directly inspired venti :10.1.1.18.8085
venti network storage system intended archival data 
venti unique sha hash block acts block identifier place block address read write operations 
venti implements write policy prohibits data deleted stored 
write policy practical part venti stores copy unique data block 
line file systems performance essential data dynamic ready change 
radically different requirements archival backup systems data immutable performance concern 
design duplicate data detected block level background 
data addressed usual block stored data accesses extra hash block index searching overheads venti 
turn determines data duplication detection coalescing effect effort done clients written data blocks storage devices 
server maintains mapping function block hashes blocks 
weaker variant write policy copy write cow guarantee consistency 
unreferenced blocks due deletion cow reclaimed 
single instance store sis detects duplicate data effect fashion file level 
technique optimized microsoft windows remote install servers store different installation images 
scenario knowledge file duplication priori files modified 
general line file systems granularity file level data duplication detection may coarse modification file cause loss benefit storage reduction 
lbfs pastiche detect data duplication granularity variable sized chunks boundary regions called anchors identified techniques rabin fingerprints :10.1.1.106.2691:10.1.1.10.8444:10.1.1.101.3227
technique suitable backup systems low bandwidth network environments reduction storage network transmission important performance 
delta compression technique effectively reduce duplicate data requirements storage network bandwidth :10.1.1.27.2114:10.1.1.31.5701:10.1.1.32.323
base version data object exists subsequent versions represented changes deltas save storage network transmission 
delta compression general requires prior knowledge data object versioning 
explore common data multiple files change base file may cause recalculating deltas files 
delta encoding resemblance detection similar files identified pairs data similarity detection techniques having specific prior knowledge :10.1.1.24.779:10.1.1.106.2691:10.1.1.150.1425
file systems provide line compression capability 
effectively improve storage efficiency technique significant run time compression decompression overheads 
line compression explores intra file compressibility take advantage common data files 
techniques naming indexing data objects content hashes systems 
sfs read file system blocks identified sha hashes block hashes hashed recursively build complex structures 
stanford digital library repository uses cyclic redundancy check crc values data objects unique handles :10.1.1.42.4367
content derived names take similar approach address issue naming managing reusable software components 
design duplicate data elimination design goal transparently reduce duplicate data storage tank possible penalizing system performance significantly 
finding data duplication spot delay duplication detection identify eliminate duplicate data server loads low 
way minimize performance impact duplicate data elimination dde 
design techniques content hashing copy cow lazy update 
duplicate data blocks detected storage tank server 
client uses collision resistant hash function digest block contents writes storage devices returns hashes server 
unique hash called fingerprint block 
server compares block fingerprints coalesces blocks fingerprint content changing corresponding file block allocation maps 
server guarantees consistency block contents fingerprints directing clients perform copy write 
server maintains count block reclamation unreferenced blocks 
techniques allow server update file block allocation maps revoking outstanding data locks 
shows basic idea duplicate data block elimination live file system 
client holds exclusive lock file shared read sr lock file lock modes described section 
file data stored blocks respectively 
duplicate block coalescing server client share view files file block allocation maps 
server finds duplicate data changes block allocation map file block updating client 
client stale view file read correct data block reclaimed immediately 
client modifies block file writes new content block keeps content block intact 
content fingerprint block consistent file right block 
duplicate data detection straightforward trusted way duplicate data detection comparison 
unfortunately expensive 
alternative method digest data contents hashing shorter fingerprints detect duplicate data comparing fingerprints 
long probability hash collisions vanishingly small confident sets data content identical fingerprints 
design duplicate data detected block level maintaining fingerprint block imposes large amount bookkeeping information system 
storage tank block level storage devices blocks basic operation units 
detection avoids unnecessary os required approaches files variable sized chunks client may read disk blocks recalculate fingerprints due data boundary mis alignments :10.1.1.10.8444:10.1.1.101.3227
data duplication detection blocks finer granularity higher possibility storage reduction techniques files 
approaches variable sized chunks require bookkeeping information block approaches tend limit average chunk size obtain reasonable chance detecting duplicate data :10.1.1.10.8444:10.1.1.101.3227
chunk approaches need totally different file block allocation map format existing server view client view rose rose block duplicate data coalescing file file file file client lock file sr lock file duplicate data coalescing file file file file lock revoke postpone free space reclamation client holds stale metadata file read wrong data client modifies file returns dirty metadata content consistent fingerprint rose green file file rose rose rose rose rose rose rose rose rose rose block block block block block block file file client writes new content file new location 
example coalescing duplicate data blocks live file system 
storage tank implementation difficult employ 
dde uses sha hash function fingerprint block data contents 
sha way secure hash function bit output 
large system contains data bytes kilobyte blocks roughly blocks probability hash collisions sha hash function orders magnitude lower probability undetectable disk error 
date known collisions hash function 
confident blocks identical sha hashes 
addition system perform comparisons coalescing blocks cross check 
storage tank data meta data management separated storage tank servers involved data path normal operations 
disks little intelligence detect duplicate data 
smarter disks extensive inter disk communications disk know local data fingerprints reduce chances detecting duplication 
design client calculates fingerprints blocks writes storage devices returns server 
software implementations sha quite efficient hashing performance bottleneck 
storage tank servers global view system appropriate data duplication detection 
consistency data content fingerprints hash data content block fingerprint attribute block 
fingerprint eventually stored server block directly accessed modified clients consistency server hash database storage device modification modification new hash value returned block update place block copy write block 
maintaining consistency fingerprint block content update place copy write 
fingerprint data content block problem 
storage tank client modify block approaches update place copy write 
update place storage tank client directly modify block block writable writes new data block 
results inconsistency server side block fingerprint block content client returns new fingerprint server shown 
fingerprint block server keeps inconsistent block content client returns latest fingerprint 
period inconsistency data duplication detection related block gives false results 
detect potential inconsistency checking data locks file block belongs 
granularity file data locks storage tank file server trust block fingerprints file exclusive data lock 
avoid erroneous duplication detection coalescing simply delay dde files exclusive shared write locks 
feasible workloads small fraction files active concurrently 
approach save storage important environments databases applications hold locks files 
approach revoke data locks files force clients return latest block fingerprints 
causes technical problems lock checking lock revocation 
check file locks block maintain reverse pointer file belongs bookkeeping information block larger 
guarantee consistency fingerprints block contents block coalescing operation revoke file locks necessary severely penalize system performance 
eliminating duplicate data blocks update place scenario inefficient best impossible 
copy write basic idea eliminate duplicate data blocks comparing fingerprints 
collision resistant hash function sufficiently large output sha fingerprints considered distinct different data 
fingerprint serve unique virtual address data content block 
mapping function virtual address physical block address implicitly provided block address 
aim mapping function nearly virtual address mapped physical address 
update place violates basic concept content addressed storage making mapping function inconsistent 
conceptually content block changed new content mapped new block original 
consequently client write modified data new blocks implies policy venti :10.1.1.18.8085
write keeps histories data unnecessary expensive line file systems 
weaker variant write copy write 
technique guarantee consistency mapping function long original blocks reclaimed shown 
fingerprint block server keeps consistent block content block reclaimed 
apparently copy write noticeable overhead normal write operations 
block modification requires free block allocation modified content needs written new block 
cost significant thought 
applications emacs microsoft word write modified file new place case extra cost cow 
server new blocks client acquires exclusive shared write lock 
promising approach alleviate extra allocation overhead cow clients maintain private storage pool behalf server allocate locally 
extra cost cow 
lazy lock revocation free space reclamation server coalesces duplicate data blocks reduces storage consumption updating file block allocation maps point copy data reclaim rest 
file block allocation map updates server check client holds data locks files 
block allocation maps held server clients inconsistent illustrated 
postpone reclamation dereferenced blocks 
clients holding stale file block allocation maps read correct data blocks 
particular time midnight file system running low free space server revokes data locks held clients frees dereferenced blocks 
process duplicate data elimination duplicate data elimination done coordination clients servers 
simply speaking clients perform copy write operations calculate return block sha hashes servers 
servers log clients activities identify coalesce duplicate data blocks background 
users unaware operations 
impact client behaviors addition normal behaviors client calculates sha fingerprints data blocks writes returns fingerprints server 
copy write section client write modified data blocks back original disk blocks modified data written newly allocated blocks 
long client holds file data lock modifications logical block written newly allocated disk block 
update server client sends latest block fingerprints block logical offsets file original physical locations modified data blocks 
data structures server discuss necessary data structure supports server facilitate duplicate data block detection elimination 
essentially count table fingerprint table secondary index maintain attributes associated blocks counts fingerprints shown arena block offset count block fingerprint block attributes secondary index bucket bucket index 
data structures storing retrieving block attributes 
block offset arena de log new fingerprint log file id block offset block offset file arena block fingerprint 

data structures logging clients activities 
dereference log new fingerprint log record clients activities shown 
scope duplicate data elimination important design decision 
larger scope higher degree data duplication providing benefit 
various reasons people may want share data working group department 
limit data duplication detection elimination file set essentially logical subset global namespace 
file set files stored different storage pools may belong different storage classes different characteristics access latency reliability availability 
sharing data storage classes result noticeable impacts quality storage service 
narrow scope dde arena provides mapping file set storage pool 
data structures discuss soon arena 
data equally important 
detecting coalescing temporary derived cached data beneficial 
storage tank provides policy storage management system easily configured store data reliable cheaper storage pools storing important data reliable storage pools 
dde arena take advantage flexibility 
arena allows logical physical mapping space file set storage pool allocate unallocated allocated reclaim free reuse unreferenced referenced remove add mark unused add remove 
block states unallocated allocated referenced unreferenced 
equivalent referencing block physical location logical offset arena 
convenience allocated physical block logical offset arena 
keep arena data structures optimal size bit integers represent logical block offsets arena 
arena contain physical blocks 
kb blocks arena manage tb storage large applications environments 
limitation capacity file set cross multiple storage pools consist multiple arenas 
count table block coalescing sharing physical block referenced multiple times different files file 
count necessary block arena 
viewpoint dde block states free block unallocated allocated block allocated unused contains valid data referenced block contains valid data hashed referenced unreferenced block allocated hashed file referencing freed reused 
illustrates states transitions 
accessing arena block allocation map know block allocated 
fortunately interested validity block fingerprints 
blocks states free allocated merged state invalid contain valid fingerprints 
count block indicates state invalid count referenced count unreferenced count 
initial state block invalid contains valid fingerprint client calculates 
count table keeps count block arena 
table organized linear array indexed bit logical block offset arena 
entry table bit integer indicating state corresponding block 
size table bytes gb 
block counts crucial data integrity update transactional 
block may contain valid data fingerprint associated 
note fingerprint block calculated written 
block written server turns feature fingerprint server 
utility running server ask clients read data blocks calculate fingerprints behalf server 
fingerprint table fingerprint table keeps unique fingerprints blocks arena 
fingerprint table associated unique physical block 
words table maintains mapping function fingerprints physical blocks 
detect coalesce duplicate data blocks merge new fingerprint log table 
fingerprint table organized linear array indexed bit logical block offset 
entry contains bit sha fingerprint 
fingerprint valid block count 
size table bytes gb fit memory 
fortunately disk block accesses sequential patterns due sequential block allocations file accesses 
organize secure fingerprint table linearly facilitate comparisons sequential block accesses 
disk blocks contain content consecutive blocks identical contents 
linear structure consecutive fingerprint comparisons efficient related entries memory 
conceptually count table fingerprint table describing block attributes merged table 
sizes potentially large block count accessed frequently fingerprint store separately optimize system memory usage 
secondary index fingerprint table linear fingerprint table favors sequential searching difficult look particular fingerprint table 
index table partial bits sha fingerprint facilitate random searching 
static hash index purpose 
hash buckets indexed bits sha fingerprint 
bucket contains bit block pointer 
size level index bytes mb fit memory 
entry bucket block contains bit arena logical block offset indicating block fingerprint associated bits sha fingerprint 
arena contains blocks average number hash entries bucket 
bucket block size kb average block utilization 
arena capacity tb multiple buckets share bucket block better storage memory utilization 
dereference log dereference log records arena logical offsets blocks deleted dereferenced due cow clients dereferenced due block coalescing server 
discuss third case greater details section 
log called semi free list blocks list freed longer 
entry log bit integer 
avoid storage leakage update log transactional 
new fingerprint log new fingerprint log records clients write activities 
entry log includes bit file id bit logical block offset file bit logical offset arena bit sha fingerprint 
appending new entries log non transactional performance purpose losing entries causes losing opportunities storage reduction 
responsibilities server server enforces client copy write behaviors marking copy file block allocation map message buffer read responds file data lock request client 
server immediately logs clients activities delete write operations 
design dde runs best effort fashion 
server lazily detects coalesces duplicate data blocks reclaims unused blocks 
maintains block counts fingerprints correspondingly 
logging activities server receives dirty file block allocation map client compares server 
block marked unused due copy write server checks referenced server side file block allocation maps 
referenced arena logical offset unused block appended dereference log possible due duplicate block coalescing lock revocation block currently referenced server side block allocation map logged dereferenced modifications corresponding file logical block 
unused block returned client logged dereferenced due block coalescing 
unused blocks logged 
written block server appends entry new fingerprint log including identifier file belongs logical block offset file logical block offset arena fingerprint 
stable log epoch current time 
log epochs 
time log epoch preprocessing periodically checkpoint dereference log new fingerprint log reasons 
data duplication detection elimination processes run best effort background fashion keep clients activities 
logging checkpointing activities allow server detect coalesce duplicate data blocks idle time 
checkpointing log epochs limit number activities server processes time 
second written blocks modified 
trying coalesce blocks beneficial 
try coalesce blocks stable epoch shown lifetimes long 
assume want merge new fingerprint log epoch fingerprint table 
random accesses fingerprint table expensive try reduce number fingerprint comparisons deleting entries new fingerprint log epoch 
find overwritten file logical blocks sorting log file id logical block offset file 
delete older entries log set block counts unreferenced 
set block counts entries log 
second scan dereference log epoch 
entry log decrease block count count set 
third compact new fingerprint log deleting entries dereference logs 
matched entries dereference log removed block counts set 
note fingerprint fingerprint table invalid block count reaches 
conceptually remove index entry secondary index 
update secondary index log preprocessing performance reasons 
postpone removal false indexes duplicate block coalescing process notices 
false index removal happens secondary index bucket block full 
merging fingerprint table detect coalesce duplicate data blocks merge compacted new fingerprint log fingerprint table 
shows processes duplication detection coalescing 
entry log check matching fingerprint fingerprint table 
insert new fingerprint table update sec index 
identical fingerprint fingerprint table check validity fingerprint 
block count fingerprint primary block fingerprint table contains valid data 
insert new fingerprint table update secondary index 
need delete false index secondary index previous matching index leads block containing invalid data 
block count fetch block allocation map file written block belongs check block referenced file 
means corresponding logical block file modified current fingerprint returned simply discard coalescing operation 
block referenced update file block allocation map referencing primary block fingerprint table checking revoking data locks file 
increase count primary block set count coalesced block 
block inserted fingerprint table adding new entry coalescing block marked read block allocation map file belongs 
free space reclamation free space reclamation process scans count table background 
logs addresses blocks counts sets counts 
particular time midnight file system running low free space revokes data locks free blocks 
case studies examined data sets studied degrees data duplication compressibility common compression techniques 
data sets summarized table 
data sets build test file servers storage tank development team 
servers distribute exchange data files hold archive primary copy important files 
server remote replica holds subset daily builds stored build subset test data test 
general oldest files deleted servers run low space 
files overwritten tend created modified purged months 
genome contains human genome sequence ucsc various bioinformatics applications 
genomic data encoded letters single letters letter combinations repeat thousands millions times fine granularities 
partial hash match secondary index 
full match fingerprint table 
ref 
count primary block 
match physical location corresponding logical block offset file 
entry compacted new fingerprint log epoch insert fingerprint table update secondary index insert fingerprint table update secondary index 
delete false index secondary index discard merging operation hash value matches partial bits 
match revoking file locks update file data descriptor referencing primary block fingerprint table increase corresponding block count set count dereferenced block 
merge new fingerprint log fingerprint table 
table 
data sets name description size gb number files file server development team file server development build team code build file server development test team testing genome human genome data local mirror installation cds ltc mirror different linux versions aggregation personal personal workstations workstations ltc mirror local ftp mirror ibm linux technology center ltc includes open source ibm software internal download 
things ftp site holds cd images iso different red hat linux installations starting rh rh 
data set aggregation personal workstations ibm almaden research center running windows 
systems development general purposes email working documents results systems analyzed separately 
data set collected size number files system 
calculated amount storage required eliminating data duplication granularity kb blocks 
compare dde common compression techniques collected compressed file sizes lzo kb data blocks 
lzo lempel ziv data compression library favors speed compression ratio 
empirically compression capability lzo similar techniques block size large 
addition calculated storage reduction achieved combining techniques dde lzo 
calculated percentage storage required eliminating file duplications 
table shows percentage storage required different data sets dde granularities kb blocks files lzo kb blocks combination dde lzo 
dde kb blocks requires fifth third original storage hold build data sets achieves times higher storage efficiency ratio amount logical data amount required physical storage lzo kb blocks 
data sets contain daily builds storage tank codes share lots codes versions data sets include small files average making lzo inefficient 
test hand average larger files mb best reduction achieved combination lzo compression dde table 
dde compression results 
storage required storage required storage required eliminating storage required combining dde name dde kb blocks file duplications lzo kb blocks lzo kb blocks build test genome ltc mirror personal workstations table 
dde compression results personal workstations 
storage required storage required storage required eliminating storage required combining dde system dde kb blocks file duplications lzo kb blocks lzo kb blocks kb blocks 
study data set greater detail section 
genomic data set encoded letters 
single letters letter combinations repeat thousands millions times quite fine granularities 
type data set find duplications granularities kilobytes dde improve storage efficiency significantly 
hand common compression techniques lzo suitable data set 
reduction dde partially due common file headers common special letter pattern fills gaps genomic sequence partially due duplicated files created analyzing applications 
lzo data set ltc mirror barely reduces storage consumption generally files linux installation cd images packaged compressed 
dde take advantage cross file duplications mainly different installation versions reduce storage consumption 
interesting data set actual installation different linux versions installation images 
plan look nearest 
studied storage requirements personal workstations different techniques individual systems shown table 
average storage required individual systems applying dde half saving due eliminating file duplications 
lzo compression provide better storage efficiency systems 
combining lzo dde kb blocks percentage storage required individual systems reduced average 
aggregating files machines potentially happens enterprise level backup applications aggregated storage requirement personal workstations dde drops significantly shown workstations table 
individual systems aggregated higher degree data duplication 
consequently dde result tremendous storage savings back systems potentially backup hundreds thousands personal workstations daily basis shows example application benefit data duplication elimination file system 
combining dde lzo kb blocks reduce storage requirement 
granularity duplicate data detection affects effectiveness dde 
table shows file level detection lose chance finding duplicate data kb blocks 
noticed dde lzo kb blocks generate better results dde solely 
general smaller block sizes favor dde finer granularities duplication elimination larger block sizes favor lzo efficient encoding 
fact dde lzo compression techniques orthogonal operated data unit granularity compression explores intra unit compressibility dde explores duplications 
detailed study shows results applying dde data set range file system block sizes 
results percentage data blocks unique 
block size varies bytes kb 
expected smallest block size works best enables detection shorter segments duplicate data 
interestingly dde effectiveness starts improve block size reaches kb 
reason file system wasting space larger blocks dde proportionally coalescing wasted space 
effect begins reduced number duplicate blocks due coarser block granularities 
note storage tank support blocks sizes smaller kb results included show savings missing 
additional space saving potential smaller blocks modest instance kb kb blocks data set 
usage pattern fifo style space management allow data simulate growing file system 
shows amount space saved dde file system starts empty grows eventually contain files data set 
files added order modified times 
kb blocks 
rightmost value chart matches space savings shown 
shown gradual improvement starting compression monotonically decreasing assert duplication data set independent time 
type curve generated files inserted random order sorted 
chart shown flat curve indicate data duplication highly correlated time strong temporal locality 
duplicate blocks new file match blocks added instance hours match blocks added month ago 
dde performs efficiently highly time correlated duplication duplicate blocks tend coalesced log epoch require fewer updates fingerprint table 
time uncorrelated duplication allows dde produce continually improving compression ratio data set grows 
data set consists kb blocks referenced 
remaining blocks unique 
shows cumulative distributions storage savings frequently occurring data blocks 
reveals unique blocks contributes total storage savings 
suggests small amount hash cache client side explore frequency data duplication save actual spot 
opportunity study recency data duplication believe storage required file system block size kb 
percentage storage required removing duplicate data 
storage required file system growth gb 
percentage unique blocks simulated growing file system 
careful design client side hash cache take advantage recency improve system performance 
furthermore spatial temporal localities duplicate data generation dominant factor dde performance server side 
build demonstrate applications substantially benefit dde 
engineers file servers exploit storage space jobs easier 
way creating hundreds views data isolated directory trees dedicated purpose 
pragmatic technology savvy users snap shot facility create views configuration management package symbolic links 
engineers employ robust familiar mechanisms available copying data applying small alterations leave results linger indefinitely 
environment file system data management application dde extends reach 
discussions duplicate data copies purpose reliability preservation performance 
typically duplicate copies stored different storage pools 
technique duplicate data elimination scoped arena data different storage pools cumulative storage saving frequent frequent duplicate blocks 
cumulative contribution total storage savings coalescing frequently occurring data blocks 
blocks sorted frequencies 
coalesced reliability preservation performance accessing data affected 
duplicate data copies inadvertent deletion corruption file data dde allows copies consuming additional storage space protects deletion corruption 
dde create exposure potential multiple file corruptions due single block error 
dde coalesces duplicate data blocks files 
result file fragmentation degrade system performance due non contiguous reads 
alleviated coalescing duplicate data sizes contiguous blocks 
section majority duplicate blocks come files cases reading files additional seek overheads 
probable dde reduce system write activities clients detect duplicate data writing storage devices discussed section 
dde potentially improve storage subsystem cache utilization unique data blocks cached 
degree data duplication vary dramatically different systems application environments 
dde technique suitable environments expected high degrees data duplication backup multiple personal workstations necessarily intended general uses 
key techniques dde content hashing copy write lazy updates 
necessary appropriate supports techniques dde applicable file systems storage tank 
particularly lazy updates minimize performance impact identifying duplicate data maintaining block metadata beneficial file systems 
working implementing duplicate data elimination storage tank 
implementation research directions explore 
technique copy write plays key role design guarantee consistency fingerprints block contents 
forces client request new block allocations file modification noticeable overheads normal write operations 
alleviate extra allocation cost due cow server new blocks client acquires exclusive shared write lock 
preallocation policy cow interesting research topic 
promising approach alleviate overhead allow client maintain small private storage pool behalf server 
extra cost cow 
current design employ quite naive coalescing policy find written block containing data block fingerprint table simply dereference new block change corresponding file block pointer primary block fingerprint table 
policy suboptimal terms efficiency 
considered file block access patterns design simplicity explicitly elaborate policies favor sequential fingerprint probing matching certain block access patterns 
research policies needed 
naive coalescing policy describe may result file fragmentation 
coalescing blocks large file desirable 
study policies minimizing file fragmentation interesting 
furthermore coalescing policy reduce storage fragmentation reusing unused blocks due lazy free space reclamation 
fingerprint block short version data 
client easily keep history write activities maintaining fingerprint cache 
client part duplicate data elimination conjunction server 
beneficially actual write operations storage saved cache hits 
far know extensive intensive studies duplicate data distributions block level levels 
better understanding data duplication file systems enormously beneficial making duplicate data coalescing policies 
design add attributes physical disk blocks count sha fingerprint block content 
provide appropriate data structures store retrieve attributes 
feasible check data integrity storage tank 
client ensure data reads data writes comparing fingerprint server calculated data reads 
disk prices drop dramatically storage precious resource computer systems 
data sets reducing storage consumption caused duplicate data significantly improve storage usage efficiency 
techniques content hashing copy write lazy lock revocation lazy free space reclamation detect coalesce duplicate data blocks line file systems significant impact system performance 
case studies show storage saved technique duplicate data elimination kb blocks application environments 
file level duplication detection sensitive changes file contents lose chance finding duplicate data finer granularities 
acknowledgments grateful robert rees wayne david pease discussions insights storage tank 
scott brandt ethan miller feng wang lan xue university california santa cruz helpful comments 
vijay sundaram university massachusetts amherst particular invaluable discussions 
terrence patrick gavin bioinformatics department ucsc providing access genome data set helping understand results 
shepherd curtis anderson provided useful feedback comments draft 
ajtai burns fagin long stockmeyer 
compactly encoding unstructured inputs differential compression 
journal association computing machinery may 
miller hollingsworth 
names package management tcl 
proceedings th annual tcl tk conference pages san diego ca sept 
usenix 
anonymous 
secure hash standard 
fips national institute standards technology apr 
bolosky goebel douceur 
single instance storage windows 
proceedings th usenix windows systems symposium pages 
usenix aug 
broder 
resemblance containment documents 
proceedings compression complexity sequences sequences pages 
ieee computer society 
burns 
data management distributed file system storage area networks 
ph dissertation department computer science university california santa cruz mar 
burns long 
efficient distributed back delta compression 
proceedings parallel distributed systems pages san jose nov 
acm 
burns rees long 
safe caching distributed file system network attached storage 
proceedings th international parallel distributed processing symposium ipdps 
ieee may 
burns rees stockmeyer long :10.1.1.101.3227
scalable session locking distributed file system 
cluster computing journal 
cox murray noble :10.1.1.101.3227
pastiche making backup cheap easy 
proceedings th symposium operating systems design implementation osdi pages boston ma dec 
crespo garcia molina :10.1.1.42.4367
archival storage digital libraries 
proceedings third acm international conference digital libraries dl pages pittsburgh pennsylvania june 
acm 
douglis iyengar :10.1.1.150.1425
application specific resemblance detection 
proceedings usenix annual technical conference pages 
usenix june 
fu kaashoek mazi res 
fast secure distributed read file system 
proceedings th symposium operating systems design implementation osdi pages san diego ca oct 
hollingsworth miller 
content derived names configuration management 
proceedings symposium software reusability ssr pages boston ma may 
ieee 
kan high performance reliable stackable compression file system 
www com product nov 
manber :10.1.1.106.2691
finding similar files large file system 
technical report tr department computer science university arizona tucson arizona oct 
menon pease rees 
ibm storage tank heterogeneous scalable san file system 
ibm systems journal 
microsoft windows server online help file 
microsoft feb 
mogul douglis feldmann krishnamurthy 
potential benefits delta encoding data compression 
proceedings sigcomm pages 
muthitacharoen chen mazi res :10.1.1.10.8444
network file system 
proceedings th acm symposium operating systems principles sosp pages oct 

windows nt file system internals developer guide 
reilly associates 

com lzo data compression library 
www com opensource lzo july 
quinlan dorward :10.1.1.18.8085
venti new approach archival storage 
proceedings conference file storage technologies fast pages monterey california usa 
usenix 
rabin 
fingerprinting random polynomials 
technical report tr center research computing technology harvard university 
spring wetherall 
protocol independent technique eliminating redundant network traffic 
proceedings conference applications technologies architectures protocols computer communication sig comm pages stockholm sweden aug 
acm press 

