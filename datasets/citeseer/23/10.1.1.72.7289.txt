journal arti cial intelligence research submitted published solving multiclass learning problems error correcting output codes thomas dietterich tgd cs orst edu department computer science hall oregon state university corvallis usa bakiri eb isa cc bh department computer science university isa town multiclass learning problems involve nding de nition unknown function range discrete set containing values classes 
de nition acquired studying collections training examples form hx existing approaches multiclass learning problems include direct application multiclass algorithms decision tree algorithms cart application binary concept learning algorithms learn individual binary functions classes application binary concept learning algorithms distributed output representations 
compares approaches new technique error correcting codes employed distributed output representation 
show output representations improve generalization performance backpropagation wide range multiclass learning tasks 
demonstrate approach robust respect changes size training sample assignment distributed representations particular classes application tting avoidance techniques decision tree pruning 
show methods error correcting code technique provide reliable class probability estimates 
taken results demonstrate error correcting output codes provide general purpose method improving performance inductive learning programs multiclass problems 

task learning examples nd approximate de nition unknown function training examples form hx cases takes values binary functions algorithms available 
example decision tree methods quinlan cart breiman friedman olshen stone construct trees leaves labeled binary values 
arti cial neural network algorithms perceptron algorithm rosenblatt error backpropagation bp algorithm rumelhart hinton williams best suited learning binary functions 
theoretical studies learning focused entirely learning binary functions valiant natarajan 
real world learning tasks unknown function takes values discrete set classes fc kg 
example medical diagnosis function map description patient possible diseases 
digit recognition ai access foundation morgan kaufmann publishers 
rights reserved 
dietterich bakiri lecun boser denker henderson howard hubbard jackel function maps hand printed digit classes 
phoneme recognition systems waibel hinton shikano lang typically classify speech segment phonemes 
decision tree algorithms easily generalized handle multiclass learning tasks 
leaf decision tree labeled classes internal nodes selected discriminate classes 
call direct multiclass approach 
connectionist algorithms di cult apply multiclass problems 
standard approach learn individual binary functions class 
assign new case classes evaluated assigned class function returns highest activation nilsson 
call class approach binary function learned class 
alternative approach explored researchers employ output code 
approach pioneered sejnowski rosenberg nettalk system 
class assigned unique binary string length refer strings codewords 
binary functions learned bit position binary strings 
training example class desired outputs binary functions speci ed codeword class arti cial neural networks functions implemented output units single network 
new values classi ed evaluating binary functions generate bit string string compared codewords assigned class codeword closest distance measure generated string example consider table shows bit distributed code class digit recognition problem 
notice row distinct class unique codeword 
applications distributed output codes bit positions columns chosen meaningful 
table gives meanings columns 
learning binary function learned column 
notice column distinct binary function learned disjunction original classes 
example vl iff 
classify new hand printed digit functions vl hl dl cc ol evaluated obtain bit string 
distance string codewords computed 
nearest codeword hamming distance counts number bits di er corresponds class 
predicts 
process mapping output string nearest codeword identical decoding step error correcting codes bose ray chaudhuri 
suggests advantage employing error correcting codes distributed representation 
idea employing error correcting distributed representations traced early research learning duda singleton 
error correcting output codes table distributed code digit recognition task 
code word class vl hl dl cc ol table meanings columns code table 
column position abbreviation meaning vl contains vertical line hl contains horizontal line dl contains diagonal line cc contains closed curve ol contains curve open left contains curve open right table bit error correcting output code class problem 
code word class dietterich bakiri table shows bit error correcting code digit recognition task 
class represented drawn error correcting code 
distributed encoding table separate boolean function learned bit position errorcorrecting code 
classify new example learned functions evaluated produce bit string 
mapped nearest codewords 
code correct errors bits 
error correcting code approach suggests view machine learning kind communications problem identity correct output class new example transmitted channel 
channel consists input features training examples learning algorithm 
errors introduced nite training sample poor choice input features aws learning process class information corrupted 
encoding class error correcting code transmitting bit separately separate run learning algorithm system may able recover errors 
perspective suggests class meaningful distributed output approaches inferior output representations constitute robust error correcting codes 
measure quality error correcting code minimum hamming distance pair code words 
minimum hamming distance code correct single bit errors 
single bit error moves unit away true codeword hamming distance 
errors nearest codeword correct codeword 
code table minimum hamming distance correct errors bit positions 
hamming distance class code class encoding output classes correct errors 
minimum hamming distance pairs codewords meaningful distributed representation tends low 
example table hamming distance codewords classes 
kinds codes new columns introduced discriminate classes 
classes di er bit position hamming distance output representations 
true distributed representation developed sejnowski rosenberg nettalk task 
compare performance error correcting code approach existing approaches direct multiclass method decision trees class method nettalk task meaningful distributed output representation approach 
show error correcting codes produce uniformly better generalization performance domains learning algorithm backpropagation neural network learning algorithm 
report series experiments designed assess robustness error correcting code changes learning task length code size training set assignment codewords classes decision tree pruning 
error correcting code approach produce reliable class probability estimates 
concludes discussion open questions raised results 
chief questions issue errors di erent bit positions output somewhat independent 
indepen error correcting output codes table data sets employed study 
number number number number name features classes training examples test examples glass fold vowel pos fold soybean isolet letter nettalk phonemes words words stresses letters letters dence error correcting output code method fail 
address question case decision tree algorithms companion kong dietterich 

methods section describes data sets learning algorithms employed study 
discusses issues involved design error correcting codes describes algorithms code design 
section concludes brief description methods applied classi cation decisions evaluate performance independent test sets 
data sets table summarizes data sets employed study 
glass vowel soybean isolet letter nettalk data sets available irvine repository machine learning databases murphy aha 
pos part speech data set provided cardie personal communication earlier version data set described cardie 
entire nettalk data set consists dictionary words pronunciations 
experiments feasible chose training set words disjoint test set words random nettalk dictionary 
focus percentage letters pronounced correctly words 
pronounce letter phoneme stress letter determined 
syntactically possible combinations phonemes stresses appear training test sets selected 

repository refers soybean data set soybean large data set ogy standardized letter data set letter recognition 
learning algorithms dietterich bakiri employed general classes learning methods algorithms learning decision trees algorithms learning feed forward networks sigmoidal units arti cial neural networks 
decision trees performed experiments release older substantially identical version program described quinlan 
changes support distributed output representations ected tree growing part algorithm 
pruning con dence factor set 
contains facility creating soft thresholds continuous features 
experimentally improved quality class probability estimates produced algorithm glass vowel isolet domains results reported domains computed soft thresholds 
neural networks employed implementations 
domains extremely fast backpropagation implementation provided neurocomputer adaptive solutions 
performs simple gradient descent xed learning rate 
gradient updated presenting training example momentum term employed 
potential limitation inputs represented bits accuracy weights represented bits accuracy 
weight update arithmetic round performs jamming forcing lowest order bit low order bits lost due shifting multiplication 
speech recognition letter recognition vowel data sets employed opt system distributed oregon graduate institute barnard cole 
implements conjugate gradient algorithm updates gradient complete pass training examples known epoch updating 
learning rate required approach 
opt attempt minimize squared error computed desired outputs network 
researchers employed error measures particularly cross entropy hinton classi cation gure merit cfm hampshire ii waibel 
researchers advocate softmax normalizing layer outputs network bridle 
con gurations theoretical support richard lippmann report squared error works just measures producing accurate posterior probability estimates 
furthermore cross entropy cfm tend easily squared error lippmann personal communication weigend 
chose minimize squared error opt systems implement 
neural network algorithm parameters chosen user 
select learning rate initial random seed number hidden units stopping criteria 
selected optimize performance validation set methodology lang hinton waibel 
training set subdivided set validation set 
training set observed generalization performance validation set determine optimal settings learning rate network size best point training 
training set mean squared error stopping point computed training performed entire training set chosen parameters stopping indicated mean squared error 
measure network performance test set 
error correcting output codes data sets procedure worked 
letter recognition data set clearly choosing poor stopping points full training set 
overcome problem employed slightly di erent procedure determine stopping epoch 
trained series progressively larger training sets subsets nal training set 
validation set determined best stopping epoch training sets 
extrapolated training sets predict best stopping epoch full training set 
glass pos data sets employed fold cross validation assess generalization performance 
chose training parameters fold fold cross validation 
creates test set contamination examples validation set data fold test set data folds 
little tting validation set little ect choice parameters stopping points 
data sets come designated test sets employed measure generalization performance 
error correcting code design de ne error correcting code matrix binary values matrix shown table 
length code number columns code 
number rows code equal number classes multiclass learning problem 
codeword row code 
error correcting output code class problem satisfy properties row separation 
codeword separated hamming distance codewords 
column separation 
bit position function uncorrelated functions learned bit positions achieved insisting hamming distance column columns large hamming distance column complement columns large 
power code correct errors directly related row separation discussed 
purpose column separation condition obvious 
columns similar identical deterministic learning algorithm applied learn similar correlated mistakes 
errorcorrecting codes succeed errors individual bit positions relatively uncorrelated number simultaneous errors bit positions small 
simultaneous errors error correcting code able correct peterson 
errors columns highly correlated bits columns complementary 
algorithms backpropagation treat class complement symmetrically 
construct identical decision trees class class interchanged 
maximum hamming distance columns attained columns complements 
column separation condition attempts ensure columns identical complementary 
dietterich bakiri table possible columns class problem 
note columns complements rst rst column discriminate classes 
code word class number classes di cult satisfy properties 
example number classes possible columns see table 
half complements half 
leaves possible columns 
zeroes ones useless discriminating rows 
result left possible columns exactly class encoding provides 
general classes usable columns removing complements zeros ones column 
classes get column code minimum inter row hamming distance 
classes get column code 
employed methods constructing error correcting output codes exhaustive technique method selects columns exhaustive code method randomized hill climbing algorithm bch codes 
choice method number classes finding single method suitable values open research problem 
describe methods turn 
exhaustive codes construct code length follows 
row ones 
row consists zeroes followed ones 
row consists zeroes followed ones followed zeroes followed ones 
row alternating runs zeroes ones 
table shows exhaustive class problem 
code inter row hamming distance columns identical complementary 
column selection exhaustive codes construct exhaustive code select subset columns 
formulate propositional satis ability problem apply gsat algorithm selman levesque mitchell attempt solution 
solution required include exactly columns desired length code ensuring hamming distance columns chosen value column represented boolean variable 
pairwise mutual error correcting output codes table exhaustive code 
row column hill climbing algorithm improving row column separation 
closest rows columns indicated lines 
lines intersect bits code words changed improve separations shown right 
exclusion constraint placed columns violate column separation condition 
support constraints extended gsat support mutual exclusion constraints ciently 
randomized hill climbing employed random search algorithm begins drawing random strings desired length pair random strings separated hamming distance binomially distributed mean 
randomly generated codes generally quite average 
improve algorithm repeatedly nds pair rows closest hamming distance pair columns extreme hamming distance close far apart 
algorithm computes codeword bits rows columns intersect changes improve row column separations shown 
hill climbing procedure reaches local maximum algorithm randomly chooses pairs rows columns tries improve separations 
combined hill climbing random choice procedure able improve minimum hamming distance separation quite substantially 
bch codes dietterich bakiri applied bch algorithm design codes bose ray chaudhuri 
bch algorithm employs algebraic methods galois eld theory design nearly optimal error correcting codes 
practical drawbacks algorithm 
published tables primitive polynomials required algorithm produce codes length largest word size employed computer memories 
second codes exhibit column separations 
third codes power 
number classes learning problem power shorten code deleting rows possible columns maintaining row column separations 
experimented various heuristic greedy algorithms code shortening 
codes nettalk isolet letter recognition domains combination simple greedy algorithms manual intervention design shortened bch codes 
data sets studied designed series error correcting codes increasing lengths 
executed learning algorithm codes 
stopped lengthening codes performance appeared leveling making classi cation decisions approach solving multiclass problems direct multiclass class errorcorrecting output coding assumes method classifying new examples 
direct multiclass approach system computes class probability estimate new example 
estimates probability example belongs ofthe classes 
chooses class having highest probability class example 
class approach decision tree neural network output unit viewed computing probability new example belongs corresponding class 
class decision tree output unit gives highest probability estimate chosen predicted class 
ties broken arbitrarily favor class comes rst class ordering 
error correcting output code approach decision tree neural network output unit viewed computing probability corresponding bit codeword 
call probability values hb ni length codewords error correcting code 
classify new example compute distance probability vector codewords error correcting code 
distance de ned lx jb jj class codeword smallest distance assigned class new example 
ties broken arbitrarily favor class comes rst class ordering 
performance relative multiclass glass error correcting output codes vowel pos soybean audiology isolet letter class ecoc nettalk multiclass performance percentage points class ecoc methods relative direct multiclass method 
asterisk indicates di erence signi cant level better 

results results experiments 
results decision trees 
consider neural networks 
report results series experiments assess robustness error correcting output code method 
decision trees shows performance domains 
horizontal line corresponds performance standard multiclass decision tree algorithm 
light bar shows performance class approach dark bar shows performance ecoc approach longest error correcting code tested 
performance displayed number percentage points pair algorithms di er 
asterisk indicates di erence statistically signi cant level test di erence proportions normal approximation binomial distribution see cochran 
gure see class method performs signi cantly worse multiclass method domains behavior statistically indistinguishable remaining domains 
encouraging observation error correcting output code approach signi cantly superior multiclass approach domains indistinguishable remaining 
dietterich bakiri nettalk domain consider performance meaningful distributed representation developed sejnowski rosenberg 
representation gave correct classi cation compared class con guration direct multiclass con guration ecoc con guration 
di erences gures statistically signi cant level better class direct multiclass con gurations statistically distinguishable 
backpropagation shows results backpropagation challenging domains 
horizontal line corresponds performance class encoding method 
bars show number percentage points error correcting output coding representation outperforms class representation 
domains ecoc encoding superior di erences statistically signi cant vowel nettalk isolet domains 
letter recognition domain encountered great di culty successfully training networks machine particularly ecoc con guration 
experiments showed problem arose fact implementation backpropagation employs xed learning rate 
switched slower opt program chooses learning rate adaptively conjugate gradient line searches 
behaved better class ecoc con gurations 
di culty training isolet ecoc con guration large networks units opt program 
sets initial random weights led local minima poor performance validation set 
nettalk task compare performance sejnowski rosenberg distributed encoding class ecoc encodings 
distributed encoding yielded performance correct compared class encoding ecoc encoding 
di erence distributed encoding class encoding statistically signi cant 
results previous results conclude distributed encoding advantages class ecoc encoding domain 
robustness results show ecoc approach performs better alternative approaches 
important questions answered recommend ecoc approach reservation results hold small samples 
decision trees learned error correcting codes larger learned class multiclass approaches 
suggests small sample sizes ecoc method may perform complex trees usually require data learned reliably 
hand experiments described covered wide range 
di erence isolet detectable test paired di erences proportions 
see cochran 
performance relative class glass error correcting output codes vowel isolet backprop ecoc letter nettalk backprop class performance ecoc method relative class backpropagation 
asterisk indicates di erence signi cant level better 
training set sizes suggests results may depend having large training set 
results depend particular assignment classes 
codewords assigned classes arbitrarily experiments reported suggests particular assignment may important 
assignments better 
results depend pruning techniques applied algorithms 
pruning methods shown improve performance multiclass domains 
ecoc approach provide class probability estimates 
backpropagation con gured provide estimates probability test example belongs possible classes 
ecoc approach 
small sample performance noted concerned small sample performance ecoc method noticed ecoc method requires larger decision trees opc method 
table compares sizes decision trees learned multiclass class ecoc con gurations letter recognition task nettalk task 
opc ecoc con gurations tables show average number leaves trees learned bit position output representation 
dietterich bakiri table size decision trees learned letter recognition task nettalk task 
letter recognition leaves bit total leaves multiclass class bit ecoc nettalk leaves bit total leaves phoneme stress phoneme stress multiclass class bit ecoc letter recognition trees learned bit ecoc times larger learned class representation 
phoneme classi cation part nettalk ecoc trees times larger opc trees 
way compare sizes trees consider total number leaves trees 
tables clearly show multiclass approach requires memory fewer total leaves opc ecoc approaches 
backpropagation di cult determine amount network resources consumed training network 
approach compare number hidden units give best generalization performance 
isolet task example class encoding attains peak validation set performance hidden unit network bit error correcting encoding attained peak validation set performance hidden unit network 
letter recognition task peak performance class encoding obtained network hidden units compared hidden units bit error correcting output code 
decision tree neural network sizes see general errorcorrecting output representation requires complex hypotheses class representation 
learning theory statistics known complex hypotheses typically require training data simple ones 
basis expect performance ecoc method poor small training sets 
test prediction measured performance function training set size larger domains nettalk letter recognition 
presents learning curves nettalk letter recognition tasks show accuracy series progressively larger training sets 
gure clear bit error correcting code consistently outperforms con gurations nearly constant margin 
shows corresponding results backpropagation nettalk letter recognition tasks 
nettalk task results sample size apparent uence bene ts error correcting output coding 
letter recognition task appears interaction 
percent correct percent correct nettalk training set size error correcting output codes bit ecoc multiclass class class bit ecoc letter recognition training set size multiclass accuracy multiclass class error correcting output coding con gurations increasing training set sizes nettalk letter recognition tasks 
note horizontal axis plotted logarithmic scale 
nettalk bit ecoc class training set size words opt bit ecoc letter recognition opt opc training set size accuracy backpropagation class error correcting output coding con gurations increasing training set sizes nettalk letter recognition tasks 
error correcting output coding works best small training sets statistically signi cant bene largest training set examples class method slightly outperforms ecoc method 
experiments conclude error correcting output coding works small samples despite increased size decision trees increased complexity training neural networks 
backpropagation letter recognition task error correcting output coding worked better small samples dietterich bakiri table random assignments codewords classes nettalk task 
column shows percentage letters correctly classi ed decision trees 
bit error correcting code replications multiclass class large ones 
ect suggests ecoc works reducing variance learning algorithm 
small samples variance higher ecoc provide bene assignment codewords classes results reported far codewords error correcting code arbitrarily assigned classes learning task 
conducted series experiments nettalk domain determine randomly reassigning codewords classes ect success ecoc 
table shows results random assignments codewords classes 
statistically signi cant variation performance di erent random assignments 
consistent similar experiments reported bakiri 
effect tree pruning pruning decision trees important technique preventing tting 
merit pruning varies domain 
shows change performance due pruning domains con gurations studied multiclass class error correcting output coding 
gure see cases pruning statistically signi cant di erence performance aside pos task decreases performance con gurations 
aside pos statistically signi cant changes involves ecoc con guration ect class con guration ects multiclass con guration 
data suggest pruning occasionally major ect con gurations 
evidence suggest pruning ects con guration 
class probability estimates applications important tohave classi er classify new cases estimate probability new case belongs classes 
example medical diagnosis simple classi er classify patient healthy input features class 
non zero probability patient life threatening disease right choice physician may prescribe therapy disease 
mundane example involves automated reading handwritten postal codes envelopes 
classi er con dent classi cation estimated performance relative pruning glass vowel error correcting output codes pos soybean audiology isolet letter nettalk multiclass class ecoc pruning change percentage points performance pruning con gurations 
horizontal line indicates performance pruning 
asterisk indicates di erence signi cant level better 
dietterich bakiri probabilities strong proceed route envelope 
uncertain envelope rejected sent attempt read postal code process envelope wilkinson geist janet 
way assess quality class probability estimates classi er compute rejection curve 
learning algorithm classi es new case require output con dence level 
plot curve showing percentage correctly classi ed test cases con dence level exceeds value 
rejection curve increases smoothly demonstrates con dence level produced algorithm transformed accurate probability measure 
class neural networks researchers di erence activity class highest activity class second highest activity measure con dence lecun 
di erence large chosen class clearly better 
di erence small chosen class nearly tied class 
measure applied class probability estimates produced 
analogous measure con dence error correcting output codes computed distance vector output probabilities bit codewords classes 
speci cally employ di erence distance second nearest codeword distance nearest codeword con dence measure 
di erence large algorithm quite con dent classi cation decision 
di erence small algorithm con dent 
compares rejection curves various con gurations backpropagation nettalk task 
curves constructed rst running test examples learned decision trees computing predicted class example con dence value prediction 
generate point curve avalue chosen parameter de nes minimum required con dence 
classi ed test examples processed determine percentage test examples con dence level rejected percentage remaining examples correctly classi ed 
value progressively incremented starting test examples rejected 
lower left portion curve shows performance algorithm small con dent cases rejected 
upper right portion curve shows performance large con dent cases classi ed 
class probability estimates produce curve rises smoothly monotonically 
decreasing region rejection curve reveals cases con dence estimate learning algorithm unrelated inversely related actual performance algorithm 
rejection curves terminate prior rejecting examples 
occurs nal increment causes examples rejected 
gives idea number examples algorithm highly con dent classi cations 
curve terminates early shows examples algorithm con dently classify 
see exception multiclass con guration rejection curves various con gurations increase fairly smoothly percent correct bit ecoc opc multiclass error correcting output codes distributed bit ecoc percent rejected backpropagation bit ecoc opc distributed bit ecoc percent rejected rejection curves various con gurations backpropagation nettalk task 
distributed curve plots behavior sejnowski rosenberg distributed representation 
producing acceptable con dence estimates 
error correcting con gurations smooth curves remain con gurations 
shows performance advantage error correcting output coding maintained con dence levels ecoc improves classi cation decisions examples just borderline ones 
similar behavior seen rejection curves backpropagation 
con gurations backpropagation give fairly smooth rejection curves 
note bit code decreases high rejection rates 
contrast bit code gives monotonic curve eventually reaches 
seen behavior cases studied extremely long error correcting codes usually best method low rejection rates high rejection rates codes intermediate length typically bits behave better 
explanation behavior 
compares rejection curves various con gurations backpropagation isolet task 
see ecoc approach markedly superior class multiclass approaches 
gure illustrates phenomenon frequently observed curve multiclass quite terminates early class surpasses 
suggests may opportunities improve class probability estimates produced multiclass trees 
note employed thresholds experiments 
backpropagation rejection curves ecoc approach consistently outperforms class approach close correct 
note con gurations backpropagation con dently classify test examples accuracy 
graphs clear error correcting approach codes intermediate length provide con dence estimates provided standard approaches multiclass problems 
percent correct bit ecoc class bit ecoc multiclass dietterich bakiri percent rejected bit ecoc class backpropagation percent rejected rejection curves various con gurations backpropagation isolet task 

experimentally compared approaches multiclass learning problems multiclass decision trees class opc approach meaningful distributed output approach error correcting output coding ecoc approach 
results clearly show ecoc approach superior approaches 
improvements provided ecoc approach quite substantial improvements order percentage points observed domains 
statistically signi cant improvements observed domains decision trees domains backpropagation 
improvements robust ecoc improves decision trees neural networks ecoc provides improvements small sample sizes improvements depend particular classes 
error correcting approach provide estimates con dence classi cation decisions accurate provided existing methods 
additional costs employing error correcting output codes 
decision trees learned ecoc generally larger complex trees constructed class multiclass approaches 
neural networks learned ecoc require hidden units longer careful training obtain improved performance see section 
factors may argue errorcorrecting output coding domains 
example domains important humans understand interpret induced decision trees ecoc methods appropriate produce complex trees 
domains training rapid completely autonomous ecoc methods backpropagation recommended potential encountering di culties training 
error correcting output codes error correcting codes intermediate length tend give better con dence estimates long error correcting codes long codes give best generalization performance 
open problems require research 
foremost important obtain deeper understanding ecoc method works 
assume learned hypotheses classi cation errors independently coding theory provides explanation individual errors corrected codewords far apart output space 
hypotheses learned algorithm training data expect errors individual hypotheses highly correlated errors corrected error correcting code 
key open problem understand classi cation errors di erent bit positions fairly independent 
error correcting output code result independence 
closely related open problem concerns relationship ecoc approach various ensemble committee boosting methods perrone cooper schapire freund :10.1.1.153.7626
methods construct multiple hypotheses vote determine classi cation example 
error correcting code viewed compact form voting certain number incorrect votes corrected 
interesting di erence standard ensemble methods ecoc approach ensemble methods hypothesis attempting predict function ecoc approach hypothesis predicts di erent function 
may reduce correlations hypotheses ective voters 
needed explore relationship 
open question concerns relationship ecoc approach exible discriminant analysis technique hastie tibshirani buja press 
method rst employs class approach neural networks applies kind discriminant analysis outputs 
discriminant analysis maps outputs dimensional space class de ned center point 
new cases classi ed mapping space nding nearest center point class 
center points similar codewords continuous space dimension 
may ecoc method kind randomized higher dimensional variant approach 
ecoc approach shows promise scaling neural networks large classi cation problems hundreds thousands classes better class method 
error correcting code length total number classes class approach requires output unit class 
networks thousands output units expensive di cult train 
studies test scaling ability di erent approaches large classi cation tasks 
authors anonymous reviewers valuable suggestions improved presentation 
authors prasad tadepalli proof reading dietterich bakiri nal manuscript 
authors gratefully acknowledge support national science foundation numbered iri cda iri 
bakiri university support doctoral research 
adaptive solutions 
back propagation guide 
tech 
rep adaptive solutions beaverton 
bakiri 

converting english text speech machine learning approach 
tech 
rep department computer science oregon state university corvallis 
barnard cole 

neural net training program optimization 
tech 
rep cse oregon graduate institute beaverton 
bose ray chaudhuri 

class error correcting binary group codes 
information control 
breiman friedman olshen stone 

classi cation regression trees 
wadsworth international group 
bridle 

training stochastic model recognition algorithms networks lead maximum mutual information estimation parameters 
touretzky 
ed neural information processing systems vol 
pp 
san francisco ca 
morgan kaufmann 
cardie 

decision trees improve case learning 
proceedings tenth international conference machine learning pp 
san francisco ca 
morgan kaufmann 
duda singleton 

function modeling experiments 
tech 
rep stanford research institute 
freund 

improved boosting algorithm implications learning complexity 
proc 
th annu 
workshop comput 
learning theory pp 

acm press new york ny 
hampshire ii waibel 

objective function improved phoneme recognition time delay neural networks 
ieee transactions neural networks 
hastie tibshirani buja 
press 
flexible discriminant analysis optimal scoring 
journal american statistical association 
hinton 

connectionist learning procedures 
arti cial intelligence 


codes 
chi res 
error correcting output codes kong dietterich 

error correcting output coding works decision trees 
tech 
rep department computer science oregon state university corvallis 
lang hinton waibel 

time delay neural network architecture isolated word recognition 
neural networks 
lecun boser denker henderson howard hubbard jackel 

backpropagation applied handwritten zip code recognition 
neural computation 
murphy aha 

uci repository machine learning databases machinereadable data repository 
tech 
rep university california irvine 
natarajan 

machine learning theoretical approach 
morgan kaufmann san mateo ca 
nilsson 

learning machines 
mcgraw hill new york 
perrone cooper 

networks disagree ensemble methods hybrid neural networks 

ed neural networks speech image processing 
chapman hall 
peterson jr 

error correcting codes 
mit press cambridge ma 
quinlan 

programs empirical learning 
morgan kaufmann san francisco ca 
richard lippmann 

neural network classi ers estimate bayesian posteriori probabilities 
neural computation 
rosenblatt 

perceptron probabilistic model information storage organization brain 
psychological review 
rumelhart hinton williams 

learning internal representations error propagation 
parallel distributed processing explorations microstructure chap 
pp 

mit press 
schapire 

strength weak learnability 
machine learning 
sejnowski rosenberg 

parallel networks learn pronounce english text 
journal complex systems 
selman levesque mitchell 

new method solving hard satis ability problems 
proceedings aaai pp 

aaai mit press 
cochran 

statistical methods 
iowa state university press ames ia eighth edition 
valiant 

theory learnable 
commun 
acm 
dietterich bakiri waibel hinton shikano lang 

phoneme recognition time delay networks 
ieee transactions acoustics speech signal processing 
weigend 

measuring ective number dimensions backpropagation training 
proceedings connectionist models summer school pp 

morgan kaufmann san francisco ca 
wilkinson geist janet 

rst census optical character recognition systems conference 
tech 
rep nistir national institute standards technology 

