
constraint neural network learning time series predictions benjamin wah qian university illinois urbana champaign usa chapter briefly surveyed previous predicting noise free piecewise chaotic time series noisy time series high frequency random noise 
noise free time series proposed constrained formulation neural network learning incorporates error learning pattern constraint new cross validation scheme allows multiple validations sets considered learning recurrent fir neural network architecture combines recurrent structure memory fir structure violation guided back propagation algorithm searching constrained space formulation 
noisy time series studied systematically edge effect due low pass filtering noisy time series developed approach incorporates constraints predicting low pass data lag period 
new constraints enable active training lag period greatly improves prediction accuracy lag period 
experimental results show significant improvements prediction accuracy standard benchmarks stock price time series 
time series ordered sequence observations time time series prediction problem prediction data horizon historical data form vector scalar 
time series predictions areas science industry commercial financial activities financial forecasts stock prices currency exchange rates product sale demand forecasting population growth earthquake activities 
general time series may exhibit nonlinearity non stationarity possibly periodic behavior 
observations contaminated noise time series noisy 
illustrates noisy nonstationary periodic time series 
characteristics described follows 

linearity 
time series linear expressed linear function historical values nonlinear 
chapter interested developing general models represent nonlinear time series continuous function wah qian noise periodicity nonstationarity fig 

example nonstationary periodic noisy time series 
stationarity 
time series stationary constant mean variance nonstationary 
nonstationarity hard model behavior may unpredictable 
chapter interested stationary time series special class nonstationary time series called piecewise chaotic time series 
time series piecewise chaotic consists regimes regime corresponds chaotic process time series collection multiple chaotic regimes 

periodicity 
time series dominant periodic components exhibit regular periodic variations 
behavior displayed example annual electricity consumption merchandise sales 
periodicity studied property eliminated effectively differencing techniques consider chapter 

random noise entire parts frequency spectrum time series 
random noise predicted study time series high frequency noise prediction low frequency components 
section survey briefly existing handling nonlinearity piecewise chaos noise time series 
section discusses newly proposed constrained formulations violation guided backpropagation developed solve constrained formulations 
section presents methods handle noisy time series constrained formulations illustrates prediction low frequency components stock prices 
drawn sect 

previous time series modeling variety time series models proposed studied decades 
section review briefly existing models potential problems applied handle nonlinear piecewise chaotic time series noise 
arma variants linear models state space models 
constraint neural network learning time series models pre defined nonlinearity nonlinear models general nonlinearity machine learning tar bilinear ar statistic unsupervised time varying parameter models reinforcement knn clustering supervised decision tree learning neural network learning fig 

classification time series models handling nonlinearity linearity classifies existing time series models linear nonlinear 
linear models linear time series may fail 
types linear models 

box jenkins arima variations autoregression ar moving average ma autoregressive moving average arma describe data linear combination historical data random processes 

exponential smoothing models smoothed data function raw data parameter model 

state space models represent inputs linear combination set state vectors evolve time linear equations 
vectors dimensions usually hard choose practice 
nonlinear models classified models predefined nonlinearity assumptions general models 
class includes bilinear autoregression time varying parameter models threshold autoregressive models 
effective modeling time series unknown nonlinear behavior 
machine learning handle nonlinear time series learns model nonlinearity assumptions 
specific methods model temporal sequences include statistic learning nearest neighbors knn reinforcement learning learning unsupervised learning clustering methods supervised learning decision trees artificial neural networks anns 
wah qian general methods learn single nonlinear objective training set 
result individual patterns help escape local optima especially gradient methods 
chapter propose constrained formulations ann learning add constraints individual patterns violated constraints help guide learning 
formulations general applied learning methods 
piecewise chaos time series piecewise chaotic regimes studied extensively 
approach identify local regimes performing learning predictions identified regime 
models approach include regime switching models hidden markov models 
approaches limited points correctly identified prediction points may hard prediction problem 
separating process steps machine learning learn regime changes reserving patterns regime change verified cross validation set 
traditional learning approaches single objective may difficulties handling cross validations multiple regime changes single objective containing sum errors cross validation sets provide guidance refinements exceeds preset threshold 
address issue formulation constrain error validation set satisfied learning 
show approach applied ann learning successful prediction regime changes testing 
fig 

symmetric fir filter taps random noise random noise uncorrelated zero mean predictable due uncorrelated nature 
presence time series model learning useful clean information especially signal noise ratio relatively low desirable eliminate noise learning 
ibm stock prices 
constraint neural network learning low pass frequency responses tap fir filter bank high pass lag days autocorrelations lag period today lag period fig 

illustration filtering process time series noisy ibm daily closing prices tap symmetric low pass fir filter de noise time series 
low pass high pass data day lag 
right panels show autocorrelation plots filtered time series literature de noising usually done low pass filtering wavelet transforms 
illustrates symmetric fir filter generate de data th filter coefficient number filter taps raw data noisy time series 
symmetric fir filter non causal filter current filtered output depends inputs 
example filtered output symmetric filter ends depends raw data ends 
dependencies data lead lag called edge effect filtered data 
shows day lag low pass high pass data closing stock prices ibm filtered tap symmetric fir filter 
edge effect unique artifact non causal filters occurs causal filters 
outputs causal filters depend inputs reflect delayed behavior original time series amount lag similar non causal filters 
overcome edge effects time series predictor predict missing filtered data lag period predicting 
time series high frequency random noise predictions limited low pass data autocorrelations high frequency samples distances longer lag period low fig 

result focus predictions low pass data chapter 
existing approaches predicting missing low pass data lag period typically impose assumptions raw data 
show example approaches 
flat extension assumes wah qian flat extension mirror extension zero extension fig 

techniques handling edge effects order compensate missing data lag period 
solid lines represent actual raw data dashed lines stand extended data average absolute errors mirror extension flat extension day index lag period fig 

average absolute errors diverge quickly predicting missing low pass data lag period days raw data 
latest observed raw data mirror extension assumes raw data mirror image history data wrap assumes period time series repeats zero extension assumes data zero 
extended raw data lowpass filtering applied obtain de data lag period 
chapter consider wrap zero extension applicable time series stationary zero mean 
shows mean absolute errors true low pass data closing stock prices ibm corresponding predicted lowpass data flat mirror extensions 
flat extension performs slightly better mirror extension case show average errors low pass data part lag period specifically days considerably larger rest lag period 
low pass values days lag period quite accurate training patterns true low pass values 
approach described sect 
de data part lag period training patterns predict patterns part lag period 
constraints raw data lag period order accurate predictions lag period 
mse number training iterations 
constraint neural network learning pattern error target training training error pattern number training errors individual progress mean squared errors patterns iterations fig 

sunspots time series trained backpropagation unconstrained formulation artificial neural networks known anns universal function approximators require knowledge process consideration 
anns modeling time series generally special structures store temporal information explicitly time delayed structures implicitly feedback structures 
examples class include neural networks fir neural networks fir nn examples include recurrent neural networks rnns 
architectures radial basis function network rbf supporting vector machines store approximate history information radial basis functions socalled supporting vectors 
time series predictions anns traditionally formulated unconstrained optimization problems minimize mean squared errors mse defined follows min oi di number output nodes ann respectively actual desired outputs ann time vector weights training data consists patterns observed extensive past research conducted designing learning algorithms unconstrained formulation order lead anns small number weights generalize 
learning algorithms limited success little guidance provided unconstrained formulation search stuck local minimum weight space 
case unconstrained objective eq 
indicate patterns violated best direction trajectory move 
wah qian illustrates lack guidance unconstrained formulation 
example ann trained backpropagation predict sunspot time series 
shows mse training decreased quickly iterations little improvement iterations 
examination weights shows frozen iterations gradients iterations small 
pattern errors fig 
shows considerably large errors patterns violated patterns identified unconstrained formulation 
propose section constrained formulation constraint pattern efficient algorithm searching constrained space 
minimizing training errors cross validations prevent overfitting ann training 
traditional learning involving cross validations generally divides available historical data disjoint training validation sets uses mse validation set sole objective 
reason validation set due limitation unconstrained formulations handle objective function 
problem faced traditional validations choosing proper crossvalidation set 
defined way long validation set prefers reserve portion historical data validation order ann generalize 
training validations sets disjoint portion patterns validation set prevents training patterns 
result ann learned access patterns time series usually important predicting 
dilemma traditional cross validations time series predictions 
problem faced traditional validations related piecewise chaotic time series 
piecewise chaotic time series behave differently points points need learned specifically order learned system generalize 
example laser time series fig 
piecewise chaotic time series points respectively 
learn points validation set segment segment right training set 
multi objective learning handled traditional single objective formulations modeled constrained formulation considers error cross validation set additional constraint 
rest chapter describe ann solutions predicting time series 
section constrained formulations learning algorithms accurately predicting stationary piecewise chaotic nonlinear time series 
sect 
design efficient 
constraint neural network learning fig 

laser piecewise chaotic time series requires validation sets regime section training learning algorithms predicting missing low pass data noisy time series lag period 
predictions noise free stationary piecewise chaotic time series predictions noise free time series ann generally carried single step iterative predictions inputs ann prediction step true observed predicted outputs previous prediction step 
approaches adopt widely evaluation metric normalized mean square error nmse defined follows nmse variance true time series period number patterns tested respectively actual desired outputs time section describe recurrent fir ann architecture constraints developed learning algorithm experimental results 
recurrent fir neural networks variety ann architectures modeling time series studied past consensus best architecture 
example horne giles concluded recurrent networks usually better finite memory machine problem 
dorffner stated recurrent neural networks wah qian recurrent node regular node bias node input recurrent fir ann unit delay fir filter fir filter fig 

structure layer double concentric circles indicate recurrent nodes circles non recurrent nodes small boxes bias nodes constant input 
unit time delay able prediction conditions simple feedforward network significantly performs best nonlinear time series 
agree best architecture problem dependent efficiency learning algorithm important network model 
recurrent non recurrent anns superior propose hybrid recurrent ann called recurrent fir neural network 
feedback link non zero delay link nodes explicit memory modeled multi tap fir filter fig 
storing history information 
shows simple layer feedback link associated unit delay 
advantage architecture store historical information rnn fir nn 
concept generalized easily existing recurrent anns elman neural network fully recurrent neural networks nonlinear autoregressive networks exogenous inputs 
constrained formulations ann learning constrained formulation consists learning objective similar traditional unconstrained formulation multiple constraints enforcing learning criterion 
describe types constraints considered may added needed 

constraint neural network learning training set test set change regime training fig 

validation sets defined training set 
validation sets cover regime changes piecewise chaotic time series constraints individual patterns 
address lack guidance search proposed new constraints account error training pattern constraint 
resulting optimization problem follows minw oi di subject hi oi di max 
constraint hi prescribes error th output unit th training pattern predefined small positive value learning accuracy achieved 
note modified way constraints satisfied objective zero learning stops 
constrained formulation beneficial difficult training scenarios violated constraints provide additional guidance search leading trajectory direction reduces constraint violations 
overcome lack guidance fig 
gradient objective function unconstrained formulation small 
search algorithm solving constrained formulation information pattern errors exactly patterns large errors 
may intentionally modify search direction increasing gradient contributions patterns large errors 
constraints cross validations 
constrained formulation leads new method cross validations shown fig 
defines multiple validation sets learning includes error validation set new constraint 
validation error ith output unit kth validation set accounted nmse iterative validations 
advantage new approach training performed historical data available excluding validation patterns training previous approaches 
considers new constraints errors iterative validations avoiding overfitting ann learned 
allows new validation set defined regime transition region training data piecewise chaotic time series leading accurate predictions regime changes 
denote iterative single step validation constraint function ith output node kth validation set hi hs 
constrained wah qian formulation minw subject hi oi di hi ei hs es oi di ei es nmse iterative single step validation error predefined small positive constants 
eq 
constrained nonlinear programming problem nlp non differentiable function hi existing lagrangian methods require differentiability functions applied 
existing methods penalty formulations difficulties convergence penalties chosen properly 
algorithms sample real space apply developed theory lagrange multipliers discrete constrained optimization inefficient solving large problem instances 
describe efficient learning algorithm called violation guided backpropagation 
violation guided backpropagation algorithm assuming discretized transform augmented lagrangian function hi hi hj discretization result approximate solution discretized minimizes may true optimal fixed error thresholds may feasible discretized feasible continuous issue ann learning error thresholds learning adjusted loose tight exist error thresholds lead feasible discretized theory lagrange multipliers discrete constrained optimization constrained local minimum discrete space equivalent saddle point discrete space local minimum subspace local maximum subspace 
look saddle points performing subspace subspace proposed fig 
violation guided back propagation algorithm 
start training prediction candidate point set initial parameters reset parameters 

constraint neural network learning accept candidate point annealing rules generate new candidates subspace gradient bp search subspace loop loop stopping conditions met 
generate new candidate subspace greedy rules update lagrangian values deterministic rules fig 

iterative search procedure employing relax tighten solve discrete constrained formulation ann time series predictions 
shaded box represents routine look saddle points discretized subspace continuous subspace loop carries subspace generating candidates subspace box accepting deterministic rules box 
box increases violation corresponding constraint larger certain threshold 
way training pattern large error larger lagrange multiplier 
learning problem large number weights training patterns essential points generated box candidates accepted box 
differentiable compute approximate gradient direction ignoring single step iterative validation errors bp find gradient approximate lagrangian function hi 
generate trial point approximate gradient step size 
way training pattern large error contribute gradient direction larger lagrange multiplier leading effective suppression constraint violations 
gradient direction computed bp heuristic step size consider constraints due cross validations search direction may lead reduced lagrangian function value may get stuck infeasible local minima subspace 
restarts help escape local minima experimental results shown uncontrolled restarts points may lead loss valuable local information collected search 
address issue propose annealing strategy box decides go current point metropolis probability exp min introduced control acceptance probability 
tolerances set relax tighten strategy proposed 
strategy observation looser constraints easier satisfy achieving larger violations convergence tighter constraints slower satisfy achieving smaller violations wah qian table 
single step iterative test performance nmse laser compared published results 
test set consists patterns 
comparison show test performance patterns 
boxed numbers indicate best results stands data available 
runs done input node hidden nodes output node feedbacks output hidden layers hidden input layers link method training single step predictions iterative predictions weights fir nn run run laser intensity expected intensity predicted intensity pattern index iterative predictions fig 

iterative predictions starting pattern ann trained constrained formulation run table convergence 
loose constraints tightening constraints gradually learning converges faster tighter tolerances 
experimental results tested formulations algorithms benchmarks including laser sunspots chaotic time series 
partial results 
laser taken fe time series competition wan fir nn won place time series 
table shows improves wan result terms single step iterative 
constraint neural network learning table 
comparison single step prediction performance nmse methods carbon copy cc linear fir nn 
carbon copy simply predicts time series data proceeding data 
training resp 
testing set shows patterns learning resp 
testing 
lorenz attractor data streams labeled respectively ikeda attractor streams real re imaginary im parts plane wave bench design methods mark set set metrics cc linear fir nn mg nmse weights mg nmse weights nmse weights nmse lorenz ikeda weights nmse re im weights predictions half weights 
shows gives accurate iterative predictions steps predicting precisely regime point phase shift 
tested nonlinear chaotic time series mackey glass mg mackey glass mg lorenz ikeda 
table compares results predictors shows anns trained constrained formulations weights achieve orders magnitude smaller 
predictions noisy time series high frequency random noise financial time series noisy behave random walks daily closing stock prices ex wah qian amples noisy time series illustrate series learned constrained formulations 
mentioned earlier de low pass filtered time series lags original series predictor needs predict missing low pass data lag period predicting 
review financial time series predictions models financial time series forecasting classified linear nonlinear expert system 
linear models popular financial time series predictions popular exponential smoothing arima models garch models linear mean nonlinear variance 
nonlinear models consist mainly nearest neighbor methods ann methods 
expert systems employ collection models including exponential smoothing methods arima methods moving average develop set rules selecting specific method select method activation certain conditions satisfied 
past years competitions known competitions competitions competitions held test forecasting accuracy including financial economic time series 
variety models including aforementioned tested 
reached consistent 
list related drawn competitions literature 
single method clearly superior methods time series tested 
existing methods outperform random walk models significantly statistically terms prediction accuracy prediction directions trends 
cases worse random walks 
prediction quality measurement dependent 
constraint lag period approach apply flat extensions generate raw time series data obtain low pass data part lag period extended raw data train including low pass data part lag period learning patterns additional constraints raw data trained predict low pass data part lag period 
order improve prediction accuracy include special constraint lag period utilizing available raw data 
raw data low pass data ann output time occur learning prediction 
low pass curve smoothed version raw data curve raw data lag period price low frequency filtering ended 
constraint neural network learning lag today low frequency target network output actual raw data fig 

illustration additional constraint lag period 
raw data center low pass data ann outputs training day iterative predictions fig 

predictions lag period tap filter generally centers true low pass curve consequently curve ann outputs fig 

observation motivates add new constraint difference raw data ann outputs lag period lag lag 
new constrained formulation ann learning noisy time series solved 
experimental results conducted experiments low pass filtering time series tap filter incurs unit lag 
extending raw data flat extensions low pass filtering extended data experimentally low pass values lag period considerably large errors fig 

shows low pass values lag period training iterative predictions 
true predictions start 
test approach lead accurate predictions data daily closing prices ibm symbol ibm symbol exxon symbol april wah qian march 
constructed predictors cc carbon copy simply copies available data ar autoregression order implementation nn unconstrained formulation trained bp input node hidden nodes output node feedbacks hidden input layers output input layers tap fir structure feedback link lnn constrained formulation constraints lag period predicted data values lag period trained structure nn ip ideal predictor lnn true values lag period 
ip establishes approximate upper bound prediction accuracy uses error free low pass data lag period available 
compare prediction accuracy nmse widely metric called hit rate financial time series predictions 
sign actual direction change sign predicted direction change 
call prediction horizon hit define hit rate follows represents number elements set shows predictors period april march 
predictions start april initial window days training prediction 
ar nn perform need predict iteratively day lag period predicting 
lnn improves significantly cc ar especially small horizons outperforms traditional nn horizons 
shows lnn errors closest ip small prediction horizons achieves slightly better nmse longer horizons 
note ip gives approximate upper bound accuracy 
plots hit rates predictors 
shows cc ar nn behave random walks day horizons chance predict correct direction price nmse nmse nmse cc ar nn lnn ip 
constraint neural network learning rate rate cc ar nn lnn ip horizon horizon ibm ibm cc ar nn lnn ip cc ar nn lnn ip horizon horizon exxon exxon cc ar nn lnn ip cc ar nn lnn ip horizon horizon nmse hit rate fig 

performance predictors tested daily closing prices ibm exxon changes 
hand lnn achieve hit rates significantly higher small horizons days 
ip performs better lnn small horizons days performs statistically lnn large horizons days 
plots predictions lnn day closing prices ibm days compared actual low pass data 
results show predictions track actual low pass data 
analyze probability random walk achieve level prediction accuracy lnn conclude lnn random walk 
random walk probability achieve hit prediction probability achieve hits predictions governed binomial probability prob hits predictions rate 

pk 


shown probability random walk achieve hits predictions hit rate probability achieve hits hit rate wah qian price low pass target prediction day index fig 

day predictions ibm low pass daily closing prices random walk achieve level prediction accuracy lnn 
considering results literature financial time series forecasting achieve day hit rates results competitive 
studied predictions noise free noisy time series anns 
noise free possibly piecewise chaotic time series proposed new constrained formulations ann learning allow multiple learning criteria prior knowledge included 
criteria include testing errors multiple validation sets model regime changes piecewise chaotic time series error ann learned validated objective testing 
proposed new architecture combines recurrent structure memory fir structure incorporates violation guided backpropagation algorithm theory lagrange multipliers discrete constrained optimization 
noisy time series high frequency random noise studied systematically edge effect due low pass filtering noisy time series 
predict missing low pass data lag period developed approach estimates low pass values lag period raw data extended flat extensions incorporates new constraints predicted low pass data lag period 
new constraints enable active training lag period greatly improves prediction accuracy period 
aoki state space modeling time series springer verlag 
constraint neural network learning dynamical recurrent neural networks prediction modeling dynamical systems 
neurocomputing ord automatic neural network modeling univariate time series 
int forecasting box jenkins time series analysis forecasting control nd ed 
holden day san francisco brooks burke benchmarks accuracy garch model estimation 
int forecasting brown smoothing forecasting prediction prentice hall englewood cliffs nj chatfield analysis time series chapman hall london edition chatfield time series forecasting chapman hall crc boca raton florida chen billings non linear system identification neural networks 
int control regime signaling techniques non stationary time series forecasting 
proc 
th int conf 
system sciences hi usa pp 
duda hart pattern classification scene analysis john wiley sons edwards magee technical analysis stock trends john magee springfield ma edition elman finding structure time 
cognitive science flores pearce expert system competition 
int forecasting geva multiscale neural network architecture time series prediction 
ieee trans 
neural networks granger andersen bilinear time series models gutjahr riedmiller daily prediction foreign exchange rate dollar german mark neural networks 
proc 
pp 
dorffner comparative study feedforward recurrent neural networks time series prediction gradient descent learning 
proc 
th european meeting cybernetics systems research pp 
haykin neural networks comprehensive foundation prentice hall nj edition schreiber software package 
www dresden mpg de predicting rank measure stock returns 
theory stochastic processes holmstrom predicting stock market 
technical report series ima tom university sweden horne giles experimental comparison recurrent neural networks 
tesauro touretzky leen eds neural information processing systems mit press cambridge ma pp 
jain murty flynn data clustering review 
acm computing surveys gardner jr anderson fletcher results focus forecasting vs exponential smoothing 
int forecasting wah qian juang rabiner hidden markov models speech recognition 
technometrics kohlmorgen ller pawelzik analysis drifting dynamics neural network hidden markov models 
advances neural information processing systems kaski time series prediction multilayer perceptron fir elman neural networks 
proc 
world congress neural networks pp 
lang hinton development time delayed neural network architecture speech recognition 
technical report cmu cs carnegie mellon university pittsburgh pa andersen newton parzen winkler accuracy extrapolation time series methods results forecasting competition 
int forecasting chatfield lawrence mills ord simmons competition real time forecasting study 
int forecasting competition results implications 
int forecasting masters neural novel hybrid algorithms time series prediction john wiley sons ny comparison accuracy short term foreign exchange forecasting methods 
int forecasting pasteels automatic arima modeling including interventions time series expert software 
int forecasting moody darken fast learning networks locally tuned processing units 
neural computation ller smola tsch sch lkopf kohlmorgen vapnik predicting time series support vector machines 
icann pp 
murtagh wavelet transform multivariate data analysis time series forecasting 
hayashi bock tanaka baba editors data science classification related methods pp 
springer verlag varying coefficient regression 
hannan krishnaiah rao eds handbook statistics north holland amsterdam pp 
quinlan induction decision trees 
machine learning ramaswamy step prediction financial time series bis working 
technical report bank settlements switzerland shang wah global optimization neural network training 
ieee computer march tong nonlinear time series dynamical system approach oxford university press oxford wah chen constrained genetic algorithms applications nonlinear constrained optimization 
proc 
int conf 
tools artificial intelligence ieee november pp 
wah qian constrained formulations neural network training applications solve spiral problem 
proc 
fifth int conf 
computer science informatics pp 
february 
constraint neural network learning wah qian time series predictions constrained formulations neural network training cross validation 
proc 
int conf 
intelligent information processing th ifip world computer congress kluwer academic press august pp 
wah qian violation guided learning constrained formulations neural network time series prediction 
proc 
int joint conference artificial intelligence ijcai aug pp 
wah qian violation guided neural network learning fo constrained formulations time series predictions 
int journal computational intelligence applications december wah qian constrained formulations algorithms stock price predictions recurrent fir neural networks 
proc 
national conf 
artificial intelligence aaai accepted appear wah wu theory discrete lagrange multipliers nonlinear discrete optimization 
principles practice constraint programming pp 
october wan temporal backpropagation fir neural networks 
ieee int joint conf 
neural networks pp 
san diego ca wan finite impulse response neural networks applications time series prediction 
ph thesis standford university watkins models delayed reinforcement learning 
ph thesis cambridge university cambridge uk weigend gershenfeld eds time series prediction forecasting understanding past addison wesley williams zipser learning algorithm continually running fully recurrent neural networks 
neural computation wu theory applications nonlinear constrained optimization lagrange multipliers 
ph thesis dept computer science univ illinois urbana il may zhang jabri flower multiresolution forecasting trading wavelet decompositions 
ieee trans 
neural networks zheng starck campbell murtagh multiscale transforms filtering financial data streams 
computational intelligence finance 
