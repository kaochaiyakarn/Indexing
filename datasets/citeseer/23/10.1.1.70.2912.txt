dependent gaussian processes phillip boyle marcus frean school mathematical computing sciences victoria university wellington wellington new zealand marcus mcs ac nz gaussian processes usually parameterised terms covariance functions 
difficult deal multiple outputs ensuring covariance matrix positive definite problematic 
alternative formulation treat gaussian processes white noise sources convolved smoothing kernels parameterise kernel 
extend gaussian processes handle multiple coupled outputs 
gaussian process regression desirable properties ease obtaining expressing uncertainty predictions ability capture wide variety behaviour simple parameterisation natural bayesian interpretation 
suggested replacements supervised neural networks non linear regression extended handle classification tasks variety ways :10.1.1.25.8841
gaussian process gp set jointly gaussian random variables completely characterised covariance matrix entries determined covariance function 
traditionally models specified parameterising covariance function function specifying covariance output values input vectors 
general needs positive definite function ensure positive definiteness covariance matrix 
gp implementations model single output variable 
attempts handle multiple outputs generally involve independent model output method known multi kriging models capture structure outputs 
example consider tightly coupled outputs shown top output simply shifted version 
detailed knowledge output sampling output sparse 
model treats outputs independent exploit obvious similarity intuitively predictions output learn output 
joint predictions possible kriging problematic clear covariance functions defined 
known positive definite autocovariance functions gaussians difficult define cross covariance functions result positive definite covariance matrices 
contrast neural network modelling handling multiple outputs routine 
alternative directly parameterising covariance functions treat gps outputs stable linear filters 
linear filter output response input defines impulse response filter denotes convolution 
provided linear filter stable gaussian white noise output process necessarily gaussian process 
possible characterise dimensional stable linear filters inputs outputs set impulse responses 
general resulting outputs dependent gaussian processes 
model multiple dependent outputs parameterising set impulse responses multiple output linear filter inferring parameter values data observe 
specifying parameterising positive definite covariance functions specify parameterise impulse responses 
restriction filter linear stable achieved requiring impulse responses absolutely integrable 
constructing gps stimulating linear filters gaussian noise equivalent constructing gps kernel convolutions 
gaussian process constructed region convolving continuous white noise process smoothing kernel 
added second white noise source representing measurement uncertainty gives model observations view gps shown graphical form 
convolution approach formulate flexible nonstationary covariance functions 
furthermore idea extended model multiple dependent output processes assuming single common latent process 
example dependent processes constructed shared dependence follows union disjoint subspaces 
dependent 
similarly dependent 
allows possess independent components 
model multiple outputs somewhat differently 
assuming single latent process defined union subspaces assume multiple latent processes defined outputs may dependent shared reliance common latent processes outputs may possess unique independent features connection latent process affects output 
dependent outputs consider outputs region obser output observations output giving data wish learn model combined data order predict shown model output linear sum stationary gaussian processes 
arises noise source unique output convolution kernel second similar arises separate noise source influences outputs different kernels 
third additive noise 
yi ui vi wi wi stationary gaussian white noise process variance independent stationary gaussian white noise processes gaussian processes ui ki vi hi xi 
gaussian process prior single output 
output sum gaussian white noise processes convolved kernel 
model dependent outputs 
noise contributions independent gaussian white noise sources 
notice forced zero independent processes control model 
parameterised gaussian kernels exp st exp st bis note offset zero allow modelling outputs coupled translated relative 
hi wi exp wish derive set functions cy ij define autocovariance cross covariance outputs separation expressed arbitrary inputs sa sb 
solving convolution integral cy ij closed form fully determined parameters gaussian kernels noise variances follows ab ab ii ii ai exp dt aid exp exp bi exp dt bid 
ij construct covariance matrices follows cij ij si sj ij si sj nj 
cy ij si ni sj cy si ni ij sj nj define positive definite symmetric covariance matrix combined output data define set hyperparameters parameterise 
calculate likelihood log yt log function learning model corresponds maximising likelihood maximising posterior probability 
alternatively simulate predictive distribution samples joint markov chain monte carlo methods 
predictive distribution point output gaussian mean variance kt ii 
cy 
example strongly dependent outputs input space consider outputs observed input space 
ai exp fi bi exp gi exp 
hyperparameters element scalar 
set gaussian priors 
generated data points samples output samples output 
samples output linearly spaced interval output uniformly spaced region 
samples taken additive gaussian noise 
build model maximised multistart conjugate gradient algorithm starts sampling initial conditions 
resulting dependent model shown independent control model coupling see 
observe dependent model learned coupling translation outputs filled output samples missing 
control model achieve consists independent gaussian processes 
example strongly dependent outputs input space consider outputs observed input space 
ai bi identity matrix 
furthermore exp 
toy example set hyperparameters element scalar 
set gaussian priors 
output independent model true function model mean output dependent model output independent model output dependent model strongly dependent outputs output simply translated version output independent gaussian noise 
solid lines represent model dotted lines true function dots samples 
shaded regions represent error bars model prediction 
top independent model outputs 
bottom dependent model 
generated data points samples output samples output 
sets samples formed uniform lattices region taken additive gaussian noise 
build model maximised 
dependent model shown independent control model 
dependent model filled output samples missing 
control model achieve filling consists independent gaussian processes 
time series forecasting consider observation multiple time series series lead predict 
simulated set time series steps series positively coupled lagged version series lag negatively coupled lagged version series lag 
observations built dependent gp model time series compared independent gp models 
dependent gp model incorporated prior belief series coupled series lags unknown 
independent gp model assumed coupling outputs consisted independent gp models 
queried models forecasts values series 
clear dependent gp model far better job forecasting dependent series 
independent model inaccurate just time steps 
inaccuracy expected knowledge series required accurately predict series 
strongly dependent outputs output simply copy output independent gaussian noise 
top independent model outputs 
bottom dependent model 
output modelled models 
output modelled dependent model dependent gp model performs learned series positively coupled lagged version series negatively coupled lagged version series 
multiple outputs non stationary kernels convolution framework described constructing gps extended build models capable modelling outputs defined dimensional input space 
general define model assume independent gaussian white noise processes 
xm outputs 
un kernels kmn autocovariance cross covariance functions output processes ij kmi ds matrix defined equation extended obvious way 
kernels need gaussian need spatially invariant stationary 
require kernels absolutely integrable 
dps 
provides large degree flexibility easy condition 
absolutely integrable kernel easier define parameterise positive definite function 
hand require closed form ij may attainable non gaussian kernels 
series series series coupled time series series series predict series 
forecasting series begins time steps 
dependent model forecast shown solid line independent control forecast shown broken line 
dependent model far better job forecasting steps series black dots 
shown gaussian process framework extended multiple output variables assuming independent 
multiple processes handled inferring convolution kernels covariance functions 
easy construct required positive definite covariance matrices outputs 
application learn spatial translations outputs 
framework developed general model data arises multiple sources shared 
examples show sparsely sampled regions possible model permits coupling outputs 
application forecasting dependent time series 
example shows learning couplings multiple time series may aid forecasting particularly series forecast dependent previous current values series 
dependent gaussian processes particularly valuable cases output expensive sample strongly second cheap 
inferring coupling independent aspects data cheap observations proxy expensive ones 
review gaussian random fields correlation functions 
tech 
rep norwegian computing center box blindern oslo norway 
boyle frean multiple output gaussian process regression 
tech 
rep victoria university wellington 
cressie statistics spatial data 
wiley 
gibbs bayesian gaussian processes classification regression 
phd thesis university cambridge cambridge 
gibbs mackay efficient implementation gaussian processes 
www inference phy cam ac uk mackay abstracts html 
gibbs mackay variational gaussian process classifiers 
ieee trans 
neural networks 
higdon space space time modelling process convolutions 
quantitative methods current environmental issues anderson barnett el eds springer verlag pp 

mackay gaussian processes replacement supervised neural networks 
nips tutorial 
mackay information theory inference learning algorithms 
cambridge university press 
neal probabilistic inference markov chain monte carlo methods 
tech 
report crg tr dept computer science univ toronto 
neal monte carlo implementation gaussian process models bayesian regression classification 
tech 
rep crg tr dept computer science univ toronto 
nonstationary gaussian processes regression spatial modelling 
phd thesis carnegie mellon university pittsburgh pennsylvania 
schervish nonstationary covariance functions gaussian process regression 
submitted nips 
rasmussen gaussian processes reinforcement learning 
advances neural information processing systems vol 

rasmussen evaluation gaussian processes methods non linear regression 
phd thesis graduate department computer science university toronto 
tipping bishop bayesian image super resolution 
advances neural information processing systems becker thrun obermayer eds vol 
pp 

williams barber bayesian classification gaussian processes 
ieee trans 
pattern analysis machine intelligence 
williams rasmussen gaussian processes regression 
advances neural information processing systems mozer hasselmo eds vol 

