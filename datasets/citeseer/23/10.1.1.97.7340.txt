reprinted journal documentation pp understanding inverse document frequency theoretical arguments idf stephen robertson microsoft research jj thomson avenue cambridge cb fb uk city university london uk term weighting function known idf proposed extremely widely usually part tf idf function 
described heuristic papers written shannon information theory seeking establish theoretical basis 
attempts reviewed shown information theory approaches problematic theoretical justifications idf tf idf traditional probabilistic model information retrieval 
karen sp jones published journal documentation called statistical interpretation term specificity application retrieval sparck jones 
measure term specificity proposed known inverse document frequency idf counting number documents collection searched contain indexed term question 
intuition query term occurs documents discriminator weight occurs documents measure heuristic implementation intuition 
intuition measure associated proved giant leap field information retrieval 
coupled tf frequency term document case better way term weighting scheme 
class weighting schemes known generically tf idf involve multiplying idf measure possibly number variants tf measure possibly number variants just raw count proved extraordinarily robust difficult beat carefully worked models theories 
way outside text retrieval methods retrieval media language processing techniques purposes 
recurring theme papers idf heuristic nature 
led authors look theoretical explanations idf works 
number papers start premise idf purely heuristic device claim provided theoretical basis quite startling papers discussed 
purpose discuss various approaches challenge demonstrate theoretical basis idf explanation effectiveness 
basic formula assume documents collection term ti occurs ni 
constitute term concern may assume terms words possibly phrases word stems 
occurs taken shorthand index term ignoring difficulties subtleties automatic indexing natural language text human assignment index terms 
measure proposed sparck jones weight applied term ti essentially idf ti log quite accurate original measure integer approximation formula logarithm specifically base 
seen base logarithm general important 
equation commonly cited form idf forms indicated 
zipf law original sparck jones small theoretical claims measure appealing intuition outlined 
wellknown zipf law concerning term frequencies zipf 
version zipf law rank words order decreasing frequency large body text plot graph log frequency log rank get straight line see 
appropriate put terms buckets dividing axis equal intervals straight line correspond equal intervals axis shown 
essentially integer approximation original idf 
argument strong 
hard see formal justification statement appropriate 

term frequency zipf law number occurrences term body continuous text term frequency idf number documents term occurs irrespective times occurs document 
distinction example event space problem discussed 
probabilities logarithms additivity fraction inside logarithm equation looks represent probability inverted 
consider probability random document ni log frequency zipf law log rank contain term robertson 
probability obvious estimate inverse fraction idf equation ti ti occurs ni light relation reasonably redefine idf terms probability regard observed idf estimate true idf idf ti log ti recall log log 
important feature term weighting schemes frequently assumed document scoring functions essentially additive 
example query terms give simple weights common simple scoring function score document sum weights query terms contains 
document containing terms score containing terms score 
clearly scoring functions considerably complex encounter addition separate term components basic assumption 
combine probabilistic interpretation idf idea addition scoring function simply elegantly assume occurrences different terms documents statistically independent addition correct thing logs 
idf log log log log idf idf represents term boolean 
argument applies number terms 
logs adding weights exactly right thing 
may note reason logs able add base logarithm matter 
course really assume terms statistically independent 
simplifying assumption suggests strongly log say linear function fraction ni equally compatible original intuition 
shannon information theory log function probability suggests connection shannon theory information communication 
shannon theory information capacity channel communication measured entropy formula nm pi log pi pi probability receiving message mi 
assumed exactly nm possible messages mi received known probabilities pi 
note constraint know pi note base logarithm units bits 
indicated base matter purposes 
model extended multiple messages coming channel example apply succession words message right 
identification information capacity measure entropy shannon subsequently 
possible interpretation shannon measure individual message carries quantified amount information ii log pi information capacity channel defined expected amount information message 
definition gives exactly equation nm piii pi log pi tempted link interpretation idf formula regarding idf measuring amount information carried term 
lines done times including author robertson 
problems view idf discussed 
useful define concepts associated shannon entropy 
messages random variables shannon definition entropy thinking discrete random variables messages 
recall definition random variable 
require event space space possible random events deterministic function outcome event 
require probability measure kind 
ideally probability measure defined event space probability associated value random variable derived probability class events map value 
class events just aggregate event probability sum probabilities elementary events class 
may define probability measure terms values random variable 
shannon case random event just arrival message 
outcome specific message mi probability pi 
event space space possible specific messages random variable just identity message 
note want deal succession words just single word redefine event space random variable easy take fixed window words hard want consider messages variable length 
event space discrete random variable values xi entropy defined exactly equation pi probability xi nm number distinct values define quantities interest mutual entropy random variables conditional entropy random variable mutual information variables kullback leibler divergence probability distributions 
closely related see 
general quantities defined entire probability distributions obviously relate single elementary aggregate events 
fairly common notion pairwise pointwise mutual information relate single events 
transition probability distribution measures pointwise measures line ii problematic see appendix illustration 
ir interpretation problem application ideas ir lies identification event space 
number entities consider environment example documents terms queries 
discussion robertson relating indication complexity issue 
essence problem hard identify single event space probability measure relevant random variables defined 
unifying event space process involves comparing mixing different measures process simple adding different terms may invalid terms shannon formalism 
obvious interpretation equation consider event space space documents 
think document list terms ignoring duplication tf point consider term turn single event message 
view problematic complete message document variable length hard see relate term event space document event space recover probability defined space 
alternatively think document vector zeros ones representing presence absence term vocabulary 
get variable length problem length document vector fixed size vocabulary 
effect decompose single event document cross product spaces elements 
cross product event spaces problematic reasons discussed robertson 
going technicalities identify new problem introduced new set probabilities element probability pi probability zero pi 
weights quantities log ti representing absence term 
matter typically ti small ti close log close zero 
serious problem 
search weighted terms typically take query terms assign weights ignore terms vocabulary 
hard see practice sense shannon model term document assumed carry information 
presence term ti carry log ti amount information irrespective query 
link amount information specific query 
justification leaving calculation 
similarity usual idf formulation component entropy stimulated researchers try connections somewhat differently link suggested 
efforts discussed briefly 
author different event spaces space documents space query terms space terms document texts 
leads restrictive assumptions relationships relevant probability distributions claim assumptions represent heuristic tf idf employs 
encounters problem single events mentioned uses conditional entropy conditional event random variable 
effect introduces event spaces example space documents contain particular term 
probabilities event space estimated data event space 
analysis may shed light complexities involved difficult see explanation value idf 
certainly ways interpret heuristic tf idf employs discussed 
siegler witbrock shorter siegler witbrock defines queries documents mappings words probabilities words event space space words different probability measures confusingly give notation 
conditional entropy mutual information defined respect conditioning event word conditioning random variable 
appear derive result idf value term exactly mutual information document collection term derivation contains obvious errors algebra 
despite log form traditional idf measure strong relationship ideas shannon information theory elusive 
relationship give strong explanation validation justification value idf tf idf term weighting method information retrieval 
interpretation papineni papineni appeal information theory ideas maximum entropy kullback leibler distance mutual information main focus proof optimal performance 
question asks document query best classifier retrieve document 
answer independence assumption give terms idf weights 
papineni fact starts claims heuristic nature idf model equivalent simplified version older relevance probabilistic model described section 
essentially assumes query relevant document query terms guaranteed document 
assumptions necessary 
papineni argument benefits depend simplification giving handle dealing dependencies terms bigrams distinguishing term weight gain including term feature classifier 
distinction earlier ir field see robertson 
classical probabilistic model ir interpretation idf function probability suggests associating probabilistic approaches information retrieval 
range different approaches ir described broadly probabilistic including number comparatively developments machine learning statistical language modelling techniques 
start discussion probability ranking principle search term weighting explicit relevance variable 
general approach discussed van rijsbergen series models described including referred rsj bm developed sparck jones walker robertson 
quite obvious idf relate relevance variable emerge 
probability ranking principle robertson states optimal performance documents ranked order probability relevance request information need calculated evidence available system 
essentially explicit connection ranking documents system may give user effectiveness system terms conventional experimental measures performance recall precision 
relevance weights relevance weighting model robertson sparck jones referred rsj shows simple assumptions probability relevance ordering achieved assigning weights query terms scoring document adding weights query terms contains 
version model evidence documents presence absence terms term frequency document discussed 
term weight may defined follows 
define probabilities term ti term weight pi document contains ti document relevant qi document contains ti document relevant rsj weight log pi qi pi qi consider estimate probabilities term weight 
initially assume purpose know documents relevant 
assumption course unrealistic search process 
consider cases partial relevance information know relevant documents 
total documents ni contain terms relevant ri relevant documents contain term obvious estimates pi ri qi ni ri form equation essentially log odds straight probabilities modification estimates appropriate 
usual modification may justified various ways discussed robertson sparck jones effect estimate rsj weight added components seen smoothing correction wi log ri ni ri ri ni ri assumptions relevance weighting model include assumptions statistical independence terms somewhat quite independence assumption section 
difference terms assumed independent set relevant documents set non relevant documents 
general independent collection documents taken fact query terms general positively correlated collection independent relevance 
positive correlation assumptions just slightly realistic section 
serve characterise relevance weighting model na bayesian model current terminology machine learning literature 
independence assumptions rsj weights additive 
best information term presence absence information probability ranking principle weight terms score documents sum weights terms documents rank documents score order 
strong result relevance weighting theory fact form proof optimal performance general papineni discussed 
query terms relevance weighting theory normally apply weights query terms 
corresponds assumption absence evidence contrary may take value rsj weight equation zero non query terms 
non query terms assumed correlated relevance implies giving zero weight leaving calculation entirely correct 
necessarily realistic assumption clear large difference probabilistic model shannon view 
probabilistic model assumption clearly ties practice restricting search query terms 
concentrate term weighting issue 
worth mentioning partial relevance information discussed section avoid assumption non query terms consider query expansion 
look evidence terms correlated relevance expand original query 
relevance weighting theory provides principled reason restrict matching query terms choose additional terms query 
noted choice terms add query determined optimal term weight searching requires related different measure effect adding term query robertson papineni refers gain papineni 
little relevance information suppose little relevance information user knows advance seen judged just documents marked relevant 
obvious way estimate pi equation smoothed form embedded equation ri number documents known relevant contain term 
appropriate estimate qi somewhat obvious base number documents known non relevant small number 
hand usually know sure documents collection non relevant usual information retrieval situation 
take complement known relevant documents 
documents nonrelevant give somewhat biased probably reliable estimate 
complement method shown 
estimation formula wi remain exactly equation ri representing known relevant documents 
correction particularly appropriate small samples 
relevance information relevance information 
concerning probability pi relating relevant documents view information 
basis probably best assume fixed value pi croft harper 
concerning qi assumption previous paragraph collection consists large extent non relevant documents 
extreme version complement method described purpose estimating qi assume documents non relevant 
qi estimated entire collection 
assumptions applied equation simple estimates equation yield ni wi log ni log estimates equation equation setting get wi log ni ni result setting point correction set 
versions look idf measures 
ni top fraction little different original equation difference little practical importance situations ni typically smaller similarly smoothing correction generally small effect 
ni top cause strange effects 
rare case term occurs half documents collection formulae defines negative weight 
somewhat odd prediction query term presence count retrieval 
original formula simply give small positive weight 
problem investigated robertson walker argue fixed pi source problem reasonable frequently occurring terms 
show modification assumption pi increases increasing frequency collection avoids anomalous behaviour removing ni numerator formulae 
equation replaced equation idf rsj weight wi log wi log ni ni apparent regard idf simple version rsj weight applicable relevance information 
fact strong justification idf 
appeals notions information theory simple na bayes model retrieval plausible simplifying assumptions 
na bayes model explicit relevance variable simplifying assumptions render variable implicit idf regard idf direct measure probability relevance 
characteristic relevance weight justification idf considerably stronger information theory justifications considered 
tf language models original sparck jones proposal idf relevance weighting model robertson sparck jones document term frequency information tf 
far arguments tf tf combined idf tf idf style measures mentioned 
tf idf style measures emerged extensive empirical studies combinations weighting factors particularly cornell group salton yang understand explain effectiveness measures need justification just tf factor want multiply log idf weight tf component 
follows denote tf frequency number occurrences term ti document consideration 
need talk multiple documents refer tf ti document dj 
dl dl refer length document number term positions summing terms document vocabulary logs additivity revisited dl tf simple independence argument section applied multiple occurrences term occurrences different terms 
assume successive occurrences single term independent number times seen term document affect probability see simply add wi time see term ti 
cumulate complete document get tf occurrences term ti simply multiply idf weight wi number occurrences tf simple argument conceals number problems 
major longer operating event space documents looking text document 
event seeing particular term relates particular term position document 
consequences problem need take account document length original basis regarded ni probability gone 
issue fatal pulled rug idea idf 
possible way get away problem fairly radical replacement idf radical principle may radical terms practical effects 
replace ni tf dl transferring probability event space documents event space term positions concatenated text documents collection 
new measure called inverse total term frequency ti log dl tf interpreted log inverse probability probability term position event space document event space 
referred inverse collection frequency idf referred collection frequency weight particular original sparck jones terminology avoided 
experiments inverse total term frequency weights tended show effective idf weights 
church gale show measures differ significantly exactly terms interested 
seen preserve document event space idf fit naturally introduce tf component 
formulations tf idf seen concerns tf example attempt information theory derivation tf idf weighting scheme seen problems formulation 
joachims approach taken joachims appeals information theory na bayes machine learning models relevance weighting theory joachims task usual ranked retrieval task categorisation task 
confronts event space problem part redefining idf somewhat similar fashion inverse total term frequency measure discussed quite concatenate texts documents takes measure term occurrences document combines measure documents different way 
replaces usual log function idf square root function 
version idf final formulation quite significantly different traditional idf 
way various assumptions classes categorisation task approximately number documents reminiscent papineni assumption just relevant document query 
analysis idf starts somewhat confusingly idf proposal consider quantity probability difficult see kind event space look probability estimate 
go consider separate cross product event spaces similar considerations discussed documents collection terms vocabulary event term occurs document document locations term positions document terms vocabulary event term occurs position 
clearly tf meaning space 
draws parallels spaces alternative idf space alternative looks different idf 
analogue tf idf formula 
language models great deal application statistical language models lm information retrieval see croft lafferty 
statistical language model successive word position document query regarded slot filled word generative process envisaged slots filled basis probabilities vocabulary 
models operate term position event space identified document event space may separate language model document 
original applications models speech recognition successful probability distribution slot typically depend small amount history normally previous words 
lm ir unigram models probabilities position document assumed independent 
important realise lm approach thinks sequence term slots positions probabilities assumed independent 
relationship models matters discussed little unclear 
standard simple language models ir ponte croft explicit idf tf achieve similar effect tf idf measure 
possible closer spirit inverse total term frequency measure defined operating similar event space 
difference simple language models direct relevance 
field explicit relevance variable lavrenko croft 
poisson model bm weighting function formulated language modelling idea came 
may recast elementary form language model 
view explored section 
tf relevance weighting model relative success reported explanation idf original relevance weighting model sense look explanation tf idf extension relevance weighting model accommodates tf 
weighting function known okapi bm developed part okapi experimental system robertson series best match functions probably best known widely extension 
poisson model brief account derivation bm 
attempt give full technical detail 
original argument robertson walker analysis various components sparck jones 

discussion idea language modelling discussed 
assume documents length assumption dropped 
assume central assumption model term word represents concept document concept 
property described term elite document 
terminology statistical model described taken bookstein swanson harter 
hidden variable observe directly 
assume text document generated simple unigram language model probability term position depend term document 
take documents particular term elite infer distribution document frequencies observe 
documents length distribution approximately poisson 
take documents term elite see poisson distribution presumably smaller mean 
course know advance consider collection observe mixture poisson distributions 
poisson mixture basic bookstein swanson harter model document term frequencies 
provides bridge event space documents cross product documents collection terms vocabulary event space term positions gives term frequencies 
term property document determines properties term positions document 
connection relevance 
relevance property document level connection term occurrences bridge query document terms 
know having done original relevance weighting model 
re relevance weighting model applying query term query term presence absence 
arguments formulate weighting scheme involves parameters query term mean poisson distribution document term frequencies elite documents ditto non elite documents mixing proportion elite non elite documents collection probability relevance probability non relevance 
full equation robertson walker 
problem scheme parameters query term need estimated general relevance hidden variables 
full equation unusable 
possible simplify point feasible think 
time introduce normalisation deal fact documents vary length 
bm bm weighting function analysis behaviour full model different values parameters 
essentially term full weight weight document gain knew term elite document 
know tf value provides probabilistic evidence give partial credit document 
credit rises monotonically zero tf approaches full weight asymptotically tf increases 
general occurrence term gives evidence successive occurrences give successively smaller increases 
bm estimates full weight usual presence rsj weight term approximates tf behaviour single global parameter controlling rate approach 
correction document length 
document length normalisation really relevant discussion completeness 
assumption reason document long verbosity part author suggest simple document length normalisation tf second reason topic coverage document suggest normalisation bm partial normalisation 
degree normalisation controlled second global parameter order normalisation relatively independent units document length measured defined terms average length documents collection 
resulting formula expressed bm weight wi tf usual rsj weight tf tf tf dl avdl dl avdl document length average document length respectively global parameters general unknown may tuned basis evaluation data 
tf idf bm weight clear formula equation expresses bm tf idf weighting scheme 
component tf component important characteristic highly non linear consequence defining ceiling contribution term document score 
second component rsj weight seen may relevance information exists reduces idf measure absence 
theoretical argument bm seen justify tf idf sense explain works just way relevance weighting theory explains idf 
basic search term weighting formula known idf proposed sparck jones heuristic grounds proved extraordinarily robust 
remains core ranking methods search engines 
developments specifically language models longer explicit component models similar effect achieved somewhat different means 
idf may disappear explicit component live spirit 
idf proved magnet researchers feel inspired replace perceive heuristic reasonably constituted theoretical argument order explain idf works 
contributions start assertion idf just heuristic known understood theoretical basis 
theoretical explanation justification idf completely straightforward 
superficially easy step interpret idf probabilistic function major problem area lies definition appropriate event space probabilities concerned 
problem undermines attempted explanations 
fertile source attempts shannon theory information idf usually interpreted specifically particular kind probabilistic function measure amount information carried term 
attempts run serious problems 
relatively simple explanation justification idf relevance weighting theory 
extends justification tf idf okapi bm model 
idf simply pure heuristic theoretical mystery 
pretty idea works 
am grateful karen sp jones useful comments discussions 
notes history range influence idf described places harman forthcoming 
association term separately relevance induces association terms collection relevance ignored 
early conducted short document surrogates short catalogue records scientific abstracts full text documents 
conditions tf important subsequently full text documents 
furthermore document length component tf measure generally unimportant typically document surrogates collection varied little length 
critical importance document length normalisation emerged 
actual definition document length somewhat complicated depending factors stopwords indexing phrases measured variety ways issues ignored discussion 

information theoretic perspective tf idf measures information processing management vol 
pp 

bookstein swanson 
probabilistic models automatic indexing journal american society information science vol 
pp 

church gale 
inverse document frequency idf measure deviations poisson yarowsky church eds third workshop large corpora acl mit pp 

croft lafferty 
eds language modelling information retrieval kluwer 
croft harper 
probabilistic models information retrieval relevance information journal documentation vol 
pp 

gray 
entropy information theory springer verlag 
harman 
forthcoming history idf influences ir fields tait ed 
harter 
probabilistic approach automatic keyword indexing parts journal american society information science vol 
pp 

joachims 
probabilistic analysis rocchio algorithm tfidf text categorization fisher ed proceedings icml th international conference machine learning morgan kaufmann publishers san francisco nashville pp 

lavrenko croft 
relevance language models croft harper kraft zobel eds sigir proceedings th annual international acm sigir conference research development information retrieval acm press new york pp 

papineni 
inverse document frequency proceedings north american association computational linguistics naacl pp 

ponte croft 
language modeling approach information retrieval croft moffat van rijsbergen wilkinson zobel eds sigir proceedings st annual international acm sigir conference research development information retrieval acm press new york pp 

robertson 
event spaces probabilistic models information retrieval information retrieval vol 
pp 
appear 
sigir workshop mathematical formal methods information retrieval submitted information retrieval 
robertson 
term specificity letter editor journal documentation vol 
pp 

robertson 
specificity weighted retrieval documentation note journal documentation vol 
pp 

robertson 
probability ranking principle information retrieval journal documentation vol 
pp 

robertson 
term selection query expansion journal documentation vol 
pp 

robertson 
overview okapi projects special issue journal documentation journal documentation vol 
pp 

robertson sparck jones 
relevance weighting search terms journal american society information science vol 
pp 

robertson walker 
simple effective approximations poisson model probabilistic weighted retrieval croft van rijsbergen eds sigir proceedings th annual international acm sigir conference research development information retrieval springer verlag pp 

robertson walker 
relevance weights little relevance information belkin narasimhalu willett eds sigir proceedings th annual international acm sigir conference research development information retrieval acm press new york pp 


frequency poisson definition probability informative callan cormack clarke hawking smeaton eds sigir proceedings th annual international acm sigir conference research development information retrieval acm press new york pp 

salton yang 
specification term values automatic indexing journal documentation vol 
pp 

shannon 
mathematical theory communication bell system technical journal vol 
pp 

siegler witbrock 
improving suitability imperfect transcriptions information retrieval spoken documents proceedings ieee international conference acoustics speech signal processing icassp ieee press piscataway nj pp 

sparck jones 
statistical interpretation term specificity application retrieval journal documentation vol 
pp 

sparck jones walker robertson 
probabilistic model information retrieval development comparative experiments information processing management vol 
pp 
part part 
van rijsbergen 
information retrieval butterworths london 
second edition 
zipf 
human behavior principle effort addison wesley reading ma 
appendix entropy pointwise information section defined shannon notion information capacity channel entropy apparently associated pointwise information measure separate message mi 
equations reproduced convenience nm pi log pi ii log pi noted definition interpreted expected value ii 
mathematically speaking expected value ii order interpretation accept sense measure amount information individual message equate capacity channel average information content messages carries 
notion appears shannon plays role results 
may explore notion example 
suppose base logarithms quantities may expressed bits 
suppose concerned sequences independent binary messages nm individual message 
bit 
message carries shannon terms bit information 
sequence independent messages carry bits 
expect 
say considerably reduced half bit 
statement mean 
answer lies shannon theoretical results 
express primary result applied example follows 
long sequence independent messages possible recode approximately binary messages loss information recipient exactly recover original sequence 
words imbalance probabilities compress data factor approximately 
powerful result 
theory tell recoding various methods exist approaching optimum different degrees merely possible 
note result little individual probabilities imbalance 
consider pointwise measure equation 
assume measure represent amount information message receipt message yield bits yield bits 
question mean 
measure plays role theory originally proposed shannon gives guidance 
am aware result developments give kind status enjoyed fact argued interpretation odds shannon theory 
shannon theory start waiting message certain probabilistic expectations 
receive discover transformation knowing knowing conveys case half bit information 
contrast pointwise view amount information depends exactly message writer information theory gray information measures important coding theorems exist operational significance intuitively pleasing aspects definitions 
exactly right 
equation definition information measure precisely operational significance 
clear said equation 
