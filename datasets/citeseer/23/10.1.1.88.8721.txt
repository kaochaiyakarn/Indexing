feature subset selection class separability genetic algorithms cant paz center applied scientific computing lawrence livermore national laboratory livermore ca llnl gov 
performance classification algorithms machine learning affected features describe labeled examples inducers 
problem feature subset selection received considerable attention 
genetic approaches problem usually follow wrapper approach treat inducer black box evaluate candidate feature subsets 
evaluations take considerable time traditional approach impractical large data sets 
describes hybrid simple genetic algorithm method class separability applied selection feature subsets classification problems 
proposed hybrid compared components feature selection wrappers widely 
objective determine proposed hybrid presents advantages methods terms accuracy speed problem 
experiments naive bayes classifier public domain artificial data sets 
experiments suggest hybrid usually finds compact feature subsets give accurate results beating execution time wrappers 
problem classification machine learning consists labeled examples induce model classifies objects set known classes 
objects described vector features may irrelevant redundant may negative effect accuracy classifier 
basic approaches feature subset selection wrapper filter methods 
wrappers treat induction algorithm black box search algorithm evaluate candidate feature subset 
giving results terms accuracy final classifier wrapper approaches computationally expensive may impractical large data sets 
filter methods independent classifier select features properties feature sets presumed class separability high correlation target 
filter methods faster wrappers filters may produce disappointing results completely ignore induction algorithm 
presents hybrid algorithm combines strengths filters wrappers attempts avoids weaknesses 
hybrid consists simple genetic algorithm sga traditional role wrapper initialized output filter method class separability metric 
objective study determine hybrid method presents advantages simple gas conventional feature selection algorithms terms accuracy speed applied feature selection problems 
experiments described public domain artificial data sets 
classifier naive bayes simple classifier induced quickly shown accuracy problems 
target maximize accuracy classification 
experiments demonstrate cases proposed hybrid algorithm finds subsets result best accuracy accuracy significantly different best finding compact feature subsets performing faster wrapper methods 
section briefly reviews previous applications eas feature subset selection 
section describes class separability filter hybridization ga section describes algorithms data sets fitness evaluation method experiments reported section 
section concludes summary discussion research directions 
feature selection reducing dimensionality vectors features describe object presents advantages 
mentioned irrelevant redundant features may affect negatively accuracy classification algorithms 
addition reducing number features may help decrease cost acquiring data classification models easier understand 
numerous techniques dimensionality reduction 
common methods seek transformations original variables lower dimensional spaces 
example principal components analysis reduces dimensions data finding orthogonal linear combinations largest variance 
mean square error sense principal components analysis yields optimal linear reduction dimensionality 
necessarily true principal components capture variance useful discriminate objects different classes 
linear combinations variables difficult interpret effect original variables class discrimination 
reasons remainder ignore methods transform features focus techniques select subsets original variables 
feature subset algorithms wrapper methods received considerable attention 
wrappers attractive seek optimize accuracy classifier tailoring solutions specific inducer domain 
search feature subset induction algorithm evaluate merit candidate subsets 
numerous search algorithms search feature subsets 
genetic algorithms usually reported deliver results exceptions reported simpler faster algorithms result higher accuracies particular data sets 
applying gas feature selection problem straightforward chromosomes individuals contain bit feature value bit determines feature classification 
wrapper approach individuals evaluated training classifiers feature subset indicated chromosome resulting accuracy calculate fitness 
siedlecki sklansky describe application gas way 
gas search feature subsets conjunction classification methods neural networks decision trees nearest neighbors rules naive bayes 
selecting feature subsets gas extract new features searching vector numeric coefficients transform linearly original features 
case value zero transformation vector equivalent avoiding feature 
combined linear transformation explicit feature selection flags chromosomes reported advantage pure transformation method 
sophisticated distribution estimation algorithms deas search optimal feature subsets 
deas explicitly identify relationships variables problem building model selected individuals model generate new solutions 
way deas avoid disruption groups related variables prevent simple ga reaching global optimum 
terms accuracy deas outperform simple gas searching feature subsets 
reason limit study simple gas 
wrappers evaluation candidate feature subsets computationally expensive large data sets 
filter methods computationally efficient offer alternative wrappers 
genetic algorithms filters regression problems optimize cost function derived correlation matrix features target value 
gas filter classification problems minimizing inconsistencies subsets features 
inconsistency examples occurs examples match respect feature subset considered class labels disagree 
demonstrated filter method efficiently identifies feature subsets predictive original set features results significantly worse 
accuracy reduced subset different better worse features 
study show proposed method reduce dimensionality data increase predictive accuracy considerably 
class separability idea measure class separability select features machine learning computer vision 
class separability filter propose calculates class separability feature kullback leibler kl distance histograms feature values 
feature histogram class 
numeric features discretized equally spaced bins size training data 
histograms normalized dividing bin count total number elements estimate probability th feature takes value th bin histogram class pj 
feature calculate class separability number classes kl distance histograms corresponding classes pj pj log pj number bins histograms 
course distribution distance metrics kl distance 
features sorted descending order distances larger distances mean better separability 
heuristically consider features redundant distances differ eliminate feature smallest distance 
eliminate irrelevant non discriminative features distances 
heuristics eliminate redundant irrelevant features calibrated artificial data sets described 
recognize heuristics may fail cases thresholds chosen adequate particular classification problem 
major disadvantage method ignores pairwise higher interactions variables 
possible features appear irrelevant discriminative considered relevant considered conjunction variables 
example consider class data displayed 
features discriminative power taken features perfectly discriminate classes 
explore combinations features decided genetic algorithm 
running filter algorithm knowledge relative importance feature considered individually 
knowledge incorporated ga relative distances initialize ga distances linearly normalized obtain probability pj th bit chromosomes initialized corresponding feature selected 
making lower upper limits pj different able explore combinations include features filter eliminated redundant irrelevant 
allows chance delete features filter identified important 
fig 

example data set feature considered discriminate classes features taken discriminate data perfectly 
feature feature ga initialized output filter ga runs wrapper feature selection algorithm 
ga manipulates population candidate feature subsets conventional ga operators 
candidate solution evaluated estimate accuracy classifier feature subset indicated chromosome best solution reported user 
methods section describes algorithms data study method evaluate fitness 
algorithms data sets ga uniform crossover probability mutation probability length chromosomes corresponds total number features problem 
population size set gambler ruin model population sizing asserts population size required reach solution particular quality 
promising solutions selected pairwise binary tournaments replacement 
algorithms terminated observing improvement best individual consecutive generations 
cant paz similar algorithms termination criterion 
compare results class separability filter gas traditional greedy feature selection algorithms 
greedy feature selection algorithms add delete single feature candidate feature subset common 
basic variants sequential forward selection sfs table 
description data experiments 
domain instances classes numeric feat 
nominal feat 
missing anneal arrhythmia euthyroid ionosphere pima segmentation soybean large random redundant sequential backward elimination 
forward selection starts empty set features 
iteration algorithm considers feature subsets feature 
feature subset highest accuracy basis iteration 
iteration algorithm tentatively adds basis feature previously selected retains feature subset results highest estimated performance 
search terminates accuracy current subset improved adding feature 
backward elimination works analogous way starting full set features tentatively deleting feature deleted previously 
classifier experiments naive bayes nb 
classifier chosen speed simplicity proposed hybrid method supervised classifiers 
nb probabilities nominal features estimated data maximum likelihood estimation observed frequencies data applying laplace correction 
numeric features assumed normal distribution 
missing values data skipped 
algorithms developed compiled version optimizations 
experiments executed single processor linux red workstation dual ghz intel xeon processors mb memory 
mersenne twister random number generator ga data partitioning 
data sets experiments described table 
exception random redundant data sets available uci repository 
random redundant artificial data sets features proposed originally 
target concept data sets features closer euclidean distance 
features generated uniformly random range 
features random random fifth ninth features repeated times redundant 
measuring fitness interested classifiers generalize fitness calculations include estimate generalization naive bayes candidate subsets 
estimate generalization network crossvalidation 
fold crossvalidation data partitioned randomly non overlapping sets dk 
iteration classifier trained di tested di 
data partitioned randomly repeated crossvalidation experiments return different results 
known methods deal noisy fitness evaluations eas chose limit uncertainty accuracy estimate repeating fold crossvalidation experiments standard deviation accuracy estimate drops maximum repetitions 
heuristic proposed kohavi john study wrapper methods feature selection adopted 
accuracy estimate fitness function 
crossvalidation expensive computationally cost prohibitive case data sets relatively small nb classifier efficient 
larger data sets inducers deal uncertainty evaluation means increasing slightly population size compensate noise evaluation sampling training data 
defer discussion possible performance improvements final section 
fitness measure include term bias search small feature subsets 
algorithms small subsets data algorithms consistently smallest subsets describe target concepts 
suggests data sets contained irrelevant redundant features decreased accuracy naive bayes 
experiments evaluate generalization accuracy feature selection methods iterations fold crossvalidation cv 
iteration data randomly divided halves 
half input feature selection algorithms 
final feature subset experiment train final nb classifier entire training data tested half data 
accuracy results table mean standard deviations tests 
determine differences algorithms statistically significant combined test proposed alpaydin 
pi denote difference accuracy rates classifiers fold th iteration cv pi pi denote mean pi pi variance pi table 
means standard deviations accuracies cv experiments 
best result significantly different best displayed bold 
domain naive filter sga sfs anneal arrhythmia euthyroid ionosphere random pima redundant segment soybean table 
means standard deviations sizes final feature subsets 
best result significantly different best bold 
domain original filter sga sfs anneal arrhythmia euthyroid ionosphere pima random redundant segmentation soybean large approximately distributed degrees freedom 
rejected null hypothesis algorithms error rate significance level 
care taken ensure algorithms training testing data folds crossvalidation experiments 
table mean accuracies obtained method 
best observed result table highlighted bold type results combined test significantly different best significance level 
immediate observations results 
feature selection algorithms result improvement accuracy nb features 
difference significant soybean large pima 
second proposed hybrid reaches highest accuracy accuracies significantly different highest 
simple ga random initialization performs reaching results significantly different best data sets 
table 
means standard deviations number feature subsets examined algorithm 
best result significantly different best bold 
domain sga sfs anneal arrhythmia euthyroid ionosphere pima random redundant segmentation soybean large terms size final feature subsets table forward sequential selection consistently smallest subsets 
expected algorithm heavily biased small subsets starts empty set adds features show improvements accuracy 
cases sfs resulted significantly worse accuracies proposed ga hybrid 
proposed hybrid significantly substantially smaller feature subsets filter sga 
table shows mean number feature subsets examined algorithm 
cases gas examine fewer subsets sfs examined fewer subsets ga initialized random 
suggests search highly biased solutions 
number examined subsets coarse surrogate execution time actual times depend number features candidate subset may vary considerably expect 
execution times user time cpu seconds entire cv experiments reported table 
filter method time reported includes time compute sort class time evaluate naive bayes feature subset filter method 
proposed filter method far fastest algorithm beating closest competitor orders magnitude 
filter significantly accurate results datasets 
wrapper methods sfs hybrid filter ga fastest 
summarizes tradeoff accuracy table execution time table data sets 
data sets omitted reduce clutter 
graph clearly shows filter orders magnitude faster methods wrappers usually result accuracy improvements 
table 
execution time cpu seconds cv experiments algorithm 
filter method fastest algorithm denoted bold type 
results italics type correspond second fastest algorithm 
domain filter sga sfs anneal arrhythmia euthyroid ionosphere pima random redundant segmentation soybean large experiments proposed ga filter hybrid feature selection classification problems 
results compared simple ga traditional sequential methods filter method simple class separability metric 
experiments considered naive bayes classifier public domain artificial data sets 
data sets tried proposed method accurate solutions solutions significantly different best 
proposed method usually second smallest feature subsets sfs performed faster simple gas sfs methods 
extended experiments evolutionary algorithms classification methods additional data sets alternative class distance metrics 
particular interesting explore methods consider feature time calculate class 
numerous opportunities improve computational efficiency algorithms deal larger data sets 
particular subsampling training sets parallelizing fitness evaluations promising alternatives 
note sfs inherently serial methods benefit parallelism gas 
addition explore efficient methods deal noisy accuracy estimates relatively expensive multiple employed 
acknowledgments conf 
performed auspices department energy university california lawrence livermore national laboratory contract 
eng 
fig 

plot accuracies vs execution time data sets 
algorithms identified labels follows filter sga sfs 
time cpu seconds accuracy euthyroid ionosphere random redundant pima 
john kohavi irrelevant features feature subset problem 
proceedings th international conference machine learning morgan kaufmann 
kohavi john wrappers feature subset selection 
artificial intelligence 
jain zongker feature selection evaluation application small sample performance 
ieee transactions pattern analysis machine intelligence 
siedlecki sklansky note genetic algorithms large scale feature selection 
pattern recognition letters 
brill brown martin genetic algorithms feature selection networks 
tech 
rep 
ipc tr university virginia institute parallel computation charlottesville 
brotherton simpson dynamic feature set training neural nets classification 
mcdonnell reynolds fogel eds evolutionary programming iv cambridge ma mit press 
bala de jong huang vafaie wechsler learning facilitate evolution features recognizing visual concepts 
evolutionary computation 
kelly davis genetic algorithm nearest neighbors classification algorithm 
belew booker eds proceedings fourth international conference genetic algorithms san mateo ca morgan kaufmann 
punch goodman pei chia shun research feature selection classification genetic algorithms 
forrest ed proceedings fifth international conference genetic algorithms san mateo ca morgan kaufmann 
punch goodman kuhn simultaneous feature scaling selection genetic algorithm 
ck ed proceedings seventh international conference genetic algorithms san francisco morgan kaufmann 
kudo sklansky comparison algorithms select features pattern classifiers 
pattern recognition 
vafaie de jong robust feature selection algorithms 
proceedings international conference tools artificial intelligence ieee computer society press 
larra aga sierra feature subset selection bayesian networks optimization 
artificial intelligence 
cant paz feature subset selection estimation distribution algorithms 
langdon cant paz mathias roy davis poli balakrishnan honavar rudolph wegener bull potter schultz miller burke eds gecco proceedings genetic evolutionary computation conference san francisco ca morgan kaufmann publishers 
punch goodman kuhn jain dimensionality reduction genetic algorithms 
ieee transactions evolutionary computation 
larra aga sierra feature subset selection bayesian networks comparison genetic sequential algorithms 
international journal approximate reasoning 
larra aga sierra feature subset selection estimation distribution algorithms 
larra aga lozano eds estimation distribution algorithms new tool evolutionary computation 
kluwer academic publishers 
embrechts lockwood bennett feature selection silico drug design genetic algorithms neural networks 
ieee mountain workshop soft computing industrial applications ieee press 
fast feature selection genetic algorithms wrapper approach 
ieee international conference evolutionary computation ieee press 
guyon elisseeff variable feature selection 
journal machine learning research 
oh lee suen analysis class separation combination features recognition 
ieee transactions pattern analysis machine intelligence 
harik cant paz goldberg miller gambler ruin problem genetic algorithms sizing populations 
evolutionary computation 
matsumoto nishimura mersenne twister dimensionally uniform pseudorandom number generator 
acm transactions modeling computer simulation 
blake merz uci repository machine learning databases 
miller goldberg genetic algorithms selection schemes varying effects noise 
evolutionary computation 
alpaydin combined cv test comparing supervised classification algorithms 
neural computation 
