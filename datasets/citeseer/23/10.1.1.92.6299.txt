fast keypoint recognition lines code pascal fua vincent computer vision laboratory cole polytechnique rale de lausanne epfl lausanne switzerland email pascal fua vincent epfl ch feature point recognition key component modern approaches object detection existing approaches require computationally expensive patch preprocessing handle perspective distortion 
show formulating problem naive bayesian classification framework preprocessing unnecessary produces algorithm simple efficient robust 
furthermore scales handle large number classes 
recognize patches surrounding keypoints classifier uses hundreds simple binary features models class posterior probabilities 
problem computationally tractable assuming independence arbitrary sets features 
strictly true demonstrate classifier performs remarkably image datasets containing significant perspective changes 

ability recognize interest points images may taken different viewpoints required address important computer vision problems 
range image registration object detection require real time performance 
standard approach addressing problem build affine invariant descriptors local image patches compare images 
usually involves fine scale selection rotation correction intensity normalization :10.1.1.14.4931
results high computational overhead requires descriptors achieve insensitivity specific kinds distortion 
shown casting wide baseline matching problem generic classification problem leads solutions computationally demanding 
approach relies offline training supported part swiss national science foundation 
phase multiple views keypoints matched train randomized trees recognize pairwise intensity comparisons 
yields fast run time performance robustness viewpoint lighting changes proved effective real time object detection 
show classic naive bayesian framework yields approach simpler faster robust state art methods discussed 
non hierarchical structures refer ferns classify patches 
consists small set binary tests returns probability patch belongs classes learned training 
responses combined naive bayesian way 
train classifier synthesizing views keypoints extracted training image appear different perspective scale 
binary tests classifier features picked completely random puts approach firmly camp techniques rely randomization achieve performance 
show particularly effective specific classification task addressing requires scale perspective invariance involves large number classes tolerate significant error rates robust statistical methods exploit information provided keypoints 
furthermore approach particularly easy implement overfit require ad hoc patch normalization allows fast potentially incremental training 

image patch classification importance image patch recognition matching images widely accepted applications ranging object recognition image retrieval pose estimation 
feature points extracted images main classes approaches proposed achieve results 
family involves computing local descriptors variant changes perspective lighting :10.1.1.14.4931
particular sift vector computed local histograms gradients works remarkably textured images benchmark approach 
second class relies statistical learning techniques model set possible appearances patch 
shot approach uses pca gaussian mixture models account perspective distortion 
addressed randomized trees rts 
set possible patches image feature changing perspective lightning conditions seen class possible train set rts recognize feature points feeding samples possible appearances synthesized warping patches training image randomly chosen homographies 
approach fast effective achieve kind object detection depicted 
note traditional classification problems close perfect method required 
recognize features succesfully robust estimator ransac detect object 
scalable approach highly desirable practical applications number keypoints large typically 
show happens performance rts tends drop ferns 
keypoints visual words image retrieval large image databases 
keypoints labeled hierarchical means clustering sift descriptors 
allows large number visual words performance measure number correctly retrieved documents number correctly classified keypoints 
concentrate localizing individual keypoints obtain pose information required tracking augmented reality applications 

naive bayesian classification shown image patches recognized basis simple randomly chosen binary tests grouped decision trees recursively partition space possible patches 
practice single tree discriminative classes 
number trees averaging votes yields results 
section argue tests chosen randomly power approach derives tree structure fact combining groups binary tests allows improved classification rates 
replacing trees non hierarchical ferns pooling answers naive bayesian manner yields better results scalability terms number classes 
naive combination strategy lets combine features key improved recognition rate 

formulation discussed section treat set possible appearances image patch surrounding keypoint class 
patch surrounding keypoint detected image task assign class 
ci set classes set binary features calculated patch trying classify 
formally looking argmax ci fn ci random variable represents class 
bayes formula yields ci fn fn ci ci fn assuming uniform prior denominator simply scaling factor independent class problem reduces finding argmax fn ci 
ci implementation value binary feature depends intensities pixel locations image patch write represents image patch 
simple require accurate classification 
complete representation joint probability eq 
feasible require estimating storing entries class 
way compress representation assume independence features 
extreme version assume complete independence fn ci ci 
completely ignores correlation features 
problem tractable accounting dependencies compromise partition features groups size groups define ferns compute joint probability features fern 
conditional probability fn ci fk ci int fern fk int index int index int index index index index index index pf shift index shift index int 
left pseudo code run time algorithm computes fn ci eq 
classify image patch index integer index computed binary features 
image rectification illumination normalization parameter tuning required 
right implementation code 
code training similar 
fk represents th fern random permutation function range follow semi naive bayesian approach modelling dependencies features 
viability approach shown context image retrieval applications 
form handled easily parameters show section fern size gives recognition rates compared full joint probability representation 
flexible performance memory trade offs changing number ferns sizes 
corresponding code highlight simplicity resulting implementation 

training experiments start training constructing set prominent keypoints lying object model 
feature point corresponds class 
fern features locations picked random 
terms fk ci estimated computing features training samples class 
exploit strong knowledge problem create virtually infinite training set small number images synthesize new views object simple rendering techniques affine deformations extract training patches class 
white noise added realism 
keypoint model gives fine sampling set possible appearances different viewing conditions 
term fk ci part full joint probability eq 
estimation involves estimating extremely large number parameters reliably estimated directly empirical probabilities practice 
order explain estimate fk ci lets introduce event fk states empirical probabilities fk reliable 
express fk ci term fk ci fk ci fk fk fk ci fk fk fk ci fk empirical probability fk ci fk ci fk taken constant equal model fk fk nk nk nk number training samples verify set features fk 
training set truly representative actual variations classes model sense tends number training samples grows yields simple way estimate fk ci 
easy check fk ci nk nk practice value really influence results soon higher 
experiments 
factor interpreted dirichlet prior class conditional probabilities modelled multinomials 
number evaluated class posteriors ratio thresholds classes simple thresholds classes number ferns evaluated number evaluated class posteriors ratio thresholds classes simple thresholds classes number ferns evaluated 
number class posteriors needs evaluated decreases rapidly probabilities thresholded ratio maximum probability 
plots show curves classes trained respectively 

images evaluation 

handling classes run time computing class probabilities takes single lookup fern final multiplication 
repeated class number classes increases quickly burdensome 
furthermore class posteriors reach small values multiplication terms final value irrelevant class selection 
principle calculate final value posterior class long selected class change 
consider strategies eliminating classes posterior evaluation term coming fern multiplied computation carried quickly 
strategy simple threshold learned training set posteriors term gets multiplied 
eliminates classes correct class 
second approach threshold ratio maximum posterior considered class posterior step 
observation class posterior classes classes thresholding simple thresholds ratio thresholds table 
percentage correctly classified image patches thresholding 
thresholds calculated training set cause misclassification test set 
case ratio thresholds loss performance significant 
fallen back large margin catch 
note second strategy requires computation maximum posterior step 
shows average number class posteriors calculated step thresholding strategies 
plots clearly show thresholds ratios maximum probability step decreases number necessary evaluations significantly 
thresholds chosen large possible causing misclassification training set 
experiments images shown percentage correctly classified image patches evaluated randomly generated images images 
observed table ratio thresholds decrease classification rate slightly generalize 

comparing ferns trees ferns random trees similar spirit differ important respects 
trees binary tests organized hierarchically posterior distributions computed additively 
contrast ferns flat compute posteriors multiplicatively 
section compare approaches 
offer theoretical insight ferns appear outperform trees training set sufficiently large 
average classification rate ferns random trees structures depth size average classification rate ferns random trees structures depth size 
percentage correctly classified image patches shown different classifier parameters fern tree methods 
independence assumption ferns allows joint utilization features resulting higher classification rate 
error bars show confidence interval assuming gaussian distribution 
average classification rate ferns ferns size random trees trees depth classes average classification rate ferns ferns size random trees trees depth classes 
classification ferns handle classes random tree methods 
figures ferns size trees depth 

ferns outperform trees evaluate performance proposed fern approach comparing results random tree implementation 
number tests ferns depth trees taken equal compare classification rate number structures ferns random trees 
particular number tests performed keypoint number joint probabilities stored 
tests images classes calculate average classification rate randomly generated test images eliminating false matches object geometry 
feature selection random repeat test times calculate mean variance classification rate perform test images 
depicted despite inaccuracy independence assumptions fern classifier outperforms combination trees 
furthermore number ferns increased random selection method cause large variations classifier performance 
investigate behavior classification rate number classes increases 
shows larger number classes affect performance ferns tree methods cope classes 
experiments trained classifiers classes different images classes image 

linking approaches show approaches equivalent training set small give insights ferns perform better large 
recall evaluate fk ci eq 
fk ci pep fk fk pe empirical probability computing product terms sum ming logarithms write logp fk ci pe log fk pe log log fk pe log fk fk small log fk fk small ape depend fk 
training set small empirical probabilities crude estimates true ones difference selecting class maximum products fk ci ferns maximum sum empirical probabilities trees 
case approaches roughly equivalent 
contrast experiments show training set sufficient large ferns perform better 
understood follows soon single fern attributes low probability class final computed probability class guaranteed low multiplicative behavior 
occur trees due additive behavior 
increases discriminating power ferns requires posterior probabilities trusted 
evaluation method section required 

results compare ferns sift widely reported effective approaches 
combines orientation estimation sift descriptors achieve viewpoint invariance 
show ferns omit costly preprocessing steps loss terms performance yield better performance 
compare classification rates computation times 

comparing matching rates compared match rates sift ferns sequence depicted mouse pad moved front moving camera cluttered background 
mouse pad undergoing large displacements produces image blur large scale perspective illumination changes 
case sift multi resolution code kindly provided david lowe computes laplacian levels octave 
contrast test ferns simple keypoint detector considers extrema laplacian octaves 
implies patches ferns roughly correctly scaled fed sift computed finely estimated scale 
words primitive simpler implement keypoint extraction method handicap really shall see 
train ferns size described section establish matches model image sequence images selecting probable class keypoint test image 
parallel compute sift descriptors keypoints extracted model images match keypoints sequence images selecting nearest sift descriptor 
retain strongest keypoints image keypoints sequence images methods 
cases robust ransac estimation compute homography images refined non linear estimation method matches compatible 
matches checked homography reprojection homography pixels retained inliers 
graph depicts number correct matches obtained methods frames sequence 
despite simplicity ferns match points sift 
shown applied ferns face images test effect non planarity 
despite planarity assumption simplicity generating synthetic training views ferns perform 
results obtained sift similar repeated 

comparing computation times difficult perform completely fair comparison ferns sift reasons 
sift reuses intermediate data keypoint extraction compute canonic scale orientations descriptors ferns rely low cost keypoint extraction 
hand distributed sift code optimized best bin tree speed nearest neighbor search 
relatively easy see approach requires computation 
performing individual tests section requires little time spent computing sums posterior probabilities 
account strategies section speed things classification keypoint requires additions number classes number ferns 
contrast sift uses additions multiplications best bin tree 
represents obvious advantage method run time taken practice 
note contrast method sift require training phase 
number inliers frame 
matching planar target frames top row 
matches obtained ferns frames 
middle row 
matches obtained sift frames 
bottom row 
number correct matches obtained methods frames sequence 
ferns match points sift 
video sequence available supplemental material 
major gain comes fact ferns require descriptors 
significant computing sift descriptors difficult part optimize takes ms pro laptop including time required convolve image 
contrast ferns take milliseconds classify keypoint classes machine 
ferns run nicely primitive keypoint extractor experiments 
keypoints extracted matched classes implementation pro laptop requires ms ferns sift frame keypoint extraction recognition images fifth time taken extraction 
corresponds theoretical hz frame rate factor time required frame acquisition 
training takes minutes 

general method image patch recognition effective object pose estimation despite severe perspective distortion 
semi naive struc 
matching face images ferns 
despite non planarity faces ferns effective 
results sift similar shown 
ture ferns adapted allows scalable simple fast implementation critical step computer vision tasks 
furthermore ferns naturally allow trade offs computing discriminative power 
computers powerful add ferns improve performance 
conversely adapt low computational power hand held systems reducing number ferns 
amit 
object detection recognition models algorithms networks 
mit press 
amit geman 
shape quantization recognition randomized trees 
neural computation 
lowe 
shape indexing approximate nearest neighbour search high dimensional spaces 
conference computer vision pattern recognition pages puerto rico 
bishop 
pattern recognition machine learning 
springer 
cker schmidt schneider 
hierarchical clustering approach large compound libraries 
journal chemical information modeling 
fei fei fergus perona 
shot learning object categories 
ieee transactions pattern analysis machine intelligence 
fergus perona zisserman 
sparse object category model efficient learning ex recognition 
conference computer vision pattern recognition july 
schneiderman 
object image retrieval statistical structure images 
conference computer vision pattern recognition 
fua 
keypoint recognition randomized trees 
ieee transactions pattern analysis machine intelligence sept 
lowe 
distinctive image features scale invariant keypoints 
international journal computer vision 
mikolajczyk tuytelaars schmid zisserman matas schaffalitzky kadir gool 
comparison affine region detectors 
international journal computer vision 

scalable recognition vocabulary tree 
conference computer vision pattern recognition 
schmid mohr 
local invariants image retrieval 
ieee transactions pattern analysis machine intelligence may 
zisserman 
video google efficient visual search videos 
category level object recognition volume lncs pages 
springer 
zheng webb 
comparative study bayes methods classification learning 
proceedings fourth australasian data mining conference pages sydney 
