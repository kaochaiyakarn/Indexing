alternative infinite mixture gaussian process experts edward simon osindero department computer science university toronto toronto osindero cs toronto edu infinite mixture model component comprises multivariate gaussian distribution input space gaussian process model output space 
model neatly able deal non stationary covariance functions discontinuities multimodality overlapping output signals 
similar rasmussen ghahramani full generative model input output space just conditional model :10.1.1.19.9008
allows deal incomplete data perform inference inverse functional mappings regression leads powerful consistent bayesian specification effective gating network different experts 
gaussian process gp models powerful tools regression function approximation predictive density estimation 
despite power flexibility suffer limitations 
computational requirements scale number data points necessitating range approximations large datasets 
problem difficult specify priors perform learning gp models require non stationary covariance functions multi modal output discontinuities 
attempts circumvent example 
particular infinite mixture gaussian process experts model proposed rasmussen ghahramani neatly addresses aforementioned key issues :10.1.1.19.9008
single gp model matrix inverted inference 
model composed multiple gp responsible subset data computational complexity inverting matrix replaced inversions smaller matrices large datasets result substantial speed may allow consider large scale problems unwieldy 
furthermore combining multiple stationary gp experts easily accommodate non stationary covariance noise levels distinctly multi modal outputs 
placing dirichlet process prior experts allow data prior beliefs may vague automatically determine number components 
alternative infinite model strongly inspired uses different formulation mixture experts style example :10.1.1.19.9008
alternative approach effectively uses posterior re psfrag replacements xi zi psfrag replacements yi left graphical model standard moe model 
expert indicators specified gating network applied inputs 
right alternative view moe model full generative model 
distribution input locations mixture model components expert 
conditioned input locations posterior responsibilities mixture component behave gating network 
mixture distribution gating network 
task hand simply output density estimation regression suggest full generative model inputs outputs preferable purely conditional model 
generative approach retains strengths number potential advantages able deal partially specified data missing input ordinates able infer inverse functional mappings input space output value :10.1.1.19.9008
generative approach affords richer consistent way specifying prior beliefs covariance structure outputs vary move input space 
example type generative model propose shown 
dirichlet process prior countably infinite number experts expert comprises parts density input space describing distribution input points associated expert gaussian process model outputs associated expert 
preliminary exposition restrict attention experts input space densities single full covariance gaussian 
simple approach demonstrates interesting performance capabilities 
elaborate setup input density associated expert infinite mixture simpler distributions instance infinite mixture gaussians allow flexible partitioning input space experts 
structure follows 
section brief overview ways thinking mixtures experts 
section give complete specification graphical depiction generative model section outline steps required perform monte carlo inference prediction 
section results simple simulations highlight salient features proposal section discuss place relation similar techniques 
mixtures experts standard mixture experts moe model gating network probabilistically mixes regression components 
subtlety gp mixture experts model iid assumptions data longer hold specify joint distributions possible assignment experts data 
set dimensional input vectors set scalar outputs set expert indicators assign data points experts 
likelihood outputs inputs specified equation gp represents gp parameters rth expert represents parameters gating network summation possible configurations indicator variables 
zi xi yi fs yr nr aw bw graphical model representation alternative infinite mixture gp experts model proposed 
xr represent ith data point set input data expert label yr represent set output data expert label words input data iid expert label sets output data iid corresponding sets input data 
lightly shaded boxes rounded corners represent hyper hyper parameters fixed text 
dp concentration parameter expert indicators variables gate hyperparameters gate component parameters gp expert parameters gp updated gp alternative view moe model experts generate inputs simply conditioned see 
alternative view employs joint mixture model input output space objective primarily estimating conditional densities outputs inputs 
gating network effectively gets specified posterior responsibilities different components mixture 
advantage perspective easily accommodate partially observed inputs allows reverse conditioning wish estimate input space output value originated 
mixture model gaussian processes experts likelihood gp description density input space encapsulated infinite mixture gaussian processes joint generative model graphical structure full generative model shown 
generative process produce iid data points simply formulated joint distribution dataset size set conditionals incrementally add data points construct complete set sample points prior specified top level hyper parameters perform operations 
sample dirichlet process concentration variable top level hyperparameters 

construct partition objects groups dirichlet process 
assignment objects denoted set indicator variables 
sample gate hyperparameters top level hyperparameters 

grouping indicators sample input space parameters conditioned defines density input space case full covariance gaussian 

parameters group sample locations input points xr 

group sample hyper parameters gp expert associated group gp 
input locations xr hyper parameters gp individual groups formulate gp output covariance matrix sample set output values yr joint gaussian distribution 
write full joint distribution model follows 
gp xr gp yr xr gp gp supplementary notation empty set hn gp delta function irrelevant dummy set parameters ensure proper normalisation 
gp components standard stationary covariance function form exp wj individual distributions equation defined follows pu fs xr xr gp ln aw bw yr xr gp yr qr qr notation ln represent normal wishart gamma log normal distributions respectively parameterizations appendix 
notation pu refers polya urn distribution 
approach similar rasmussen input data mean covariance provide automatic normalisation dataset 
incorporate additional hyperparameters fs allow prior beliefs variation location size relative data covariance 
monte carlo updates integrals summations required inference learning operations model analytically intractable necessitate monte carlo approximations 
fortunately necessary updates relatively straightforward carry markov chain monte carlo mcmc scheme employing gibbs sampling hybrid monte carlo 
note model predictive density depends entire set test locations input space 
transductive behaviour follows non iid nature model influence test locations posterior distribution mixture parameters 
consequently marginal predictive distribution location depend locations making simultaneous predictions 
may may desired 
situations ability incorporate additional information input density test time may beneficial 
straightforward effectively ignore new information simply compute set independent single location predictions 
set test locations training data pairs top level hyper parameters iterate conditional updates produce predictive distribution unknown outputs 
parameter updates conjugate prior distributions noted 
update indicators cycling data sampling indicator variable time 
algorithm explore new experts 

update input space parameters 

update gp hyper params hybrid monte carlo 

update gate hyperparameters 
note updated slice sampling 

update dp hyperparameter data augmentation technique escobar west 

resample missing output values cycling experts jointly sampling missing outputs associated gp 
perform preliminary runs estimate longest auto covariance time max posterior estimates burn period times timescale samples max iterations 
simulations auto covariance time typically complete update cycles burn period iterations collect samples 
experiments samples prior give example data drawn model multi modal non stationary 
artificial dataset confirm mcmc algorithm performs able recover sensible posterior distributions 
posterior histograms inferred parameters shown see clustered true values 
primarily convenience 
valid samples burn period considered independent obtain accurate estimator 
count set samples model prior 
different marker styles indicate sets points different experts 
posterior distribution log true value indicated dashed line top distribution occupied experts bottom 
note posterior mass located vicinity true values 
inference toy data illustrate features model constructed toy dataset consisting continuous functions added different levels noise 
functions count 
noise sd 
noise sd 
noise sd sin 
noise sd resulting data non stationary noise levels non stationary covariance discontinuities significant multi modality 
shows results dataset single gp comparison 
see order account entire data set single gp forced infer unnecessarily high level noise function 
single gp unable capture multi modality non stationarity data distribution 
contrast model able deal challenges 
full generative model input output space able model infer input locations particular output value 
number applications relevant example wanted sample candidate locations evaluate function trying optimise 
provide simple illustration 
choose output levels conditioned output having values sample input location 
inference plausible model able suggest locations input space maximal output value seen training data 
regression simple real world dataset apply model algorithm motorcycle dataset 
commonly dataset gp community serves useful basis comparison 
particular easy see model compares standard gp :10.1.1.19.9008
compares performance model single gp 
particular note median model closely resembles mean single gp model able accurately model low noise level training data single gp results toy dataset 
training data shown predictive mean stationary covariance gp median predictive distribution model 
small dots samples model samples location evaluated equally spaced locations range plotted small amount jitter aid visualisation 
illustrate predictive density model 
solid lines show sd interval regular gp 
circular markers ordinates show samples reverse conditioning sample abscissa locations test ordinate set training data 
left side dataset 
remainder dataset noise level modeled model single gp similar model better able capture behaviour data ms difficult exact comparison speculate model realistically modeling noise dataset inferring overly flat gp expert location :10.1.1.19.9008
report expert adjacency matrix closely resembles :10.1.1.19.9008
discussion alternative framework infinite mixture gp experts 
feel proposed model carries strengths augments desirable additional features :10.1.1.19.9008
pseudo likelihood objective function adapt gating network defined guaranteed lead self consistent distribution results may depend order updates performed model incorporates consistent bayesian density formulation input output spaces definition :10.1.1.19.9008
furthermore general framework naturally able specify priors partitioning space different expert components 
full joint model infer inverse functional mappings 
considerable gains allowing input density models powerful 
easier arbitrary regions space share covariance structures areas controlled particular expert tend local 
consequently potentially undesirable aspect current model strong clustering input space lead infer expert components single gp job modelling data 
elegant way extending model way separate infinite mixture distribution input density expert incorporating hierarchical dp prior infinite set experts allow information shared 
regard applications interesting explore model capability infer inverse functional mappings useful optimisation active learning context 
note focused small examples far inference techniques scale larger problems acceleration training data time ms acceleration time ms motorcycle impact data median model point wise predictive distribution predictive mean stationary covariance gp model 
small dots samples model samples location evaluated equally spaced locations range plotted small amount jitter aid visualisation 
solid lines show sd interval regular gp 
practical tasks 
acknowledgments ben marlin sharing slice sampling code carl rasmussen making minimize available 
rasmussen ghahramani :10.1.1.19.9008
infinite mixtures gaussian process experts 
advances neural information processing systems pages 
mit press 
tresp 
mixture gaussian processes 
advances neural information processing systems volume 
mit press 
ghahramani jordan 
supervised learning incomplete data em approach 
advances neural information processing systems pages 
morgan kaufmann 
xu jordan hinton 
alternative model mixtures experts 
advances neural information processing systems pages 
mit press 
rasmussen 
infinite gaussian mixture model 
advances neural information processing systems volume pages 
mit press 
jacobs jordan hinton 
adaptive mixture local experts 
neural computation 
gelman carlin stern rubin 
bayesian data analysis 
chapman hall nd edition 
blackwell macqueen 
ferguson distributions polya urn schemes 
annals statistics 
neal 
markov chain sampling methods dirichlet process mixture models 
journal computational graphical statistics 
neal 
probabilistic inference markov chain monte carlo methods 
technical report crg tr university toronto 
neal 
slice sampling discussion 
annals statistics 
escobar west 
computing bayesian nonparametric hierarchical models 
practical nonparametric semiparametric bayesian statistics number lecture notes statistics 
springer verlag 
silverman 
aspects spline smoothing approach non parametric regression curve fitting 
royal society 
ser 

