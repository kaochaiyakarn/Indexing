non linear cca pca alignment local models jakob verbeek sam roweis nikos vlassis informatics institute university amsterdam department computer science university toronto propose non linear canonical correlation analysis cca method works coordinating aligning mixtures linear models 
way cca extends idea pca extends methods non linear dimensionality reduction case multiple embeddings underlying low dimensional coordinates observed lying different high dimensional manifold 
show special case method applied single manifold reduces laplacian eigenmaps algorithm 
previous alignment schemes mixture models estimated parameters model estimated closed form local optima learning 
experimental results illustrate viability approach non linear extension cca 
interested data lies close low dimensional manifold embedded possibly non linearly euclidean space higher dimension 
data kind generated observations high dimensional number underlying degrees freedom small 
typical example images object different conditions pose lighting 
simpler example fig 
data ir lies dimensional manifold 
want recover structure data manifold unroll data manifold data expressed underlying latent coordinates coordinates manifold 
learning low dimensional latent representations may desirable different reasons compression storage communication visualization high dimensional data preprocessing data analysis prediction tasks 
unsupervised nonlinear feature extraction pursued complementary directions 
various nonparametric spectral methods isomap lle kernel pca laplacian eigenmaps proposed reduce dimensionality fixed training set way maximally preserve certain inter point relationships methods generally provide functional mappings high low dimensional spaces valid training data :10.1.1.19.9400
consider method integrate local feature extractors single global representation similar approaches global coordination automatic alignment manifold charting 
methods deliver training functional mapping convert previously unseen high dimensional observations low dimensional global coordinates 
algorithms method performs non linear feature extraction minimizing convex objective function critical points characterized eigenvectors matrix 
algorithms generally simple efficient needs construct matrix local feature analysis training data computes largest smallest eigenvectors standard numerical methods 
contrast methods generative topographic mapping self organizing maps prone local optima objective function :10.1.1.130.110
method intuitions earlier idea learn mixture latent variable density models original training data mixture component acts local feature extractor 
example may mixture factor analyzers mixture principal component analyzers pca 
mixture learned local feature extractors coordinated finding model suitable linear mapping offset latent variable space single global low dimensional coordinate system 
local feature extractors coordinating linear maps provide global non linear map data space latent space back 
learning mixture driven density signal want place models near training points post coordination driven idea different models place significant weight point agree mapping global space 
algorithm developed section builds coordination methods 
cross entropy unimodal approximation true posterior global coordinates encourage agreement 
attempt simultaneously learn mixture model coordinate causes severe problems local minima 
fix specific mixture study computations involved coordinating local representations 
setting showing section laplacian eigenmaps special case algorithms applied single manifold :10.1.1.19.9400
go section extend algorithm setting multiple different observation spaces available related underlying global space different nonlinear embeddings 
naturally gives rise nonlinear version weighted canonical correlation analysis cca 
results experiments section conclude general discussion section 
non linear pca aligning local feature extractors consider data set 
xn collection local feature extractors fs 
fs vector containing zero features produced model feature extractor provides activity signal representing confidence modeling point 
convert activities posterior responsibilities simple softmax eas 
ear experts components mixture setting activities logarithm posteriors mixture recover exactly posteriors 
consider relationship representation data representation data global latent space find 
denote latent global coordinates data denote locally extracted features 
unobserved latent coordinate corresponding data point xn conditioned assume density xn xn gns gaussian distribution mean covariance 
mean gns xn sum component offset latent space linear transformation implemented fs xn 
homogeneous coordinates write ls fs xn gns 
consider posterior distribution latent coordinates data 
fixed set local feature extractors corresponding activities interested finding linear maps ls give rise consistent projections data latent space 
consistent mean similar components large posterior 
predictions perfect agreement posterior gaussian 
mixture gaussians measure similarity predictions measuring posterior mixture resembles gaussian 
achieve consistent alignment introduce objective function 
lk sums kullback leibler kl divergence measuring unimodal distribution objective defined qn xn qn gn distributes independently set xn objective sums data point xn kullback leibler divergence qn weighted posterior xn qn xn const 
easy derive order minimize objective gn obtain gn denotes identity matrix 
skipping additive multiplicative constants respect linear maps ls objective simplifies gn gns gns 
main attraction setup objective quadratic function linear maps ls 
extra notation obtain clearer form objective function linear maps 
un qn 
nk 

lk 
note gn unl expected projection coordinates computed 
gn ul 
define block diagonal matrix blocks ds ns 
objective written tr 
objective function invariant translation rotation global latent space re scaling latent space changes objective monotonically 

solutions unique respect translation rotation scaling impose constraints transl 
gn rot 
scale gn gn data ir local charts indicated axes left 
data representation ir generated optimizing objective function 
expected latent coordinates gn plotted right 
columns minimizing characterized generalized eigenvectors uv dv uv 
value objective function sum corresponding eigenvalues 
smallest eigenvalue zero corresponding mapping data latent coordinate 
embedding uninformative constant select eigenvectors corresponding second st smallest eigenvalues obtain best embedding dimensions 
note mentioned framework enables feature extractors provide different numbers features 
fig 
give illustration applying procedure simple manifold 
plots show original data algorithm left dimensional latent coordinates gn algorithm right 
laplacian eigenmaps special case consider special case algorithm section features extracted 
information mixture model provides posterior probabilities collected matrix ns xn 
case gns 
tr adjacency matrix st diagonal degree matrix ss 
optimization constrains ast zero mean identity covariance leads generalized eigenproblem av dv optimization problem exactly laplacian eigenmaps algorithm applied mixture components data points :10.1.1.19.9400
feature extractors setting applied mixture models model data hard design feature extractors data numerical categorical features 
mixture densities latent variables mixtures multinomials mixtures hidden markov models notice manner mixture model provides soft grouping data posteriors adjacency matrix groups 
non linear cca aligning local feature extractors canonical correlation analysis cca data analysis method finds correspondences sets measurements 
data provided tuples corresponding measurements different spaces 
sets measurements obtained employing different sensors measurements phenomenon 
main interest develop nonlinear extension cca works different measurements come separate nonlinear manifolds share underlying global coordinate system 
non linear cca trained find shared low dimensional embedding manifolds exploiting pairwise correspondence provided data set 
models different purposes sensor fusion denoising filling missing data predicting measurement space measurement space 
important aspect learning setup multiple sensors function regularization helping avoid overfitting 
cca zero mean sets points 
xn irp 
yn irq aim find linear maps map members respectively real line correlation linearly transformed variables maximized 
easily shown equivalent minimizing constraint 
easily generalized sets need zero mean allowing translation 
generalize mapping ir real line requiring sum covariance matrices projections identity 
cca readily extended take account point sets show 
generalized cca setting multiple point sets allowing translations linear mappings ir objective minimize squared distance tuples projections constraint 
denote projection th point th point set gns gn cca gns gns 
minimize error function gns gn 
objective equation coincides cca different constraints imposed optimization cca objective previous sections equivalent 
regard alignment procedure weighted form cca 
suggests coordination technique non linear cca 
achieved quite easily modifying objective function 
consider different point sets having mixture locally valid linear projections global latent space shared mixture components point sets 
minimize weighted sum squared distances pairs projections pairs projections due point set pairs combine projections different point sets 
index ranging different observation spaces write ns posterior component observation observation space similarly ns denote projection due component space average projection due observation space denoted gc qc ns 
index range mixture components observation spaces xn corresponds yn corresponds 
average projection gn gc 
objective rewritten gn gn ns ns 
data charts indicated bars left middle 
latent coordinates vert 
coordinate generating curve hor 
right 
observe objective sums point set consistency projections summand point set consistency projections second summand 
technique get stable results chart coordination procedure single manifold discussed section 
robustness variation mixture fitting improved sets charts fitted manifold 
align sets charts optimizing 
aligns charts set time sure different sets aligned charts aligned providing important regularization point modeled local models 
note charts responsibilities obtained mixture pca factor analyzers local linear mappings latent space induce gaussian mixture latent space 
mixture compute responsibilities components latent coordinates 
linear map data latent space compute pseudo inverse projecting back 
averaging individual back projections responsibilities computed latent space obtain projection latent space data space 
total map observation space 
generated reconstructions experiments reported 
linear cca data non linearly embedded reconstructions poor linear cca map low dimensional linear subspace 
illustrative example non linear cca point sets ir point set generated shaped curve second point set generated arc see fig 

point sets added gaussian noise learned component mixture model sets 
rightmost panel fig 
clearly succesfully discovered latent coordinates plotted coordinate generating curve 
describe challenging experiments 
experiment data sets know share underlying degrees freedom 
images face varying gaze left right 
cut images half obtain sets images 
trained system image halves pixels 
image halves modeled mixture components 
fig 
generated right half images left half shown 
second experiment concerns appearance pose estimation object 
point set consists pixel representation images object point set contains corresponding pose camera object 
pose parameters identity extract features just component space 
training data collected moving camera half sphere centered object 
mixture pca trained image data aligned pose parameters dimensional latent space 
right panel fig 
shows reconstructions images conditioned various pose inputs left image pair reconstruction pose right image 
going way input image estimate pose peters sharing images recorded institute neural computation ruhr university bochum germany 
right half images generated left half trained model left 
image reconstructions pose parameters right 
absolute errors longitude cases cases 
third experiment images second experiment replace direct low dimensional supervision signal pose parameters high dimensional form images object corresponding poses 
trained mixture pca image sets images pixels set aligned dimensional latent space 
comparing pose object pose nearest latent space image object std 
dev 
error latitude longitude errors test cases rest errors std 
dev 
view object reconstruct corresponding view second object fig 
shows obtained reconstruction results 
experiments reconstructions data included training 
discussion extended alignment methods single manifold nonlinear dimensionality reduction perform non linear cca measurements multiple manifolds 
shown close relationship laplacian eigenmaps degenerate case single manifold feature extractors zero dimensionality :10.1.1.19.9400
related method coordinate local charts proposed lle cost function opposed cross entropy term means need just set local feature extractors posteriors need able compute reconstruction weights collected weight matrix 
weights indicate reconstruct data point nearest neighbors 
computing weights requires access original data directly just interface mixture model 
defining sensible weights right number neighbors straightforward especially data non euclidean spaces 
furthermore computing weights costs principle need find nearest neighbors running time linear number data points 
considered find low dimensional representations multiple point sets simultaneously correspondences point sets 
generalization lle problem closely related non linear cca model 
extended case know points set points correspond set 
multiple sets charts image set corresponding image second set closest image second set latent space reconstruction 
data set similar spirit self correspondence technique data split overlapping sets stabilize generalized lle 
acknowledgments jjv nv supported technology foundation stw applied science division nwo technology program dutch ministry economic affairs 
str supported part learning project iris canada nserc 
tenenbaum de silva langford 
global geometric framework nonlinear dimensionality reduction 
science december 
roweis saul 
nonlinear dimensionality reduction locally linear embedding 
science december 
sch lkopf smola ller 
nonlinear component analysis kernel eigenvalue problem 
neural computation 
belkin niyogi :10.1.1.19.9400
laplacian eigenmaps spectral techniques embedding clustering 
advances neural information processing systems 
mit press 
roweis saul hinton 
global coordination local linear models 
advances neural information processing systems 
mit press 
teh roweis 
automatic alignment local representations 
advances neural information processing systems 
mit press 
brand 
charting manifold 
advances neural information processing systems 
mit press 
bishop svens williams :10.1.1.130.110
gtm generative topographic mapping 
neural computation 
kohonen 
self organizing maps 
springer 
ham lee saul 
learning high dimensional correspondences low dimensional manifolds 
proc 
international conf 
machine learning 
appear 
peters von der malsburg 
measure pose robustness object views 
image vision computing 
