feature selection high dimensional data fast correlation filter solution lei yu asu edu huan liu asu edu department computer science engineering arizona state university tempe az usa feature selection preprocessing step machine learning effective reducing dimensionality removing irrelevant data increasing learning accuracy improving result comprehensibility 
increase dimensionality data poses severe challenge existing feature selection methods respect efficiency effectiveness 
introduce novel concept predominant correlation propose fast filter method identify relevant features redundancy relevant features pairwise correlation analysis 
efficiency effectiveness method demonstrated extensive comparisons methods real world data high dimensionality 

feature selection frequently preprocessing step machine learning 
process choosing subset original features feature space optimally reduced certain evaluation criterion 
feature selection fertile field research development proven effective removing irrelevant redundant features increasing efficiency learning tasks improving learning performance predictive accuracy enhancing comprehensibility learned results blum langley dash liu kohavi john 
years data increasingly larger number instances number features applications genome projects xing text categorization yang pederson image retrieval rui customer relationship management ng liu 
may cause serious problems machine learning algorithms respect scalability learning performance 
example high dimensional data data sets hundreds thousands features contain high degree irrelevant redundant information may greatly degrade performance learning algorithms 
feature selection necessary machine learning tasks facing high dimensional data nowadays 
trend size dimensionality poses severe challenges feature selection algorithms 
research efforts feature selection focused challenges handling huge number instances liu dealing high dimensional data das xing 
concerned feature selection high dimensional data 
review models feature selection explain filter solution suitable high dimensional data review efforts feature selection high dimensional data 
feature selection algorithms fall broad categories filter model wrapper model das kohavi john 
filter model relies general characteristics training data select features involving learning algorithm 
wrapper model requires predetermined learning algorithm feature selection uses performance evaluate determine features selected 
new subset features wrapper model needs learn hypothesis classifier 
tends find features better suited predetermined learning algorithm resulting superior learning performance tends computationally expensive filter model langley 
number features large filter model usually chosen due computational efficiency 
proceedings twentieth international conference machine learning icml washington dc 
combine advantages models algorithms hybrid model proposed deal high dimensional data das ng xing 
algorithms goodness measure feature subsets data characteristics choose best subsets cardinality cross validation exploited decide final best subset different cardinalities 
algorithms mainly focus combining filter wrapper algorithms achieve best possible performance particular learning algorithm similar time complexity filter algorithms 
focus filter model aim develop new feature selection algorithm effectively remove irrelevant redundant features costly computation currently available algorithms 
section review current algorithms filter model point problems context high dimensionality 
section describe correlation measures form base method evaluating feature relevance redundancy 
section propose method selects features classification novel concept predominant correlation fast algorithm quadratic time complexity 
section evaluate efficiency effectiveness algorithm extensive experiments various real world data sets comparing representative feature selection algorithms discuss implications findings 
section conclude possible extensions 

related filter model different feature selection algorithms categorized groups feature weighting algorithms subset search algorithms evaluate goodness features individually feature subsets 
discuss advantages shortcomings representative algorithms group 
feature weighting algorithms assign weights features individually rank relevance target concept 
number different definitions feature relevance machine learning literature blum langley kohavi john 
feature selected weight relevance greater threshold value 
known algorithm relies relevance evaluation relief kira rendell 
key idea relief estimate relevance features values distinguish instances different classes near 
relief randomly samples number instances training set updates relevance estimation feature difference selected instance nearest instances opposite classes 
time complexity relief data set instances features 
constant time complexity mn scalable data sets huge number instances high dimensionality 
relief help removing redundant features 
long features deemed relevant class concept selected highly correlated kira rendell 
algorithms group similar problems relief 
capture relevance features target concept discover redundancy features 
empirical evidence feature selection literature shows irrelevant features redundant features affect speed accuracy learning algorithms eliminated hall kohavi john 
context feature selection high dimensional data may exist redundant features pure relevance feature weighting algorithms meet need feature selection 
subset search algorithms search candidate feature subsets guided certain evaluation measure liu motoda captures goodness subset 
optimal near optimal subset selected search stops 
existing evaluation measures shown effective removing irrelevant redundant features include consistency measure dash correlation measure hall hall 
consistency measure attempts find minimum number features separate classes consistently full set features 
inconsistency defined instances having feature values different class labels 
dash 
different search strategies exhaustive heuristic random search combined evaluation measure form different algorithms 
time complexity exponential terms data dimensionality exhaustive search quadratic heuristic search 
complexity linear number iterations random search experiments show order find best feature subset number iterations required quadratic number features dash 
hall correlation measure applied evaluate ness feature subsets hypothesis feature subset contains features highly correlated class uncorrelated 
underlying algorithm named cfs exploits heuristic search 
quadratic higher time complexity terms dimensionality existing subset search algorithms strong scalability deal high dimensional data 
overcome problems algorithms groups meet demand feature selection high dimensional data develop novel algorithm effectively identify irrelevant redundant features time complexity subset search algorithms 

correlation measures section discuss evaluate goodness features classification 
general feature relevant class concept redundant relevant features 
adopt correlation variables goodness measure definition feature highly correlated class highly correlated features 
words correlation feature class high relevant predictive class correlation relevant features reach level predicted relevant features regarded feature classification task 
sense problem feature selection boils find suitable measure correlations features sound procedure select features measure 
exist broadly approaches measure correlation random variables 
classical linear correlation information theory 
approach known measure linear correlation coefficient 
pair variables linear correlation coefficient formula xi xi yi yi xi xi yi yi xi mean yi mean value lies inclusive 
completely correlated takes value totally independent zero 
symmetrical measure variables 
measures category basically variations formula square regression error maximal information compression index mitra 
benefits choosing linear correlation feature goodness measure classification 
helps remove features near zero linear correlation class 
second helps reduce redundancy selected features 
known data linearly separable original representation linearly separable group linearly dependent features removed das 
safe assume linear correlation features real world 
linear correlation measures may able capture correlations linear nature 
limitation calculation requires features contain numerical values 
overcome shortcomings solution adopt approach choose correlation measure information theoretical concept entropy measure uncertainty random variable 
entropy variable defined xi log xi entropy observing values variable defined yj xi yj log xi yj xi prior probabilities values xi yi posterior probabilities values amount entropy decreases reflects additional information provided called information gain quinlan ig 
measure feature regarded correlated feature feature ig ig 
information gain measure theorem 
theorem information gain symmetrical random variables proof sketch prove ig ig need prove 
easily derived 
symmetry desired property measure correlations features 
information gain biased favor features values 
furthermore values normalized ensure comparable affect 
choose symmetrical uncertainty press defined follows 
ig su compensates information gain bias features values normalizes values range value indicating knowledge value completely predicts value value indicating independent 
addition treats pair features symmetrically 
entropy measures require nominal features applied measure correlations continuous features values discretized properly advance fayyad irani liu 
symmetrical uncertainty 

correlation filter approach 
methodology symmetrical uncertainty su goodness measure ready develop procedure select features classification correlation analysis features including class 
involves aspects decide feature relevant class decide relevant feature redundant considering relevant features 
answer question userdefined threshold su value method feature weighting algorithms relief 
specifically suppose data set contains features class sui denote su value measures correlation feature fi class named correlation subset relevant features decided threshold su value fi sui 
answer second question complicated may involve analysis pairwise correlations features named correlation results time complexity associated number features existing algorithms 
solve problem propose method 
correlations captured su values order decide relevant feature redundant need find reasonable way decide threshold level correlations 
words need decide level correlation features high cause redundancy may removed feature fi value sui quantifies extent fi correlated predictive class examine value fj obtain quantified estimations extent fi correlated predicted rest relevant features possible identify highly correlated features fi straightforward manner decide threshold su value equal similar 
features method sounds reasonable try determine highly correlated features concept considering concept 
context set relevant features identified class concept try determine highly correlated features feature fi reasonable correlation level fi class concept sui 
reason lies common phenomenon feature correlated concept class certain level may correlated concepts features higher level 
correlation feature class concept larger threshold thereof making feature relevant class concept correlation means predominant 
precise define concept predominant correlation follows 
definition predominant correlation 
correlation feature fi fi class predominant iff sui fj exists fj sui exists fj feature fi call redundant peer fi spi denote set redundant peers fi 
fi spi spi divide spi parts spi spi spi fj fj spi sui fj fj spi sui 
spi definition predominant feature 
feature predominant class iff correlation class predominant predominant removing redundant peers 
definitions feature predominant predicting class concept feature selection classification process identifies predominant features class concept removes rest 
propose heuristics effectively identify predominant features remove redundant ones relevant features having identify redundant peers feature avoids pairwise analysis correlations relevant features 
assumption developing heuristics features redundant needs removed removing relevant class concept keeps information predict class reducing redundancy data 
heuristic spi 
treat fi feature remove features spi skip identifying redundant peers 
heuristic spi 
process features making decision fi 
spi predominant follow heuristic remove fi decide remove features spi features heuristic starting point 
feature largest sui value predominant feature starting point remove features 

algorithm analysis methodology develop algorithm named fast correlation filter 
data set input fn training data set predefined threshold output optimal subset calculate sui fi sui append fi list order list descending sui value fp list fq list fp fq null fq sup remove fq list fq list fq list fq null fp fp list fp null list 
algorithm fq features class algorithm finds set predominant features class concept 
consists major parts 
part line calculates su value feature selects relevant features list predefined threshold orders descending order su values 
second part line processes ordered list list remove redundant features keeps predominant ones selected relevant features 
heuristic feature fp determined predominant feature filter features ranked lower fp fp redundant peers 
iteration starts element heuristic line continues follows 
re list features right fp list fp happens redundant peer feature fq fq removed list heuristic 
round filtering features fp algorithm take currently remaining feature right fp new line repeat filtering process 
algorithm stops feature removed list part algorithm linear time complexity terms number features second part iteration predominant feature fp identified previous round remove large number features redundant peers fp current iteration 
best case remaining features fp ranked list removed worst case 
average assume half remaining features removed iteration 
time complexity second part log terms calculation su pair features linear term number instances data set complexity mn log 

empirical study objective section evaluate proposed algorithm terms speed number selected features learning accuracy selected features 

experiment setup experiments choose representative feature selection algorithms comparison 
feature weighting algorithm relieff extension relief searches nearest neighbors robust noise handles multiple classes kononenko sub set search algorithms exploit sequential forward search utilize correlation measure consistency measure guide search denoted respectively 
variation cfs algorithm mentioned section 
reason prefer cfs experiments hall initial experiments show cfs produces slightly better results sequential forward search runs faster cfs best search nodes expansion suitable high dimensional data 
addition feature selection algorithms select different learning algorithms quinlan nbc witten frank evaluate accuracy selected features feature selection algorithm 
table 
summary bench mark data sets 
title features instances classes lung cancer promoters splice coil chemical musk arrhythmia isolet multi features experiments conducted weka implementation existing algorithms implemented weka environment witten frank 
data sets selected uci machine learning repository blake merz uci kdd archive bay 
summary data sets table 
data set run feature selection algorithms relieff respectively record running time number selected features algorithm 
apply nbc original data set newly obtained data set containing selected features algorithm record accuracy fold cross validation 

results discussions table records running time number selected features feature selection algorithm 
relieff parameter set neighbors set instances experiments 
table observe algorithm running times different data sets consistent previous time complexity analysis 
averaged values row table clear runs significantly faster degrees algorithms verifies superior computational efficiency 
interesting relieff unexpectedly slow time complexity mn fixed sample size reason lies searching nearest neighbors involves distance calculation time consuming calculation symmetrical uncertainty values 
table clear achieves highest level dimensionality reduction selecting number features exception consistent theoretical analysis ability identify redundant features 
tables show learning accuracy nbc respectively different feature sets 
averaged accuracy data sets observe general improves accuracy nbc algorithms enhance accuracy level 
individual accuracy values observe data sets maintain increase accuracy 
experimental results suggest practical feature selection classification high dimensional data 
efficiently achieve high degree dimensionality reduction enhance classification accuracy predominant features 

propose novel concept predominant correlation introduce efficient way analyzing feature redundancy design fast filter approach 
new feature selection algorithm implemented evaluated extensive experiments comparing related feature selection algorithms 
feature selection results verified applying different classification algorithms data feature selection 
approach demonstrates efficiency effectiveness dealing high dimensional data classification 
extend data higher dimensionality thousands features 
study detail redundant features role classification combine feature discretization algorithms smoothly handle data different feature types 
table 
running time ms number selected features feature selection algorithm 
title running time selected features relieff relieff lung cancer promoters splice coil chemical musk arrhythmia isolet multi features average table 
accuracy selected features feature selection algorithm 
title full set relieff lung cancer promoters splice coil chemical musk arrhythmia isolet multi features average gratefully anonymous reviewers area chair constructive comments 
part supported nsf prop ecr ford asu liu 
bay 

uci kdd archive 
kdd ics uci edu 
blake merz 

uci repository machine learning databases 
www ics uci edu mlearn mlrepository html 
blum langley 

selection relevant features examples machine learning 
artificial intelligence 
das 

filters wrappers boosting hybrid feature selection 
proceedings eighteenth international conference machine learning pp 

das 

feature selection linear dependence measure 
ieee transactions computers 
dash liu 

feature selection classifications 
intelligent data analysis international journal 
dash liu motoda 

consistency feature selection 
proceedings fourth pacific asia conference knowledge discovery data mining pp 

springer verlag 
fayyad irani 

multi interval discretization continuous valued attributes classification learning 
proceedings thirteenth table 
accuracy nbc selected features feature selection algorithm 
title full set relieff lung cancer promoters splice coil chemical musk arrhythmia isolet multi features average international joint conference artificial intelligence pp 

morgan kaufmann 
hall 

correlation feature selection machine learning 
doctoral dissertation university waikato dept computer science 
hall 

correlation feature selection discrete numeric class machine learning 
proceedings seventeenth international conference machine learning pp 

kira rendell 

feature selection problem traditional methods new algorithm 
proceedings tenth national conference artificial intelligence pp 

menlo park aaai press mit press 
kohavi john 

wrappers feature subset selection 
artificial intelligence 
kononenko 

estimating attributes analysis extension relief 
proceedings european conference machine learning pp 

catania italy berlin springer verlag 
langley 

selection relevant features machine learning 
proceedings aaai fall symposium relevance 
aaai press 
liu tan dash 

discretization enabling technique 
data mining knowledge discovery 
liu motoda 

feature selection knowledge discovery data mining 
boston kluwer academic publishers 
liu motoda yu 

feature selection selective sampling 
proceedings nineteenth international conference machine learning pp 

mitra murthy pal 

unsupervised feature selection feature similarity 
ieee transactions pattern analysis machine intelligence 
ng 

feature selection learning exponentially irrelevant features training examples 
proceedings fifteenth international conference machine learning pp 

ng liu 

customer retention data mining 
ai review 
press flannery teukolsky vetterling 

numerical recipes cambridge university press cambridge 
quinlan 

programs machine learning 
morgan kaufmann 
rui huang chang 

image retrieval current techniques promising directions open issues 
journal visual communication image representation 
witten frank 

data mining machine learning tools techniques java implementations 
morgan kaufmann publishers 
xing jordan karp 

feature selection high dimensional genomic microarray data 
proceedings eighteenth international conference machine learning pp 

yang pederson 

comparative study feature selection text categorization 
proceedings fourteenth international conference machine learning pp 

